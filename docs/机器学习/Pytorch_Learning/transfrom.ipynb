{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84386168-925c-480e-8657-6e3a7d4fd7aa",
   "metadata": {},
   "source": [
    "详见[Pytorch官方Transformers](https://docs.pytorch.ac.cn/tutorials/beginner/basics/transforms_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08428101-b497-4c0d-a9f0-2510cc39098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ce9ab-2d86-44a6-a3e7-2624174ff3bb",
   "metadata": {},
   "source": [
    "## 🖼️ TorchVision 介绍\n",
    "\n",
    "**TorchVision** 是 PyTorch 官方支持的、专门用于**计算机视觉 (Computer Vision)** 任务的库。它是 PyTorch 生态系统中不可或缺的一部分，极大地简化了计算机视觉研究和应用的开发过程。\n",
    "\n",
    "TorchVision 主要由三个核心模块构成，共同为视觉任务提供高效的支持：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `torchvision.datasets` (数据集)\n",
    "\n",
    "这个模块提供了大量**预先准备好的、常用的公共数据集**，你可以直接下载和使用，无需自己编写复杂的数据加载代码。\n",
    "\n",
    "* **核心功能:** 方便地加载标准数据集进行训练和测试。\n",
    "* **常见数据集示例:**\n",
    "    * **Image Classification (图像分类):** `MNIST`, `CIFAR10`, `ImageNet` (需要手动下载和组织文件)。\n",
    "    * **Object Detection (目标检测):** `CocoDetection`, `Kitti`.\n",
    "    * **Semantic Segmentation (语义分割):** `VOCSegmentation`.\n",
    "\n",
    "### 2. `torchvision.models` (预训练模型)\n",
    "\n",
    "这个模块提供了大量**预训练的、高性能的神经网络模型**。这些模型通常在大型数据集（如 ImageNet）上训练过，可以直接用于特征提取或作为**迁移学习 (Transfer Learning)** 的起点。\n",
    "\n",
    "* **核心功能:** 快速获取和使用经过验证的经典或先进模型。\n",
    "* **常见模型架构示例:**\n",
    "    * **Classification (分类):** `ResNet` (如 `resnet50`), `VGG`, `MobileNet`, `EfficientNet`.\n",
    "    * **Object Detection (目标检测):** `Faster R-CNN`, `SSD`, `RetinaNet`.\n",
    "    * **Segmentation (分割):** `FCN`, `DeepLabV3`.\n",
    "\n",
    "### 3. `torchvision.transforms` (数据变换/预处理)\n",
    "\n",
    "这个模块提供了各种**图像预处理和数据增强 (Data Augmentation)** 操作。在将原始图像输入到神经网络之前，通常需要进行这些处理。\n",
    "\n",
    "* **核心功能:** 对 PIL Image 或 Tensor 进行各种操作，如尺寸调整、裁剪、归一化等。\n",
    "* **常见操作示例:**\n",
    "    * `ToTensor()`: 将 PIL Image 或 NumPy 数组转换成 PyTorch Tensor。\n",
    "    * `Normalize()`: 对 Tensor 进行标准化处理 (减去均值，除以标准差)。\n",
    "    * `Resize()`: 改变图像尺寸。\n",
    "    * `RandomCrop()`: 随机裁剪，用于数据增强。\n",
    "    * `Compose()`: 将多个变换操作串联起来。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "\n",
    "TorchVision 是 PyTorch 用户进行计算机视觉项目时的**标配工具箱**，它通过提供即插即用的数据集、模型和预处理工具，帮助开发者：\n",
    "\n",
    "1.  **快速启动项目:** 利用预训练模型和标准数据集快速构建原型。\n",
    "2.  **保证数据处理一致性:** 使用统一的 `transforms` API 进行数据预处理。\n",
    "3.  **节省资源:** 避免重复实现常见模型的代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34005954-5c17-4805-b7e8-bc00f0265b60",
   "metadata": {},
   "source": [
    "# transform\n",
    "##  `torchvision.transforms` 介绍\n",
    "\n",
    "`torchvision.transforms` 是 **TorchVision** 库中专门用于图像**预处理 (Preprocessing)** 和**数据增强 (Data Augmentation)** 的核心模块。\n",
    "\n",
    "在将原始图像数据送入神经网络之前，我们几乎总是需要进行一系列的转换操作，而 `transforms` 模块就是为此设计的。\n",
    "\n",
    "### 核心目的\n",
    "\n",
    "1.  **统一输入格式:** 将不同格式的输入（如 PIL Image, NumPy Array）转换为 PyTorch 模型所需的 `Tensor` 格式。\n",
    "2.  **标准化:** 对像素值进行归一化，使其均值接近 0、标准差接近 1，有助于模型训练收敛。\n",
    "3.  **数据增强:** 通过随机变换（如裁剪、翻转、旋转），增加训练数据的多样性，从而提高模型的**泛化能力**，减少过拟合。\n",
    "\n",
    "### 常见操作分类及示例\n",
    "\n",
    "#### 1\\. 基础转换 (Normalization & Format)\n",
    "\n",
    "| 转换方法 | 作用 | 目的 |\n",
    "| :--- | :--- | :--- |\n",
    "| `ToTensor()` | 将 PIL Image 或 NumPy `ndarray` 转换为 `FloatTensor`。 | **必须的操作**，将数据转换为模型能处理的 PyTorch 张量。 |\n",
    "| `Normalize(mean, std)` | 根据给定的均值和标准差对 Tensor 进行标准化。 | 将像素值缩放到一个合适的范围，加速模型收敛。 |\n",
    "| `Resize(size)` | 将输入图像调整到指定的尺寸。 | 确保所有输入图像尺寸统一。 |\n",
    "\n",
    "#### 2\\. 数据增强 (Augmentation)\n",
    "\n",
    "| 转换方法 | 作用 | 目的 |\n",
    "| :--- | :--- | :--- |\n",
    "| `RandomCrop(size)` | 从图像中随机裁剪一块指定大小的区域。 | 增加训练样本的多样性和平移不变性。 |\n",
    "| `RandomHorizontalFlip(p=0.5)` | 以给定的概率水平随机翻转图像。 | 增加数据的对称性。 |\n",
    "| `RandomRotation(degrees)` | 随机旋转图像一个角度范围。 | 增加模型的旋转不变性。 |\n",
    "| `ColorJitter(brightness=0.2, ...)`| 随机改变图像的亮度、对比度、饱和度和色调。 | 使模型对光照变化更具鲁棒性。 |\n",
    "\n",
    "#### 3\\. 组合操作\n",
    "\n",
    "| 转换方法 | 作用 | 目的 |\n",
    "| :--- | :--- | :--- |\n",
    "| `Compose([...])` | **将多个转换操作按顺序串联起来。** | 这是最常用的方法，用于定义一个完整的预处理流程。 |\n",
    "\n",
    "### `Compose` 示例 (标准的预处理流程)\n",
    "\n",
    "在实际应用中，我们通常使用 `transforms.Compose` 将多个操作组合成一个流程：\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义一个标准的预处理流程\n",
    "transform = transforms.Compose([\n",
    "    # 1. 调整图像尺寸到 256x256\n",
    "    transforms.Resize(256),  \n",
    "    \n",
    "    # 2. 从 256x256 的图像中随机裁剪 224x224 的区域 (用于数据增强)\n",
    "    transforms.RandomCrop(224), \n",
    "    \n",
    "    # 3. 以 50% 的概率水平翻转图像 (用于数据增强)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "    # 4. 必须操作：将图像转换为 PyTorch Tensor\n",
    "    transforms.ToTensor(),      \n",
    "    \n",
    "    # 5. 必须操作：进行标准化 (使用 ImageNet 的均值和标准差作为示例)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# 假设 image 是一个 PIL 图像对象\n",
    "# processed_tensor = transform(image)\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "`torchvision.transforms` 模块通过提供一个清晰、灵活的 API，使得深度学习工程师可以方便地：\n",
    "\n",
    "  * **构建**模型所需的输入张量。\n",
    "  * **应用**各种数据增强技术来改进模型性能和鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9f911e-019d-4e75-8f88-902956e9488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换图片\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0dfc1c-4864-4a64-bc3d-5795ab8fa36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "img_path = 'dataset/train/ants/0013035.jpg'\n",
    "img = Image.open(img_path)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2c800b-7cdd-4351-88cd-4d5971fcaf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tensor_trans = transforms.ToTensor() #实例化类\n",
    "img_tensor = tensor_trans(img) #调用类(就是调用__call__函数)\n",
    "print(type(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba80909-9ded-4c7d-962b-b31407c22b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
      "         ...,\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
      "         ...,\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa52d6f-f19a-4cce-9ac0-e3fa97b39bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg_tensor\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1dbe4-b209-4ee9-b6f9-90858d61ffeb",
   "metadata": {},
   "source": [
    "说明：维度0 深度（通道数）；维度1 行数（高），维度2 列数（宽）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a30e4d-ffce-4a79-b374-fd197021d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "# 将特征转换为归一化的张量，并将标签转换为独热编码的张量\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    # 把输入的y列表对应的位置都写 1，其他则为 0，并统一长度为 10\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) #y should be Python List\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542f038-bd65-4d29-be79-6e39a031295e",
   "metadata": {},
   "source": [
    "这段代码中的 `target_transform` 部分是 PyTorch 数据预处理中一个简洁且高效的技巧，它实现了**将标量标签（类别 ID）转换为独热编码 (One-Hot Encoding)**。\n",
    "\n",
    "让我们把这段代码拆解开来，一步步详细解释。\n",
    "\n",
    "-----\n",
    "\n",
    "##  代码解析：目标转换 (Target Transform)\n",
    "\n",
    "```python\n",
    "target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    "```\n",
    "\n",
    "### 1\\. `Lambda(...)` 的作用\n",
    "\n",
    "  * **回顾：** `Lambda` 是 `torchvision.transforms` 模块中的一个类，它允许您将任何 **自定义的、单行可执行的函数** 封装成一个转换操作。\n",
    "  * **输入：** 这里的 `Lambda` 接收一个匿名函数 `lambda y: ...`。\n",
    "  * **`y`：** 在 FashionMNIST 数据集中，`y` 就是当前的**目标标签 (Target Label)**，它是一个表示类别索引的 **整数标量**（例如：T恤衫是 `0`，裤子是 `1`，鞋子是 `9`）。\n",
    "\n",
    "### 2\\. Lambda 函数体：核心的独热编码逻辑\n",
    "\n",
    "Lambda 函数体执行了三个步骤来完成独热编码：\n",
    "\n",
    "#### A. 创建一个全零向量 (容器)\n",
    "\n",
    "```python\n",
    "torch.zeros(10, dtype=torch.float)\n",
    "```\n",
    "\n",
    "  * **`torch.zeros(10, ...)`：** 创建一个包含 10 个元素的 **一维零张量**。\n",
    "      * 为什么是 10？因为 **FashionMNIST 数据集有 10 个类别**。\n",
    "      * 这个张量就是我们最终的独热编码容器，形状是 `(10,)`。\n",
    "  * **`dtype=torch.float`：** 确保张量的数据类型是浮点数，这通常是深度学习中损失函数（如交叉熵损失）所要求的输入格式。\n",
    "\n",
    "#### B. 核心操作：`.scatter_()`\n",
    "\n",
    "这个操作是实现独热编码的关键。它将一个数值 **分散 (scatter)** 写入目标张量的指定索引位置。\n",
    "\n",
    "  * **操作对象：** 刚才创建的那个全零张量。\n",
    "  * **语法回顾：** `target.scatter_(dim, index, value)`\n",
    "\n",
    "#### C. 理解 `.scatter_()` 的参数\n",
    "\n",
    "| 参数 | 表达式 | 实际值 (假设 $y=3$) | 作用 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`dim`** | `0` | `0` | **分散维度：** 因为目标张量是 1 维的 `(10,)`，所以只有 `dim=0` 可选。操作将沿着这个唯一的维度进行。 |\n",
    "| **`index`** | `torch.tensor(y)` | `torch.tensor(3)` | **索引张量：** `y` 是一个 Python 整数（如 3），但 `.scatter_` 要求索引必须是 PyTorch 张量。因此，我们将其包装成一个单元素张量。 |\n",
    "| **`value`** | `value=1` | `1` | **写入值：** 要写入到目标张量指定位置的值。独热编码要求将对应类别位置的值设置为 1。 |\n",
    "\n",
    "### 3\\. 完整过程演示（假设 $y=3$）\n",
    "\n",
    "假设当前样本的标签 $y=3$（即第 4 个类别）：\n",
    "\n",
    "1.  **初始零张量 (Target):**\n",
    "    $$T = [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]$$\n",
    "    (形状 $(10$, $)$)\n",
    "\n",
    "2.  **索引张量 (Index):**\n",
    "    $$I = [3]$$\n",
    "    (形状 $(1$, $)$)\n",
    "\n",
    "3.  **执行 `.scatter_(0, I, value=1)`:**\n",
    "    这个操作意味着：将值 `1` 写入到 $T$ 中，位置由 $I$ 指定，沿着 $dim=0$。\n",
    "\n",
    "      * $T[I[0]] = 1$\n",
    "      * $T[3] = 1$\n",
    "\n",
    "4.  **最终独热编码结果:**\n",
    "    $$T_{final} = [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]$$\n",
    "\n",
    "-----\n",
    "\n",
    "##  总结：这段代码的意义\n",
    "\n",
    "这段 `target_transform` 代码的目的是将原始的 **整数类别标签**（如 `3`）转换成 **长度为类别总数 (10) 的向量**，其中只有对应类别的索引位置为 `1`，其他位置为 `0`。\n",
    "\n",
    "**为什么需要这样做？**\n",
    "\n",
    "  * **交叉熵损失 (Cross-Entropy Loss)：** 在 PyTorch 中，虽然 `nn.CrossEntropyLoss` 优化器可以直接接收整数 ID 标签，但当涉及到其他类型的损失函数、自定义激活函数（如 Sigmoid 输出）或执行某些特殊的梯度计算时，**独热编码** 的格式（One-Hot Vector）是更通用和标准的标签格式。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
