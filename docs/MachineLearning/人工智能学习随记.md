# 人工智能学习随记

我是跟着吴恩达机器学习来学习人工智能的，以下是我的一些理解、笔记和线路。

# 1. Python基础

这个不用废话，肯定是最先学的。

还有一些补充得库要学：

- [Python库](../Addition/Python库/Python库.md)

# 2. 环境搭建

- 学会使用pip命令

- 部署Python环境

- 部署conda环境

- Python或conda中环境(env)与隔离的概念，为什么要这样做，学会环境管理。

- PyCharm和jupyter notebook的基本使用。

# 3. 机器学习基础

了解机器学习的基本原理，包括：

- 监督学习、无监督学习的概念

- 线性回归模型

- 成本函数和损失函数

- 梯度、梯度下降
- 代码上如何实现梯度下降

- 学习率

- 特征（features）

- 向量化，学会numpy

- 特征缩放

- 多元线性回归

- 多项式回归

- 逻辑回归(Logistic)

- 决策边界

- 二元类别——Sigmoid函数（或使用二元交叉熵损失）

- 过拟合及解决方案

- 正则化的实现

# 4. 监督学习

## 人工神经网络 (Artificial Neural Network, ANN)

- 什么是深度学习（或神经网络）

- 神经元的概念

- 神经网络的层、神经元数以及各层之间的连接

- **多层感知机 (Multi-Layer Perceptron)**

- 前向传播的概念以及实现代码

- 使用TensorFlow

- 神经网络的高效实现方式——矩阵并行运算（张量）

- 激活函数的概念以及选择。为什么要激活函数？

- ReLU激活函数，特点。（提供转折点）

- 分类问题之多类别的实现——Softmax激活函数（或使用多分类交叉熵）

- 卷积层

- 计算图——反向传播

- 抉择；模型评估；模型选择、训练与验证

- 诊断偏差和方差，如何解决？

- 错误分析方法

- 数据增加技巧，如何获得更多数据？

- 迁移学习：预训练&微调

- 机器学习项目流程

- 模型评估：精确率和召回率，混淆矩阵。

- 权衡Precision/recall numbers：$F_1\;score$ (调和平均，强调较小值)

## 卷积神经网络(Convolutional Neural Network, CNN)

- 传统神经网络的弊端：参数爆炸、空间结构忽略（展平后丢失了像素点间空间邻近的关系）

- 设计特点：**卷积层(Convolutional Layer)**、**非线性激活函数ReLU**、**池化层(Pooling Layer)**

- 架构：\[CONV -> ReLU -> POOL] $\rightarrow$ \[CONV -> ReLU -> POOL] $\rightarrow$ $\dots$ $\rightarrow$ \[FC/MLP]

|**组件 (Layer)**|**作用 (Function)**|**优势 (Benefit)**|**PyTorch 对应模块 (Example)**|
|---|---|---|---|
|**卷积层** (Conv)|提取局部特征，实现参数共享|减少参数量，保留空间信息|`nn.Conv2d`|
|**激活函数** (ReLU)|引入非线性|拟合复杂模式|`nn.ReLU`|
|**池化层** (Pooling)|降采样，实现平移不变性|降低计算量，防止过拟合|`nn.MaxPool2d`|
|**全连接层** (FC)|对提取的特征进行分类或回归|完成最终的决策输出|`nn.Linear`|

## 循环神经网络 (Recurrent Neural Network, RNN)

- MLP与CNN：输入和输出的数据之间都是相互独立的。

- 为什么需要RNN？数据前后的依赖性，数据是按顺序排列的，且**前一个数据点会影响后一个数据点**。应用：文本、语音、股市等。
### RNN 的“循环”机制

**核心思想：** **为网络赋予记忆能力。** 它通过一个“循环”结构，将当前时间步的计算结果，作为**隐藏状态 (Hidden State)** 传递给下一个时间步。

#### 传统网络 (MLP/CNN) 的输入方式

- **特点:** **整体打包**，**一次性输入**。
    
- **示例:** 如果你有一个 10 个词的句子，每个词是 100 维的向量。
    
    - MLP/CNN 会将这 10 个词的向量**拼接**起来，形成一个 $10 \times 100 = 1000$ 维的巨大输入向量，然后一次性喂给网络。
        
- **缺点:** 网络参数会爆炸；更重要的是，它将整个序列视为一个**固定尺寸**的输入，**无法处理变长序列**。
    

#### 循环网络 (RNN) 的输入方式

- **特点:** **分次输入**，**共享网络**。
    
- **简短说明:** RNN 单元（包含权重 $W_{hh}$ 和 $W_{xh}$）在时间维度上被**重复使用**。
    
    - **步骤一:** 输入第一个词的向量 $X_1$ 和初始隐藏状态 $H_0$。
        
    - **步骤二:** 计算得到 $H_1$，将 $H_1$ 传给下一步。
        
    - **步骤三:** 输入第二个词的向量 $X_2$ 和 $H_1$。
        
    - ... 依次进行，直到序列结束。

#### 1. 结构特点：时间展开与共享权重

- **循环体 (Recurrence):** RNN 的核心是一个循环单元，它接收两个输入：
    
    1. 当前时间步 $t$ 的**输入数据** $X_t$。
        
    2. 上一个时间步 $t-1$ 传来的**隐藏状态** $H_{t-1}$（即“记忆”）。
        
- **时间展开 (Unrolling):** 虽然结构上只有一个单元，但在计算时，我们会将它在时间维度上展开，形成一个长链。
    
- **参数共享 (Weight Sharing):** 在这个展开的长链中，每一步使用的**权重矩阵 $W$ 都是同一套**。这大大减少了参数量，并确保了模型能够学习到跨时间的通用模式。
    
#### 2. 工作原理：状态更新

在每个时间步 $t$，RNN 单元会根据输入 $X_t$ 和旧状态 $H_{t-1}$ 来计算新的隐藏状态 $H_t$ 和输出 $Y_t$。

$$H_t = \text{Activation}(W_{hh} H_{t-1} + W_{xh} X_t + b_h)$$

$$Y_t = \text{Activation}(W_{hy} H_t + b_y)$$

- **$W_{hh}$:** 作用于上一个隐藏状态的权重（决定旧记忆如何影响新记忆）。
    
- **$W_{xh}$:** 作用于当前输入的权重（决定新输入如何影响新记忆）。
    
- **$H_t$:** 承载了从序列开始到当前时间 $t$ 的所有历史信息。
    

---

### RNN 的主要问题：梯度不稳定

虽然 RNN 解决了序列依赖问题，但它有一个致命的缺陷，尤其在处理很长的序列时：

#### 1. 梯度消失 (Vanishing Gradient)

- **简短说明:** 在反向传播过程中，因为权重在不同时间步被**重复相乘**，如果权重值较小（小于1），梯度会呈指数级衰减，导致信息无法回传到序列的初始部分。
    
- **结果:** 网络无法学习到**长距离依赖**。模型会“忘记”序列开头的信息。
    

#### 2. 梯度爆炸 (Exploding Gradient)

- **简短说明:** 如果权重值较大（大于1），梯度会呈指数级增长，导致权重更新过大，网络震荡，模型训练失败。
    
- **解决方法:** 通常通过**梯度裁剪 (Gradient Clipping)** 来限制梯度的最大值。
    

---

### RNN 结构概览

|**特性 (Feature)**|**描述 (Description)**|**作用 (Function)**|
|---|---|---|
|**循环连接**|将 $t-1$ 步的**隐藏状态**传给 $t$ 步|赋予网络“记忆”能力|
|**参数共享**|所有时间步使用**同一套权重**|减少参数量，学习通用时序模式|
|**隐藏状态 ($H_t$)**|序列历史信息的**压缩表示**|模型的内部记忆|
|**反向传播**|**BPTT** (Backpropagation Through Time)|训练网络的独特方式|

**总结:** RNN 是处理序列数据的基石，但其长距离依赖问题促使更高级的变体，如 **LSTM** 和 **GRU** 的出现，它们通过更复杂的门控机制来精确控制信息的流动。

## 自注意机制(Self Attention Mechanism)

- 补充：[自然语言处理](#6.%20补充：自然语言处理)

- RNN（长距离局限）与CNN的局限（局部感知的局限）

- 问题：若输入的特征数不确定，怎么办？（比如一句句子）


**核心思想：** **无需依赖序列的距离，直接计算序列中任意两个元素之间的相互依赖程度。**

简而言之，它允许模型在处理序列的某个词时，去“关注”序列中**所有其他词**对当前词的重要性，并根据这个重要性（权重）来加权当前的特征表示。

### 工作原理：三要素和计算步骤

自注意力机制的核心在于使用三个从输入向量 $\mathbf{X}$ 导出的向量：**查询（Query, Q）**、**键（Key, K）**、和**值（Value, V）**。

对于输入序列中的每一个词向量 $\mathbf{x}_i$，我们通过三个不同的线性变换（矩阵乘法）来生成对应的 $\mathbf{q}_i$, $\mathbf{k}_i$, 和 $\mathbf{v}_i$。
$$\mathbf{Q} = \mathbf{X} \mathbf{W}_Q$$
$$\mathbf{K} = \mathbf{X} \mathbf{W}_K$$
$$\mathbf{V} = \mathbf{X} \mathbf{W}_V$$
- $\mathbf{X}$: 输入序列（词向量组成的矩阵）。
    
- $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$: 三个独立的、需要学习的**权重矩阵**。它们将输入特征投影到 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ **空间**。
    
> **类比理解：** 想象你在看一段文字。

- **Q (Query, 查询):** 你当前关注的这个词，带着 **“我想找什么”** 的需求。
    
- **K (Key, 键):** 序列中所有其他词，带着 **“我有什么内容可以被匹配”** 的标签。
    
- **V (Value, 值):** 序列中所有其他词，带着 **“如果我被匹配了，我的实际价值是什么”** 的内容。
    
> **类比:** 想象你在一个图书馆里搜索（**Q**），你用你的查询去匹配书架上所有的书名和摘要（**K**），找到匹配度高的书（高**注意力权重**），然后取出书中的内容（**V**）。

允许 $\mathbf{Q}$ 和 $\mathbf{K}$ 仅负责**确定重要性（寻址）**，而 $\mathbf{V}$ 则负责**提供内容**。理论上，模型可以使用 $\mathbf{K}$ 向量的一个子集来匹配 $\mathbf{Q}$，但使用 $\mathbf{V}$ 向量的另一个子集来提供最终融合的特征，从而增强模型的表达能力。
### 计算步骤 (Scaled Dot-Product Attention)

自注意力的计算过程可以概括为以下四个步骤：
#### 1. 计算相似度（点积）
$$Similarity = Q \cdot K^T$$

- **简短说明:** 用查询向量 $\mathbf{Q}$ 与所有键向量 $\mathbf{K}$ 进行点积操作。
    
- **成果:** 得到一个**分数 (Score)** 矩阵，分数越高，表示 $\mathbf{Q}$ 和 $\mathbf{K}$ 对应元素之间的相关性越强。
    
#### 2. 缩放 (Scaling)
$$Scaled = \frac{Similarity}{\sqrt{d_k}}$$

- **简短说明:** 将分数除以 $\mathbf{K}$ 向量维度 $d_k$ 的平方根。
    
- **成果:** 避免点积结果过大，使得 Softmax 函数的梯度在反向传播时过于平坦，保持训练的稳定性。
    
#### 3. 归一化（Softmax）
$$Weights = \text{Softmax}(Scaled)$$

- **简短说明:** 对缩放后的分数进行 Softmax 归一化。
    
- **成果:** 得到最终的**注意力权重 (Attention Weights)**，这些权重是**概率分布**，表示序列中每个词对当前词的相对重要性。
    
#### 4. 加权求和
$$Output = Weights \cdot V$$

- **简短说明:** 用得到的注意力权重去加权所有的值向量 $\mathbf{V}$。
    
- **成果:** 得到当前词的**新的特征表示**，这个表示融合了序列中所有其他词的加权信息。

## 决策树模型（Decision Tree）

- 树的概念

- 学习过程

- 纯度的测定：信息熵

- 选择分类依据：信息增益（如何计算？）

- 独热编码

- 连续值特征得处理

- 补充：回归树(基于方差增益分叉)

- 决策树的缺点：敏感

- 决策树擅长什么：离散的结构化数据（比如表格）、快速训练、可解释性（白箱）
## 随机森林(Random Forest Algorithm)

- 树的集成、有放回采样

- 随机森林算法(Random Forest Algorithm)
## XGBoost(Extreme Gradient Boosting)

- 刻意学习

- **XGBClassifier**

- **XGBRegressor**
# 5. 无监督学习


# 6. 补充：自然语言处理

## 自然语言处理 (NLP) 核心概述

### 方向：让机器理解人类的语言

**核心思想:** NLP 是一门交叉学科，旨在让计算机能够像人类一样理解、解释、生成和处理人类使用的自然语言（如中文、英文等）。

- **输入:** 通常是文本（句子、文档）或语音（需要先转录成文本）。
    
- **输出:** 可以是机器翻译、问答系统、情感评分、摘要等。
### NLP 的核心挑战

人类语言充满**歧义**、**上下文依赖**和**复杂结构**，这是机器理解语言的主要难点：

1. **词汇歧义 (Lexical Ambiguity):** 一个词可能有多个意思（例如：“苹果”可以是水果，也可以是公司）。
    
2. **句法歧义 (Syntactic Ambiguity):** 句子结构可能导致多种解释（例如：“小明看到那个女孩拿着望远镜。”）。
    
3. **语义理解 (Semantic Understanding):** 机器不仅要知道词的意思，还要知道句子表达的**真实含义**。

## NLP 的经典处理流程与技术

NLP 的技术栈非常庞大，但可以从浅层处理到深度理解进行划分。

### 一、基础处理 (浅层)

这些步骤是大多数 NLP 任务的起点：

- **数据清洗**：将无关词汇删除，比如￥%*@
	 
- **分词 (Tokenization):** 将连续的文本分解成有意义的最小单元，即**词 (Token)**。
    
    - _中文:_ 将句子切分成词语（如：“我/爱/学习/NLP”）。
        
    - _英文:_ 主要按空格和标点符号切分。
        
- **词性标注 (Part-of-Speech Tagging, POS):** 标注每个词在句子中的词性（如：名词、动词、形容词）。
    
- **命名实体识别 (Named Entity Recognition, NER):** 识别文本中具有特定意义的实体，如人名、地名、组织名、日期等。
    

### 二、特征表示

机器无法直接处理文字，需要将词语转化为**数值向量**。

#### 1. 稀疏表示：词袋模型 (Bag-of-Words, BoW)

- **简短说明:** 将文本表示为一个**高维、稀疏**的向量。向量的维度是整个词汇表的大小。
    
- **方法:** 向量的每一维对应一个词，其值通常是该词在文本中出现的次数（词频 TF）或 TF-IDF 值。
    
- **局限性:**
    
    - **维度灾难:** 词汇表一大，向量维度就非常高。
        
    - **缺乏语义:** 无法体现“猫”和“狗”虽然不同，但都是动物这种**语义相似性**。它只关心词汇是否存在，不关心顺序。
        
#### 2. 密集表示：词嵌入 (Word Embedding)

- **核心思想:** **“词义由上下文决定”** (Distributional Hypothesis)。出现在相似上下文中的词，其含义也相似。
    
- **简短说明:** 将每个词映射为一个**低维、密集**的实数向量（通常维度在 50 到 300 之间）。
    
- 成果: 具有相似语义的词，在向量空间中的距离会比较近。著名的 **“词向量代数”** 现象就体现在此：
    $$\text{Vector}(\text{King}) - \text{Vector}(\text{Man}) + \text{Vector}(\text{Woman}) \approx \text{Vector}(\text{Queen})$$
- 典型方法：**Word2Vec**, **GloVe** (Global Vectors for Word Representation), **ELMo**/**BERT** (上下文相关的词嵌入)

#### 3. 独热编码(One-Hot Encoding)

处理无序的类别特征

- **核心思想:** 当我们遇到不能直接用数值大小来比较的**离散型、非数值型**特征时（例如：颜色、国家、职业），我们需要将它们转换成数值格式，以便机器学习模型处理。
	
- **工作原理**：创建虚拟变量
	
	- **简短说明:** 对于一个具有 $N$ 个不同类别的特征，独热编码会将其转换为一个长度为 $N$ 的**向量**。
    
- **编码方式:** 向量的 $N$ 个维度中，只有一个维度被设置为 **1**，表示该样本属于这个类别；其余 $N-1$ 个维度都设置为 **0**。
    

> 示例：颜色
> 
> 假设特征“颜色”有三个类别：{红, 绿, 蓝} ($N=3$)。
> 
> - “红” 被编码为： $[1, 0, 0]$
>     
> - “绿” 被编码为： $[0, 1, 0]$
>     
> - “蓝” 被编码为： $[0, 0, 1]$
>     

- **优势**：消除序数关系
	
- **简短说明:** 将一个类别特征展开成多个独立的二元（0/1）特征。
    
- **成果:** 保证了所有类别之间的**距离相等**，避免模型对类别数据产生错误的序数（大小）理解。
### 三、深度学习与模型 (深层)

现代 NLP 依赖于强大的深度学习模型来处理复杂的序列信息：

- **RNN/LSTM/GRU:** 用于处理**序列数据**，捕捉时间或顺序上的依赖关系，尤其擅长处理变长输入。
    
- **Attention/Transformer:**
    
    - **自注意力机制**克服了 RNN 的长距离依赖问题。
        
    - **Transformer 架构** (基于 Attention) 成为 NLP 的主流，支持高度并行化训练。
        
- **预训练模型 (Pre-trained Models):**
    
    - 通过在海量数据上预先训练（如**掩码语言建模**），模型学习到通用的语言知识和结构。
        
    - **代表:** **BERT**, **GPT** (Generative Pre-trained Transformer)。
        
    - **应用:** 通过**微调 (Fine-tuning)**，能快速适应各种下游任务。
        

---

## 知识整理：NLP 的主要应用领域

|**应用领域 (Field)**|**目标 (Goal)**|**典型模型 (Model Focus)**|
|---|---|---|
|**机器翻译** (MT)|将一种语言自动翻译成另一种语言|Encoder-Decoder, Transformer|
|**情感分析** (SA)|判断文本表达的情绪或态度（正向/负向）|深度学习分类模型，BERT 微调|
|**文本生成** (TG)|自动生成连贯、有意义的文本（如写文章）|GPT 系列 (大型语言模型 LLMs)|
|**问答系统** (QA)|根据给定文本或知识库回答用户提出的问题|抽取式 QA，生成式 QA|
|**语音识别** (ASR)|将人类语音转化为文本|RNN/CTC, Transformer|

**总结:** NLP 的发展经历了从基于规则和统计的方法，到基于浅层机器学习，再到如今由 **Transformer** 和**大型语言模型 (LLMs)** 驱动的深度学习时代。