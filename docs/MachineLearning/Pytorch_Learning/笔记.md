# Pytorch学习笔记

本来我是不想记笔记的，但是我学着学着发现，如果不简要记些东西，下次查看的时候难以连贯思路，因此写一个我的学习路径和思考。



## 张量

- 前置知识：Numpy，机器学习基础

- 多维数组

- 形状的理解

- 主要属性

- 在深度学习中的作用

### PyTorch 张量常见功能映射表

| 功能类别                   | 数学/NumPy 概念                 | PyTorch 对应函数/方法                                   | 常见用途和说明                                               |
| :------------------------- | :------------------------------ | :------------------------------------------------------ | :----------------------------------------------------------- |
| **I. 创建与初始化**        | 零矩阵                          | `torch.zeros(shape)`                                    | 创建一个所有元素都为 0 的张量。                              |
|                            | '1'矩阵                         | `torch.ones(n, m)`                                      | 创建一个 $n \times m$ 的全'1'矩阵。                          |
|                            | 单位矩阵                        | `torch.eye(n, m)`                                       | 创建一个 $n \times m$ 的单位矩阵。                           |
|                            | 随机数                          | `torch.rand(shape)`                                     | 创建服从 $[0, 1)$ 均匀分布的张量。                           |
|                            | 正态分布随机数                  | `torch.randn(shape)`                                    | 创建服从标准正态分布 ($\mu=0, \sigma=1$) 的张量。            |
|                            | 基于现有张量创建                | `torch.ones_like(tensor)`                               | 根据另一个张量的形状创建新张量。                             |
|                            | 从 Python/NumPy 创建            | `torch.tensor(data)`, `torch.from_numpy(ndarray)`       | 将 Python 列表、数组或 NumPy 转换成张量。                    |
| **II. 形状和维度操作**     | 形状查询                        | `tensor.shape` 或 `tensor.size()`                       | 返回张量的形状（元组）。                                     |
|                            | 维度重塑 (NumPy: `reshape`)     | `tensor.view(new_shape)` 或 `tensor.reshape(new_shape)` | 改变张量的形状。**`view`** 要求内存连续，**`reshape`** 更灵活。 |
|                            | 增加/减少维度                   | `torch.squeeze()`, `torch.unsqueeze(dim)`               | 移除（维度为 1）或增加维度（如将向量转为 Batch ）。          |
|                            | 维度转置 (NumPy: `T`)           | `tensor.permute(dims)`, `tensor.transpose(dim0, dim1)`  | 交换张量的维度位置（常用于 $HWC \to CWH$）。                 |
|                            | 张量拼接 (NumPy: `concatenate`) | `torch.cat((t1, t2), dim=0)`                            | 沿指定维度将多个张量拼接起来（维度增加）。                   |
|                            | 张量堆叠 (NumPy: `stack`)       | `torch.stack((t1, t2), dim=0)`                          | 沿一个新的维度将多个张量堆叠起来（维度数量增加）。           |
|                            | 基于索引的元素分散操作          | `target.scatter_(dim, index, src,value (optional))`     | 将一个 src 张量（源数据）中的值，根据一个 index 张量（索引）的指示，分散写入（覆盖或累加）到调用该方法的 目标张量（self，即 target）的指定位置;如果不使用 src 张量，可以传入一个标量值，所有写入操作都使用这个固定值。 |
| **III. 数学运算**          | 元素级加减乘除                  | `t1 + t2`, `t1 * t2`, `torch.add(t1, t2, out=t3)`       | 对应元素进行运算。                                           |
|                            | 矩阵乘法 (NumPy: `@` 或 `dot`)  | `torch.matmul(t1, t2)` 或 `t1 @ t2`                     | 适用于矩阵、向量乘法，也支持 Batch 矩阵乘法。                |
|                            | 向量内积 (NumPy: `dot`)         | `torch.dot(vec1, vec2)`                                 | 仅用于一维张量（向量）。                                     |
|                            | 求和 (NumPy: `sum`)             | `torch.sum(tensor, dim=None)`                           | 对所有元素或指定维度求和。                                   |
|                            | 平均值                          | `torch.mean(tensor, dim=None)`                          | 对所有元素或指定维度求平均值。                               |
| **IV. 梯度控制与设备管理** | 设备查询                        | `tensor.device`                                         | 返回张量所在的设备（如 `cpu` 或 `cuda:0`）。                 |
|                            | 设备移动                        | `tensor.to('cuda')`, `tensor.cpu()`                     | 将张量在 CPU 和 GPU 之间移动。                               |
|                            | 梯度追踪                        | `tensor.requires_grad`                                  | 布尔值，指示是否需要计算梯度（默认为 `False`）。             |
|                            | 停止梯度追踪                    | `with torch.no_grad():` 或 `tensor.detach()`            | 在不需要计算梯度时（如评估阶段）使用，节省计算和内存。       |
| **V. 索引和切片**          | 标量值                          | `tensor.item()`                                         | 从只包含一个元素的张量中获取 Python 标量值。                 |
|                            | 标准索引/切片                   | `tensor[0]`, `tensor[:, 2:5]`                           | 与 NumPy 索引规则相同。                                      |
|                            | 掩码索引                        | `tensor[tensor > 5]`                                    | 使用布尔张量作为索引（获取满足条件的元素）。                 |

---

### PyTorch 与 NumPy 的关键区别

1.  **设备支持：** PyTorch Tensors 默认在 CPU 上创建，但可以使用 `.cuda()` 或 `.to('cuda')` 轻松移动到 GPU 上进行高性能计算。NumPy $ndarray$ 只能在 CPU 上运行。
2.  **梯度追踪：** PyTorch Tensors 可以使用 `requires_grad=True` 启用 **自动求导 (Autograd)**，这是构建神经网络的核心功能，而 NumPy 不具备此功能。
3.  **原地操作 (In-place Operations)：** PyTorch 提供了许多以 `_` 结尾的方法（例如 `add_()`），这些是**原地操作**，会直接修改张量本身的值。在涉及 Autograd 时使用原地操作需要小心，可能会破坏计算图。

## 数据集和数据加载器

- `Dataset`和`Dataloader`是什么？用于管理和加载数据的 **类**。
  - `Dataset`包含对数据基本的操作，比如按路径读取源和标签，获取样本方法，获取数据集大小方法等。
  - `Dataloader` 可以实现小批量传递样本。`DataLoader` 是一个可迭代对象，它通过简单的 API 为我们抽象了这些复杂性。
  - 我们可以自己继承`Dataset`和`Dataloader`写我们的数据集和数据加载器，但通常我们用开源库。

- 为什么要Dataset和Dataloader？

## Transform 转换

- 什么是`TorchVision`

- 数据并不总是以机器学习算法训练所需的最终处理形式提供。我们使用 **transforms** 来对数据进行一些操作，使其适合训练。

- 比如，``torchvision.transforms`` 是一个工具包，里面实现了各种转化的方法,
  - **常见操作示例:**
    - `ToTensor()`: 将 PIL Image 或 NumPy 数组转换成 PyTorch Tensor。
    - `Normalize()`: 对 Tensor 进行标准化处理 (减去均值，除以标准差)。
    - `Resize()`: 改变图像尺寸。
    - `RandomCrop()`: 随机裁剪，用于数据增强。
    - `Compose()`: 将多个变换操作串联起来。

## 构建神经网络

- 前置知识：神经网络

- `torch.nn` 是 PyTorch 的神经网络模块，提供了：
  - **预定义层**：全连接层、卷积层、池化层等
  - **损失函数**：各种损失计算
  - **容器**：组织网络结构的容器 `nn.Sequential`
  - **工具函数**：初始化、正则化等
- `nn.Module` ；定义我们自己的神经网络：在 `__init__` 中初始化神经网络层。每个 `nn.Module` 子类都在 `forward` 方法中实现对输入数据的操作。

- 用于训练的设备选择；`.to(device)`

- `nn.Flatten`、`nn.Linear`、`nn.ReLU` 、`nn.Softmax`......

- 模型参数的查看

## 自动微分`torch.autograd`

- 使用 `loss.backward()` 计算，然后从 `w.grad` 和 `b.grad` 中检索值。

- 禁用梯度跟踪：默认情况下，所有 `requires_grad=True` 的张量都会跟踪它们的计算历史并支持梯度计算。但是，在某些情况下我们不需要这样做，例如，当我们训练好模型并只想将其应用于某些输入数据时，即我们只想通过网络进行*前向*计算。我们可以将计算代码用 `torch.no_grad()` 块包围来停止跟踪计算。
