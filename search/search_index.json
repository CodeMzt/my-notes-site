{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfe0 \u9996\u9875","text":""},{"location":"#_1","title":"\ud83d\ude80 \u6b22\u8fce\u6765\u5230\u6211\u7684\u5d4c\u5165\u5f0f\u4e0e\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u7ad9","text":"<p>Mzt2006 \u7684\u6280\u672f\u5907\u5fd8\u5f55\u4e0e\u5b66\u4e60\u8bb0\u5f55</p>"},{"location":"#_2","title":"\ud83d\udc4b \u5173\u4e8e\u672c\u7ad9","text":"<p>\u672c\u7f51\u7ad9\u662f\u6211\u4e2a\u4eba\u5728 \u5d4c\u5165\u5f0f\u7cfb\u7edf\uff08Embedded Systems\uff09\u548c \u673a\u5668\u5b66\u4e60\uff08Machine Learning\uff09\u9886\u57df\u5b66\u4e60\u4e0e\u5b9e\u8df5\u7684\u77e5\u8bc6\u4ed3\u5e93\u3002</p> <p>\u5185\u5bb9\u6db5\u76d6\u4ece\u5e95\u5c42\u786c\u4ef6\u67b6\u6784\uff08ARM \u6c47\u7f16\u3001Cortex-M\uff09\u5230\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\uff08FreeRTOS\uff09\uff0c\u518d\u5230\u4e0a\u5c42\u7b97\u6cd5\uff08\u7535\u63a7\u3001\u89c6\u89c9\uff09\u7684\u5b8c\u6574\u6280\u672f\u6808\u3002\u65e8\u5728\u7cfb\u7edf\u5316\u5730\u6574\u7406\u548c\u56de\u987e\u590d\u6742\u7684\u77e5\u8bc6\u70b9\uff0c\u4f9b\u81ea\u5df1\u67e5\u9605\uff0c\u4e5f\u6b22\u8fce\u540c\u884c\u4ea4\u6d41\u3002</p>"},{"location":"#_3","title":"\ud83d\udcda \u4e3b\u8981\u5b66\u4e60\u6a21\u5757","text":"<p>\u4ee5\u4e0b\u662f\u672c\u7ad9\u6700\u6838\u5fc3\u7684\u4e09\u4e2a\u77e5\u8bc6\u677f\u5757\uff0c\u4f60\u53ef\u4ee5\u70b9\u51fb\u6807\u9898\u8fdb\u5165\u5404\u9886\u57df\u7684\u6df1\u5ea6\u5b66\u4e60\u3002</p> \u6a21\u5757 \u7126\u70b9\u5185\u5bb9 \u5173\u952e\u8bcd \ud83d\udcbb \u5d4c\u5165\u5f0f\u8f6f\u4ef6 ARM \u67b6\u6784\u3001RTOS \u6838\u5fc3\u673a\u5236\u3001FreeRTOS \u6e90\u7801\u5206\u6790\u3001HAL \u5e93\u5f00\u53d1\u3001\u5e95\u5c42\u901a\u4fe1\u534f\u8bae\u3002 #ARM #FreeRTOS #STM32 #ContextSwitching \ud83e\udd16 \u673a\u5668\u5b66\u4e60 Pytorch \u6846\u67b6\u5b66\u4e60\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3001\u5f20\u91cf\u64cd\u4f5c\u3001\u6570\u636e\u5904\u7406\u4e0e\u53ef\u89c6\u5316\u3002 #PyTorch #DataScience #NeuralNetwork #Tensorboard \ud83d\udca1 \u7535\u63a7\u4e0e\u89c6\u89c9 PID\u3001LQR\u3001SMC \u7b49\u7ecf\u5178\u63a7\u5236\u7b97\u6cd5\uff0c\u673a\u5668\u89c6\u89c9\u57fa\u7840\u53ca\u5d4c\u5165\u5f0f\u89c6\u89c9\u5e94\u7528\uff08K230\uff09\u3002 #ControlAlgos #CV #Robotics"},{"location":"#quick-start","title":"\ud83c\udfaf \u5feb\u901f\u5f00\u59cb\uff08Quick Start\uff09","text":""},{"location":"#rtos","title":"\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf (RTOS)","text":"<p>\u4ece FreeRTOS \u7684\u6838\u5fc3\u6982\u5ff5\u5f00\u59cb\uff0c\u7406\u89e3\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u7684\u5e76\u53d1\u548c\u540c\u6b65\u673a\u5236\u3002</p> <ul> <li>FreeRTOS \u7b80\u4ecb</li> <li>\u540c\u6b65\u3001\u4e92\u65a5\u4e0e\u901a\u4fe1</li> <li>\u4efb\u52a1\u8c03\u5ea6</li> </ul>"},{"location":"#_4","title":"\u786c\u4ef6\u67b6\u6784\u57fa\u7840","text":"<p>\u4ece\u6700\u5e95\u5c42\u7406\u89e3\u5904\u7406\u5668\u548c\u7f16\u7a0b\u7684\u539f\u7406\u3002</p> <ul> <li>ARM \u67b6\u6784\u6982\u8ff0</li> <li>\u6c47\u7f16\u8bed\u8a00\u5165\u95e8</li> <li>\u5185\u5b58\u7ba1\u7406</li> </ul>"},{"location":"#pytorch","title":"Pytorch \u5165\u95e8","text":"<p>\u5feb\u901f\u4e86\u89e3 Pytorch \u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002</p> <ul> <li>Pytorch - \u5f20\u91cf</li> <li>Pytorch - \u795e\u7ecf\u7f51\u7edc</li> </ul>"},{"location":"#_5","title":"\u2728 \u7f51\u7ad9\u7279\u6027","text":"<ul> <li>\ud83c\udf10 \u4e2d\u82f1\u6df7\u5408\u547d\u540d: \u6587\u4ef6\u8def\u5f84\u4f7f\u7528\u82f1\u6587\uff0c\u786e\u4fdd\u8de8\u5e73\u53f0\u90e8\u7f72\u7a33\u5b9a\uff1b\u6807\u9898\u548c\u5185\u5bb9\u4f7f\u7528\u4e2d\u6587\uff0c\u65b9\u4fbf\u9605\u8bfb\u3002</li> <li>\ud83c\udf19 \u660e\u6697\u6a21\u5f0f: \u652f\u6301\u4e00\u952e\u5207\u6362\u660e\u4eae\u548c\u6697\u9ed1\u6a21\u5f0f\uff0c\u9002\u914d\u4e0d\u540c\u9605\u8bfb\u73af\u5883\u3002</li> <li>\ud83d\udd0d \u589e\u5f3a\u641c\u7d22: \u542f\u7528\u641c\u7d22\u5efa\u8bae\u548c\u9ad8\u4eae\u529f\u80fd\uff0c\u5e2e\u52a9\u5feb\u901f\u5b9a\u4f4d\u77e5\u8bc6\u70b9\u3002</li> <li>\ud83d\udcc4 Jupyter \u517c\u5bb9: \u76f4\u63a5\u5c55\u793a <code>.ipynb</code> \u7b14\u8bb0\u5185\u5bb9\uff0c\u65b9\u4fbf\u67e5\u9605\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u3002</li> </ul>"},{"location":"#_6","title":"\ud83d\udee0\ufe0f \u672c\u5730\u5f00\u53d1\u4e0e\u8d21\u732e","text":""},{"location":"#_7","title":"\u73af\u5883\u914d\u7f6e","text":"<pre><code># \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/CodeMzt/my-notes-site.git\ncd tech-notes\n\n# \u5b89\u88c5\u6587\u6863\u5de5\u5177\uff08\u53ef\u9009\uff09\npip install mkdocs mkdocs-material\n</code></pre>"},{"location":"#_8","title":"\u6587\u6863\u7ed3\u6784","text":"<pre><code>tech-notes/\n\u251c\u2500\u2500 docs/                    # \u5b8c\u6574\u6587\u6863\u76ee\u5f55\uff08\u4e3b\u5185\u5bb9\uff09\n\u2502   \u251c\u2500\u2500 README.md           # \ud83d\udccd \u5b8c\u6574\u9996\u9875\uff08\u4e3b\u6587\u6863\uff09\n\u2502   \u251c\u2500\u2500 EmbeddedSoft/       # \u5d4c\u5165\u5f0f\u8f6f\u4ef6\u6a21\u5757\n\u2502   \u251c\u2500\u2500 MachineLearning/    # \u673a\u5668\u5b66\u4e60\u6a21\u5757\n\u2502   \u251c\u2500\u2500 Control_Vision/     # \u7535\u63a7\u4e0e\u89c6\u89c9\u6a21\u5757\n\u2502   \u2514\u2500\u2500 Projects/          # \u9879\u76ee\u5b9e\u6218\n\u251c\u2500\u2500 assets/                 # \u9759\u6001\u8d44\u6e90\n\u251c\u2500\u2500 .github/               # GitHub \u5de5\u4f5c\u6d41\n\u2514\u2500\u2500 README.md              # \u5f53\u524d\u6587\u4ef6\uff08\u5bfc\u822a\u9875\uff09\n</code></pre>"},{"location":"#_9","title":"\ud83d\udcdd \u5982\u4f55\u8d21\u732e","text":"<p>\u6b22\u8fce\u5bf9\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3001\u673a\u5668\u5b66\u4e60\u3001\u63a7\u5236\u7b97\u6cd5\u7b49\u9886\u57df\u611f\u5174\u8da3\u7684\u5f00\u53d1\u8005\u4e00\u8d77\u5b8c\u5584\u8fd9\u4e2a\u77e5\u8bc6\u5e93\uff01</p>"},{"location":"#_10","title":"\u8d21\u732e\u6d41\u7a0b","text":"<ol> <li> <p>Fork \u4ed3\u5e93 <code>bash    # \u70b9\u51fb GitHub \u53f3\u4e0a\u89d2 Fork \u6309\u94ae    # \u514b\u9686\u4f60\u7684 fork    git clone https://github.com/\u4f60\u7684\u7528\u6237\u540d/my-notes-site.git</code></p> </li> <li> <p>\u521b\u5efa\u5206\u652f <code>bash    git checkout -b feature/\u4e3b\u9898\u540d\u79f0    # \u4f8b\u5982\uff1afeature/add-rtos-tutorial</code></p> </li> <li> <p>\u6dfb\u52a0\u6216\u4fee\u6539\u5185\u5bb9</p> </li> <li>\u6280\u672f\u7b14\u8bb0\uff1a\u5728 <code>docs/</code> \u5bf9\u5e94\u76ee\u5f55\u4e0b\u6dfb\u52a0 <code>.md</code> \u6587\u4ef6</li> <li>\u4ee3\u7801\u793a\u4f8b\uff1a\u5728 <code>examples/</code> \u76ee\u5f55\u6dfb\u52a0\u4ee3\u7801\u6587\u4ef6</li> <li> <p>\u4fee\u590d\u9519\u8bef\uff1a\u76f4\u63a5\u4fee\u6539\u6709\u95ee\u9898\u7684\u5730\u65b9</p> </li> <li> <p>\u63d0\u4ea4\u66f4\u6539 <code>bash    git add .    git commit -m \"\u7c7b\u578b: \u63cf\u8ff0\u6027\u4fe1\u606f\"    # \u7c7b\u578b\u53ef\u9009: docs, feat, fix, style, refactor    # \u4f8b\u5982: docs: \u65b0\u589eFreeRTOS\u4efb\u52a1\u8c03\u5ea6\u8be6\u89e3</code></p> </li> <li> <p>\u53d1\u8d77 Pull Request</p> </li> <li>\u540c\u6b65\u4e3b\u4ed3\u5e93\u6700\u65b0\u53d8\u66f4</li> <li>\u63a8\u9001\u5230\u4f60\u7684\u5206\u652f</li> <li>\u5728 GitHub \u521b\u5efa Pull Request</li> </ol>"},{"location":"#_11","title":"\u8d21\u732e\u6307\u5357","text":"<ul> <li>\u6587\u6863\u89c4\u8303</li> <li>\u4f7f\u7528\u4e2d\u6587\u4e3a\u4e3b\uff0c\u6280\u672f\u672f\u8bed\u4fdd\u7559\u82f1\u6587</li> <li>\u6587\u4ef6\u8def\u5f84\u4f7f\u7528\u82f1\u6587\uff0c\u907f\u514d\u7a7a\u683c\u548c\u7279\u6b8a\u5b57\u7b26</li> <li>\u56fe\u7247\u8d44\u6e90\u5b58\u653e\u5728 <code>assets/images/</code> \u5bf9\u5e94\u5b50\u76ee\u5f55</li> <li> <p>\u4ee3\u7801\u5757\u6807\u6ce8\u8bed\u8a00\u7c7b\u578b\uff08\u5982 <code>c,</code>python\uff09</p> </li> <li> <p>\u5185\u5bb9\u8981\u6c42</p> </li> <li>\u786e\u4fdd\u6280\u672f\u51c6\u786e\u6027\uff0c\u6709\u5b98\u65b9\u6587\u6863\u6216\u5b9e\u9a8c\u9a8c\u8bc1</li> <li>\u590d\u6742\u7684\u7406\u8bba\u9700\u914d\u56fe\u8bf4\u660e</li> <li>\u4ee3\u7801\u793a\u4f8b\u5e94\u6709\u6ce8\u91ca\u548c\u4f7f\u7528\u8bf4\u660e</li> <li> <p>\u5982\u679c\u662f\u7ffb\u8bd1\u5185\u5bb9\uff0c\u6ce8\u660e\u539f\u6587\u51fa\u5904</p> </li> <li> <p>\u63d0\u4ea4\u4fe1\u606f\u89c4\u8303   ```   \u7c7b\u578b(\u6a21\u5757): \u7b80\u77ed\u63cf\u8ff0</p> </li> </ul> <p>\u8be6\u7ec6\u8bf4\u660e\uff08\u53ef\u9009\uff09\uff1a   - \u4fee\u6539\u7684\u5185\u5bb9   - \u89e3\u51b3\u7684\u95ee\u9898   - \u76f8\u5173issue\u7f16\u53f7</p> <p>\u7c7b\u578b\u8bf4\u660e\uff1a   - docs: \u6587\u6863\u66f4\u65b0   - feat: \u65b0\u529f\u80fd/\u5185\u5bb9   - fix: \u9519\u8bef\u4fee\u590d   - style: \u683c\u5f0f\u8c03\u6574   - refactor: \u91cd\u6784\u5185\u5bb9   ```</p>"},{"location":"#_12","title":"\ud83c\udfc6 \u8d21\u732e\u8005\u540d\u5355","text":"<p>\u611f\u8c22\u6240\u6709\u4e3a\u8fd9\u4e2a\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u5f00\u53d1\u8005\uff01</p> <p></p>"},{"location":"#_13","title":"\ud83d\udd27 \u5f00\u53d1\u5de5\u5177\u63a8\u8350","text":""},{"location":"#_14","title":"\u6587\u6863\u7f16\u5199","text":"<ul> <li>VS Code + Markdown All in One \u63d2\u4ef6</li> <li>Typora - \u5373\u65f6\u9884\u89c8 Markdown \u7f16\u8f91\u5668</li> <li>Draw.io - \u6280\u672f\u56fe\u8868\u7ed8\u5236</li> </ul>"},{"location":"#_15","title":"\u5d4c\u5165\u5f0f\u5f00\u53d1","text":"<ul> <li>STM32CubeIDE - ARM \u5f00\u53d1\u73af\u5883</li> <li>FreeRTOS Kernel - \u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf</li> <li>Wireshark - \u534f\u8bae\u5206\u6790</li> </ul>"},{"location":"#_16","title":"\u673a\u5668\u5b66\u4e60","text":"<ul> <li>Jupyter Notebook - \u4ea4\u4e92\u5f0f\u5b9e\u9a8c</li> <li>PyTorch/TensorFlow - \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6</li> <li>MLflow - \u5b9e\u9a8c\u8ddf\u8e2a</li> </ul>"},{"location":"#_17","title":"\ud83d\udcc4 \u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u6587\u6863\u91c7\u7528 CC BY-NC-SA 4.0 \u8bb8\u53ef\u8bc1\uff0c\u4ee3\u7801\u793a\u4f8b\u91c7\u7528 MIT \u8bb8\u53ef\u8bc1\u3002</p>"},{"location":"#_18","title":"\ud83d\udcee \u8054\u7cfb\u4e0e\u53cd\u9988","text":"<ul> <li>\u95ee\u9898\u53cd\u9988: GitHub Issues</li> <li>\u6280\u672f\u8ba8\u8bba: \u6b22\u8fce\u5728 Issues \u4e2d\u53d1\u8d77\u8ba8\u8bba</li> <li>\u5185\u5bb9\u5efa\u8bae: \u76f4\u63a5\u63d0\u4ea4 Pull Request \u6216 Issue</li> </ul> <p>\u2b50 \u5982\u679c\u8fd9\u4e2a\u9879\u76ee\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u8bf7\u7ed9\u4e2a Star\uff01 \ud83d\udcda \u5b8c\u6574\u6587\u6863\u8bf7\u8bbf\u95ee\uff1a\u9996\u9875</p> <p>\u6301\u7eed\u66f4\u65b0\u4e2d...*</p>"},{"location":"DeepLearning/part1/Assignment/","title":"Practice Lab: Neural Networks for Handwritten Digit Recognition, Binary","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport matplotlib.pyplot as plt\nfrom autils import *\n%matplotlib inline\n\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense import matplotlib.pyplot as plt from autils import * %matplotlib inline  import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) <p>Tensorflow and Keras Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by Fran\u00e7ois Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface.</p> <p></p> In\u00a0[2]: Copied! <pre># load dataset\nX, y = load_data()\n</pre> # load dataset X, y = load_data() <p></p> In\u00a0[3]: Copied! <pre>print ('The first element of X is: ', X[0])\n</pre> print ('The first element of X is: ', X[0]) <pre>The first element of X is:  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  8.56059680e-06\n  1.94035948e-06 -7.37438725e-04 -8.13403799e-03 -1.86104473e-02\n -1.87412865e-02 -1.87572508e-02 -1.90963542e-02 -1.64039011e-02\n -3.78191381e-03  3.30347316e-04  1.27655229e-05  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  1.16421569e-04  1.20052179e-04\n -1.40444581e-02 -2.84542484e-02  8.03826593e-02  2.66540339e-01\n  2.73853746e-01  2.78729541e-01  2.74293607e-01  2.24676403e-01\n  2.77562977e-02 -7.06315478e-03  2.34715414e-04  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  1.28335523e-17 -3.26286765e-04 -1.38651604e-02\n  8.15651552e-02  3.82800381e-01  8.57849775e-01  1.00109761e+00\n  9.69710638e-01  9.30928598e-01  1.00383757e+00  9.64157356e-01\n  4.49256553e-01 -5.60408259e-03 -3.78319036e-03  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.10620915e-06\n  4.36410675e-04 -3.95509940e-03 -2.68537241e-02  1.00755014e-01\n  6.42031710e-01  1.03136838e+00  8.50968614e-01  5.43122379e-01\n  3.42599738e-01  2.68918777e-01  6.68374643e-01  1.01256958e+00\n  9.03795598e-01  1.04481574e-01 -1.66424973e-02  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.59875260e-05\n -3.10606987e-03  7.52456076e-03  1.77539831e-01  7.92890120e-01\n  9.65626503e-01  4.63166079e-01  6.91720680e-02 -3.64100526e-03\n -4.12180405e-02 -5.01900656e-02  1.56102907e-01  9.01762651e-01\n  1.04748346e+00  1.51055252e-01 -2.16044665e-02  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  5.87012352e-05 -6.40931373e-04\n -3.23305249e-02  2.78203465e-01  9.36720163e-01  1.04320956e+00\n  5.98003217e-01 -3.59409041e-03 -2.16751770e-02 -4.81021923e-03\n  6.16566793e-05 -1.23773318e-02  1.55477482e-01  9.14867477e-01\n  9.20401348e-01  1.09173902e-01 -1.71058007e-02  0.00000000e+00\n  0.00000000e+00  1.56250000e-04 -4.27724104e-04 -2.51466503e-02\n  1.30532561e-01  7.81664862e-01  1.02836583e+00  7.57137601e-01\n  2.84667194e-01  4.86865128e-03 -3.18688725e-03  0.00000000e+00\n  8.36492601e-04 -3.70751123e-02  4.52644165e-01  1.03180133e+00\n  5.39028101e-01 -2.43742611e-03 -4.80290033e-03  0.00000000e+00\n  0.00000000e+00 -7.03635621e-04 -1.27262443e-02  1.61706648e-01\n  7.79865383e-01  1.03676705e+00  8.04490400e-01  1.60586724e-01\n -1.38173339e-02  2.14879493e-03 -2.12622549e-04  2.04248366e-04\n -6.85907627e-03  4.31712963e-04  7.20680947e-01  8.48136063e-01\n  1.51383408e-01 -2.28404366e-02  1.98971950e-04  0.00000000e+00\n  0.00000000e+00 -9.40410539e-03  3.74520505e-02  6.94389110e-01\n  1.02844844e+00  1.01648066e+00  8.80488426e-01  3.92123945e-01\n -1.74122413e-02 -1.20098039e-04  5.55215142e-05 -2.23907271e-03\n -2.76068376e-02  3.68645493e-01  9.36411169e-01  4.59006723e-01\n -4.24701797e-02  1.17356610e-03  1.88929739e-05  0.00000000e+00\n  0.00000000e+00 -1.93511951e-02  1.29999794e-01  9.79821705e-01\n  9.41862388e-01  7.75147704e-01  8.73632241e-01  2.12778350e-01\n -1.72353349e-02  0.00000000e+00  1.09937426e-03 -2.61793751e-02\n  1.22872879e-01  8.30812662e-01  7.26501773e-01  5.24441863e-02\n -6.18971913e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00 -9.36563862e-03  3.68349741e-02  6.99079299e-01\n  1.00293583e+00  6.05704402e-01  3.27299224e-01 -3.22099249e-02\n -4.83053002e-02 -4.34069138e-02 -5.75151144e-02  9.55674190e-02\n  7.26512627e-01  6.95366966e-01  1.47114481e-01 -1.20048679e-02\n -3.02798203e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00 -6.76572712e-04 -6.51415556e-03  1.17339359e-01\n  4.21948410e-01  9.93210937e-01  8.82013974e-01  7.45758734e-01\n  7.23874268e-01  7.23341725e-01  7.20020340e-01  8.45324959e-01\n  8.31859739e-01  6.88831870e-02 -2.77765012e-02  3.59136710e-04\n  7.14869281e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  1.53186275e-04  3.17353553e-04 -2.29167177e-02\n -4.14402914e-03  3.87038450e-01  5.04583435e-01  7.74885876e-01\n  9.90037446e-01  1.00769478e+00  1.00851440e+00  7.37905042e-01\n  2.15455291e-01 -2.69624864e-02  1.32506127e-03  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.36366422e-04\n -2.26031454e-03 -2.51994485e-02 -3.73889910e-02  6.62121228e-02\n  2.91134498e-01  3.23055726e-01  3.06260315e-01  8.76070942e-02\n -2.50581917e-02  2.37438725e-04  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  6.20939216e-18  6.72618320e-04 -1.13151411e-02\n -3.54641066e-02 -3.88214912e-02 -3.71077412e-02 -1.33524928e-02\n  9.90964718e-04  4.89176960e-05  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n</pre> In\u00a0[4]: Copied! <pre>print ('The first element of y is: ', y[0,0])\nprint ('The last element of y is: ', y[-1,0])\n</pre> print ('The first element of y is: ', y[0,0]) print ('The last element of y is: ', y[-1,0]) <pre>The first element of y is:  0\nThe last element of y is:  1\n</pre> <p></p> In\u00a0[5]: Copied! <pre>print ('The shape of X is: ' + str(X.shape))\nprint ('The shape of y is: ' + str(y.shape))\n</pre> print ('The shape of X is: ' + str(X.shape)) print ('The shape of y is: ' + str(y.shape)) <pre>The shape of X is: (1000, 400)\nThe shape of y is: (1000, 1)\n</pre> <p></p> In\u00a0[7]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1)\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Display the label above the image\n    ax.set_title(y[random_index,0])\n    ax.set_axis_off()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1)  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Display the label above the image     ax.set_title(y[random_index,0])     ax.set_axis_off() <p></p> <ul> <li><p>The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $1$ output unit in layer 3.</p> <ul> <li><p>Recall that the dimensions of these parameters are determined as follows:</p> <ul> <li>If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then<ul> <li>$W$ will be of dimension $s_{in} \\times s_{out}$.</li> <li>$b$ will a vector with $s_{out}$ elements</li> </ul> </li> </ul> </li> <li><p>Therefore, the shapes of <code>W</code>, and <code>b</code>,  are</p> <ul> <li>layer1: The shape of <code>W1</code> is (400, 25) and the shape of <code>b1</code> is (25,)</li> <li>layer2: The shape of <code>W2</code> is (25, 15) and the shape of <code>b2</code> is: (15,)</li> <li>layer3: The shape of <code>W3</code> is (15, 1) and the shape of <code>b3</code> is: (1,)</li> </ul> </li> </ul> </li> </ul> <p>Note: The bias vector <code>b</code> could be represented as a 1-D (n,) or 2-D (n,1) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention.</p> <p></p> <p>Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's output dimensions and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the <code>model.fit</code> statment below.</p> <p>Note: It is also possible to add an input layer that specifies the input dimension of the first layer. For example: <code>tf.keras.Input(shape=(400,)),    #specify input shape</code> We will include that here to illuminate some model sizing.</p> <p></p> In\u00a0[8]: Copied! <pre># UNQ_C1\n# GRADED CELL: Sequential model\n\nmodel = Sequential(\n    [               \n        tf.keras.Input(shape=(400,)),    #specify input size\n        ### START CODE HERE ### \n        tf.keras.layers.Dense(25, activation=\"sigmoid\"),\n        tf.keras.layers.Dense(15, activation=\"sigmoid\"),\n        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n        ### END CODE HERE ### \n    ], name = \"my_model\" \n)                            \n</pre> # UNQ_C1 # GRADED CELL: Sequential model  model = Sequential(     [                        tf.keras.Input(shape=(400,)),    #specify input size         ### START CODE HERE ###          tf.keras.layers.Dense(25, activation=\"sigmoid\"),         tf.keras.layers.Dense(15, activation=\"sigmoid\"),         tf.keras.layers.Dense(1, activation=\"sigmoid\")         ### END CODE HERE ###      ], name = \"my_model\"  )                              In\u00a0[9]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"my_model\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense (Dense)                        \u2502 (None, 25)                  \u2502          10,025 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                      \u2502 (None, 15)                  \u2502             390 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                      \u2502 (None, 1)                   \u2502              16 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 10,431 (40.75 KB)\n</pre> <pre> Trainable params: 10,431 (40.75 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> Expected Output (Click to Expand)  The `model.summary()` function displays a useful summary of the model. Because we have specified an input layer size, the shape of the weight and bias arrays are determined and the total number of parameters per layer can be shown. Note, the names of the layers may vary as they are auto-generated.    <pre><code>Model: \"my_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 25)                10025     \n_________________________________________________________________\ndense_1 (Dense)              (None, 15)                390       \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 16        \n=================================================================\nTotal params: 10,431\nTrainable params: 10,431\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> Click for hints As described in the lecture:  <pre>model = Sequential(                      \n    [                                   \n        tf.keras.Input(shape=(400,)),    # specify input size (optional)\n        Dense(25, activation='sigmoid'), \n        Dense(15, activation='sigmoid'), \n        Dense(1,  activation='sigmoid')  \n    ], name = \"my_model\"                                    \n)                                       \n</pre> In\u00a0[10]: Copied! <pre># UNIT TESTS\nfrom public_tests import * \n\ntest_c1(model)\n</pre> # UNIT TESTS from public_tests import *   test_c1(model) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[10], line 4\n      1 # UNIT TESTS\n      2 from public_tests import * \n----&gt; 4 test_c1(model)\n\nFile D:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week1\\C2W1A1\\public_tests.py:10, in test_c1(target)\n      7 def test_c1(target):\n      8     assert len(target.layers) == 3, \\\n      9         f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n---&gt; 10     assert target.input.shape.as_list() == [None, 400], \\\n     11         f\"Wrong input shape. Expected [None,  400] but got {target.input.shape.as_list()}\"\n     12     i = 0\n     13     expected = [[Dense, [None, 25], sigmoid],\n     14                 [Dense, [None, 15], sigmoid],\n     15                 [Dense, [None, 1], sigmoid]]\n\nFile D:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\src\\ops\\operation.py:337, in Operation.input(self)\n    327 @property\n    328 def input(self):\n    329     \"\"\"Retrieves the input tensor(s) of a symbolic operation.\n    330 \n    331     Only returns the tensor(s) corresponding to the *first time*\n   (...)\n    335         Input tensor or list of input tensors.\n    336     \"\"\"\n--&gt; 337     return self._get_node_attribute_at_index(0, \"input_tensors\", \"input\")\n\nFile D:\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\src\\ops\\operation.py:368, in Operation._get_node_attribute_at_index(self, node_index, attr, attr_name)\n    352 \"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\n    353 \n    354 This is used to implement the properties:\n   (...)\n    365     The operation's attribute `attr` at the node of index `node_index`.\n    366 \"\"\"\n    367 if not self._inbound_nodes:\n--&gt; 368     raise AttributeError(\n    369         f\"The layer {self.name} has never been called \"\n    370         f\"and thus has no defined {attr_name}.\"\n    371     )\n    372 if not len(self._inbound_nodes) &gt; node_index:\n    373     raise ValueError(\n    374         f\"Asked to get {attr_name} at node \"\n    375         f\"{node_index}, but the operation has only \"\n    376         f\"{len(self._inbound_nodes)} inbound nodes.\"\n    377     )\n\nAttributeError: The layer my_model has never been called and thus has no defined input.</pre> <p>The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays as shown below.</p> In\u00a0[11]: Copied! <pre>L1_num_params = 400 * 25 + 25  # W1 parameters  + b1 parameters\nL2_num_params = 25 * 15 + 15   # W2 parameters  + b2 parameters\nL3_num_params = 15 * 1 + 1     # W3 parameters  + b3 parameters\nprint(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params, \",  L3 params = \", L3_num_params )\n</pre> L1_num_params = 400 * 25 + 25  # W1 parameters  + b1 parameters L2_num_params = 25 * 15 + 15   # W2 parameters  + b2 parameters L3_num_params = 15 * 1 + 1     # W3 parameters  + b3 parameters print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params, \",  L3 params = \", L3_num_params ) <pre>L1 params =  10025 , L2 params =  390 ,  L3 params =  16\n</pre> <p>Let's further examine the weights to verify that tensorflow produced the same dimensions as we calculated above.</p> In\u00a0[12]: Copied! <pre>[layer1, layer2, layer3] = model.layers\n</pre> [layer1, layer2, layer3] = model.layers In\u00a0[13]: Copied! <pre>#### Examine Weights shapes\nW1,b1 = layer1.get_weights()\nW2,b2 = layer2.get_weights()\nW3,b3 = layer3.get_weights()\nprint(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\nprint(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\nprint(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n</pre> #### Examine Weights shapes W1,b1 = layer1.get_weights() W2,b2 = layer2.get_weights() W3,b3 = layer3.get_weights() print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\") print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\") print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\") <pre>W1 shape = (400, 25), b1 shape = (25,)\nW2 shape = (25, 15), b2 shape = (15,)\nW3 shape = (15, 1), b3 shape = (1,)\n</pre> <p>Expected Output</p> <pre><code>W1 shape = (400, 25), b1 shape = (25,)  \nW2 shape = (25, 15), b2 shape = (15,)  \nW3 shape = (15, 1), b3 shape = (1,)\n</code></pre> <p><code>xx.get_weights</code> returns a NumPy array. One can also access the weights directly in their tensor form. Note the shape of the tensors in the final layer.</p> In\u00a0[14]: Copied! <pre>print(model.layers[2].weights)\n</pre> print(model.layers[2].weights) <pre>[&lt;Variable path=my_model/dense_2/kernel, shape=(15, 1), dtype=float32, value=[[ 0.15700519]\n [ 0.5334124 ]\n [-0.0971905 ]\n [ 0.46561807]\n [ 0.1878447 ]\n [ 0.17097592]\n [-0.41699964]\n [-0.15640718]\n [-0.29133296]\n [ 0.07087266]\n [ 0.5918456 ]\n [ 0.5490479 ]\n [-0.5716338 ]\n [ 0.19458014]\n [ 0.13206285]]&gt;, &lt;Variable path=my_model/dense_2/bias, shape=(1,), dtype=float32, value=[0.]&gt;]\n</pre> <p>The following code will define a loss function and run gradient descent to fit the weights of the model to the training data. This will be explained in more detail in the following week.</p> In\u00a0[15]: Copied! <pre>model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X,y,\n    epochs=20\n)\n</pre> model.compile(     loss=tf.keras.losses.BinaryCrossentropy(),     optimizer=tf.keras.optimizers.Adam(0.001), )  model.fit(     X,y,     epochs=20 ) <pre>Epoch 1/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step - loss: 0.6535   \nEpoch 2/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.5163 \nEpoch 3/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.3775 \nEpoch 4/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.2688 \nEpoch 5/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.1996 \nEpoch 6/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 0.1556 \nEpoch 7/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.1257 \nEpoch 8/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.1043 \nEpoch 9/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0879 \nEpoch 10/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0754 \nEpoch 11/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0657 \nEpoch 12/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0578 \nEpoch 13/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0515 \nEpoch 14/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0463 \nEpoch 15/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0421 \nEpoch 16/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0386 \nEpoch 17/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0357 \nEpoch 18/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0331 \nEpoch 19/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0309 \nEpoch 20/20\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 1ms/step - loss: 0.0289 \n</pre> Out[15]: <pre>&lt;keras.src.callbacks.history.History at 0x26333314a00&gt;</pre> <p>To run the model on an example to make a prediction, use Keras <code>predict</code>. The input to <code>predict</code> is an array so the single example is reshaped to be two dimensional.</p> In\u00a0[16]: Copied! <pre>prediction = model.predict(X[0].reshape(1,400))  # a zero\nprint(f\" predicting a zero: {prediction}\")\nprediction = model.predict(X[500].reshape(1,400))  # a one\nprint(f\" predicting a one:  {prediction}\")\n</pre> prediction = model.predict(X[0].reshape(1,400))  # a zero print(f\" predicting a zero: {prediction}\") prediction = model.predict(X[500].reshape(1,400))  # a one print(f\" predicting a one:  {prediction}\") <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 41ms/step\n predicting a zero: [[0.03235131]]\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n predicting a one:  [[0.9875155]]\n</pre> <p>The output of the model is interpreted as a probability. In the first example above, the input is a zero. The model predicts the probability that the input is a one is nearly zero. In the second example, the input is a one. The model predicts the probability that the input is a one is nearly one. As in the case of logistic regression, the probability is compared to a threshold to make a final prediction.</p> In\u00a0[17]: Copied! <pre>if prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint(f\"prediction after threshold: {yhat}\")\n</pre> if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print(f\"prediction after threshold: {yhat}\") <pre>prediction after threshold: 1\n</pre> <p>Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run.</p> In\u00a0[18]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Predict using the Neural Network\n    prediction = model.predict(X[random_index].reshape(1,400))\n    if prediction &gt;= 0.5:\n        yhat = 1\n    else:\n        yhat = 0\n    \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n    ax.set_axis_off()\nfig.suptitle(\"Label, yhat\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Predict using the Neural Network     prediction = model.predict(X[random_index].reshape(1,400))     if prediction &gt;= 0.5:         yhat = 1     else:         yhat = 0          # Display the label above the image     ax.set_title(f\"{y[random_index,0]},{yhat}\")     ax.set_axis_off() fig.suptitle(\"Label, yhat\", fontsize=16) plt.show() <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n</pre> <p></p> <p></p> In\u00a0[19]: Copied! <pre># UNQ_C2\n# GRADED FUNCTION: my_dense\n\ndef my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n### START CODE HERE ### \n    for i in range(units):\n        w = W[:,i]\n        z=np.dot(w,a_in) + b[i]\n        a_out[i]=g(z)\n### END CODE HERE ### \n    return(a_out)\n</pre> # UNQ_C2 # GRADED FUNCTION: my_dense  def my_dense(a_in, W, b, g):     \"\"\"     Computes dense layer     Args:       a_in (ndarray (n, )) : Data, 1 example        W    (ndarray (n,j)) : Weight matrix, n features per unit, j units       b    (ndarray (j, )) : bias vector, j units         g    activation function (e.g. sigmoid, relu..)     Returns       a_out (ndarray (j,))  : j units     \"\"\"     units = W.shape[1]     a_out = np.zeros(units) ### START CODE HERE ###      for i in range(units):         w = W[:,i]         z=np.dot(w,a_in) + b[i]         a_out[i]=g(z) ### END CODE HERE ###      return(a_out)  In\u00a0[20]: Copied! <pre># Quick Check\nx_tst = 0.1*np.arange(1,3,1).reshape(2,)  # (1 examples, 2 features)\nW_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features)\nb_tst = 0.1*np.arange(1,4,1).reshape(3,)  # (3 features)\nA_tst = my_dense(x_tst, W_tst, b_tst, sigmoid)\nprint(A_tst)\n</pre> # Quick Check x_tst = 0.1*np.arange(1,3,1).reshape(2,)  # (1 examples, 2 features) W_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features) b_tst = 0.1*np.arange(1,4,1).reshape(3,)  # (3 features) A_tst = my_dense(x_tst, W_tst, b_tst, sigmoid) print(A_tst) <pre>[0.54735762 0.57932425 0.61063923]\n</pre> <p>Expected Output</p> <pre><code>[0.54735762 0.57932425 0.61063923]\n</code></pre> Click for hints As described in the lecture:  <pre>def my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):             \n        w =                            # Select weights for unit j. These are in column j of W\n        z =                            # dot product of w and a_in + b\n        a_out[j] =                     # apply activation to z\n    return(a_out)\n</pre> Click for more hints <pre>def my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):             \n        w = W[:,j]                     \n        z = np.dot(w, a_in) + b[j]     \n        a_out[j] = g(z)                \n    return(a_out)\n</pre> In\u00a0[24]: Copied! <pre># UNIT TESTS\ntest_c2(my_dense)\n</pre> # UNIT TESTS test_c2(my_dense) <pre>All tests passed!\n</pre> <p>The following cell builds a three-layer neural network utilizing the <code>my_dense</code> subroutine above.</p> In\u00a0[25]: Copied! <pre>def my_sequential(x, W1, b1, W2, b2, W3, b3):\n    a1 = my_dense(x,  W1, b1, sigmoid)\n    a2 = my_dense(a1, W2, b2, sigmoid)\n    a3 = my_dense(a2, W3, b3, sigmoid)\n    return(a3)\n</pre> def my_sequential(x, W1, b1, W2, b2, W3, b3):     a1 = my_dense(x,  W1, b1, sigmoid)     a2 = my_dense(a1, W2, b2, sigmoid)     a3 = my_dense(a2, W3, b3, sigmoid)     return(a3) <p>We can copy trained weights and biases from Tensorflow.</p> In\u00a0[26]: Copied! <pre>W1_tmp,b1_tmp = layer1.get_weights()\nW2_tmp,b2_tmp = layer2.get_weights()\nW3_tmp,b3_tmp = layer3.get_weights()\n</pre> W1_tmp,b1_tmp = layer1.get_weights() W2_tmp,b2_tmp = layer2.get_weights() W3_tmp,b3_tmp = layer3.get_weights() In\u00a0[27]: Copied! <pre># make predictions\nprediction = my_sequential(X[0], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nif prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint( \"yhat = \", yhat, \" label= \", y[0,0])\nprediction = my_sequential(X[500], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nif prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint( \"yhat = \", yhat, \" label= \", y[500,0])\n</pre> # make predictions prediction = my_sequential(X[0], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print( \"yhat = \", yhat, \" label= \", y[0,0]) prediction = my_sequential(X[500], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print( \"yhat = \", yhat, \" label= \", y[500,0]) <pre>yhat =  0  label=  0\nyhat =  1  label=  1\n</pre> <p>Run the following cell to see predictions from both the Numpy model and the Tensorflow model. This takes a moment to run.</p> In\u00a0[28]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n\n    # Predict using the Neural Network implemented in Numpy\n    my_prediction = my_sequential(X[random_index], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\n    my_yhat = int(my_prediction &gt;= 0.5)\n\n    # Predict using the Neural Network implemented in Tensorflow\n    tf_prediction = model.predict(X[random_index].reshape(1,400))\n    tf_yhat = int(tf_prediction &gt;= 0.5)\n    \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{tf_yhat},{my_yhat}\")\n    ax.set_axis_off() \nfig.suptitle(\"Label, yhat Tensorflow, yhat Numpy\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')      # Predict using the Neural Network implemented in Numpy     my_prediction = my_sequential(X[random_index], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )     my_yhat = int(my_prediction &gt;= 0.5)      # Predict using the Neural Network implemented in Tensorflow     tf_prediction = model.predict(X[random_index].reshape(1,400))     tf_yhat = int(tf_prediction &gt;= 0.5)          # Display the label above the image     ax.set_title(f\"{y[random_index,0]},{tf_yhat},{my_yhat}\")     ax.set_axis_off()  fig.suptitle(\"Label, yhat Tensorflow, yhat Numpy\", fontsize=16) plt.show() <p></p> In\u00a0[29]: Copied! <pre>x = X[0].reshape(-1,1)         # column vector (400,1)\nz1 = np.matmul(x.T,W1) + b1    # (1,400)(400,25) = (1,25)\na1 = sigmoid(z1)\nprint(a1.shape)\n</pre> x = X[0].reshape(-1,1)         # column vector (400,1) z1 = np.matmul(x.T,W1) + b1    # (1,400)(400,25) = (1,25) a1 = sigmoid(z1) print(a1.shape) <pre>(1, 25)\n</pre> <p>You can take this a step further and compute all the units for all examples in one Matrix-Matrix operation.</p>  The full operation is $\\mathbf{Z}=\\mathbf{XW}+\\mathbf{b}$. This will utilize NumPy broadcasting to expand $\\mathbf{b}$ to $m$ rows. If this is unfamiliar, a short tutorial is provided at the end of the notebook.    <p></p> In\u00a0[31]: Copied! <pre># UNQ_C3\n# GRADED FUNCTION: my_dense_v\n\ndef my_dense_v(A_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      A_in (ndarray (m,n)) : Data, m examples, n features each\n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (1,j)) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      A_out (ndarray (m,j)) : m examples, j units\n    \"\"\"\n### START CODE HERE ### \n    A_out = g(np.matmul(A_in,W) + b)\n    \n### END CODE HERE ### \n    return(A_out)\n</pre> # UNQ_C3 # GRADED FUNCTION: my_dense_v  def my_dense_v(A_in, W, b, g):     \"\"\"     Computes dense layer     Args:       A_in (ndarray (m,n)) : Data, m examples, n features each       W    (ndarray (n,j)) : Weight matrix, n features per unit, j units       b    (ndarray (1,j)) : bias vector, j units         g    activation function (e.g. sigmoid, relu..)     Returns       A_out (ndarray (m,j)) : m examples, j units     \"\"\" ### START CODE HERE ###      A_out = g(np.matmul(A_in,W) + b)      ### END CODE HERE ###      return(A_out) In\u00a0[32]: Copied! <pre>X_tst = 0.1*np.arange(1,9,1).reshape(4,2) # (4 examples, 2 features)\nW_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features)\nb_tst = 0.1*np.arange(1,4,1).reshape(1,3) # (1, 3 features)\nA_tst = my_dense_v(X_tst, W_tst, b_tst, sigmoid)\nprint(A_tst)\n</pre> X_tst = 0.1*np.arange(1,9,1).reshape(4,2) # (4 examples, 2 features) W_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features) b_tst = 0.1*np.arange(1,4,1).reshape(1,3) # (1, 3 features) A_tst = my_dense_v(X_tst, W_tst, b_tst, sigmoid) print(A_tst) <pre>tf.Tensor(\n[[0.54735762 0.57932425 0.61063923]\n [0.57199613 0.61301418 0.65248946]\n [0.5962827  0.64565631 0.6921095 ]\n [0.62010643 0.67699586 0.72908792]], shape=(4, 3), dtype=float64)\n</pre> <p>Expected Output</p> <pre><code>[[0.54735762 0.57932425 0.61063923]\n [0.57199613 0.61301418 0.65248946]\n [0.5962827  0.64565631 0.6921095 ]\n [0.62010643 0.67699586 0.72908792]]\n</code></pre> Click for hints     In matrix form, this can be written in one or two lines.   <pre><code>   Z = np.matmul of A_in and W plus b    \n   A_out is g(Z)  </code></pre> Click for code <pre>def my_dense_v(A_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      A_in (ndarray (m,n)) : Data, m examples, n features each\n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j,1)) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      A_out (ndarray (m,j)) : m examples, j units\n    \"\"\"\n    Z = np.matmul(A_in,W) + b    \n    A_out = g(Z)                 \n    return(A_out)\n</pre> In\u00a0[33]: Copied! <pre># UNIT TESTS\ntest_c3(my_dense_v)\n</pre> # UNIT TESTS test_c3(my_dense_v) <pre>All tests passed!\n</pre> <p>The following cell builds a three-layer neural network utilizing the <code>my_dense_v</code> subroutine above.</p> In\u00a0[34]: Copied! <pre>def my_sequential_v(X, W1, b1, W2, b2, W3, b3):\n    A1 = my_dense_v(X,  W1, b1, sigmoid)\n    A2 = my_dense_v(A1, W2, b2, sigmoid)\n    A3 = my_dense_v(A2, W3, b3, sigmoid)\n    return(A3)\n</pre> def my_sequential_v(X, W1, b1, W2, b2, W3, b3):     A1 = my_dense_v(X,  W1, b1, sigmoid)     A2 = my_dense_v(A1, W2, b2, sigmoid)     A3 = my_dense_v(A2, W3, b3, sigmoid)     return(A3) <p>We can again copy trained weights and biases from Tensorflow.</p> In\u00a0[35]: Copied! <pre>W1_tmp,b1_tmp = layer1.get_weights()\nW2_tmp,b2_tmp = layer2.get_weights()\nW3_tmp,b3_tmp = layer3.get_weights()\n</pre> W1_tmp,b1_tmp = layer1.get_weights() W2_tmp,b2_tmp = layer2.get_weights() W3_tmp,b3_tmp = layer3.get_weights() <p>Let's make a prediction with the new model. This will make a prediction on all of the examples at once. Note the shape of the output.</p> In\u00a0[36]: Copied! <pre>Prediction = my_sequential_v(X, W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nPrediction.shape\n</pre> Prediction = my_sequential_v(X, W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) Prediction.shape Out[36]: <pre>TensorShape([1000, 1])</pre> <p>We'll apply a threshold of 0.5 as before, but to all predictions at once.</p> In\u00a0[37]: Copied! <pre>Yhat = (Prediction &gt;= 0.5).numpy().astype(int)\nprint(\"predict a zero: \",Yhat[0], \"predict a one: \", Yhat[500])\n</pre> Yhat = (Prediction &gt;= 0.5).numpy().astype(int) print(\"predict a zero: \",Yhat[0], \"predict a one: \", Yhat[500]) <pre>predict a zero:  [0] predict a one:  [1]\n</pre> <p>Run the following cell to see predictions. This will use the predictions we just calculated above. This takes a moment to run.</p> In\u00a0[38]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8, 8, figsize=(8, 8))\nfig.tight_layout(pad=0.1, rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i, ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20, 20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n   \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")\n    ax.set_axis_off() \nfig.suptitle(\"Label, Yhat\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8, 8, figsize=(8, 8)) fig.tight_layout(pad=0.1, rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i, ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20, 20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')         # Display the label above the image     ax.set_title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")     ax.set_axis_off()  fig.suptitle(\"Label, Yhat\", fontsize=16) plt.show() <p>You can see how one of the misclassified images looks.</p> In\u00a0[39]: Copied! <pre>fig = plt.figure(figsize=(1, 1))\nerrors = np.where(y != Yhat)\nrandom_index = errors[0][0]\nX_random_reshaped = X[random_index].reshape((20, 20)).T\nplt.imshow(X_random_reshaped, cmap='gray')\nplt.title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")\nplt.axis('off')\nplt.show()\n</pre> fig = plt.figure(figsize=(1, 1)) errors = np.where(y != Yhat) random_index = errors[0][0] X_random_reshaped = X[random_index].reshape((20, 20)).T plt.imshow(X_random_reshaped, cmap='gray') plt.title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\") plt.axis('off') plt.show() <p></p> <p></p> <p>In the last example,  $\\mathbf{Z}=\\mathbf{XW} + \\mathbf{b}$ utilized NumPy broadcasting to expand the vector $\\mathbf{b}$. If you are not familiar with NumPy Broadcasting, this short tutorial is provided.</p> <p>$\\mathbf{XW}$  is a matrix-matrix operation with dimensions $(m,j_1)(j_1,j_2)$ which results in a matrix with dimension  $(m,j_2)$. To that, we add a vector $\\mathbf{b}$ with dimension $(1,j_2)$.  $\\mathbf{b}$ must be expanded to be a $(m,j_2)$ matrix for this element-wise operation to make sense. This expansion is accomplished for you by NumPy broadcasting.</p> <p>Broadcasting applies to element-wise operations. Its basic operation is to 'stretch' a smaller dimension by replicating elements to match a larger dimension.</p> <p>More specifically: When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when</p> <ul> <li>they are equal, or</li> <li>one of them is 1</li> </ul> <p>If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.</p> <p>Here are some examples:</p> Calculating Broadcast Result shape <p>The graphic below describes expanding dimensions. Note the red text below:</p> Broadcast notionally expands arguments to match for element wise operations <p>The graphic above shows NumPy expanding the arguments to match before the final operation. Note that this is a notional description. The actual mechanics of NumPy operation choose the most efficient implementation.</p> <p>For each of the following examples, try to guess the size of the result before running the example.</p> In\u00a0[40]: Copied! <pre>a = np.array([1,2,3]).reshape(-1,1)  #(3,1)\nb = 5\nprint(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\")\n</pre> a = np.array([1,2,3]).reshape(-1,1)  #(3,1) b = 5 print(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\") <pre>(a + b).shape: (3, 1), \na + b = \n[[6]\n [7]\n [8]]\n</pre> <p>Note that this applies to all element-wise operations:</p> In\u00a0[41]: Copied! <pre>a = np.array([1,2,3]).reshape(-1,1)  #(3,1)\nb = 5\nprint(f\"(a * b).shape: {(a * b).shape}, \\na * b = \\n{a * b}\")\n</pre> a = np.array([1,2,3]).reshape(-1,1)  #(3,1) b = 5 print(f\"(a * b).shape: {(a * b).shape}, \\na * b = \\n{a * b}\") <pre>(a * b).shape: (3, 1), \na * b = \n[[ 5]\n [10]\n [15]]\n</pre> Row-Column Element-Wise Operations In\u00a0[42]: Copied! <pre>a = np.array([1,2,3,4]).reshape(-1,1)\nb = np.array([1,2,3]).reshape(1,-1)\nprint(a)\nprint(b)\nprint(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\")\n</pre> a = np.array([1,2,3,4]).reshape(-1,1) b = np.array([1,2,3]).reshape(1,-1) print(a) print(b) print(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\") <pre>[[1]\n [2]\n [3]\n [4]]\n[[1 2 3]]\n(a + b).shape: (4, 3), \na + b = \n[[2 3 4]\n [3 4 5]\n [4 5 6]\n [5 6 7]]\n</pre> <p>This is the scenario in the dense layer you built above. Adding a 1-D vector $b$ to a (m,j) matrix.</p> Matrix + 1-D Vector In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part1/Assignment/#practice-lab-neural-networks-for-handwritten-digit-recognition-binary","title":"Practice Lab: Neural Networks for Handwritten Digit Recognition, Binary\u00b6","text":"<p>In this exercise, you will use a neural network to recognize the hand-written digits zero and one.</p>"},{"location":"DeepLearning/part1/Assignment/#outline","title":"Outline\u00b6","text":"<ul> <li> 1 - Packages </li> <li> 2 - Neural Networks<ul> <li> 2.1 Problem Statement</li> <li> 2.2 Dataset</li> <li> 2.3 Model representation</li> <li> 2.4 Tensorflow Model Implementation<ul> <li> Exercise 1</li> </ul> </li> <li> 2.5 NumPy Model Implementation (Forward Prop in NumPy)<ul> <li> Exercise 2</li> </ul> </li> <li> 2.6 Vectorized NumPy Model Implementation (Optional)<ul> <li> Exercise 3</li> </ul> </li> <li> 2.7 Congratulations!</li> <li> 2.8 NumPy Broadcasting Tutorial (Optional)</li> </ul> </li> </ul>"},{"location":"DeepLearning/part1/Assignment/#1-packages","title":"1 - Packages\u00b6","text":"<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p> <ul> <li>numpy is the fundamental package for scientific computing with Python.</li> <li>matplotlib is a popular library to plot graphs in Python.</li> <li>tensorflow a popular platform for machine learning.</li> </ul>"},{"location":"DeepLearning/part1/Assignment/#2-neural-networks","title":"2 - Neural Networks\u00b6","text":"<p>In Course 1, you implemented logistic regression. This was extended to handle non-linear boundaries using polynomial regression. For even more complex scenarios such as image recognition, neural networks are preferred.</p> <p></p>"},{"location":"DeepLearning/part1/Assignment/#21-problem-statement","title":"2.1 Problem Statement\u00b6","text":"<p>In this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.</p> <p>This exercise will show you how the methods you have learned can be used for this classification task.</p> <p></p>"},{"location":"DeepLearning/part1/Assignment/#22-dataset","title":"2.2 Dataset\u00b6","text":"<p>You will start by loading the dataset for this task.</p> <ul> <li><p>The <code>load_data()</code> function shown below loads the data into variables <code>X</code> and <code>y</code></p> </li> <li><p>The data set contains 1000 training examples of handwritten digits $^1$, here limited to zero and one.</p> <ul> <li>Each training example is a 20-pixel x 20-pixel grayscale image of the digit.<ul> <li>Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.</li> <li>The 20 by 20 grid of pixels is \u201cunrolled\u201d into a 400-dimensional vector.</li> <li>Each training example becomes a single row in our data matrix <code>X</code>.</li> <li>This gives us a 1000 x 400 matrix <code>X</code> where every row is a training example of a handwritten digit image.</li> </ul> </li> </ul> </li> </ul> <p>$$X =  \\left(\\begin{array}{cc}  --- (x^{(1)}) --- \\\\ --- (x^{(2)}) --- \\\\ \\vdots \\\\  --- (x^{(m)}) ---  \\end{array}\\right)$$</p> <ul> <li>The second part of the training set is a 1000 x 1 dimensional vector <code>y</code> that contains labels for the training set<ul> <li><code>y = 0</code> if the image is of the digit <code>0</code>, <code>y = 1</code> if the image is of the digit <code>1</code>.</li> </ul> </li> </ul> <p>$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub></p>"},{"location":"DeepLearning/part1/Assignment/#221-view-the-variables","title":"2.2.1 View the variables\u00b6","text":"<p>Let's get more familiar with your dataset.</p> <ul> <li>A good place to start is to print out each variable and see what it contains.</li> </ul> <p>The code below prints elements of the variables <code>X</code> and <code>y</code>.</p>"},{"location":"DeepLearning/part1/Assignment/#222-check-the-dimensions-of-your-variables","title":"2.2.2 Check the dimensions of your variables\u00b6","text":"<p>Another way to get familiar with your data is to view its dimensions. Please print the shape of <code>X</code> and <code>y</code> and see how many training examples you have in your dataset.</p>"},{"location":"DeepLearning/part1/Assignment/#223-visualizing-the-data","title":"2.2.3 Visualizing the Data\u00b6","text":"<p>You will begin by visualizing a subset of the training set.</p> <ul> <li>In the cell below, the code randomly selects 64 rows from <code>X</code>, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.</li> <li>The label for each image is displayed above the image</li> </ul>"},{"location":"DeepLearning/part1/Assignment/#23-model-representation","title":"2.3 Model representation\u00b6","text":"<p>The neural network you will use in this assignment is shown in the figure below.</p> <ul> <li>This has three dense layers with sigmoid activations.<ul> <li>Recall that our inputs are pixel values of digit images.</li> <li>Since the images are of size $20\\times20$, this gives us $400$ inputs</li> </ul> </li> </ul>"},{"location":"DeepLearning/part1/Assignment/#24-tensorflow-model-implementation","title":"2.4 Tensorflow Model Implementation\u00b6","text":""},{"location":"DeepLearning/part1/Assignment/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Below, using Keras Sequential model and Dense Layer with a sigmoid activation to construct the network described above.</p>"},{"location":"DeepLearning/part1/Assignment/#25-numpy-model-implementation-forward-prop-in-numpy","title":"2.5 NumPy Model Implementation (Forward Prop in NumPy)\u00b6","text":"<p>As described in lecture, it is possible to build your own dense layer using NumPy. This can then be utilized to build a multi-layer neural network.</p>"},{"location":"DeepLearning/part1/Assignment/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Below, build a dense layer subroutine. The example in lecture utilized a for loop to visit each unit (<code>j</code>) in the layer and perform the dot product of the weights for that unit (<code>W[:,j]</code>) and sum the bias for the unit (<code>b[j]</code>) to form <code>z</code>. An activation function <code>g(z)</code> is then applied to that result. This section will not utilize some of the matrix operations described in the optional lectures. These will be explored in a later section.</p>"},{"location":"DeepLearning/part1/Assignment/#26-vectorized-numpy-model-implementation-optional","title":"2.6 Vectorized NumPy Model Implementation (Optional)\u00b6","text":"<p>The optional lectures described vector and matrix operations that can be used to speed the calculations. Below describes a layer operation that computes the output for all units in a layer on a given input example:</p> <p>We can demonstrate this using the examples <code>X</code> and the <code>W1</code>,<code>b1</code> parameters above. We use <code>np.matmul</code> to perform the matrix multiply. Note, the dimensions of x and W must be compatible as shown in the diagram above.</p>"},{"location":"DeepLearning/part1/Assignment/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Below, compose a new <code>my_dense_v</code> subroutine that performs the layer calculations for a matrix of examples. This will utilize <code>np.matmul()</code>.</p>"},{"location":"DeepLearning/part1/Assignment/#27-congratulations","title":"2.7 Congratulations!\u00b6","text":"<p>You have successfully built and utilized a neural network.</p>"},{"location":"DeepLearning/part1/Assignment/#28-numpy-broadcasting-tutorial-optional","title":"2.8 NumPy Broadcasting Tutorial (Optional)\u00b6","text":""},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/","title":"Optional Lab - Simple Neural Network","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom lab_utils_common import dlc, sigmoid\nfrom lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from lab_utils_common import dlc, sigmoid from lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) In\u00a0[2]: Copied! <pre>X,Y = load_coffee_data();\nprint(X.shape, Y.shape)\n</pre> X,Y = load_coffee_data(); print(X.shape, Y.shape) <pre>(200, 2) (200, 1)\n</pre> <p>Let's plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. Coffee Roasting at Home suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius. Of course, as the temperature rises, the duration should shrink.</p> In\u00a0[3]: Copied! <pre>plt_roast(X,Y)\n</pre> plt_roast(X,Y) In\u00a0[4]: Copied! <pre>print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\nprint(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\nnorm_l = tf.keras.layers.Normalization(axis=-1)\nnorm_l.adapt(X)  # learns mean, variance\nXn = norm_l(X)\nprint(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\nprint(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")\n</pre> print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\") print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\") norm_l = tf.keras.layers.Normalization(axis=-1) norm_l.adapt(X)  # learns mean, variance Xn = norm_l(X) print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\") print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\") <pre>Temperature Max, Min pre normalization: 284.99, 151.32\nDuration    Max, Min pre normalization: 15.45, 11.51\nTemperature Max, Min post normalization: 1.66, -1.69\nDuration    Max, Min post normalization: 1.79, -1.70\n</pre> <p>As described in lecture, it is possible to build your own dense layer using NumPy. This can then be utilized to build a multi-layer neural network.</p> <p>In the first optional lab, you constructed a neuron in NumPy and in Tensorflow and noted their similarity. A layer simply contains multiple neurons/units. As described in lecture, one can utilize a for loop to visit each unit (<code>j</code>) in the layer and perform the dot product of the weights for that unit (<code>W[:,j]</code>) and sum the bias for the unit (<code>b[j]</code>) to form <code>z</code>. An activation function <code>g(z)</code> can then be applied to that result. Let's try that below to build a \"dense layer\" subroutine.</p> In\u00a0[5]: Copied! <pre>def my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units|\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):               \n        w = W[:,j]                                    \n        z = np.dot(w, a_in) + b[j]         \n        a_out[j] = g(z)               \n    return(a_out)\n</pre> def my_dense(a_in, W, b, g):     \"\"\"     Computes dense layer     Args:       a_in (ndarray (n, )) : Data, 1 example        W    (ndarray (n,j)) : Weight matrix, n features per unit, j units       b    (ndarray (j, )) : bias vector, j units         g    activation function (e.g. sigmoid, relu..)     Returns       a_out (ndarray (j,))  : j units|     \"\"\"     units = W.shape[1]     a_out = np.zeros(units)     for j in range(units):                        w = W[:,j]                                             z = np.dot(w, a_in) + b[j]                  a_out[j] = g(z)                    return(a_out) <p>The following cell builds a two-layer neural network utilizing the <code>my_dense</code> subroutine above.</p> In\u00a0[6]: Copied! <pre>def my_sequential(x, W1, b1, W2, b2):\n    a1 = my_dense(x,  W1, b1, sigmoid)\n    a2 = my_dense(a1, W2, b2, sigmoid)\n    return(a2)\n</pre> def my_sequential(x, W1, b1, W2, b2):     a1 = my_dense(x,  W1, b1, sigmoid)     a2 = my_dense(a1, W2, b2, sigmoid)     return(a2) <p>We can copy trained weights and biases from the previous lab in Tensorflow.</p> In\u00a0[7]: Copied! <pre>W1_tmp = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] )\nb1_tmp = np.array( [-9.82, -9.28,  0.96] )\nW2_tmp = np.array( [[-31.18], [-27.59], [-32.56]] )\nb2_tmp = np.array( [15.41] )\n</pre> W1_tmp = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] ) b1_tmp = np.array( [-9.82, -9.28,  0.96] ) W2_tmp = np.array( [[-31.18], [-27.59], [-32.56]] ) b2_tmp = np.array( [15.41] ) <p>Let's start by writing a routine similar to Tensorflow's <code>model.predict()</code>. This will take a matrix $X$ with all $m$ examples in the rows and make a prediction by running the model.</p> In\u00a0[8]: Copied! <pre>def my_predict(X, W1, b1, W2, b2):\n    m = X.shape[0]\n    p = np.zeros((m,1))\n    for i in range(m):\n        p[i,0] = my_sequential(X[i], W1, b1, W2, b2)\n    return(p)\n</pre> def my_predict(X, W1, b1, W2, b2):     m = X.shape[0]     p = np.zeros((m,1))     for i in range(m):         p[i,0] = my_sequential(X[i], W1, b1, W2, b2)     return(p) <p>We can try this routine on two examples:</p> In\u00a0[9]: Copied! <pre>X_tst = np.array([\n    [200,13.9],  # postive example\n    [200,17]])   # negative example\nX_tstn = norm_l(X_tst)  # remember to normalize\npredictions = my_predict(X_tstn, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\n</pre> X_tst = np.array([     [200,13.9],  # postive example     [200,17]])   # negative example X_tstn = norm_l(X_tst)  # remember to normalize predictions = my_predict(X_tstn, W1_tmp, b1_tmp, W2_tmp, b2_tmp) <pre>C:\\Users\\\u7814Ma\\AppData\\Local\\Temp\\ipykernel_27012\\3168763570.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p[i,0] = my_sequential(X[i], W1, b1, W2, b2)\n</pre> <p>To convert the probabilities to a decision, we apply a threshold:</p> In\u00a0[10]: Copied! <pre>yhat = np.zeros_like(predictions)\nfor i in range(len(predictions)):\n    if predictions[i] &gt;= 0.5:\n        yhat[i] = 1\n    else:\n        yhat[i] = 0\nprint(f\"decisions = \\n{yhat}\")\n</pre> yhat = np.zeros_like(predictions) for i in range(len(predictions)):     if predictions[i] &gt;= 0.5:         yhat[i] = 1     else:         yhat[i] = 0 print(f\"decisions = \\n{yhat}\") <pre>decisions = \n[[1.]\n [0.]]\n</pre> <p>This can be accomplished more succinctly:</p> In\u00a0[11]: Copied! <pre>yhat = (predictions &gt;= 0.5).astype(int)\nprint(f\"decisions = \\n{yhat}\")\n</pre> yhat = (predictions &gt;= 0.5).astype(int) print(f\"decisions = \\n{yhat}\") <pre>decisions = \n[[1]\n [0]]\n</pre> <p>This graph shows the operation of the whole network and is identical to the Tensorflow result from the previous lab. The left graph is the raw output of the final layer represented by the blue shading. This is overlaid on the training data represented by the X's and O's. The right graph is the output of the network after a decision threshold. The X's and O's here correspond to decisions made by the network.</p> In\u00a0[12]: Copied! <pre>netf= lambda x : my_predict(norm_l(x),W1_tmp, b1_tmp, W2_tmp, b2_tmp)\nplt_network(X,Y,netf)\n</pre> netf= lambda x : my_predict(norm_l(x),W1_tmp, b1_tmp, W2_tmp, b2_tmp) plt_network(X,Y,netf) <pre>C:\\Users\\\u7814Ma\\AppData\\Local\\Temp\\ipykernel_27012\\3168763570.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  p[i,0] = my_sequential(X[i], W1, b1, W2, b2)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#optional-lab-simple-neural-network","title":"Optional Lab - Simple Neural Network\u00b6","text":"<p>In this lab, we will build a small neural network using Numpy. It will be the same \"coffee roasting\" network you implemented in Tensorflow.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#dataset","title":"DataSet\u00b6","text":"<p>This is the same data set as the previous lab.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#normalize-data","title":"Normalize Data\u00b6","text":"<p>To match the previous lab, we'll normalize the data. Refer to that lab for more details</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#numpy-model-forward-prop-in-numpy","title":"Numpy Model (Forward Prop in NumPy)\u00b6","text":"Let's build the \"Coffee Roasting Network\" described in lecture. There are two layers with sigmoid activations."},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#predictions","title":"Predictions\u00b6","text":"<p>Once you have a trained model, you can then use it to make predictions. Recall that the output of our model is a probability. In this case, the probability of a good roast. To make a decision, one must apply the probability to a threshold. In this case, we will use 0.5</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#network-function","title":"Network function\u00b6","text":""},{"location":"DeepLearning/part1/CoffeeRoasting_Numpy/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have built a small neural network in NumPy. Hopefully this lab revealed the fairly simple and familiar functions which make up a layer in a neural network.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/","title":"Optional Lab - Simple Neural Network","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom lab_utils_common import dlc\nfrom lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from lab_utils_common import dlc from lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0)  In\u00a0[3]: Copied! <pre>X,Y = load_coffee_data();\nprint(X.shape, Y.shape)\n</pre> X,Y = load_coffee_data(); print(X.shape, Y.shape) <pre>(200, 2) (200, 1)\n</pre> <p>Let's plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. Coffee Roasting at Home suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius. Of course, as temperature rises, the duration should shrink.</p> In\u00a0[4]: Copied! <pre>plt_roast(X,Y)\n</pre> plt_roast(X,Y) In\u00a0[5]: Copied! <pre>print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\nprint(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\nnorm_l = tf.keras.layers.Normalization(axis=-1)\nnorm_l.adapt(X)  # learns mean, variance\nXn = norm_l(X)\nprint(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\nprint(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")\n</pre> print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\") print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\") norm_l = tf.keras.layers.Normalization(axis=-1) norm_l.adapt(X)  # learns mean, variance Xn = norm_l(X) print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\") print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\") <pre>Temperature Max, Min pre normalization: 284.99, 151.32\nDuration    Max, Min pre normalization: 15.45, 11.51\nTemperature Max, Min post normalization: 1.66, -1.69\nDuration    Max, Min post normalization: 1.79, -1.70\n</pre> <p>Tile/copy our data to increase the training set size and reduce the number of training epochs.</p> In\u00a0[6]: Copied! <pre>Xt = np.tile(Xn,(1000,1))\nYt= np.tile(Y,(1000,1))   \nprint(Xt.shape, Yt.shape)   \n</pre> Xt = np.tile(Xn,(1000,1)) Yt= np.tile(Y,(1000,1))    print(Xt.shape, Yt.shape)    <pre>(200000, 2) (200000, 1)\n</pre> In\u00a0[9]: Copied! <pre>tf.random.set_seed(1234)  # applied to achieve consistent results\nmodel = Sequential(\n    [\n        tf.keras.Input(shape=(2,)),\n        Dense(3, activation='sigmoid', name = 'layer1'),\n        Dense(1, activation='sigmoid', name = 'layer2')\n     ]\n)\n</pre> tf.random.set_seed(1234)  # applied to achieve consistent results model = Sequential(     [         tf.keras.Input(shape=(2,)),         Dense(3, activation='sigmoid', name = 'layer1'),         Dense(1, activation='sigmoid', name = 'layer2')      ] ) <p>Note 1: The <code>tf.keras.Input(shape=(2,)),</code> specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the <code>model.fit</code> statement. Note 2: Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.</p> <p>The <code>model.summary()</code> provides a description of the network:</p> In\u00a0[10]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"sequential_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 layer1 (Dense)                       \u2502 (None, 3)                   \u2502               9 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 layer2 (Dense)                       \u2502 (None, 1)                   \u2502               4 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 13 (52.00 B)\n</pre> <pre> Trainable params: 13 (52.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays as shown below.</p> In\u00a0[11]: Copied! <pre>L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters\nL2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters\nprint(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  )\n</pre> L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  ) <pre>L1 params =  9 , L2 params =  4\n</pre> <p>Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:</p> <ul> <li>In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.</li> <li>In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element.</li> </ul> In\u00a0[12]: Copied! <pre>W1, b1 = model.get_layer(\"layer1\").get_weights()\nW2, b2 = model.get_layer(\"layer2\").get_weights()\nprint(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\nprint(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)\n</pre> W1, b1 = model.get_layer(\"layer1\").get_weights() W2, b2 = model.get_layer(\"layer2\").get_weights() print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1) print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2) <pre>W1(2, 3):\n [[-1.1   0.83  0.11]\n [ 0.38 -0.59 -0.93]] \nb1(3,): [0. 0. 0.]\nW2(3, 1):\n [[-0.13]\n [-0.15]\n [-0.04]] \nb2(1,): [0.]\n</pre> <p>The following statements will be described in detail in Week2. For now:</p> <ul> <li>The <code>model.compile</code> statement defines a loss function and specifies a compile optimization.</li> <li>The <code>model.fit</code> statement runs gradient descent and fits the weights to the data.</li> </ul> In\u00a0[13]: Copied! <pre>model.compile(\n    loss = tf.keras.losses.BinaryCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n)\n\nmodel.fit(\n    Xt,Yt,            \n    epochs=10,\n)\n</pre> model.compile(     loss = tf.keras.losses.BinaryCrossentropy(),     optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), )  model.fit(     Xt,Yt,                 epochs=10, ) <pre>Epoch 1/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 736us/step - loss: 0.2273\nEpoch 2/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 720us/step - loss: 0.1179\nEpoch 3/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 714us/step - loss: 0.1089\nEpoch 4/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 715us/step - loss: 0.0326\nEpoch 5/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 711us/step - loss: 0.0139\nEpoch 6/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 730us/step - loss: 0.0094\nEpoch 7/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 742us/step - loss: 0.0066\nEpoch 8/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 733us/step - loss: 0.0047\nEpoch 9/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 729us/step - loss: 0.0034\nEpoch 10/10\n6250/6250 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 715us/step - loss: 0.0025\n</pre> Out[13]: <pre>&lt;keras.src.callbacks.history.History at 0x25d0fab3310&gt;</pre> In\u00a0[14]: Copied! <pre>W1, b1 = model.get_layer(\"layer1\").get_weights()\nW2, b2 = model.get_layer(\"layer2\").get_weights()\nprint(\"W1:\\n\", W1, \"\\nb1:\", b1)\nprint(\"W2:\\n\", W2, \"\\nb2:\", b2)\n</pre> W1, b1 = model.get_layer(\"layer1\").get_weights() W2, b2 = model.get_layer(\"layer2\").get_weights() print(\"W1:\\n\", W1, \"\\nb1:\", b1) print(\"W2:\\n\", W2, \"\\nb2:\", b2) <pre>W1:\n [[-10.54  -0.06  14.89]\n [ -0.19  -9.11  12.43]] \nb1: [-11.49 -11.4    1.93]\nW2:\n [[-46.52]\n [-43.18]\n [-39.23]] \nb2: [24.66]\n</pre> <p>Next, we will load some saved weights from a previous training run. This is so that this notebook remains robust to changes in Tensorflow over time. Different training runs can produce somewhat different results and the discussion below applies to a particular solution. Feel free to re-run the notebook with this cell commented out to see the difference.</p> In\u00a0[20]: Copied! <pre>W1 = np.array(\n    [[-10.54,-0.06,14.89],\n [ -0.19,-9.11,12.43]]  )\nb1 = np.array([-11.49,-11.4 , 1.93])\nW2 = np.array([\n    [-46.52],\n [-43.18],\n [-39.23]])\nb2 = np.array([24.66])\nmodel.get_layer(\"layer1\").set_weights([W1,b1])\nmodel.get_layer(\"layer2\").set_weights([W2,b2])\n</pre> W1 = np.array(     [[-10.54,-0.06,14.89],  [ -0.19,-9.11,12.43]]  ) b1 = np.array([-11.49,-11.4 , 1.93]) W2 = np.array([     [-46.52],  [-43.18],  [-39.23]]) b2 = np.array([24.66]) model.get_layer(\"layer1\").set_weights([W1,b1]) model.get_layer(\"layer2\").set_weights([W2,b2]) <p>Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples. Recall, we have normalized the input features so we must normalize our test data as well. To make a prediction, you apply the <code>predict</code> method.</p> In\u00a0[21]: Copied! <pre>X_test = np.array([\n    [200,13.9],  # postive example\n    [200,17]])   # negative example\nX_testn = norm_l(X_test)\npredictions = model.predict(X_testn)\nprint(\"predictions = \\n\", predictions)\n</pre> X_test = np.array([     [200,13.9],  # postive example     [200,17]])   # negative example X_testn = norm_l(X_test) predictions = model.predict(X_testn) print(\"predictions = \\n\", predictions) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step\npredictions = \n [[9.73e-01]\n [4.54e-07]]\n</pre> <p>To convert the probabilities to a decision, we apply a threshold:</p> In\u00a0[22]: Copied! <pre>yhat = np.zeros_like(predictions)\nfor i in range(len(predictions)):\n    if predictions[i] &gt;= 0.5:\n        yhat[i] = 1\n    else:\n        yhat[i] = 0\nprint(f\"decisions = \\n{yhat}\")\n</pre> yhat = np.zeros_like(predictions) for i in range(len(predictions)):     if predictions[i] &gt;= 0.5:         yhat[i] = 1     else:         yhat[i] = 0 print(f\"decisions = \\n{yhat}\") <pre>decisions = \n[[1.]\n [0.]]\n</pre> <p>This can be accomplished more succinctly:</p> In\u00a0[23]: Copied! <pre>yhat = (predictions &gt;= 0.5).astype(int)\nprint(f\"decisions = \\n{yhat}\")\n</pre> yhat = (predictions &gt;= 0.5).astype(int) print(f\"decisions = \\n{yhat}\") <pre>decisions = \n[[1]\n [0]]\n</pre> In\u00a0[25]: Copied! <pre>plt_layer(X,Y.reshape(-1,),W1,b1,norm_l)\n</pre> plt_layer(X,Y.reshape(-1,),W1,b1,norm_l) <p>The shading shows that each unit is responsible for a different \"bad roast\" region. unit 0 has larger values when the temperature is too low. unit 1 has larger values when the duration is too short and unit 2 has larger values for bad combinations of time/temp. It is worth noting that the network learned these functions on its own through the process of gradient descent. They are very much the same sort of functions a person might choose to make the same decisions.</p> <p>The function plot of the final layer is a bit more difficult to visualize. It's inputs are the output of the first layer. We know that the first layer uses sigmoids so their output range is between zero and one. We can create a 3-D plot that calculates the output for all possible combinations of the three inputs. This is shown below. Above, high output values correspond to 'bad roast' area's. Below, the maximum output is in area's where the three inputs are small values corresponding to 'good roast' area's.</p> In\u00a0[26]: Copied! <pre>plt_output_unit(W2,b2)\n</pre> plt_output_unit(W2,b2) <p>The final graph shows the whole network in action. The left graph is the raw output of the final layer represented by the blue shading. This is overlaid on the training data represented by the X's and O's. The right graph is the output of the network after a decision threshold. The X's and O's here correspond to decisions made by the network. The following takes a moment to run</p> In\u00a0[27]: Copied! <pre>netf= lambda x : model.predict(norm_l(x))\nplt_network(X,Y,netf)\n</pre> netf= lambda x : model.predict(norm_l(x)) plt_network(X,Y,netf) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 35ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 17ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 16ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n</pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n7/7 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#optional-lab-simple-neural-network","title":"Optional Lab - Simple Neural Network\u00b6","text":"<p>In this lab we will build a small neural network using Tensorflow.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#dataset","title":"DataSet\u00b6","text":""},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#normalize-data","title":"Normalize Data\u00b6","text":"<p>Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. The procedure below uses a Keras normalization layer. It has the following steps:</p> <ul> <li>create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.</li> <li>'adapt' the data. This learns the mean and variance of the data set and saves the values internally.</li> <li>normalize the data. It is important to apply normalization to any future data that utilizes the learned model.</li> </ul>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#tensorflow-model","title":"Tensorflow Model\u00b6","text":""},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#model","title":"Model\u00b6","text":"Let's build the \"Coffee Roasting Network\" described in lecture. There are two layers with sigmoid activations as shown below:"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#updated-weights","title":"Updated Weights\u00b6","text":"<p>After fitting, the weights have been updated:</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#predictions","title":"Predictions\u00b6","text":"<p>Once you have a trained model, you can then use it to make predictions. Recall that the output of our model is a probability. In this case, the probability of a good roast. To make a decision, one must apply the probability to a threshold. In this case, we will use 0.5</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#epochs-and-batches","title":"Epochs and batches\u00b6","text":"<p>In the <code>compile</code> statement above, the number of <code>epochs</code> was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:</p> <pre><code>Epoch 1/10\n6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n</code></pre> <p>The first line, <code>Epoch 1/10</code>, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line <code>6250/6250 [====</code> is describing which batch has been executed.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#layer-functions","title":"Layer Functions\u00b6","text":"<p>Let's examine the functions of the units to determine their role in the coffee roasting decision. We will plot the output of each node for all values of the inputs (duration,temp). Each unit is a logistic function whose output can range from zero to one. The shading in the graph represents the output value.</p> <p>Note: In labs we typically number things starting at zero while the lectures may start with 1.</p>"},{"location":"DeepLearning/part1/CoffeeRoasting_TF/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have built a small neural network in Tensorflow. The network demonstrated the ability of neural networks to handle complex decisions by dividing the decisions between multiple units.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/","title":"Optional Lab - Neurons and Layers","text":"In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom tensorflow.keras.activations import sigmoid\nfrom lab_utils_common import dlc\nfrom lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic\nplt.style.use('./deeplearning.mplstyle')\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras import Sequential from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy from tensorflow.keras.activations import sigmoid from lab_utils_common import dlc from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic plt.style.use('./deeplearning.mplstyle') import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) In\u00a0[4]: Copied! <pre>X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet)\nY_train = np.array([[300.0], [500.0]], dtype=np.float32)       #(price in 1000s of dollars)\n\nfig, ax = plt.subplots(1,1)\nax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\nax.legend( fontsize='xx-large')\nax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\nax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')\nplt.show()\n</pre> X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet) Y_train = np.array([[300.0], [500.0]], dtype=np.float32)       #(price in 1000s of dollars)  fig, ax = plt.subplots(1,1) ax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\") ax.legend( fontsize='xx-large') ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large') ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large') plt.show() <p>We can define a layer with one neuron or unit and compare it to the familiar linear regression function.</p> In\u00a0[5]: Copied! <pre>linear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', )\n</pre> linear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', ) <p>Let's examine the weights.</p> In\u00a0[6]: Copied! <pre>linear_layer.get_weights()\n</pre> linear_layer.get_weights() Out[6]: <pre>[]</pre> <p>There are no weights as the weights are not yet instantiated. Let's try the model on one example in <code>X_train</code>. This will trigger the instantiation of the weights. Note, the input to the layer must be 2-D, so we'll reshape it.</p> In\u00a0[8]: Copied! <pre>a1 = linear_layer(X_train[0].reshape(1,1))\nprint(a1)\n</pre> a1 = linear_layer(X_train[0].reshape(1,1)) print(a1) <pre>tf.Tensor([[1.29]], shape=(1, 1), dtype=float32)\n</pre> <p>The result is a tensor (another name for an array) with a shape of (1,1) or one entry. Now let's look at the weights and bias. These weights are randomly initialized to small numbers and the bias defaults to being initialized to zero.</p> In\u00a0[9]: Copied! <pre>w, b= linear_layer.get_weights()\nprint(f\"w = {w}, b={b}\")\n</pre> w, b= linear_layer.get_weights() print(f\"w = {w}, b={b}\") <pre>w = [[1.29]], b=[0.]\n</pre> <p>A linear regression model (1) with a single input feature will have a single weight and bias. This matches the dimensions of our <code>linear_layer</code> above.</p> <p>The weights are initialized to random values so let's set them to some known values.</p> In\u00a0[10]: Copied! <pre>set_w = np.array([[200]])\nset_b = np.array([100])\n\n# set_weights takes a list of numpy arrays\nlinear_layer.set_weights([set_w, set_b])\nprint(linear_layer.get_weights())\n</pre> set_w = np.array([[200]]) set_b = np.array([100])  # set_weights takes a list of numpy arrays linear_layer.set_weights([set_w, set_b]) print(linear_layer.get_weights()) <pre>[array([[200.]], dtype=float32), array([100.], dtype=float32)]\n</pre> <p>Let's compare equation (1) to the layer output.</p> In\u00a0[11]: Copied! <pre>a1 = linear_layer(X_train[0].reshape(1,1))\nprint(a1)\nalin = np.dot(set_w,X_train[0].reshape(1,1)) + set_b\nprint(alin)\n</pre> a1 = linear_layer(X_train[0].reshape(1,1)) print(a1) alin = np.dot(set_w,X_train[0].reshape(1,1)) + set_b print(alin) <pre>tf.Tensor([[300.]], shape=(1, 1), dtype=float32)\n[[300.]]\n</pre> <p>They produce the same values! Now, we can use our linear layer to make predictions on our training data.</p> In\u00a0[12]: Copied! <pre>prediction_tf = linear_layer(X_train)\nprediction_np = np.dot( X_train, set_w) + set_b\n</pre> prediction_tf = linear_layer(X_train) prediction_np = np.dot( X_train, set_w) + set_b In\u00a0[13]: Copied! <pre>plt_linear(X_train, Y_train, prediction_tf, prediction_np)\n</pre> plt_linear(X_train, Y_train, prediction_tf, prediction_np) In\u00a0[14]: Copied! <pre>X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\nY_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n</pre> X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix In\u00a0[15]: Copied! <pre>pos = Y_train == 1\nneg = Y_train == 0\nX_train[pos]\n</pre> pos = Y_train == 1 neg = Y_train == 0 X_train[pos] Out[15]: <pre>array([3., 4., 5.], dtype=float32)</pre> In\u00a0[16]: Copied! <pre>pos = Y_train == 1\nneg = Y_train == 0\n\nfig,ax = plt.subplots(1,1,figsize=(4,3))\nax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\nax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n              edgecolors=dlc[\"dlblue\"],lw=3)\n\nax.set_ylim(-0.08,1.1)\nax.set_ylabel('y', fontsize=12)\nax.set_xlabel('x', fontsize=12)\nax.set_title('one variable plot')\nax.legend(fontsize=12)\nplt.show()\n</pre> pos = Y_train == 1 neg = Y_train == 0  fig,ax = plt.subplots(1,1,figsize=(4,3)) ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\") ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',                edgecolors=dlc[\"dlblue\"],lw=3)  ax.set_ylim(-0.08,1.1) ax.set_ylabel('y', fontsize=12) ax.set_xlabel('x', fontsize=12) ax.set_title('one variable plot') ax.legend(fontsize=12) plt.show() In\u00a0[18]: Copied! <pre>model = Sequential(\n    [\n        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n    ]\n)\n</pre> model = Sequential(     [         tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')     ] ) <p><code>model.summary()</code> shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.</p> In\u00a0[19]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"sequential_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 L1 (Dense)                           \u2502 (None, 1)                   \u2502               2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 2 (8.00 B)\n</pre> <pre> Trainable params: 2 (8.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[22]: Copied! <pre>logistic_layer = model.get_layer('L1')\nw,b = logistic_layer.get_weights()\nprint(w,b)\nprint(w.shape,b.shape)\n</pre> logistic_layer = model.get_layer('L1') w,b = logistic_layer.get_weights() print(w,b) print(w.shape,b.shape) <pre>[[0.38]] [0.]\n(1, 1) (1,)\n</pre> <p>Let's set the weight and bias to some known values.</p> In\u00a0[23]: Copied! <pre>set_w = np.array([[2]])\nset_b = np.array([-4.5])\n# set_weights takes a list of numpy arrays\nlogistic_layer.set_weights([set_w, set_b])\nprint(logistic_layer.get_weights())\n</pre> set_w = np.array([[2]]) set_b = np.array([-4.5]) # set_weights takes a list of numpy arrays logistic_layer.set_weights([set_w, set_b]) print(logistic_layer.get_weights()) <pre>[array([[2.]], dtype=float32), array([-4.5], dtype=float32)]\n</pre> <p>Let's compare equation (2) to the layer output.</p> In\u00a0[25]: Copied! <pre>a1 = model.predict(X_train[0].reshape(1,1))\nprint(a1)\nalog = sigmoidnp(np.dot(set_w,X_train[0].reshape(1,1)) + set_b)\nprint(alog)\n</pre> a1 = model.predict(X_train[0].reshape(1,1)) print(a1) alog = sigmoidnp(np.dot(set_w,X_train[0].reshape(1,1)) + set_b) print(alog) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n[[0.01]]\n[[0.01]]\n</pre> <p>They produce the same values! Now, we can use our logistic layer and NumPy model to make predictions on our training data.</p> In\u00a0[26]: Copied! <pre>plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg)\n</pre> plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg) <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 27ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 18ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 23ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 24ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 19ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 22ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 20ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 21ms/step\n</pre> <p>The shading above reflects the output of the sigmoid which varies from 0 to 1.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#optional-lab-neurons-and-layers","title":"Optional Lab - Neurons and Layers\u00b6","text":"<p>In this lab we will explore the inner workings of neurons/units and layers. In particular, the lab will draw parallels to the models you have mastered in Course 1, the regression/linear model and the logistic model. The lab will introduce Tensorflow and demonstrate how these models are implemented in that framework.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#packages","title":"Packages\u00b6","text":"<p>Tensorflow and Keras Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by Fran\u00e7ois Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#neuron-without-activation-regressionlinear-model","title":"Neuron without activation - Regression/Linear Model\u00b6","text":""},{"location":"DeepLearning/part1/Neurons_and_Layers/#dataset","title":"DataSet\u00b6","text":"<p>We'll use an example from Course 1, linear regression on house prices.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#regressionlinear-model","title":"Regression/Linear Model\u00b6","text":"<p>The function implemented by a neuron with no activation is the same as in Course 1, linear regression: $$ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b \\tag{1}$$</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#neuron-with-sigmoid-activation","title":"Neuron with Sigmoid activation\u00b6","text":"<p>The function implemented by a neuron/unit with a sigmoid activation is the same as in Course 1, logistic  regression: $$ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\tag{2}$$ where $$g(x) = sigmoid(x)$$</p> <p>Let's set $w$ and $b$ to some known values and check the model.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#dataset","title":"DataSet\u00b6","text":"<p>We'll use an example from Course 1, logistic regression.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#logistic-neuron","title":"Logistic Neuron\u00b6","text":"<p>We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (2) above. This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The Sequential model is a convenient means of constructing these models.</p>"},{"location":"DeepLearning/part1/Neurons_and_Layers/#congratulations","title":"Congratulations!\u00b6","text":"<p>You built a very simple neural network and have explored the similarities of a neuron to the linear and logistic regression from Course 1.</p>"},{"location":"DeepLearning/part1/autils/","title":"Autils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def load_data():\n    X = np.load(\"data/X.npy\")\n    y = np.load(\"data/y.npy\")\n    X = X[0:1000]\n    y = y[0:1000]\n    return X, y\n</pre> def load_data():     X = np.load(\"data/X.npy\")     y = np.load(\"data/y.npy\")     X = X[0:1000]     y = y[0:1000]     return X, y In\u00a0[\u00a0]: Copied! <pre>def load_weights():\n    w1 = np.load(\"data/w1.npy\")\n    b1 = np.load(\"data/b1.npy\")\n    w2 = np.load(\"data/w2.npy\")\n    b2 = np.load(\"data/b2.npy\")\n    return w1, b1, w2, b2\n</pre> def load_weights():     w1 = np.load(\"data/w1.npy\")     b1 = np.load(\"data/b1.npy\")     w2 = np.load(\"data/w2.npy\")     b2 = np.load(\"data/b2.npy\")     return w1, b1, w2, b2 In\u00a0[\u00a0]: Copied! <pre>def sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n</pre> def sigmoid(x):     return 1. / (1. + np.exp(-x))"},{"location":"DeepLearning/part1/lab_coffee_utils/","title":"Lab coffee utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.activations import sigmoid\nfrom matplotlib import cm\nimport matplotlib.colors as colors\nfrom lab_utils_common import dlc\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.activations import sigmoid from matplotlib import cm import matplotlib.colors as colors from lab_utils_common import dlc In\u00a0[\u00a0]: Copied! <pre>def load_coffee_data():\n    \"\"\" Creates a coffee roasting data set.\n        roasting duration: 12-15 minutes is best\n        temperature range: 175-260C is best\n    \"\"\"\n    rng = np.random.default_rng(2)\n    X = rng.random(400).reshape(-1,2)\n    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n    Y = np.zeros(len(X))\n    \n    i=0\n    for t,d in X:\n        y = -3/(260-175)*t + 21\n        if (t &gt; 175 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d&lt;=y ):\n            Y[i] = 1\n        else:\n            Y[i] = 0\n        i += 1\n\n    return (X, Y.reshape(-1,1))\n</pre> def load_coffee_data():     \"\"\" Creates a coffee roasting data set.         roasting duration: 12-15 minutes is best         temperature range: 175-260C is best     \"\"\"     rng = np.random.default_rng(2)     X = rng.random(400).reshape(-1,2)     X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best     X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best     Y = np.zeros(len(X))          i=0     for t,d in X:         y = -3/(260-175)*t + 21         if (t &gt; 175 and t &lt; 260 and d &gt; 12 and d &lt; 15 and d&lt;=y ):             Y[i] = 1         else:             Y[i] = 0         i += 1      return (X, Y.reshape(-1,1)) In\u00a0[\u00a0]: Copied! <pre>def plt_roast(X,Y):\n    Y = Y.reshape(-1,)\n    colormap = np.array(['r', 'b'])\n    fig, ax = plt.subplots(1,1,)\n    ax.scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )\n    ax.scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none', \n               edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")\n    tr = np.linspace(175,260,50)\n    ax.plot(tr, (-3/85) * tr + 21, color=dlc[\"dlpurple\"],linewidth=1)\n    ax.axhline(y=12,color=dlc[\"dlpurple\"],linewidth=1)\n    ax.axvline(x=175,color=dlc[\"dlpurple\"],linewidth=1)\n    ax.set_title(f\"Coffee Roasting\", size=16)\n    ax.set_xlabel(\"Temperature \\n(Celsius)\",size=12)\n    ax.set_ylabel(\"Duration \\n(minutes)\",size=12)\n    ax.legend(loc='upper right')\n    plt.show()\n</pre> def plt_roast(X,Y):     Y = Y.reshape(-1,)     colormap = np.array(['r', 'b'])     fig, ax = plt.subplots(1,1,)     ax.scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )     ax.scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none',                 edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")     tr = np.linspace(175,260,50)     ax.plot(tr, (-3/85) * tr + 21, color=dlc[\"dlpurple\"],linewidth=1)     ax.axhline(y=12,color=dlc[\"dlpurple\"],linewidth=1)     ax.axvline(x=175,color=dlc[\"dlpurple\"],linewidth=1)     ax.set_title(f\"Coffee Roasting\", size=16)     ax.set_xlabel(\"Temperature \\n(Celsius)\",size=12)     ax.set_ylabel(\"Duration \\n(minutes)\",size=12)     ax.legend(loc='upper right')     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_prob(ax,fwb):\n    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n    #setup useful ranges and common linspaces\n    x0_space  = np.linspace(150, 285 , 40)\n    x1_space  = np.linspace(11.5, 15.5 , 40)\n\n    # get probability for x0,x1 ranges\n    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n    z = np.zeros_like(tmp_x0)\n    for i in range(tmp_x0.shape[0]):\n        for j in range(tmp_x1.shape[1]):\n            x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])\n            z[i,j] = fwb(x)\n\n\n    cmap = plt.get_cmap('Blues')\n    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n    ax.figure.colorbar(pcm, ax=ax)\n</pre> def plt_prob(ax,fwb):     \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"     #setup useful ranges and common linspaces     x0_space  = np.linspace(150, 285 , 40)     x1_space  = np.linspace(11.5, 15.5 , 40)      # get probability for x0,x1 ranges     tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)     z = np.zeros_like(tmp_x0)     for i in range(tmp_x0.shape[0]):         for j in range(tmp_x1.shape[1]):             x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])             z[i,j] = fwb(x)       cmap = plt.get_cmap('Blues')     new_cmap = truncate_colormap(cmap, 0.0, 0.5)     pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,                    norm=cm.colors.Normalize(vmin=0, vmax=1),                    cmap=new_cmap, shading='nearest', alpha = 0.9)     ax.figure.colorbar(pcm, ax=ax) In\u00a0[\u00a0]: Copied! <pre>def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n    \"\"\" truncates color map \"\"\"\n    new_cmap = colors.LinearSegmentedColormap.from_list(\n        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n        cmap(np.linspace(minval, maxval, n)))\n    return new_cmap\n</pre> def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):     \"\"\" truncates color map \"\"\"     new_cmap = colors.LinearSegmentedColormap.from_list(         'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),         cmap(np.linspace(minval, maxval, n)))     return new_cmap In\u00a0[\u00a0]: Copied! <pre>def plt_layer(X,Y,W1,b1,norm_l):\n    Y = Y.reshape(-1,)\n    fig,ax = plt.subplots(1,W1.shape[1], figsize=(16,4))\n    for i in range(W1.shape[1]):\n        layerf= lambda x : sigmoid(np.dot(norm_l(x),W1[:,i]) + b1[i])\n        plt_prob(ax[i], layerf)\n        ax[i].scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )\n        ax[i].scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none', \n                   edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")\n        tr = np.linspace(175,260,50)\n        ax[i].plot(tr, (-3/85) * tr + 21, color=dlc[\"dlpurple\"],linewidth=2)\n        ax[i].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=2)\n        ax[i].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=2)\n        ax[i].set_title(f\"Layer 1, unit {i}\")\n        ax[i].set_xlabel(\"Temperature \\n(Celsius)\",size=12)\n    ax[0].set_ylabel(\"Duration \\n(minutes)\",size=12)\n    plt.show()\n</pre> def plt_layer(X,Y,W1,b1,norm_l):     Y = Y.reshape(-1,)     fig,ax = plt.subplots(1,W1.shape[1], figsize=(16,4))     for i in range(W1.shape[1]):         layerf= lambda x : sigmoid(np.dot(norm_l(x),W1[:,i]) + b1[i])         plt_prob(ax[i], layerf)         ax[i].scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )         ax[i].scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none',                     edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")         tr = np.linspace(175,260,50)         ax[i].plot(tr, (-3/85) * tr + 21, color=dlc[\"dlpurple\"],linewidth=2)         ax[i].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=2)         ax[i].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=2)         ax[i].set_title(f\"Layer 1, unit {i}\")         ax[i].set_xlabel(\"Temperature \\n(Celsius)\",size=12)     ax[0].set_ylabel(\"Duration \\n(minutes)\",size=12)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_network(X,Y,netf):\n    fig, ax = plt.subplots(1,2,figsize=(16,4))\n    Y = Y.reshape(-1,)\n    plt_prob(ax[0], netf)\n    ax[0].scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )\n    ax[0].scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none', \n                   edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")\n    ax[0].plot(X[:,0], (-3/85) * X[:,0] + 21, color=dlc[\"dlpurple\"],linewidth=1)\n    ax[0].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=1)\n    ax[0].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=1)\n    ax[0].set_xlabel(\"Temperature \\n(Celsius)\",size=12)\n    ax[0].set_ylabel(\"Duration \\n(minutes)\",size=12)\n    ax[0].legend(loc='upper right')\n    ax[0].set_title(f\"network probability\")\n\n    ax[1].plot(X[:,0], (-3/85) * X[:,0] + 21, color=dlc[\"dlpurple\"],linewidth=1)\n    ax[1].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=1)\n    ax[1].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=1)\n    fwb = netf(X)\n    yhat = (fwb &gt; 0.5).astype(int)\n    ax[1].scatter(X[yhat[:,0]==1,0],X[yhat[:,0]==1,1], s=70, marker='x', c='orange', label=\"Predicted Good Roast\" )\n    ax[1].scatter(X[yhat[:,0]==0,0],X[yhat[:,0]==0,1], s=100, marker='o', facecolors='none', \n                   edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")\n    ax[1].set_title(f\"network decision\")\n    ax[1].set_xlabel(\"Temperature \\n(Celsius)\",size=12)\n    ax[1].set_ylabel(\"Duration \\n(minutes)\",size=12)\n    ax[1].legend(loc='upper right')\n</pre> def plt_network(X,Y,netf):     fig, ax = plt.subplots(1,2,figsize=(16,4))     Y = Y.reshape(-1,)     plt_prob(ax[0], netf)     ax[0].scatter(X[Y==1,0],X[Y==1,1], s=70, marker='x', c='red', label=\"Good Roast\" )     ax[0].scatter(X[Y==0,0],X[Y==0,1], s=100, marker='o', facecolors='none',                     edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")     ax[0].plot(X[:,0], (-3/85) * X[:,0] + 21, color=dlc[\"dlpurple\"],linewidth=1)     ax[0].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=1)     ax[0].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=1)     ax[0].set_xlabel(\"Temperature \\n(Celsius)\",size=12)     ax[0].set_ylabel(\"Duration \\n(minutes)\",size=12)     ax[0].legend(loc='upper right')     ax[0].set_title(f\"network probability\")      ax[1].plot(X[:,0], (-3/85) * X[:,0] + 21, color=dlc[\"dlpurple\"],linewidth=1)     ax[1].axhline(y= 12, color=dlc[\"dlpurple\"], linewidth=1)     ax[1].axvline(x=175, color=dlc[\"dlpurple\"], linewidth=1)     fwb = netf(X)     yhat = (fwb &gt; 0.5).astype(int)     ax[1].scatter(X[yhat[:,0]==1,0],X[yhat[:,0]==1,1], s=70, marker='x', c='orange', label=\"Predicted Good Roast\" )     ax[1].scatter(X[yhat[:,0]==0,0],X[yhat[:,0]==0,1], s=100, marker='o', facecolors='none',                     edgecolors=dlc[\"dldarkblue\"],linewidth=1,  label=\"Bad Roast\")     ax[1].set_title(f\"network decision\")     ax[1].set_xlabel(\"Temperature \\n(Celsius)\",size=12)     ax[1].set_ylabel(\"Duration \\n(minutes)\",size=12)     ax[1].legend(loc='upper right') In\u00a0[\u00a0]: Copied! <pre>def plt_output_unit(W,b):\n    \"\"\" plots a single unit function with 3 inputs \"\"\"\n    steps = 10\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    x_ = np.linspace(0., 1., steps)\n    y_ = np.linspace(0., 1., steps)\n    z_ = np.linspace(0., 1., steps)\n    x, y, z = np.meshgrid(x_, y_, z_, indexing='ij')\n    d = np.zeros((steps,steps,steps))\n    cmap = plt.get_cmap('Blues')\n    for i in range(steps):\n        for j in range(steps):\n            for k in range(steps):\n                v = np.array([x[i,j,k],y[i,j,k],z[i,j,k]])\n                d[i,j,k] = tf.keras.activations.sigmoid(np.dot(v,W[:,0])+b).numpy()\n    pcm = ax.scatter(x, y, z, c=d, cmap=cmap, alpha = 1 )\n    ax.set_xlabel(\"unit 0\"); \n    ax.set_ylabel(\"unit 1\"); \n    ax.set_zlabel(\"unit 2\"); \n    ax.view_init(30, -120)\n    ax.figure.colorbar(pcm, ax=ax)\n    ax.set_title(f\"Layer 2, output unit\")\n\n    plt.show()\n</pre> def plt_output_unit(W,b):     \"\"\" plots a single unit function with 3 inputs \"\"\"     steps = 10     fig = plt.figure()     ax = fig.add_subplot(projection='3d')     x_ = np.linspace(0., 1., steps)     y_ = np.linspace(0., 1., steps)     z_ = np.linspace(0., 1., steps)     x, y, z = np.meshgrid(x_, y_, z_, indexing='ij')     d = np.zeros((steps,steps,steps))     cmap = plt.get_cmap('Blues')     for i in range(steps):         for j in range(steps):             for k in range(steps):                 v = np.array([x[i,j,k],y[i,j,k],z[i,j,k]])                 d[i,j,k] = tf.keras.activations.sigmoid(np.dot(v,W[:,0])+b).numpy()     pcm = ax.scatter(x, y, z, c=d, cmap=cmap, alpha = 1 )     ax.set_xlabel(\"unit 0\");      ax.set_ylabel(\"unit 1\");      ax.set_zlabel(\"unit 2\");      ax.view_init(30, -120)     ax.figure.colorbar(pcm, ax=ax)     ax.set_title(f\"Layer 2, output unit\")      plt.show()"},{"location":"DeepLearning/part1/lab_neurons_utils/","title":"Lab neurons utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nfrom matplotlib import cm\nimport matplotlib.colors as colors\nfrom lab_utils_common import dlc\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') from matplotlib import cm import matplotlib.colors as colors from lab_utils_common import dlc In\u00a0[\u00a0]: Copied! <pre>def plt_prob_1d(ax,fwb):\n    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n    #setup useful ranges and common linspaces\n    x_space  = np.linspace(0, 5 , 50)\n    y_space  = np.linspace(0, 1 , 50)\n\n    # get probability for x range, extend to y\n    z = np.zeros((len(x_space),len(y_space)))\n    for i in range(len(x_space)):\n        x = np.array([[x_space[i]]])\n        z[:,i] = fwb(x)\n\n    cmap = plt.get_cmap('Blues')\n    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n    pcm = ax.pcolormesh(x_space, y_space, z,\n                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n    ax.figure.colorbar(pcm, ax=ax)\n</pre> def plt_prob_1d(ax,fwb):     \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"     #setup useful ranges and common linspaces     x_space  = np.linspace(0, 5 , 50)     y_space  = np.linspace(0, 1 , 50)      # get probability for x range, extend to y     z = np.zeros((len(x_space),len(y_space)))     for i in range(len(x_space)):         x = np.array([[x_space[i]]])         z[:,i] = fwb(x)      cmap = plt.get_cmap('Blues')     new_cmap = truncate_colormap(cmap, 0.0, 0.5)     pcm = ax.pcolormesh(x_space, y_space, z,                    norm=cm.colors.Normalize(vmin=0, vmax=1),                    cmap=new_cmap, shading='nearest', alpha = 0.9)     ax.figure.colorbar(pcm, ax=ax) In\u00a0[\u00a0]: Copied! <pre>def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n    \"\"\" truncates color map \"\"\"\n    new_cmap = colors.LinearSegmentedColormap.from_list(\n        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n        cmap(np.linspace(minval, maxval, n)))\n    return new_cmap\n</pre> def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):     \"\"\" truncates color map \"\"\"     new_cmap = colors.LinearSegmentedColormap.from_list(         'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),         cmap(np.linspace(minval, maxval, n)))     return new_cmap In\u00a0[\u00a0]: Copied! <pre>def sigmoidnp(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Parameters\n    ----------\n    z : array_like\n        A scalar or numpy array of any size.\n\n    Returns\n    -------\n     g : array_like\n         sigmoid(z)\n    \"\"\"\n    z = np.clip( z, -500, 500 )           # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n\n    return g\n</pre> def sigmoidnp(z):     \"\"\"     Compute the sigmoid of z      Parameters     ----------     z : array_like         A scalar or numpy array of any size.      Returns     -------      g : array_like          sigmoid(z)     \"\"\"     z = np.clip( z, -500, 500 )           # protect against overflow     g = 1.0/(1.0+np.exp(-z))      return g In\u00a0[\u00a0]: Copied! <pre>def plt_linear(X_train, Y_train, prediction_tf, prediction_np):\n    fig, ax = plt.subplots(1,2, figsize=(16,4))\n    ax[0].scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\n    ax[0].plot(X_train, prediction_tf,  c=dlc['dlblue'], label=\"model output\")\n    ax[0].text(1.6,350,r\"y=$200 x + 100$\", fontsize='xx-large', color=dlc['dlmagenta'])\n    ax[0].legend(fontsize='xx-large')\n    ax[0].set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\n    ax[0].set_xlabel('Size (1000 sqft)', fontsize='xx-large')\n    ax[0].set_title(\"Tensorflow prediction\",fontsize='xx-large')\n\n    ax[1].scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\n    ax[1].plot(X_train, prediction_np,  c=dlc['dlblue'], label=\"model output\")\n    ax[1].text(1.6,350,r\"y=$200 x + 100$\", fontsize='xx-large', color=dlc['dlmagenta'])\n    ax[1].legend(fontsize='xx-large')\n    ax[1].set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\n    ax[1].set_xlabel('Size (1000 sqft)', fontsize='xx-large')\n    ax[1].set_title(\"Numpy prediction\",fontsize='xx-large')\n    plt.show()\n</pre> def plt_linear(X_train, Y_train, prediction_tf, prediction_np):     fig, ax = plt.subplots(1,2, figsize=(16,4))     ax[0].scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")     ax[0].plot(X_train, prediction_tf,  c=dlc['dlblue'], label=\"model output\")     ax[0].text(1.6,350,r\"y=$200 x + 100$\", fontsize='xx-large', color=dlc['dlmagenta'])     ax[0].legend(fontsize='xx-large')     ax[0].set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')     ax[0].set_xlabel('Size (1000 sqft)', fontsize='xx-large')     ax[0].set_title(\"Tensorflow prediction\",fontsize='xx-large')      ax[1].scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")     ax[1].plot(X_train, prediction_np,  c=dlc['dlblue'], label=\"model output\")     ax[1].text(1.6,350,r\"y=$200 x + 100$\", fontsize='xx-large', color=dlc['dlmagenta'])     ax[1].legend(fontsize='xx-large')     ax[1].set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')     ax[1].set_xlabel('Size (1000 sqft)', fontsize='xx-large')     ax[1].set_title(\"Numpy prediction\",fontsize='xx-large')     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):\n    fig,ax = plt.subplots(1,2,figsize=(16,4))\n\n    layerf= lambda x : model.predict(x)\n    plt_prob_1d(ax[0], layerf)\n\n    ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n    ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n                  edgecolors=dlc[\"dlblue\"],lw=3)\n\n    ax[0].set_ylim(-0.08,1.1)\n    ax[0].set_xlim(-0.5,5.5)\n    ax[0].set_ylabel('y', fontsize=16)\n    ax[0].set_xlabel('x', fontsize=16)\n    ax[0].set_title('Tensorflow Model', fontsize=20)\n    ax[0].legend(fontsize=16)\n\n    layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)\n    plt_prob_1d(ax[1], layerf)\n\n    ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n    ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n                  edgecolors=dlc[\"dlblue\"],lw=3)\n\n    ax[1].set_ylim(-0.08,1.1)\n    ax[1].set_xlim(-0.5,5.5)\n    ax[1].set_ylabel('y', fontsize=16)\n    ax[1].set_xlabel('x', fontsize=16)\n    ax[1].set_title('Numpy Model', fontsize=20)\n    ax[1].legend(fontsize=16)\n    plt.show()\n</pre> def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):     fig,ax = plt.subplots(1,2,figsize=(16,4))      layerf= lambda x : model.predict(x)     plt_prob_1d(ax[0], layerf)      ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")     ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',                    edgecolors=dlc[\"dlblue\"],lw=3)      ax[0].set_ylim(-0.08,1.1)     ax[0].set_xlim(-0.5,5.5)     ax[0].set_ylabel('y', fontsize=16)     ax[0].set_xlabel('x', fontsize=16)     ax[0].set_title('Tensorflow Model', fontsize=20)     ax[0].legend(fontsize=16)      layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)     plt_prob_1d(ax[1], layerf)      ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")     ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',                    edgecolors=dlc[\"dlblue\"],lw=3)      ax[1].set_ylim(-0.08,1.1)     ax[1].set_xlim(-0.5,5.5)     ax[1].set_ylabel('y', fontsize=16)     ax[1].set_xlabel('x', fontsize=16)     ax[1].set_title('Numpy Model', fontsize=20)     ax[1].legend(fontsize=16)     plt.show()"},{"location":"DeepLearning/part1/lab_utils_common/","title":"Lab utils common","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>lab_utils_common contains common routines and variable definitions used by all the labs in this week. by contrast, specific, large plotting routines will be in separate files and are generally imported into the week where they are used. those files will import this file</p> In\u00a0[\u00a0]: Copied! <pre>import copy\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch\nfrom ipywidgets import Output\nfrom matplotlib.widgets import Button, CheckButtons\n\nnp.set_printoptions(precision=2)\n\ndlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\nplt.style.use('./deeplearning.mplstyle')\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Parameters\n    ----------\n    z : array_like\n        A scalar or numpy array of any size.\n\n    Returns\n    -------\n     g : array_like\n         sigmoid(z)\n    \"\"\"\n    z = np.clip( z, -500, 500 )           # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n\n    return g\n</pre> import copy import math import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from ipywidgets import Output from matplotlib.widgets import Button, CheckButtons  np.set_printoptions(precision=2)  dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC' dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] plt.style.use('./deeplearning.mplstyle')  def sigmoid(z):     \"\"\"     Compute the sigmoid of z      Parameters     ----------     z : array_like         A scalar or numpy array of any size.      Returns     -------      g : array_like          sigmoid(z)     \"\"\"     z = np.clip( z, -500, 500 )           # protect against overflow     g = 1.0/(1.0+np.exp(-z))      return g <p>Regression Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>def predict_logistic(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return sigmoid(X @ w + b)\n\ndef predict_linear(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return X @ w + b\n\ndef compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n    \"\"\"\n    Computes cost using logistic loss, non-matrix version\n\n    Args:\n      X (ndarray): Shape (m,n)  matrix of examples with n features\n      y (ndarray): Shape (m,)   target values\n      w (ndarray): Shape (n,)   parameters for prediction\n      b (scalar):               parameter  for prediction\n      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n      safe : (boolean)          True-selects under/overflow safe algorithm\n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m,n = X.shape\n    cost = 0.0\n    for i in range(m):\n        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n        if safe:  #avoids overflows\n            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n        else:\n            f_wb_i = sigmoid(z_i)                                                   #(n,)\n            cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n    cost = cost/m\n\n    reg_cost = 0\n    if lambda_ != 0:\n        for j in range(n):\n            reg_cost += (w[j]**2)                                               # scalar\n        reg_cost = (lambda_/(2*m))*reg_cost\n\n    return cost + reg_cost\n\n\ndef log_1pexp(x, maximum=20):\n    ''' approximate log(1+exp^x)\n        https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice\n    Args:\n    x   : (ndarray Shape (n,1) or (n,)  input\n    out : (ndarray Shape matches x      output ~= np.log(1+exp(x))\n    '''\n\n    out  = np.zeros_like(x,dtype=float)\n    i    = x &lt;= maximum\n    ni   = np.logical_not(i)\n\n    out[i]  = np.log(1 + np.exp(x[i]))\n    out[ni] = x[ni]\n    return out\n\n\ndef compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):\n    \"\"\"\n    Computes the cost using  using matrices\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model\n      b : (scalar )                       Values of parameter of the model\n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns:\n      total_cost: (scalar)                cost\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n    if logistic:\n        if safe:  #safe from overflow\n            z = X @ w + b                                                           #(m,n)(n,1)=(m,1)\n            cost = -(y * z) + log_1pexp(z)\n            cost = np.sum(cost)/m                                                   # (scalar)\n        else:\n            f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)\n            cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)\n            cost = cost[0,0]                                                        # scalar\n    else:\n        f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)\n        cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar\n\n    reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar\n\n    total_cost = cost + reg_cost                                                # scalar\n\n    return total_cost                                                           # scalar\n\ndef compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):\n    \"\"\"\n    Computes the gradient using matrices\n\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model\n      b : (scalar )                       Values of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n    Returns\n      dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w\n      dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n\n    f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)\n    err   = f_wb - y                                              # (m,1)\n    dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)\n    dj_db = (1/m) * np.sum(err)                                   # scalar\n\n    dj_dw += (lambda_/m) * w        # regularize                  # (n,1)\n\n    return dj_db, dj_dw                                           # scalar, (n,1)\n\ndef gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True, Trace=True):\n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking\n    num_iters gradient steps with learning rate alpha\n\n    Args:\n      X (ndarray):    Shape (m,n)         matrix of examples\n      y (ndarray):    Shape (m,) or (m,1) target value of each example\n      w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model\n      b_in (scalar):                      Initial value of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n      alpha (float):                      Learning rate\n      num_iters (int):                    number of iterations to run gradient descent\n\n    Returns:\n      w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape\n      b (scalar):                         Updated value of parameter\n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    w = w.reshape(-1,1)      #prep for matrix operations\n    y = y.reshape(-1,1)\n    last_cost = np.Inf\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        # Save cost J at each iteration\n        ccost = compute_cost_matrix(X, y, w, b, logistic, lambda_)\n        if Trace and i&lt;100000:      # prevent resource exhaustion\n            J_history.append( ccost )\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            if verbose: print(f\"Iteration {i:4d}: Cost {ccost}   \")\n            if verbose ==2: print(f\"dj_db, dj_dw = {dj_db: 0.3f}, {dj_dw.reshape(-1)}\")\n\n            if ccost == last_cost:\n                alpha = alpha/10\n                print(f\" alpha now {alpha}\")\n            last_cost = ccost\n\n    return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing\n\ndef zscore_normalize_features(X):\n    \"\"\"\n    computes  X, zcore normalized by column\n\n    Args:\n      X (ndarray): Shape (m,n) input data, m examples, n features\n\n    Returns:\n      X_norm (ndarray): Shape (m,n)  input normalized by column\n      mu (ndarray):     Shape (n,)   mean of each feature\n      sigma (ndarray):  Shape (n,)   standard deviation of each feature\n    \"\"\"\n    # find the mean of each column/feature\n    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n    # find the standard deviation of each column/feature\n    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n    # element-wise, subtract mu for that column from each example, divide by std for that column\n    X_norm = (X - mu) / sigma\n\n    return X_norm, mu, sigma\n\n#check our work\n#from sklearn.preprocessing import scale\n#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n</pre> def predict_logistic(X, w, b):     \"\"\" performs prediction \"\"\"     return sigmoid(X @ w + b)  def predict_linear(X, w, b):     \"\"\" performs prediction \"\"\"     return X @ w + b  def compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):     \"\"\"     Computes cost using logistic loss, non-matrix version      Args:       X (ndarray): Shape (m,n)  matrix of examples with n features       y (ndarray): Shape (m,)   target values       w (ndarray): Shape (n,)   parameters for prediction       b (scalar):               parameter  for prediction       lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization       safe : (boolean)          True-selects under/overflow safe algorithm     Returns:       cost (scalar): cost     \"\"\"      m,n = X.shape     cost = 0.0     for i in range(m):         z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()         if safe:  #avoids overflows             cost += -(y[i] * z_i ) + log_1pexp(z_i)         else:             f_wb_i = sigmoid(z_i)                                                   #(n,)             cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar     cost = cost/m      reg_cost = 0     if lambda_ != 0:         for j in range(n):             reg_cost += (w[j]**2)                                               # scalar         reg_cost = (lambda_/(2*m))*reg_cost      return cost + reg_cost   def log_1pexp(x, maximum=20):     ''' approximate log(1+exp^x)         https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice     Args:     x   : (ndarray Shape (n,1) or (n,)  input     out : (ndarray Shape matches x      output ~= np.log(1+exp(x))     '''      out  = np.zeros_like(x,dtype=float)     i    = x &lt;= maximum     ni   = np.logical_not(i)      out[i]  = np.log(1 + np.exp(x[i]))     out[ni] = x[ni]     return out   def compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):     \"\"\"     Computes the cost using  using matrices     Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model       b : (scalar )                       Values of parameter of the model       verbose : (Boolean) If true, print out intermediate value f_wb     Returns:       total_cost: (scalar)                cost     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D     if logistic:         if safe:  #safe from overflow             z = X @ w + b                                                           #(m,n)(n,1)=(m,1)             cost = -(y * z) + log_1pexp(z)             cost = np.sum(cost)/m                                                   # (scalar)         else:             f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)             cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)             cost = cost[0,0]                                                        # scalar     else:         f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)         cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar      reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar      total_cost = cost + reg_cost                                                # scalar      return total_cost                                                           # scalar  def compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):     \"\"\"     Computes the gradient using matrices      Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model       b : (scalar )                       Values of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero     Returns       dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w       dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D      f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)     err   = f_wb - y                                              # (m,1)     dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)     dj_db = (1/m) * np.sum(err)                                   # scalar      dj_dw += (lambda_/m) * w        # regularize                  # (n,1)      return dj_db, dj_dw                                           # scalar, (n,1)  def gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True, Trace=True):     \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking     num_iters gradient steps with learning rate alpha      Args:       X (ndarray):    Shape (m,n)         matrix of examples       y (ndarray):    Shape (m,) or (m,1) target value of each example       w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model       b_in (scalar):                      Initial value of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero       alpha (float):                      Learning rate       num_iters (int):                    number of iterations to run gradient descent      Returns:       w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape       b (scalar):                         Updated value of parameter     \"\"\"     # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in     w = w.reshape(-1,1)      #prep for matrix operations     y = y.reshape(-1,1)     last_cost = np.Inf      for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)          # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw         b = b - alpha * dj_db          # Save cost J at each iteration         ccost = compute_cost_matrix(X, y, w, b, logistic, lambda_)         if Trace and i&lt;100000:      # prevent resource exhaustion             J_history.append( ccost )          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters / 10) == 0:             if verbose: print(f\"Iteration {i:4d}: Cost {ccost}   \")             if verbose ==2: print(f\"dj_db, dj_dw = {dj_db: 0.3f}, {dj_dw.reshape(-1)}\")              if ccost == last_cost:                 alpha = alpha/10                 print(f\" alpha now {alpha}\")             last_cost = ccost      return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing  def zscore_normalize_features(X):     \"\"\"     computes  X, zcore normalized by column      Args:       X (ndarray): Shape (m,n) input data, m examples, n features      Returns:       X_norm (ndarray): Shape (m,n)  input normalized by column       mu (ndarray):     Shape (n,)   mean of each feature       sigma (ndarray):  Shape (n,)   standard deviation of each feature     \"\"\"     # find the mean of each column/feature     mu     = np.mean(X, axis=0)                 # mu will have shape (n,)     # find the standard deviation of each column/feature     sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)     # element-wise, subtract mu for that column from each example, divide by std for that column     X_norm = (X - mu) / sigma      return X_norm, mu, sigma  #check our work #from sklearn.preprocessing import scale #scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True) <p>Common Plotting Routines #####################################################</p> In\u00a0[\u00a0]: Copied! <pre>def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n    \"\"\" plots logistic data with two axis \"\"\"\n    # Find Indices of Positive and Negative Examples\n    pos = y == 1\n    neg = y == 0\n    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n    neg = neg.reshape(-1,)\n\n    # Plot examples\n    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n    ax.legend(loc=loc)\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\ndef plt_tumor_data(x, y, ax):\n    \"\"\" plots tumor data on one axis \"\"\"\n    pos = y == 1\n    neg = y == 0\n\n    ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n    ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n    ax.set_ylim(-0.175,1.1)\n    ax.set_ylabel('y')\n    ax.set_xlabel('Tumor Size')\n    ax.set_title(\"Logistic Regression on Categorical Data\")\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\n# Draws a threshold at 0.5\ndef draw_vthresh(ax,x):\n    \"\"\" draws a threshold \"\"\"\n    ylim = ax.get_ylim()\n    xlim = ax.get_xlim()\n    ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)\n    ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)\n    ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',\n                xytext=[30,5],textcoords='offset points')\n    d = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(d)\n    ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',\n                 xytext=[-50,5],textcoords='offset points', ha='left')\n    f = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(f)\n\n\n#-----------------------------------------------------\n# common interactive plotting routines\n#-----------------------------------------------------\n\nclass button_manager:\n    ''' Handles some missing features of matplotlib check buttons\n    on init:\n        creates button, links to button_click routine,\n        calls call_on_click with active index and firsttime=True\n    on click:\n        maintains single button on state, calls call_on_click\n    '''\n\n    #@output.capture()  # debug\n    def __init__(self,fig, dim, labels, init, call_on_click):\n        '''\n        dim: (list)     [leftbottom_x,bottom_y,width,height]\n        labels: (list)  for example ['1','2','3','4','5','6']\n        init: (list)    for example [True, False, False, False, False, False]\n        '''\n        self.fig = fig\n        self.ax = plt.axes(dim)  #lx,by,w,h\n        self.init_state = init\n        self.call_on_click = call_on_click\n        self.button  = CheckButtons(self.ax,labels,init)\n        self.button.on_clicked(self.button_click)\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True),firsttime=True)\n\n    #@output.capture()  # debug\n    def reinit(self):\n        self.status = self.init_state\n        self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status\n\n    #@output.capture()  # debug\n    def button_click(self, event):\n        ''' maintains one-on state. If on-button is clicked, will process correctly '''\n        #new_status = self.button.get_status()\n        #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]\n        #newidx = new.index(True)\n        self.button.eventson = False\n        self.button.set_active(self.status.index(True))  #turn off old or reenable if same\n        self.button.eventson = True\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True))\n</pre>  def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):     \"\"\" plots logistic data with two axis \"\"\"     # Find Indices of Positive and Negative Examples     pos = y == 1     neg = y == 0     pos = pos.reshape(-1,)  #work with 1D or 1D y vectors     neg = neg.reshape(-1,)      # Plot examples     ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)     ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)     ax.legend(loc=loc)      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  def plt_tumor_data(x, y, ax):     \"\"\" plots tumor data on one axis \"\"\"     pos = y == 1     neg = y == 0      ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")     ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)     ax.set_ylim(-0.175,1.1)     ax.set_ylabel('y')     ax.set_xlabel('Tumor Size')     ax.set_title(\"Logistic Regression on Categorical Data\")      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  # Draws a threshold at 0.5 def draw_vthresh(ax,x):     \"\"\" draws a threshold \"\"\"     ylim = ax.get_ylim()     xlim = ax.get_xlim()     ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)     ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)     ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',                 xytext=[30,5],textcoords='offset points')     d = FancyArrowPatch(         posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(d)     ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',                  xytext=[-50,5],textcoords='offset points', ha='left')     f = FancyArrowPatch(         posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(f)   #----------------------------------------------------- # common interactive plotting routines #-----------------------------------------------------  class button_manager:     ''' Handles some missing features of matplotlib check buttons     on init:         creates button, links to button_click routine,         calls call_on_click with active index and firsttime=True     on click:         maintains single button on state, calls call_on_click     '''      #@output.capture()  # debug     def __init__(self,fig, dim, labels, init, call_on_click):         '''         dim: (list)     [leftbottom_x,bottom_y,width,height]         labels: (list)  for example ['1','2','3','4','5','6']         init: (list)    for example [True, False, False, False, False, False]         '''         self.fig = fig         self.ax = plt.axes(dim)  #lx,by,w,h         self.init_state = init         self.call_on_click = call_on_click         self.button  = CheckButtons(self.ax,labels,init)         self.button.on_clicked(self.button_click)         self.status = self.button.get_status()         self.call_on_click(self.status.index(True),firsttime=True)      #@output.capture()  # debug     def reinit(self):         self.status = self.init_state         self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status      #@output.capture()  # debug     def button_click(self, event):         ''' maintains one-on state. If on-button is clicked, will process correctly '''         #new_status = self.button.get_status()         #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]         #newidx = new.index(True)         self.button.eventson = False         self.button.set_active(self.status.index(True))  #turn off old or reenable if same         self.button.eventson = True         self.status = self.button.get_status()         self.call_on_click(self.status.index(True))"},{"location":"DeepLearning/part1/public_tests/","title":"Public tests","text":"In\u00a0[\u00a0]: Copied! <pre># UNIT TESTS\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Dense\n</pre> # UNIT TESTS from tensorflow.keras.activations import sigmoid from tensorflow.keras.layers import Dense In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def test_c1(target):\n    assert len(target.layers) == 3, \\\n        f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n    assert target.input.shape.as_list() == [None, 400], \\\n        f\"Wrong input shape. Expected [None,  400] but got {target.input.shape.as_list()}\"\n    i = 0\n    expected = [[Dense, [None, 25], sigmoid],\n                [Dense, [None, 15], sigmoid],\n                [Dense, [None, 1], sigmoid]]\n\n    for layer in target.layers:\n        assert type(layer) == expected[i][0], \\\n            f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"\n        assert layer.output.shape.as_list() == expected[i][1], \\\n            f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"\n        assert layer.activation == expected[i][2], \\\n            f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"\n        i = i + 1\n\n    print(\"\\033[92mAll tests passed!\")\n</pre> def test_c1(target):     assert len(target.layers) == 3, \\         f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"     assert target.input.shape.as_list() == [None, 400], \\         f\"Wrong input shape. Expected [None,  400] but got {target.input.shape.as_list()}\"     i = 0     expected = [[Dense, [None, 25], sigmoid],                 [Dense, [None, 15], sigmoid],                 [Dense, [None, 1], sigmoid]]      for layer in target.layers:         assert type(layer) == expected[i][0], \\             f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"         assert layer.output.shape.as_list() == expected[i][1], \\             f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"         assert layer.activation == expected[i][2], \\             f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"         i = i + 1      print(\"\\033[92mAll tests passed!\") In\u00a0[\u00a0]: Copied! <pre>def test_c2(target):\n    \n    def linear(a):\n        return a\n    \n    def linear_times3(a):\n        return a * 3\n    \n    x_tst = np.array([1., 2., 3., 4.])  # (1 examples, 3 features)\n    W_tst = np.array([[1., 2.], [1., 2.], [1., 2.], [1., 2.]]) # (3 input features, 2 output features)\n    b_tst = np.array([0., 0.])  # (2 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert A_tst.shape[0] == len(b_tst)\n    assert np.allclose(A_tst, [10., 20.]), \\\n        \"Wrong output. Check the dot product\"\n    \n    b_tst = np.array([3., 5.])  # (2 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert np.allclose(A_tst, [13., 25.]), \\\n        \"Wrong output. Check the bias term in the formula\"\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear_times3)\n    assert np.allclose(A_tst, [39., 75.]), \\\n        \"Wrong output. Did you apply the activation function at the end?\"\n    \n    print(\"\\033[92mAll tests passed!\")  \n</pre> def test_c2(target):          def linear(a):         return a          def linear_times3(a):         return a * 3          x_tst = np.array([1., 2., 3., 4.])  # (1 examples, 3 features)     W_tst = np.array([[1., 2.], [1., 2.], [1., 2.], [1., 2.]]) # (3 input features, 2 output features)     b_tst = np.array([0., 0.])  # (2 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert A_tst.shape[0] == len(b_tst)     assert np.allclose(A_tst, [10., 20.]), \\         \"Wrong output. Check the dot product\"          b_tst = np.array([3., 5.])  # (2 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert np.allclose(A_tst, [13., 25.]), \\         \"Wrong output. Check the bias term in the formula\"          A_tst = target(x_tst, W_tst, b_tst, linear_times3)     assert np.allclose(A_tst, [39., 75.]), \\         \"Wrong output. Did you apply the activation function at the end?\"          print(\"\\033[92mAll tests passed!\")   In\u00a0[\u00a0]: Copied! <pre>def test_c3(target):\n    \n    def linear(a):\n        return a\n    \n    def linear_times3(a):\n        return a * 3\n    \n    x_tst = np.array([1., 2., 3., 4.])  # (1 examples, 3 features)\n    W_tst = np.array([[1., 2.], [1., 2.], [1., 2.], [1., 2.]]) # (3 input features, 2 output features)\n    b_tst = np.array([0., 0.])  # (2 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert A_tst.shape[0] == len(b_tst)\n    assert np.allclose(A_tst, [10., 20.]), \\\n        \"Wrong output. Check the dot product\"\n    \n    b_tst = np.array([3., 5.])  # (2 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert np.allclose(A_tst, [13., 25.]), \\\n        \"Wrong output. Check the bias term in the formula\"\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear_times3)\n    assert np.allclose(A_tst, [39., 75.]), \\\n        \"Wrong output. Did you apply the activation function at the end?\"\n    \n    x_tst = np.array([[1., 2., 3., 4.], [5., 6., 7., 8.]])  # (2 examples, 4 features)\n    W_tst = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12]]) # (3 input features, 2 output features)\n    b_tst = np.array([0., 0., 0.])  # (2 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert A_tst.shape == (2, 3)\n    assert np.allclose(A_tst, [[ 70.,  80.,  90.], [158., 184., 210.]]), \\\n        \"Wrong output. Check the dot product\"\n    \n    b_tst = np.array([3., 5., 6])  # (3 features)\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear)\n    assert np.allclose(A_tst, [[ 73.,  85.,  96.], [161., 189., 216.]]), \\\n        \"Wrong output. Check the bias term in the formula\"\n    \n    A_tst = target(x_tst, W_tst, b_tst, linear_times3)\n    assert np.allclose(A_tst, [[ 219.,  255.,  288.], [483., 567., 648.]]), \\\n        \"Wrong output. Did you apply the activation function at the end?\"\n    \n    print(\"\\033[92mAll tests passed!\")  \n</pre> def test_c3(target):          def linear(a):         return a          def linear_times3(a):         return a * 3          x_tst = np.array([1., 2., 3., 4.])  # (1 examples, 3 features)     W_tst = np.array([[1., 2.], [1., 2.], [1., 2.], [1., 2.]]) # (3 input features, 2 output features)     b_tst = np.array([0., 0.])  # (2 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert A_tst.shape[0] == len(b_tst)     assert np.allclose(A_tst, [10., 20.]), \\         \"Wrong output. Check the dot product\"          b_tst = np.array([3., 5.])  # (2 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert np.allclose(A_tst, [13., 25.]), \\         \"Wrong output. Check the bias term in the formula\"          A_tst = target(x_tst, W_tst, b_tst, linear_times3)     assert np.allclose(A_tst, [39., 75.]), \\         \"Wrong output. Did you apply the activation function at the end?\"          x_tst = np.array([[1., 2., 3., 4.], [5., 6., 7., 8.]])  # (2 examples, 4 features)     W_tst = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12]]) # (3 input features, 2 output features)     b_tst = np.array([0., 0., 0.])  # (2 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert A_tst.shape == (2, 3)     assert np.allclose(A_tst, [[ 70.,  80.,  90.], [158., 184., 210.]]), \\         \"Wrong output. Check the dot product\"          b_tst = np.array([3., 5., 6])  # (3 features)          A_tst = target(x_tst, W_tst, b_tst, linear)     assert np.allclose(A_tst, [[ 73.,  85.,  96.], [161., 189., 216.]]), \\         \"Wrong output. Check the bias term in the formula\"          A_tst = target(x_tst, W_tst, b_tst, linear_times3)     assert np.allclose(A_tst, [[ 219.,  255.,  288.], [483., 567., 648.]]), \\         \"Wrong output. Did you apply the activation function at the end?\"          print(\"\\033[92mAll tests passed!\")"},{"location":"DeepLearning/part1/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre># C2_W1 Utilities\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n</pre> # C2_W1 Utilities import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs In\u00a0[\u00a0]: Copied! <pre>def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n</pre> def sigmoid(x):     return 1 / (1 + np.exp(-x)) In\u00a0[\u00a0]: Copied! <pre># Plot  multi-class training points\ndef plot_mc_data(X, y, class_labels=None, legend=False,size=40):\n    classes = np.unique(y)\n    for i in classes:\n        label = class_labels[i] if class_labels else \"class {}\".format(i)\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1],  cmap=plt.cm.Paired,\n                    edgecolor='black', s=size, label=label)\n    if legend: plt.legend()\n</pre> # Plot  multi-class training points def plot_mc_data(X, y, class_labels=None, legend=False,size=40):     classes = np.unique(y)     for i in classes:         label = class_labels[i] if class_labels else \"class {}\".format(i)         idx = np.where(y == i)         plt.scatter(X[idx, 0], X[idx, 1],  cmap=plt.cm.Paired,                     edgecolor='black', s=size, label=label)     if legend: plt.legend() In\u00a0[\u00a0]: Copied! <pre>#Plot a multi-class categorical decision boundary\n# This version handles a non-vector prediction (adds a for-loop over points)\ndef plot_cat_decision_boundary(X,predict , class_labels=None, legend=False, vector=True):\n\n    # create a mesh to points to plot\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = max(x_max-x_min, y_max-y_min)/200\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    points = np.c_[xx.ravel(), yy.ravel()]\n\n    #make predictions for each point in mesh\n    if vector:\n        Z = predict(points)\n    else:\n        Z = np.zeros((len(points),))\n        for i in range(len(points)):\n            Z[i] = predict(points[i].reshape(1,2))\n    Z = Z.reshape(xx.shape)\n\n    #contour plot highlights boundaries between values - classes in this case\n    plt.figure()\n    plt.contour(xx, yy, Z, colors='g') \n    plt.axis('tight')\n</pre> #Plot a multi-class categorical decision boundary # This version handles a non-vector prediction (adds a for-loop over points) def plot_cat_decision_boundary(X,predict , class_labels=None, legend=False, vector=True):      # create a mesh to points to plot     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1     h = max(x_max-x_min, y_max-y_min)/200     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))     points = np.c_[xx.ravel(), yy.ravel()]      #make predictions for each point in mesh     if vector:         Z = predict(points)     else:         Z = np.zeros((len(points),))         for i in range(len(points)):             Z[i] = predict(points[i].reshape(1,2))     Z = Z.reshape(xx.shape)      #contour plot highlights boundaries between values - classes in this case     plt.figure()     plt.contour(xx, yy, Z, colors='g')      plt.axis('tight')"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/","title":"Neural Networks for Handwritten Digit Recognition, Binary","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport matplotlib.pyplot as plt\nfrom autils import *\n%matplotlib inline\n\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense import matplotlib.pyplot as plt from autils import * %matplotlib inline  import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) <p>Tensorflow and Keras Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by Fran\u00e7ois Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># load dataset\nX, y = load_data()\n</pre> # load dataset X, y = load_data() <p></p> In\u00a0[\u00a0]: Copied! <pre>print ('The first element of X is: ', X[0])\n</pre> print ('The first element of X is: ', X[0]) In\u00a0[\u00a0]: Copied! <pre>print ('The first element of y is: ', y[0,0])\nprint ('The last element of y is: ', y[-1,0])\n</pre> print ('The first element of y is: ', y[0,0]) print ('The last element of y is: ', y[-1,0]) <p></p> In\u00a0[\u00a0]: Copied! <pre>print ('The shape of X is: ' + str(X.shape))\nprint ('The shape of y is: ' + str(y.shape))\n</pre> print ('The shape of X is: ' + str(X.shape)) print ('The shape of y is: ' + str(y.shape)) <p></p> In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1)\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Display the label above the image\n    ax.set_title(y[random_index,0])\n    ax.set_axis_off()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1)  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Display the label above the image     ax.set_title(y[random_index,0])     ax.set_axis_off() <p></p> <ul> <li><p>The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $1$ output unit in layer 3.</p> <ul> <li><p>Recall that the dimensions of these parameters are determined as follows:</p> <ul> <li>If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then<ul> <li>$W$ will be of dimension $s_{in} \\times s_{out}$.</li> <li>$b$ will a vector with $s_{out}$ elements</li> </ul> </li> </ul> </li> <li><p>Therefore, the shapes of <code>W</code>, and <code>b</code>,  are</p> <ul> <li>layer1: The shape of <code>W1</code> is (400, 25) and the shape of <code>b1</code> is (25,)</li> <li>layer2: The shape of <code>W2</code> is (25, 15) and the shape of <code>b2</code> is: (15,)</li> <li>layer3: The shape of <code>W3</code> is (15, 1) and the shape of <code>b3</code> is: (1,)</li> </ul> </li> </ul> </li> </ul> <p>Note: The bias vector <code>b</code> could be represented as a 1-D (n,) or 2-D (n,1) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention.</p> <p></p> <p>Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's output dimensions and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the <code>model.fit</code> statment below.</p> <p>Note: It is also possible to add an input layer that specifies the input dimension of the first layer. For example: <code>tf.keras.Input(shape=(400,)),    #specify input shape</code> We will include that here to illuminate some model sizing.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># UNQ_C1\n# GRADED CELL: Sequential model\n\nmodel = Sequential(\n    [               \n        tf.keras.Input(shape=(400,)),    #specify input size\n        ### START CODE HERE ### \n        \n        ### END CODE HERE ### \n    ], name = \"my_model\" \n)                            \n</pre> # UNQ_C1 # GRADED CELL: Sequential model  model = Sequential(     [                        tf.keras.Input(shape=(400,)),    #specify input size         ### START CODE HERE ###                   ### END CODE HERE ###      ], name = \"my_model\"  )                              In\u00a0[\u00a0]: Copied! <pre>model.summary()\n</pre> model.summary() Expected Output (Click to Expand)  The `model.summary()` function displays a useful summary of the model. Because we have specified an input layer size, the shape of the weight and bias arrays are determined and the total number of parameters per layer can be shown. Note, the names of the layers may vary as they are auto-generated.    <pre><code>Model: \"my_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 25)                10025     \n_________________________________________________________________\ndense_1 (Dense)              (None, 15)                390       \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 16        \n=================================================================\nTotal params: 10,431\nTrainable params: 10,431\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> Click for hints As described in the lecture:  <pre>model = Sequential(                      \n    [                                   \n        tf.keras.Input(shape=(400,)),    # specify input size (optional)\n        Dense(25, activation='sigmoid'), \n        Dense(15, activation='sigmoid'), \n        Dense(1,  activation='sigmoid')  \n    ], name = \"my_model\"                                    \n)                                       \n</pre> In\u00a0[\u00a0]: Copied! <pre># UNIT TESTS\nfrom public_tests import * \n\ntest_c1(model)\n</pre> # UNIT TESTS from public_tests import *   test_c1(model) <p>The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays as shown below.</p> In\u00a0[\u00a0]: Copied! <pre>L1_num_params = 400 * 25 + 25  # W1 parameters  + b1 parameters\nL2_num_params = 25 * 15 + 15   # W2 parameters  + b2 parameters\nL3_num_params = 15 * 1 + 1     # W3 parameters  + b3 parameters\nprint(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params, \",  L3 params = \", L3_num_params )\n</pre> L1_num_params = 400 * 25 + 25  # W1 parameters  + b1 parameters L2_num_params = 25 * 15 + 15   # W2 parameters  + b2 parameters L3_num_params = 15 * 1 + 1     # W3 parameters  + b3 parameters print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params, \",  L3 params = \", L3_num_params ) <p>Let's further examine the weights to verify that tensorflow produced the same dimensions as we calculated above.</p> In\u00a0[\u00a0]: Copied! <pre>[layer1, layer2, layer3] = model.layers\n</pre> [layer1, layer2, layer3] = model.layers In\u00a0[\u00a0]: Copied! <pre>#### Examine Weights shapes\nW1,b1 = layer1.get_weights()\nW2,b2 = layer2.get_weights()\nW3,b3 = layer3.get_weights()\nprint(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\nprint(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\nprint(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n</pre> #### Examine Weights shapes W1,b1 = layer1.get_weights() W2,b2 = layer2.get_weights() W3,b3 = layer3.get_weights() print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\") print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\") print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\") <p>Expected Output</p> <pre><code>W1 shape = (400, 25), b1 shape = (25,)  \nW2 shape = (25, 15), b2 shape = (15,)  \nW3 shape = (15, 1), b3 shape = (1,)\n</code></pre> <p><code>xx.get_weights</code> returns a NumPy array. One can also access the weights directly in their tensor form. Note the shape of the tensors in the final layer.</p> In\u00a0[\u00a0]: Copied! <pre>print(model.layers[2].weights)\n</pre> print(model.layers[2].weights) <p>The following code will define a loss function and run gradient descent to fit the weights of the model to the training data. This will be explained in more detail in the following week.</p> In\u00a0[\u00a0]: Copied! <pre>model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X,y,\n    epochs=20\n)\n</pre> model.compile(     loss=tf.keras.losses.BinaryCrossentropy(),     optimizer=tf.keras.optimizers.Adam(0.001), )  model.fit(     X,y,     epochs=20 ) <p>To run the model on an example to make a prediction, use Keras <code>predict</code>. The input to <code>predict</code> is an array so the single example is reshaped to be two dimensional.</p> In\u00a0[\u00a0]: Copied! <pre>prediction = model.predict(X[0].reshape(1,400))  # a zero\nprint(f\" predicting a zero: {prediction}\")\nprediction = model.predict(X[500].reshape(1,400))  # a one\nprint(f\" predicting a one:  {prediction}\")\n</pre> prediction = model.predict(X[0].reshape(1,400))  # a zero print(f\" predicting a zero: {prediction}\") prediction = model.predict(X[500].reshape(1,400))  # a one print(f\" predicting a one:  {prediction}\") <p>The output of the model is interpreted as a probability. In the first example above, the input is a zero. The model predicts the probability that the input is a one is nearly zero. In the second example, the input is a one. The model predicts the probability that the input is a one is nearly one. As in the case of logistic regression, the probability is compared to a threshold to make a final prediction.</p> In\u00a0[\u00a0]: Copied! <pre>if prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint(f\"prediction after threshold: {yhat}\")\n</pre> if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print(f\"prediction after threshold: {yhat}\") <p>Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run.</p> In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Predict using the Neural Network\n    prediction = model.predict(X[random_index].reshape(1,400))\n    if prediction &gt;= 0.5:\n        yhat = 1\n    else:\n        yhat = 0\n    \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n    ax.set_axis_off()\nfig.suptitle(\"Label, yhat\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Predict using the Neural Network     prediction = model.predict(X[random_index].reshape(1,400))     if prediction &gt;= 0.5:         yhat = 1     else:         yhat = 0          # Display the label above the image     ax.set_title(f\"{y[random_index,0]},{yhat}\")     ax.set_axis_off() fig.suptitle(\"Label, yhat\", fontsize=16) plt.show() <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre># UNQ_C2\n# GRADED FUNCTION: my_dense\n\ndef my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n### START CODE HERE ### \n        \n### END CODE HERE ### \n    return(a_out)\n</pre> # UNQ_C2 # GRADED FUNCTION: my_dense  def my_dense(a_in, W, b, g):     \"\"\"     Computes dense layer     Args:       a_in (ndarray (n, )) : Data, 1 example        W    (ndarray (n,j)) : Weight matrix, n features per unit, j units       b    (ndarray (j, )) : bias vector, j units         g    activation function (e.g. sigmoid, relu..)     Returns       a_out (ndarray (j,))  : j units     \"\"\"     units = W.shape[1]     a_out = np.zeros(units) ### START CODE HERE ###           ### END CODE HERE ###      return(a_out)  In\u00a0[\u00a0]: Copied! <pre># Quick Check\nx_tst = 0.1*np.arange(1,3,1).reshape(2,)  # (1 examples, 2 features)\nW_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features)\nb_tst = 0.1*np.arange(1,4,1).reshape(3,)  # (3 features)\nA_tst = my_dense(x_tst, W_tst, b_tst, sigmoid)\nprint(A_tst)\n</pre> # Quick Check x_tst = 0.1*np.arange(1,3,1).reshape(2,)  # (1 examples, 2 features) W_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features) b_tst = 0.1*np.arange(1,4,1).reshape(3,)  # (3 features) A_tst = my_dense(x_tst, W_tst, b_tst, sigmoid) print(A_tst) <p>Expected Output</p> <pre><code>[0.54735762 0.57932425 0.61063923]\n</code></pre> Click for hints As described in the lecture:  <pre>def my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):             \n        w =                            # Select weights for unit j. These are in column j of W\n        z =                            # dot product of w and a_in + b\n        a_out[j] =                     # apply activation to z\n    return(a_out)\n</pre> Click for more hints <pre>def my_dense(a_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      a_in (ndarray (n, )) : Data, 1 example \n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j, )) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      a_out (ndarray (j,))  : j units\n    \"\"\"\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):             \n        w = W[:,j]                     \n        z = np.dot(w, a_in) + b[j]     \n        a_out[j] = g(z)                \n    return(a_out)\n</pre> In\u00a0[\u00a0]: Copied! <pre># UNIT TESTS\ntest_c2(my_dense)\n</pre> # UNIT TESTS test_c2(my_dense) <p>The following cell builds a three-layer neural network utilizing the <code>my_dense</code> subroutine above.</p> In\u00a0[\u00a0]: Copied! <pre>def my_sequential(x, W1, b1, W2, b2, W3, b3):\n    a1 = my_dense(x,  W1, b1, sigmoid)\n    a2 = my_dense(a1, W2, b2, sigmoid)\n    a3 = my_dense(a2, W3, b3, sigmoid)\n    return(a3)\n</pre> def my_sequential(x, W1, b1, W2, b2, W3, b3):     a1 = my_dense(x,  W1, b1, sigmoid)     a2 = my_dense(a1, W2, b2, sigmoid)     a3 = my_dense(a2, W3, b3, sigmoid)     return(a3) <p>We can copy trained weights and biases from Tensorflow.</p> In\u00a0[\u00a0]: Copied! <pre>W1_tmp,b1_tmp = layer1.get_weights()\nW2_tmp,b2_tmp = layer2.get_weights()\nW3_tmp,b3_tmp = layer3.get_weights()\n</pre> W1_tmp,b1_tmp = layer1.get_weights() W2_tmp,b2_tmp = layer2.get_weights() W3_tmp,b3_tmp = layer3.get_weights() In\u00a0[\u00a0]: Copied! <pre># make predictions\nprediction = my_sequential(X[0], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nif prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint( \"yhat = \", yhat, \" label= \", y[0,0])\nprediction = my_sequential(X[500], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nif prediction &gt;= 0.5:\n    yhat = 1\nelse:\n    yhat = 0\nprint( \"yhat = \", yhat, \" label= \", y[500,0])\n</pre> # make predictions prediction = my_sequential(X[0], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print( \"yhat = \", yhat, \" label= \", y[0,0]) prediction = my_sequential(X[500], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) if prediction &gt;= 0.5:     yhat = 1 else:     yhat = 0 print( \"yhat = \", yhat, \" label= \", y[500,0]) <p>Run the following cell to see predictions from both the Numpy model and the Tensorflow model. This takes a moment to run.</p> In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n\n    # Predict using the Neural Network implemented in Numpy\n    my_prediction = my_sequential(X[random_index], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\n    my_yhat = int(my_prediction &gt;= 0.5)\n\n    # Predict using the Neural Network implemented in Tensorflow\n    tf_prediction = model.predict(X[random_index].reshape(1,400))\n    tf_yhat = int(tf_prediction &gt;= 0.5)\n    \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{tf_yhat},{my_yhat}\")\n    ax.set_axis_off() \nfig.suptitle(\"Label, yhat Tensorflow, yhat Numpy\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(8,8)) fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')      # Predict using the Neural Network implemented in Numpy     my_prediction = my_sequential(X[random_index], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )     my_yhat = int(my_prediction &gt;= 0.5)      # Predict using the Neural Network implemented in Tensorflow     tf_prediction = model.predict(X[random_index].reshape(1,400))     tf_yhat = int(tf_prediction &gt;= 0.5)          # Display the label above the image     ax.set_title(f\"{y[random_index,0]},{tf_yhat},{my_yhat}\")     ax.set_axis_off()  fig.suptitle(\"Label, yhat Tensorflow, yhat Numpy\", fontsize=16) plt.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>x = X[0].reshape(-1,1)         # column vector (400,1)\nz1 = np.matmul(x.T,W1) + b1    # (1,400)(400,25) = (1,25)\na1 = sigmoid(z1)\nprint(a1.shape)\n</pre> x = X[0].reshape(-1,1)         # column vector (400,1) z1 = np.matmul(x.T,W1) + b1    # (1,400)(400,25) = (1,25) a1 = sigmoid(z1) print(a1.shape) <p>You can take this a step further and compute all the units for all examples in one Matrix-Matrix operation.</p>  The full operation is $\\mathbf{Z}=\\mathbf{XW}+\\mathbf{b}$. This will utilize NumPy broadcasting to expand $\\mathbf{b}$ to $m$ rows. If this is unfamiliar, a short tutorial is provided at the end of the notebook.    <p></p> In\u00a0[\u00a0]: Copied! <pre># UNQ_C3\n# GRADED FUNCTION: my_dense_v\n\ndef my_dense_v(A_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      A_in (ndarray (m,n)) : Data, m examples, n features each\n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j,1)) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      A_out (ndarray (m,j)) : m examples, j units\n    \"\"\"\n### START CODE HERE ### \n    \n    \n### END CODE HERE ### \n    return(A_out)\n</pre> # UNQ_C3 # GRADED FUNCTION: my_dense_v  def my_dense_v(A_in, W, b, g):     \"\"\"     Computes dense layer     Args:       A_in (ndarray (m,n)) : Data, m examples, n features each       W    (ndarray (n,j)) : Weight matrix, n features per unit, j units       b    (ndarray (j,1)) : bias vector, j units         g    activation function (e.g. sigmoid, relu..)     Returns       A_out (ndarray (m,j)) : m examples, j units     \"\"\" ### START CODE HERE ###            ### END CODE HERE ###      return(A_out) In\u00a0[\u00a0]: Copied! <pre>X_tst = 0.1*np.arange(1,9,1).reshape(4,2) # (4 examples, 2 features)\nW_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features)\nb_tst = 0.1*np.arange(1,4,1).reshape(1,3) # (3 features, 1)\nA_tst = my_dense_v(X_tst, W_tst, b_tst, sigmoid)\nprint(A_tst)\n</pre> X_tst = 0.1*np.arange(1,9,1).reshape(4,2) # (4 examples, 2 features) W_tst = 0.1*np.arange(1,7,1).reshape(2,3) # (2 input features, 3 output features) b_tst = 0.1*np.arange(1,4,1).reshape(1,3) # (3 features, 1) A_tst = my_dense_v(X_tst, W_tst, b_tst, sigmoid) print(A_tst) <p>Expected Output</p> <pre><code>[[0.54735762 0.57932425 0.61063923]\n [0.57199613 0.61301418 0.65248946]\n [0.5962827  0.64565631 0.6921095 ]\n [0.62010643 0.67699586 0.72908792]]\n</code></pre> Click for hints     In matrix form, this can be written in one or two lines.   <pre><code>   Z = np.matmul of A_in and W plus b    \n   A_out is g(Z)  </code></pre> Click for code <pre>def my_dense_v(A_in, W, b, g):\n    \"\"\"\n    Computes dense layer\n    Args:\n      A_in (ndarray (m,n)) : Data, m examples, n features each\n      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n      b    (ndarray (j,1)) : bias vector, j units  \n      g    activation function (e.g. sigmoid, relu..)\n    Returns\n      A_out (ndarray (m,j)) : m examples, j units\n    \"\"\"\n    Z = np.matmul(A_in,W) + b    \n    A_out = g(Z)                 \n    return(A_out)\n</pre> In\u00a0[\u00a0]: Copied! <pre># UNIT TESTS\ntest_c3(my_dense_v)\n</pre> # UNIT TESTS test_c3(my_dense_v) <p>The following cell builds a three-layer neural network utilizing the <code>my_dense_v</code> subroutine above.</p> In\u00a0[\u00a0]: Copied! <pre>def my_sequential_v(X, W1, b1, W2, b2, W3, b3):\n    A1 = my_dense_v(X,  W1, b1, sigmoid)\n    A2 = my_dense_v(A1, W2, b2, sigmoid)\n    A3 = my_dense_v(A2, W3, b3, sigmoid)\n    return(A3)\n</pre> def my_sequential_v(X, W1, b1, W2, b2, W3, b3):     A1 = my_dense_v(X,  W1, b1, sigmoid)     A2 = my_dense_v(A1, W2, b2, sigmoid)     A3 = my_dense_v(A2, W3, b3, sigmoid)     return(A3) <p>We can again copy trained weights and biases from Tensorflow.</p> In\u00a0[\u00a0]: Copied! <pre>W1_tmp,b1_tmp = layer1.get_weights()\nW2_tmp,b2_tmp = layer2.get_weights()\nW3_tmp,b3_tmp = layer3.get_weights()\n</pre> W1_tmp,b1_tmp = layer1.get_weights() W2_tmp,b2_tmp = layer2.get_weights() W3_tmp,b3_tmp = layer3.get_weights() <p>Let's make a prediction with the new model. This will make a prediction on all of the examples at once. Note the shape of the output.</p> In\u00a0[\u00a0]: Copied! <pre>Prediction = my_sequential_v(X, W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp )\nPrediction.shape\n</pre> Prediction = my_sequential_v(X, W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp ) Prediction.shape <p>We'll apply a threshold of 0.5 as before, but to all predictions at once.</p> In\u00a0[\u00a0]: Copied! <pre>Yhat = (Prediction &gt;= 0.5).numpy().astype(int)\nprint(\"predict a zero: \",Yhat[0], \"predict a one: \", Yhat[500])\n</pre> Yhat = (Prediction &gt;= 0.5).numpy().astype(int) print(\"predict a zero: \",Yhat[0], \"predict a one: \", Yhat[500]) <p>Run the following cell to see predictions. This will use the predictions we just calculated above. This takes a moment to run.</p> In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8, 8, figsize=(8, 8))\nfig.tight_layout(pad=0.1, rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i, ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20, 20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n   \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")\n    ax.set_axis_off() \nfig.suptitle(\"Label, Yhat\", fontsize=16)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8, 8, figsize=(8, 8)) fig.tight_layout(pad=0.1, rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]  for i, ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20, 20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')         # Display the label above the image     ax.set_title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")     ax.set_axis_off()  fig.suptitle(\"Label, Yhat\", fontsize=16) plt.show() <p>You can see how one of the misclassified images looks.</p> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(1, 1))\nerrors = np.where(y != Yhat)\nrandom_index = errors[0][0]\nX_random_reshaped = X[random_index].reshape((20, 20)).T\nplt.imshow(X_random_reshaped, cmap='gray')\nplt.title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\")\nplt.axis('off')\nplt.show()\n</pre> fig = plt.figure(figsize=(1, 1)) errors = np.where(y != Yhat) random_index = errors[0][0] X_random_reshaped = X[random_index].reshape((20, 20)).T plt.imshow(X_random_reshaped, cmap='gray') plt.title(f\"{y[random_index,0]}, {Yhat[random_index, 0]}\") plt.axis('off') plt.show() <p></p> <p></p> <p>In the last example,  $\\mathbf{Z}=\\mathbf{XW} + \\mathbf{b}$ utilized NumPy broadcasting to expand the vector $\\mathbf{b}$. If you are not familiar with NumPy Broadcasting, this short tutorial is provided.</p> <p>$\\mathbf{XW}$  is a matrix-matrix operation with dimensions $(m,j_1)(j_1,j_2)$ which results in a matrix with dimension  $(m,j_2)$. To that, we add a vector $\\mathbf{b}$ with dimension $(j_2,)$.  $\\mathbf{b}$ must be expanded to be a $(m,j_2)$ matrix for this element-wise operation to make sense. This expansion is accomplished for you by NumPy broadcasting.</p> <p>Broadcasting applies to element-wise operations. Its basic operation is to 'stretch' a smaller dimension by replicating elements to match a larger dimension.</p> <p>More specifically: When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when</p> <ul> <li>they are equal, or</li> <li>one of them is 1</li> </ul> <p>If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.</p> <p>Here are some examples:</p> Calculating Broadcast Result shape <p>The graphic below describes expanding dimensions. Note the red text below:</p> Broadcast notionally expands arguments to match for element wise operations <p>The graphic above shows NumPy expanding the arguments to match before the final operation. Note that this is a notional description. The actual mechanics of NumPy operation choose the most efficient implementation.</p> <p>For each of the following examples, try to guess the size of the result before running the example.</p> In\u00a0[\u00a0]: Copied! <pre>a = np.array([1,2,3]).reshape(-1,1)  #(3,1)\nb = 5\nprint(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\")\n</pre> a = np.array([1,2,3]).reshape(-1,1)  #(3,1) b = 5 print(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\") <p>Note that this applies to all element-wise operations:</p> In\u00a0[\u00a0]: Copied! <pre>a = np.array([1,2,3]).reshape(-1,1)  #(3,1)\nb = 5\nprint(f\"(a * b).shape: {(a * b).shape}, \\na * b = \\n{a * b}\")\n</pre> a = np.array([1,2,3]).reshape(-1,1)  #(3,1) b = 5 print(f\"(a * b).shape: {(a * b).shape}, \\na * b = \\n{a * b}\") Row-Column Element-Wise Operations In\u00a0[\u00a0]: Copied! <pre>a = np.array([1,2,3,4]).reshape(-1,1)\nb = np.array([1,2,3]).reshape(1,-1)\nprint(a)\nprint(b)\nprint(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\")\n</pre> a = np.array([1,2,3,4]).reshape(-1,1) b = np.array([1,2,3]).reshape(1,-1) print(a) print(b) print(f\"(a + b).shape: {(a + b).shape}, \\na + b = \\n{a + b}\") <p>This is the scenario in the dense layer you built above. Adding a 1-D vector $b$ to a (m,j) matrix.</p> Matrix + 1-D Vector In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#neural-networks-for-handwritten-digit-recognition-binary","title":"Neural Networks for Handwritten Digit Recognition, Binary\u00b6","text":"<p>In this exercise, you will use a neural network to recognize the hand-written digits zero and one.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#outline","title":"Outline\u00b6","text":"<ul> <li> 1 - Packages </li> <li> 2 - Neural Networks<ul> <li> 2.1 Problem Statement</li> <li> 2.2 Dataset</li> <li> 2.3 Model representation</li> <li> 2.4 Tensorflow Model Implementation<ul> <li> Exercise 1</li> </ul> </li> <li> 2.5 NumPy Model Implementation (Forward Prop in NumPy)<ul> <li> Exercise 2</li> </ul> </li> <li> 2.6 Vectorized NumPy Model Implementation (Optional)<ul> <li> Exercise 3</li> </ul> </li> <li> 2.7 Congratulations!</li> <li> 2.8 NumPy Broadcasting Tutorial (Optional)</li> </ul> </li> </ul>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#1-packages","title":"1 - Packages\u00b6","text":"<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p> <ul> <li>numpy is the fundamental package for scientific computing with Python.</li> <li>matplotlib is a popular library to plot graphs in Python.</li> <li>tensorflow a popular platform for machine learning.</li> </ul>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#2-neural-networks","title":"2 - Neural Networks\u00b6","text":"<p>In Course 1, you implemented logistic regression. This was extended to handle non-linear boundaries using polynomial regression. For even more complex scenarios such as image recognition, neural networks are preferred.</p> <p></p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#21-problem-statement","title":"2.1 Problem Statement\u00b6","text":"<p>In this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.</p> <p>This exercise will show you how the methods you have learned can be used for this classification task.</p> <p></p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#22-dataset","title":"2.2 Dataset\u00b6","text":"<p>You will start by loading the dataset for this task.</p> <ul> <li><p>The <code>load_data()</code> function shown below loads the data into variables <code>X</code> and <code>y</code></p> </li> <li><p>The data set contains 1000 training examples of handwritten digits $^1$, here limited to zero and one.</p> <ul> <li>Each training example is a 20-pixel x 20-pixel grayscale image of the digit.<ul> <li>Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.</li> <li>The 20 by 20 grid of pixels is \u201cunrolled\u201d into a 400-dimensional vector.</li> <li>Each training example becomes a single row in our data matrix <code>X</code>.</li> <li>This gives us a 1000 x 400 matrix <code>X</code> where every row is a training example of a handwritten digit image.</li> </ul> </li> </ul> </li> </ul> <p>$$X =  \\left(\\begin{array}{cc}  --- (x^{(1)}) --- \\\\ --- (x^{(2)}) --- \\\\ \\vdots \\\\  --- (x^{(m)}) ---  \\end{array}\\right)$$</p> <ul> <li>The second part of the training set is a 1000 x 1 dimensional vector <code>y</code> that contains labels for the training set<ul> <li><code>y = 0</code> if the image is of the digit <code>0</code>, <code>y = 1</code> if the image is of the digit <code>1</code>.</li> </ul> </li> </ul> <p>$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub></p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#221-view-the-variables","title":"2.2.1 View the variables\u00b6","text":"<p>Let's get more familiar with your dataset.</p> <ul> <li>A good place to start is to print out each variable and see what it contains.</li> </ul> <p>The code below prints elements of the variables <code>X</code> and <code>y</code>.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#222-check-the-dimensions-of-your-variables","title":"2.2.2 Check the dimensions of your variables\u00b6","text":"<p>Another way to get familiar with your data is to view its dimensions. Please print the shape of <code>X</code> and <code>y</code> and see how many training examples you have in your dataset.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#223-visualizing-the-data","title":"2.2.3 Visualizing the Data\u00b6","text":"<p>You will begin by visualizing a subset of the training set.</p> <ul> <li>In the cell below, the code randomly selects 64 rows from <code>X</code>, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.</li> <li>The label for each image is displayed above the image</li> </ul>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#23-model-representation","title":"2.3 Model representation\u00b6","text":"<p>The neural network you will use in this assignment is shown in the figure below.</p> <ul> <li>This has three dense layers with sigmoid activations.<ul> <li>Recall that our inputs are pixel values of digit images.</li> <li>Since the images are of size $20\\times20$, this gives us $400$ inputs</li> </ul> </li> </ul>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#24-tensorflow-model-implementation","title":"2.4 Tensorflow Model Implementation\u00b6","text":""},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Below, using Keras Sequential model and Dense Layer with a sigmoid activation to construct the network described above.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#25-numpy-model-implementation-forward-prop-in-numpy","title":"2.5 NumPy Model Implementation (Forward Prop in NumPy)\u00b6","text":"<p>As described in lecture, it is possible to build your own dense layer using NumPy. This can then be utilized to build a multi-layer neural network.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Below, build a dense layer subroutine. The example in lecture utilized a for loop to visit each unit (<code>j</code>) in the layer and perform the dot product of the weights for that unit (<code>W[:,j]</code>) and sum the bias for the unit (<code>b[j]</code>) to form <code>z</code>. An activation function <code>g(z)</code> is then applied to that result. This section will not utilize some of the matrix operations described in the optional lectures. These will be explored in a later section.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#26-vectorized-numpy-model-implementation-optional","title":"2.6 Vectorized NumPy Model Implementation (Optional)\u00b6","text":"<p>The optional lectures described vector and matrix operations that can be used to speed the calculations. Below describes a layer operation that computes the output for all units in a layer on a given input example:</p> <p>We can demonstrate this using the examples <code>X</code> and the <code>W1</code>,<code>b1</code> parameters above. We use <code>np.matmul</code> to perform the matrix multiply. Note, the dimensions of x and W must be compatible as shown in the diagram above.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Below, compose a new <code>my_dense_v</code> subroutine that performs the layer calculations for a matrix of examples. This will utilize <code>np.matmul()</code>.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#27-congratulations","title":"2.7 Congratulations!\u00b6","text":"<p>You have successfully built and utilized a neural network.</p>"},{"location":"DeepLearning/part1/archive/C2_W1_Assignment-Copy1/#28-numpy-broadcasting-tutorial-optional","title":"2.8 NumPy Broadcasting Tutorial (Optional)\u00b6","text":""},{"location":"DeepLearning/part2/Assignment/","title":"Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.activations import linear, relu, sigmoid\n%matplotlib widget\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\n\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n\nfrom public_tests import * \n\nfrom autils import *\nfrom lab_utils_softmax import plt_softmax\nnp.set_printoptions(precision=2)\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.activations import linear, relu, sigmoid %matplotlib widget import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle')  import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0)  from public_tests import *   from autils import * from lab_utils_softmax import plt_softmax np.set_printoptions(precision=2) In\u00a0[2]: Copied! <pre>plt_act_trio()\n</pre> plt_act_trio() <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre>  The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.      The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? This enables multiple units to contribute to to the resulting function without interfering. This is examined more in the supporting optional lab.     <p>The softmax function can be written: $$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$</p> <p>Where $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ and N is the number of feature/categories in the output layer.</p> <p></p> In\u00a0[7]: Copied! <pre># UNQ_C1\n# GRADED CELL: my_softmax\n\ndef my_softmax(z):  \n    \"\"\" Softmax converts a vector of values to a probability distribution.\n    Args:\n      z (ndarray (N,))  : input data, N features\n    Returns:\n      a (ndarray (N,))  : softmax of z\n    \"\"\"    \n    ### START CODE HERE ### \n    ez = np.exp(z)\n    a = ez/np.sum(ez)\n    ### END CODE HERE ### \n    return a\n</pre> # UNQ_C1 # GRADED CELL: my_softmax  def my_softmax(z):       \"\"\" Softmax converts a vector of values to a probability distribution.     Args:       z (ndarray (N,))  : input data, N features     Returns:       a (ndarray (N,))  : softmax of z     \"\"\"         ### START CODE HERE ###      ez = np.exp(z)     a = ez/np.sum(ez)     ### END CODE HERE ###      return a In\u00a0[8]: Copied! <pre>z = np.array([1., 2., 3., 4.])\na = my_softmax(z)\natf = tf.nn.softmax(z)\nprint(f\"my_softmax(z):         {a}\")\nprint(f\"tensorflow softmax(z): {atf}\")\n\n# BEGIN UNIT TEST  \ntest_my_softmax(my_softmax)\n# END UNIT TEST  \n</pre> z = np.array([1., 2., 3., 4.]) a = my_softmax(z) atf = tf.nn.softmax(z) print(f\"my_softmax(z):         {a}\") print(f\"tensorflow softmax(z): {atf}\")  # BEGIN UNIT TEST   test_my_softmax(my_softmax) # END UNIT TEST   <pre>my_softmax(z):         [0.03 0.09 0.24 0.64]\ntensorflow softmax(z): [0.03 0.09 0.24 0.64]\n All tests passed.\n</pre> Click for hints     One implementation uses for loop to first build the denominator and then a second loop to calculate each output.  <pre>def my_softmax(z):  \n    N = len(z)\n    a =                     # initialize a to zeros \n    ez_sum =                # initialize sum to zero\n    for k in range(N):      # loop over number of outputs             \n        ez_sum +=           # sum exp(z[k]) to build the shared denominator      \n    for j in range(N):      # loop over number of outputs again                \n        a[j] =              # divide each the exp of each output by the denominator   \n    return(a)\n</pre> Click for code <pre>def my_softmax(z):  \n    N = len(z)\n    a = np.zeros(N)\n    ez_sum = 0\n    for k in range(N):                \n        ez_sum += np.exp(z[k])       \n    for j in range(N):                \n        a[j] = np.exp(z[j])/ez_sum   \n    return(a)\n\nOr, a vector implementation:\n\ndef my_softmax(z):  \n    ez = np.exp(z)              \n    a = ez/np.sum(ez)           \n    return(a)\n</pre> <p>Below, vary the values of the <code>z</code> inputs. Note in particular how the exponential in the numerator magnifies small differences in the values. Note as well that the output values sum to one.</p> In\u00a0[9]: Copied! <pre>plt.close(\"all\")\nplt_softmax(my_softmax)\n</pre> plt.close(\"all\") plt_softmax(my_softmax) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p></p> In\u00a0[10]: Copied! <pre># load dataset\nX, y = load_data()\n</pre> # load dataset X, y = load_data() In\u00a0[\u00a0]: Copied! <pre>print ('The first element of X is: ', X[0])\n</pre> print ('The first element of X is: ', X[0]) In\u00a0[11]: Copied! <pre>print ('The first element of y is: ', y[0,0])\nprint ('The last element of y is: ', y[-1,0])\n</pre> print ('The first element of y is: ', y[0,0]) print ('The last element of y is: ', y[-1,0]) <pre>The first element of y is:  0\nThe last element of y is:  9\n</pre> In\u00a0[12]: Copied! <pre>print ('The shape of X is: ' + str(X.shape))\nprint ('The shape of y is: ' + str(y.shape))\n</pre> print ('The shape of X is: ' + str(X.shape)) print ('The shape of y is: ' + str(y.shape)) <pre>The shape of X is: (5000, 400)\nThe shape of y is: (5000, 1)\n</pre> In\u00a0[13]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(5,5))\nfig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\n\n#fig.tight_layout(pad=0.5)\nwidgvis(fig)\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Display the label above the image\n    ax.set_title(y[random_index,0])\n    ax.set_axis_off()\n    fig.suptitle(\"Label, image\", fontsize=14)\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(5,5)) fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]  #fig.tight_layout(pad=0.5) widgvis(fig) for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Display the label above the image     ax.set_title(y[random_index,0])     ax.set_axis_off()     fig.suptitle(\"Label, image\", fontsize=14) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p></p> <ul> <li><p>The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $10$ output units in layer 3, one for each digit.</p> <ul> <li><p>Recall that the dimensions of these parameters is determined as follows:</p> <ul> <li>If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then<ul> <li>$W$ will be of dimension $s_{in} \\times s_{out}$.</li> <li>$b$ will be a vector with $s_{out}$ elements</li> </ul> </li> </ul> </li> <li><p>Therefore, the shapes of <code>W</code>, and <code>b</code>,  are</p> <ul> <li>layer1: The shape of <code>W1</code> is (400, 25) and the shape of <code>b1</code> is (25,)</li> <li>layer2: The shape of <code>W2</code> is (25, 15) and the shape of <code>b2</code> is: (15,)</li> <li>layer3: The shape of <code>W3</code> is (15, 10) and the shape of <code>b3</code> is: (10,)</li> </ul> </li> </ul> </li> </ul> <p>Note: The bias vector <code>b</code> could be represented as a 1-D (n,) or 2-D (n,1) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention:</p> <p></p> <p>Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's output dimensions and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the <code>model.fit</code> statement below.</p> <p>Note: It is also possible to add an input layer that specifies the input dimension of the first layer. For example: <code>tf.keras.Input(shape=(400,)),    #specify input shape</code> We will include that here to illuminate some model sizing.</p> <p></p> <p></p> In\u00a0[16]: Copied! <pre># UNQ_C2\n# GRADED CELL: Sequential model\ntf.random.set_seed(1234) # for consistent results\nmodel = Sequential(\n    [               \n        ### START CODE HERE ### \n        tf.keras.layers.InputLayer((400,)),\n        tf.keras.layers.Dense(25, activation=\"relu\", name=\"L1\"),\n        tf.keras.layers.Dense(15, activation=\"relu\", name=\"L2\"),\n        tf.keras.layers.Dense(10, activation=\"linear\", name=\"L3\")\n        ### END CODE HERE ### \n    ], name = \"my_model\" \n)\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n</pre> # UNQ_C2 # GRADED CELL: Sequential model tf.random.set_seed(1234) # for consistent results model = Sequential(     [                        ### START CODE HERE ###          tf.keras.layers.InputLayer((400,)),         tf.keras.layers.Dense(25, activation=\"relu\", name=\"L1\"),         tf.keras.layers.Dense(15, activation=\"relu\", name=\"L2\"),         tf.keras.layers.Dense(10, activation=\"linear\", name=\"L3\")         ### END CODE HERE ###      ], name = \"my_model\"  ) model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) In\u00a0[17]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"my_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n L1 (Dense)                  (None, 25)                10025     \n                                                                 \n L2 (Dense)                  (None, 15)                390       \n                                                                 \n L3 (Dense)                  (None, 10)                160       \n                                                                 \n=================================================================\nTotal params: 10,575\nTrainable params: 10,575\nNon-trainable params: 0\n_________________________________________________________________\n</pre> Expected Output (Click to expand) The `model.summary()` function displays a useful summary of the model. Note, the names of the layers may vary as they are auto-generated unless the name is specified.      <pre><code>Model: \"my_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nL1 (Dense)                   (None, 25)                10025     \n_________________________________________________________________\nL2 (Dense)                   (None, 15)                390       \n_________________________________________________________________\nL3 (Dense)                   (None, 10)                160       \n=================================================================\nTotal params: 10,575\nTrainable params: 10,575\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> Click for hints <pre>tf.random.set_seed(1234)\nmodel = Sequential(\n    [               \n        ### START CODE HERE ### \n        tf.keras.Input(shape=(400,)),     # @REPLACE \n        Dense(25, activation='relu', name = \"L1\"), # @REPLACE \n        Dense(15, activation='relu',  name = \"L2\"), # @REPLACE  \n        Dense(10, activation='linear', name = \"L3\"),  # @REPLACE \n        ### END CODE HERE ### \n    ], name = \"my_model\" \n)\n</pre> In\u00a0[18]: Copied! <pre># BEGIN UNIT TEST     \ntest_model(model, 10, 400)\n# END UNIT TEST     \n</pre> # BEGIN UNIT TEST      test_model(model, 10, 400) # END UNIT TEST      <pre>All tests passed!\n</pre> <p>The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays as shown below.</p> <p>Let's further examine the weights to verify that tensorflow produced the same dimensions as we calculated above.</p> In\u00a0[19]: Copied! <pre>[layer1, layer2, layer3] = model.layers\n</pre> [layer1, layer2, layer3] = model.layers In\u00a0[20]: Copied! <pre>#### Examine Weights shapes\nW1,b1 = layer1.get_weights()\nW2,b2 = layer2.get_weights()\nW3,b3 = layer3.get_weights()\nprint(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\nprint(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\nprint(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n</pre> #### Examine Weights shapes W1,b1 = layer1.get_weights() W2,b2 = layer2.get_weights() W3,b3 = layer3.get_weights() print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\") print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\") print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\") <pre>W1 shape = (400, 25), b1 shape = (25,)\nW2 shape = (25, 15), b2 shape = (15,)\nW3 shape = (15, 10), b3 shape = (10,)\n</pre> <p>Expected Output</p> <pre><code>W1 shape = (400, 25), b1 shape = (25,)  \nW2 shape = (25, 15), b2 shape = (15,)  \nW3 shape = (15, 10), b3 shape = (10,)\n</code></pre> <p>The following code:</p> <ul> <li>defines a loss function, <code>SparseCategoricalCrossentropy</code> and indicates the softmax should be included with the  loss calculation by adding <code>from_logits=True</code>)</li> <li>defines an optimizer. A popular choice is Adaptive Moment (Adam) which was described in lecture.</li> </ul> In\u00a0[21]: Copied! <pre>model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n)\n\nhistory = model.fit(\n    X,y,\n    epochs=40\n)\n</pre> model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), )  history = model.fit(     X,y,     epochs=40 ) <pre>Epoch 1/40\n157/157 [==============================] - 1s 2ms/step - loss: 1.7094\nEpoch 2/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.7480\nEpoch 3/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.4428\nEpoch 4/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.3463\nEpoch 5/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2977\nEpoch 6/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2630\nEpoch 7/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2361\nEpoch 8/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2131\nEpoch 9/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.2004\nEpoch 10/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1805\nEpoch 11/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1692\nEpoch 12/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1580\nEpoch 13/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1507\nEpoch 14/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1396\nEpoch 15/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1289\nEpoch 16/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1255\nEpoch 17/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1154\nEpoch 18/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1102\nEpoch 19/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.1016\nEpoch 20/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0970\nEpoch 21/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0926\nEpoch 22/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0891\nEpoch 23/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0828\nEpoch 24/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0785\nEpoch 25/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0755\nEpoch 26/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0713\nEpoch 27/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0701\nEpoch 28/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0617\nEpoch 29/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0578\nEpoch 30/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0550\nEpoch 31/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0511\nEpoch 32/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0499\nEpoch 33/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0462\nEpoch 34/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0437\nEpoch 35/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0422\nEpoch 36/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0396\nEpoch 37/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0366\nEpoch 38/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0344\nEpoch 39/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0312\nEpoch 40/40\n157/157 [==============================] - 0s 2ms/step - loss: 0.0294\n</pre> In\u00a0[22]: Copied! <pre>plot_loss_tf(history)\n</pre> plot_loss_tf(history) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> In\u00a0[23]: Copied! <pre>image_of_two = X[1015]\ndisplay_digit(image_of_two)\n\nprediction = model.predict(image_of_two.reshape(1,400))  # prediction\n\nprint(f\" predicting a Two: \\n{prediction}\")\nprint(f\" Largest Prediction index: {np.argmax(prediction)}\")\n</pre> image_of_two = X[1015] display_digit(image_of_two)  prediction = model.predict(image_of_two.reshape(1,400))  # prediction  print(f\" predicting a Two: \\n{prediction}\") print(f\" Largest Prediction index: {np.argmax(prediction)}\") <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <pre> predicting a Two: \n[[ -7.99  -2.23   0.77  -2.41 -11.66 -11.15  -9.53  -3.36  -4.42  -7.17]]\n Largest Prediction index: 2\n</pre> <p>The largest output is prediction[2], indicating the predicted digit is a '2'. If the problem only requires a selection, that is sufficient. Use NumPy argmax to select it. If the problem requires a probability, a softmax is required:</p> In\u00a0[24]: Copied! <pre>prediction_p = tf.nn.softmax(prediction)\n\nprint(f\" predicting a Two. Probability vector: \\n{prediction_p}\")\nprint(f\"Total of predictions: {np.sum(prediction_p):0.3f}\")\n</pre> prediction_p = tf.nn.softmax(prediction)  print(f\" predicting a Two. Probability vector: \\n{prediction_p}\") print(f\"Total of predictions: {np.sum(prediction_p):0.3f}\") <pre> predicting a Two. Probability vector: \n[[1.42e-04 4.49e-02 8.98e-01 3.76e-02 3.61e-06 5.97e-06 3.03e-05 1.44e-02\n  5.03e-03 3.22e-04]]\nTotal of predictions: 1.000\n</pre> <p>To return an integer representing the predicted target, you want the index of the largest probability. This is accomplished with the Numpy argmax function.</p> In\u00a0[25]: Copied! <pre>yhat = np.argmax(prediction_p)\n\nprint(f\"np.argmax(prediction_p): {yhat}\")\n</pre> yhat = np.argmax(prediction_p)  print(f\"np.argmax(prediction_p): {yhat}\") <pre>np.argmax(prediction_p): 2\n</pre> <p>Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run.</p> In\u00a0[26]: Copied! <pre>import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(5,5))\nfig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\nwidgvis(fig)\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n    \n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n    \n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n    \n    # Predict using the Neural Network\n    prediction = model.predict(X[random_index].reshape(1,400))\n    prediction_p = tf.nn.softmax(prediction)\n    yhat = np.argmax(prediction_p)\n    \n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{yhat}\",fontsize=10)\n    ax.set_axis_off()\nfig.suptitle(\"Label, yhat\", fontsize=14)\nplt.show()\n</pre> import warnings warnings.simplefilter(action='ignore', category=FutureWarning) # You do not need to modify anything in this cell  m, n = X.shape  fig, axes = plt.subplots(8,8, figsize=(5,5)) fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top] widgvis(fig) for i,ax in enumerate(axes.flat):     # Select random indices     random_index = np.random.randint(m)          # Select rows corresponding to the random indices and     # reshape the image     X_random_reshaped = X[random_index].reshape((20,20)).T          # Display the image     ax.imshow(X_random_reshaped, cmap='gray')          # Predict using the Neural Network     prediction = model.predict(X[random_index].reshape(1,400))     prediction_p = tf.nn.softmax(prediction)     yhat = np.argmax(prediction_p)          # Display the label above the image     ax.set_title(f\"{y[random_index,0]},{yhat}\",fontsize=10)     ax.set_axis_off() fig.suptitle(\"Label, yhat\", fontsize=14) plt.show() <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p>Let's look at some of the errors.</p> <p>Note: increasing the number of training epochs can eliminate the errors on this data set.</p> In\u00a0[27]: Copied! <pre>print( f\"{display_errors(model,X,y)} errors out of {len(X)} images\")\n</pre> print( f\"{display_errors(model,X,y)} errors out of {len(X)} images\") <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <pre>15 errors out of 5000 images\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/Assignment/#practice-lab-neural-networks-for-handwritten-digit-recognition-multiclass","title":"Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass\u00b6","text":"<p>In this exercise, you will use a neural network to recognize the hand-written digits 0-9.</p>"},{"location":"DeepLearning/part2/Assignment/#outline","title":"Outline\u00b6","text":"<ul> <li> 1 - Packages </li> <li> 2 - ReLU Activation</li> <li> 3 - Softmax Function<ul> <li> Exercise 1</li> </ul> </li> <li> 4 - Neural Networks<ul> <li> 4.1 Problem Statement</li> <li> 4.2 Dataset</li> <li> 4.3 Model representation</li> <li> 4.4 Tensorflow Model Implementation</li> <li> 4.5 Softmax placement<ul> <li> Exercise 2</li> </ul> </li> </ul> </li> </ul>"},{"location":"DeepLearning/part2/Assignment/#1-packages","title":"1 - Packages\u00b6","text":"<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p> <ul> <li>numpy is the fundamental package for scientific computing with Python.</li> <li>matplotlib is a popular library to plot graphs in Python.</li> <li>tensorflow a popular platform for machine learning.</li> </ul>"},{"location":"DeepLearning/part2/Assignment/#2-relu-activation","title":"2 - ReLU Activation\u00b6","text":"<p>This week, a new activation was introduced, the Rectified Linear Unit (ReLU). $$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$</p>"},{"location":"DeepLearning/part2/Assignment/#3-softmax-function","title":"3 - Softmax Function\u00b6","text":"<p>A multiclass neural network generates N outputs. One output is selected as the predicted answer. In the output layer, a vector $\\mathbf{z}$ is generated by a linear function which is fed into a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will sum to 1. They can be interpreted as probabilities. The larger inputs to the softmax will correspond to larger output probabilities.</p>"},{"location":"DeepLearning/part2/Assignment/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Let's create a NumPy implementation:</p>"},{"location":"DeepLearning/part2/Assignment/#4-neural-networks","title":"4 - Neural Networks\u00b6","text":"<p>In last weeks assignment, you implemented a neural network to do binary classification. This week you will extend that to multiclass classification. This will utilize the softmax activation.</p> <p></p>"},{"location":"DeepLearning/part2/Assignment/#41-problem-statement","title":"4.1 Problem Statement\u00b6","text":"<p>In this exercise, you will use a neural network to recognize ten handwritten digits, 0-9. This is a multiclass classification task where one of n choices is selected. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks.</p> <p></p>"},{"location":"DeepLearning/part2/Assignment/#42-dataset","title":"4.2 Dataset\u00b6","text":"<p>You will start by loading the dataset for this task.</p> <ul> <li><p>The <code>load_data()</code> function shown below loads the data into variables <code>X</code> and <code>y</code></p> </li> <li><p>The data set contains 5000 training examples of handwritten digits $^1$.</p> <ul> <li>Each training example is a 20-pixel x 20-pixel grayscale image of the digit.<ul> <li>Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.</li> <li>The 20 by 20 grid of pixels is \u201cunrolled\u201d into a 400-dimensional vector.</li> <li>Each training examples becomes a single row in our data matrix <code>X</code>.</li> <li>This gives us a 5000 x 400 matrix <code>X</code> where every row is a training example of a handwritten digit image.</li> </ul> </li> </ul> </li> </ul> <p>$$X =  \\left(\\begin{array}{cc}  --- (x^{(1)}) --- \\\\ --- (x^{(2)}) --- \\\\ \\vdots \\\\  --- (x^{(m)}) ---  \\end{array}\\right)$$</p> <ul> <li>The second part of the training set is a 5000 x 1 dimensional vector <code>y</code> that contains labels for the training set<ul> <li><code>y = 0</code> if the image is of the digit <code>0</code>, <code>y = 4</code> if the image is of the digit <code>4</code> and so on.</li> </ul> </li> </ul> <p>$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub></p>"},{"location":"DeepLearning/part2/Assignment/#421-view-the-variables","title":"4.2.1 View the variables\u00b6","text":"<p>Let's get more familiar with your dataset.</p> <ul> <li>A good place to start is to print out each variable and see what it contains.</li> </ul> <p>The code below prints the first element in the variables <code>X</code> and <code>y</code>.</p>"},{"location":"DeepLearning/part2/Assignment/#422-check-the-dimensions-of-your-variables","title":"4.2.2 Check the dimensions of your variables\u00b6","text":"<p>Another way to get familiar with your data is to view its dimensions. Please print the shape of <code>X</code> and <code>y</code> and see how many training examples you have in your dataset.</p>"},{"location":"DeepLearning/part2/Assignment/#423-visualizing-the-data","title":"4.2.3 Visualizing the Data\u00b6","text":"<p>You will begin by visualizing a subset of the training set.</p> <ul> <li>In the cell below, the code randomly selects 64 rows from <code>X</code>, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.</li> <li>The label for each image is displayed above the image</li> </ul>"},{"location":"DeepLearning/part2/Assignment/#43-model-representation","title":"4.3 Model representation\u00b6","text":"<p>The neural network you will use in this assignment is shown in the figure below.</p> <ul> <li>This has two dense layers with ReLU activations followed by an output layer with a linear activation.<ul> <li>Recall that our inputs are pixel values of digit images.</li> <li>Since the images are of size $20\\times20$, this gives us $400$ inputs</li> </ul> </li> </ul>"},{"location":"DeepLearning/part2/Assignment/#44-tensorflow-model-implementation","title":"4.4 Tensorflow Model Implementation\u00b6","text":""},{"location":"DeepLearning/part2/Assignment/#45-softmax-placement","title":"4.5 Softmax placement\u00b6","text":"<p>As described in the lecture and the optional softmax lab, numerical stability is improved if the softmax is grouped with the loss function rather than the output layer during training. This has implications when building the model and using the model. Building:</p> <ul> <li>The final Dense layer should use a 'linear' activation. This is effectively no activation.</li> <li>The <code>model.compile</code> statement will indicate this by including <code>from_logits=True</code>. <code>loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) </code></li> <li>This does not impact the form of the target. In the case of SparseCategorialCrossentropy, the target is the expected digit, 0-9.</li> </ul> <p>Using the model:</p> <ul> <li>The outputs are not probabilities. If output probabilities are desired, apply a softmax function.</li> </ul>"},{"location":"DeepLearning/part2/Assignment/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Below, using Keras Sequential model and Dense Layer with a ReLU activation to construct the three layer network described above.</p>"},{"location":"DeepLearning/part2/Assignment/#epochs-and-batches","title":"Epochs and batches\u00b6","text":"<p>In the <code>compile</code> statement above, the number of <code>epochs</code> was set to 100. This specifies that the entire data set should be applied during training 100 times.  During training, you see output describing the progress of training that looks like this:</p> <pre><code>Epoch 1/100\n157/157 [==============================] - 0s 1ms/step - loss: 2.2770\n</code></pre> <p>The first line, <code>Epoch 1/100</code>, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 5000 examples in our data set or roughly 157 batches. The notation on the 2nd line <code>157/157 [====</code> is describing which batch has been executed.</p>"},{"location":"DeepLearning/part2/Assignment/#loss-cost","title":"Loss  (cost)\u00b6","text":"<p>In course 1, we learned to track the progress of gradient descent by monitoring the cost. Ideally, the cost will decrease as the number of iterations of the algorithm increases. Tensorflow refers to the cost as <code>loss</code>. Above, you saw the loss displayed each epoch as <code>model.fit</code> was executing. The .fit method returns a variety of metrics including the loss. This is captured in the <code>history</code> variable above. This can be used to examine the loss in a plot as shown below.</p>"},{"location":"DeepLearning/part2/Assignment/#prediction","title":"Prediction\u00b6","text":"<p>To make a prediction, use Keras <code>predict</code>. Below, X[1015] contains an image of a two.</p>"},{"location":"DeepLearning/part2/Assignment/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have successfully built and utilized a neural network to do multiclass classification.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/","title":"Optional Lab - Multi-class Classification","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib widget\nfrom sklearn.datasets import make_blobs\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nnp.set_printoptions(precision=2)\nfrom lab_utils_multiclass_TF import *\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt %matplotlib widget from sklearn.datasets import make_blobs import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense np.set_printoptions(precision=2) from lab_utils_multiclass_TF import * import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) In\u00a0[3]: Copied! <pre># make 4-class dataset for classification\nclasses = 4\nm = 100\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nstd = 1.0\nX_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)\n</pre> # make 4-class dataset for classification classes = 4 m = 100 centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]] std = 1.0 X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30) In\u00a0[6]: Copied! <pre>print(X_train.shape)\n</pre> print(X_train.shape) <pre>(100, 2)\n</pre> In\u00a0[7]: Copied! <pre>plt_mc(X_train,y_train,classes, centers, std=std)\n</pre> plt_mc(X_train,y_train,classes, centers, std=std)                      Figure                  <p>Each dot represents a training example. The axis (x0,x1) are the inputs and the color represents the class the example is associated with. Once trained, the model will be presented with a new example, (x0,x1), and will predict the class.</p> <p>While generated, this data set is representative of many real-world classification problems. There are several input features (x0,...,xn) and several output categories. The model is trained to use the input features to predict the correct output category.</p> In\u00a0[8]: Copied! <pre># show classes in data set\nprint(f\"unique classes {np.unique(y_train)}\")\n# show how classes are represented\nprint(f\"class representation {y_train[:10]}\")\n# show shapes of our dataset\nprint(f\"shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}\")\n</pre> # show classes in data set print(f\"unique classes {np.unique(y_train)}\") # show how classes are represented print(f\"class representation {y_train[:10]}\") # show shapes of our dataset print(f\"shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}\") <pre>unique classes [0 1 2 3]\nclass representation [3 3 3 0 3 3 3 3 2 0]\nshape of X_train: (100, 2), shape of y_train: (100,)\n</pre> In\u00a0[9]: Copied! <pre>tf.random.set_seed(1234)  # applied to achieve consistent results\nmodel = Sequential(\n    [\n        Dense(2, activation = 'relu',   name = \"L1\"),\n        Dense(4, activation = 'linear', name = \"L2\")\n    ]\n)\n</pre> tf.random.set_seed(1234)  # applied to achieve consistent results model = Sequential(     [         Dense(2, activation = 'relu',   name = \"L1\"),         Dense(4, activation = 'linear', name = \"L2\")     ] ) <p>The statements below compile and train the network. Setting <code>from_logits=True</code> as an argument to the loss function specifies that the output activation was linear rather than a softmax.</p> In\u00a0[10]: Copied! <pre>model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(0.01),\n)\n\nmodel.fit(\n    X_train,y_train,\n    epochs=200\n)\n</pre> model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=tf.keras.optimizers.Adam(0.01), )  model.fit(     X_train,y_train,     epochs=200 ) <pre>Epoch 1/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 10ms/step - loss: 2.7849\nEpoch 2/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 2.5192 \nEpoch 3/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 2.2868 \nEpoch 4/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 2.0815 \nEpoch 5/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 1.9020 \nEpoch 6/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.7498 \nEpoch 7/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.6209 \nEpoch 8/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.5123 \nEpoch 9/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.4221 \nEpoch 10/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 1.3488 \nEpoch 11/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.2882 \nEpoch 12/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.2367 \nEpoch 13/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.1925 \nEpoch 14/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.1554 \nEpoch 15/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.1239 \nEpoch 16/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.0964 \nEpoch 17/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.0723 \nEpoch 18/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.0505 \nEpoch 19/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 1.0307 \nEpoch 20/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 1.0123 \nEpoch 21/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.9948 \nEpoch 22/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.9780 \nEpoch 23/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.9620 \nEpoch 24/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.9462 \nEpoch 25/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.9304 \nEpoch 26/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.9148 \nEpoch 27/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.8994 \nEpoch 28/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.8842 \nEpoch 29/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.8694 \nEpoch 30/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.8550 \nEpoch 31/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.8408 \nEpoch 32/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.8269 \nEpoch 33/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.8134 \nEpoch 34/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.8005 \nEpoch 35/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.7880 \nEpoch 36/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.7762 \nEpoch 37/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.7649 \nEpoch 38/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.7540 \nEpoch 39/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.7434 \nEpoch 40/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.7332 \nEpoch 41/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.7232 \nEpoch 42/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.7135 \nEpoch 43/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.7041 \nEpoch 44/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6950 \nEpoch 45/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.6861 \nEpoch 46/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.6774 \nEpoch 47/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.6690 \nEpoch 48/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.6609 \nEpoch 49/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.6530 \nEpoch 50/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6454 \nEpoch 51/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6379 \nEpoch 52/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.6307 \nEpoch 53/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6237 \nEpoch 54/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6168 \nEpoch 55/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6100 \nEpoch 56/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.6033 \nEpoch 57/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.5968 \nEpoch 58/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5904 \nEpoch 59/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5840 \nEpoch 60/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5778 \nEpoch 61/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.5717 \nEpoch 62/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.5657 \nEpoch 63/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5598 \nEpoch 64/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.5540 \nEpoch 65/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5483 \nEpoch 66/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5427 \nEpoch 67/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5371 \nEpoch 68/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.5315 \nEpoch 69/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.5260 \nEpoch 70/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.5204 \nEpoch 71/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.5145 \nEpoch 72/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.5083 \nEpoch 73/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.5020 \nEpoch 74/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4951 \nEpoch 75/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4879 \nEpoch 76/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4808 \nEpoch 77/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4738 \nEpoch 78/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4669 \nEpoch 79/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.4602 \nEpoch 80/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.4536 \nEpoch 81/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.4470 \nEpoch 82/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4401 \nEpoch 83/200\n</pre> <pre>4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.4330 \nEpoch 84/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4260 \nEpoch 85/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4192 \nEpoch 86/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4127 \nEpoch 87/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.4061 \nEpoch 88/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3994 \nEpoch 89/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3929 \nEpoch 90/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3866 \nEpoch 91/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3804 \nEpoch 92/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3742 \nEpoch 93/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3681 \nEpoch 94/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3615 \nEpoch 95/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3549 \nEpoch 96/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3485 \nEpoch 97/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3424 \nEpoch 98/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3365 \nEpoch 99/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3309 \nEpoch 100/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3255 \nEpoch 101/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3204 \nEpoch 102/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.3151 \nEpoch 103/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3094 \nEpoch 104/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.3037 \nEpoch 105/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2982 \nEpoch 106/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2928 \nEpoch 107/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2876 \nEpoch 108/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2826 \nEpoch 109/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2778 \nEpoch 110/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2733 \nEpoch 111/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2689 \nEpoch 112/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2646 \nEpoch 113/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2606 \nEpoch 114/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2567 \nEpoch 115/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2528 \nEpoch 116/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2485 \nEpoch 117/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2443 \nEpoch 118/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2402 \nEpoch 119/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2362 \nEpoch 120/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2324 \nEpoch 121/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2288 \nEpoch 122/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2253 \nEpoch 123/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2219 \nEpoch 124/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 9ms/step - loss: 0.2187 \nEpoch 125/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2155 \nEpoch 126/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.2124 \nEpoch 127/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.2094 \nEpoch 128/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2066 \nEpoch 129/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2038 \nEpoch 130/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.2011 \nEpoch 131/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1986 \nEpoch 132/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1961 \nEpoch 133/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1938 \nEpoch 134/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1915 \nEpoch 135/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.1893 \nEpoch 136/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.1872 \nEpoch 137/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.1849 \nEpoch 138/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1822 \nEpoch 139/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1793 \nEpoch 140/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1763 \nEpoch 141/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1734 \nEpoch 142/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1705 \nEpoch 143/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1678 \nEpoch 144/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1651 \nEpoch 145/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1627 \nEpoch 146/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1603 \nEpoch 147/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1581 \nEpoch 148/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1559 \nEpoch 149/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1539 \nEpoch 150/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1519 \nEpoch 151/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.1501 \nEpoch 152/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1482 \nEpoch 153/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1461 \nEpoch 154/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1439 \nEpoch 155/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1416 \nEpoch 156/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1391 \nEpoch 157/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1365 \nEpoch 158/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1338 \nEpoch 159/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1312 \nEpoch 160/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1283 \nEpoch 161/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1253 \nEpoch 162/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1223 \nEpoch 163/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1193 \nEpoch 164/200\n</pre> <pre>4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1163 \nEpoch 165/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.1134 \nEpoch 166/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1105 \nEpoch 167/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1077 \nEpoch 168/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.1049 \nEpoch 169/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.1022 \nEpoch 170/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0995 \nEpoch 171/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0969 \nEpoch 172/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0944 \nEpoch 173/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0919 \nEpoch 174/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0894 \nEpoch 175/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0871 \nEpoch 176/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.0848 \nEpoch 177/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0825 \nEpoch 178/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0804 \nEpoch 179/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0783 \nEpoch 180/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0763 \nEpoch 181/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0743 \nEpoch 182/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0724 \nEpoch 183/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0705 \nEpoch 184/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0688 \nEpoch 185/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0670 \nEpoch 186/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0654 \nEpoch 187/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0638 \nEpoch 188/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0622 \nEpoch 189/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0607 \nEpoch 190/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0592 \nEpoch 191/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.0578 \nEpoch 192/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0565 \nEpoch 193/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0552 \nEpoch 194/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0539 \nEpoch 195/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 8ms/step - loss: 0.0528 \nEpoch 196/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0516 \nEpoch 197/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0505 \nEpoch 198/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 0.0495 \nEpoch 199/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0484 \nEpoch 200/200\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 0.0474 \n</pre> Out[10]: <pre>&lt;keras.src.callbacks.history.History at 0x1ebcfe11330&gt;</pre> <p>With the model trained, we can see how the model has classified the training data.</p> In\u00a0[11]: Copied! <pre>plt_cat_mc(X_train, y_train, model, classes)\n</pre> plt_cat_mc(X_train, y_train, model, classes) <pre>184/184 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 882us/step\n</pre>                      Figure                  <p>Above, the decision boundaries show how the model has partitioned the input space.  This very simple model has had no trouble classifying the training data. How did it accomplish this? Let's look at the network in more detail.</p> <p>Below, we will pull the trained weights from the model and use that to plot the function of each of the network units. Further down, there is a more detailed explanation of the results. You don't need to know these details to successfully use neural networks, but it may be helpful to gain more intuition about how the layers combine to solve a classification problem.</p> In\u00a0[12]: Copied! <pre># gather the trained parameters from the first layer\nl1 = model.get_layer(\"L1\")\nW1,b1 = l1.get_weights()\n</pre> # gather the trained parameters from the first layer l1 = model.get_layer(\"L1\") W1,b1 = l1.get_weights() In\u00a0[13]: Copied! <pre># plot the function of the first layer\nplt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)\n</pre> # plot the function of the first layer plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes) <pre>D:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\nD:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n</pre>                      Figure                  In\u00a0[14]: Copied! <pre># gather the trained parameters from the output layer\nl2 = model.get_layer(\"L2\")\nW2, b2 = l2.get_weights()\n# create the 'new features', the training examples after L1 transformation\nXl2 = np.maximum(0, np.dot(X_train,W1) + b1)\n\nplt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,\n                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))\n</pre> # gather the trained parameters from the output layer l2 = model.get_layer(\"L2\") W2, b2 = l2.get_weights() # create the 'new features', the training examples after L1 transformation Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)  plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,                         x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1]))) <pre>D:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\nD:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\nD:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\nD:\\AI\\Machine-Learning-Specialization-Coursera\\C2 - Advanced Learning Algorithms\\week2\\optional-labs\\lab_utils_multiclass_TF.py:63: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n  ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n</pre>                      Figure                  Layer 2, the output layer   <p>The dots in these graphs are the training examples translated by the first layer. One way to think of this is the first layer has created a new set of features for evaluation by the 2nd layer. The axes in these plots are the outputs of the previous layer $a^{[1]}_0$ and $a^{[1]}_1$. As predicted above, classes 0 and 1 (blue and green) have  $a^{[1]}_0 = 0$ while classes 0 and 2 (blue and orange) have $a^{[1]}_1 = 0$. Once again, the intensity of the background color indicates the highest values. Unit 0 will produce its maximum value for values near (0,0), where class 0 (blue) has been mapped. Unit 1 produces its highest values in the upper left corner selecting class 1 (green). Unit 2 targets the lower right corner where class 2 (orange) resides. Unit 3 produces its highest values in the upper right selecting our final class (purple).</p> <p>One other aspect that is not obvious from the graphs is that the values have been coordinated between the units. It is not sufficient for a unit to produce a maximum value for the class it is selecting for, it must also be the highest value of all the units for points in that class. This is done by the implied softmax function that is part of the loss function (<code>SparseCategoricalCrossEntropy</code>). Unlike other activation functions, the softmax works across all the outputs.</p> <p>You can successfully use neural networks without knowing the details of what each unit is up to. Hopefully, this example has provided some intuition about what is happening under the hood.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/Multiclass_TF/#optional-lab-multi-class-classification","title":"Optional Lab - Multi-class Classification\u00b6","text":""},{"location":"DeepLearning/part2/Multiclass_TF/#11-goals","title":"1.1 Goals\u00b6","text":"<p>In this lab, you will explore an example of multi-class classification using neural networks.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#12-tools","title":"1.2 Tools\u00b6","text":"<p>You will use some plotting routines. These are stored in <code>lab_utils_multiclass_TF.py</code> in this directory.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#20-multi-class-classification","title":"2.0 Multi-class Classification\u00b6","text":"<p>Neural Networks are often used to classify data. Examples are neural networks:</p> <ul> <li>take in photos and classify subjects in the photos as {dog,cat,horse,other}</li> <li>take in a sentence and classify the 'parts of speech' of its elements: {noun, verb, adjective etc..}</li> </ul> <p>A network of this type will have multiple units in its final layer. Each output is associated with a category. When an input example is applied to the network, the output with the highest value is the category predicted. If the output is applied to a softmax function, the output of the softmax will provide probabilities of the input being in each category.</p> <p>In this lab you will see an example of building a multiclass network in Tensorflow. We will then take a look at how the neural network makes its predictions.</p> <p>Let's start by creating a four-class data set.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#21-prepare-and-visualize-our-data","title":"2.1 Prepare and visualize our data\u00b6","text":"<p>We will use Scikit-Learn <code>make_blobs</code> function to make a training data set with 4 categories as shown in the plot below.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#22-model","title":"2.2 Model\u00b6","text":"This lab will use a 2-layer network as shown. Unlike the binary classification networks, this network has four outputs, one for each class. Given an input example, the output with the highest value is the predicted class of the input.     <p>Below is an example of how to construct this network in Tensorflow. Notice the output layer uses a <code>linear</code> rather than a <code>softmax</code> activation. While it is possible to include the softmax in the output layer, it is more numerically stable if linear outputs are passed to the loss function during training. If the model is used to predict probabilities, the softmax can be applied at that point.</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#explanation","title":"Explanation\u00b6","text":"Layer 1  <p>These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ($x_0,x_1$) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output, $a^{[1]}_j$ being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu.</p> <p>Unit 0 has separated classes 0 and 1 from classes 2 and 3. Points to the left of the line (classes 0 and 1) will output zero, while points to the right will output a value greater than zero. Unit 1 has separated classes 0 and 2 from classes 1 and 3. Points above the line (classes 0 and 2 ) will output a zero, while points below will output a value greater than zero. Let's see how this works out in the next layer!</p>"},{"location":"DeepLearning/part2/Multiclass_TF/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have learned to build and operate a neural network for multiclass classification.</p>"},{"location":"DeepLearning/part2/Relu/","title":"Optional Lab - ReLU activation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LeakyReLU\nfrom tensorflow.keras.activations import linear, relu, sigmoid\n%matplotlib widget\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\nfrom autils import plt_act_trio\nfrom lab_utils_relu import *\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, LeakyReLU from tensorflow.keras.activations import linear, relu, sigmoid %matplotlib widget from matplotlib.widgets import Slider from lab_utils_common import dlc from autils import plt_act_trio from lab_utils_relu import * import warnings warnings.simplefilter(action='ignore', category=UserWarning)  In\u00a0[2]: Copied! <pre>plt_act_trio()\n</pre> plt_act_trio()                      Figure                   The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.      The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? Let's examine this below.     <p>The exercise will use the network below in a regression problem where you must model a piecewise linear target :  The network has 3 units in the first layer. Each is required to form the target. Unit 0 is pre-programmed and fixed to map the first segment. You will modify weights and biases in unit 1 and 2 to model the 2nd and 3rd segment. The output unit is also fixed and simply sums the outputs of the first layer.</p> <p>Using the sliders below, modify weights and bias to match the target. Hints: Start with <code>w1</code> and <code>b1</code> and leave <code>w2</code> and <code>b2</code> zero until you match the 2nd segment. Clicking rather than sliding is quicker.  If you have trouble, don't worry, the text below will describe this in more detail.</p> In\u00a0[3]: Copied! <pre>_ = plt_relu_ex()\n</pre> _ = plt_relu_ex()                      Figure                  <p>The goal of this exercise is to appreciate how the ReLU's non-linear behavior provides the needed ability to turn functions off until they are needed. Let's see how this worked in this example.  The plots on the right contain the output of the units in the first layer. Starting at the top, unit 0 is responsible for the first segment marked with a 1. Both the linear function $z$ and the function following the ReLU $a$ are shown. You can see that the ReLU cuts off the function after the interval [0,1]. This is important as it prevents Unit 0 from interfering with the following segment.</p> <p>Unit 1 is responsible for the 2nd segment. Here the ReLU kept this unit quiet until after x is 1. Since the first unit is not contributing, the slope for unit 1, $w^{[1]}_1$, is just the slope of the target line. The bias must be adjusted to keep the output negative until x has reached 1. Note how the contribution of Unit 1 extends to the 3rd segment as well.</p> <p>Unit 2 is responsible for the 3rd segment. The ReLU again zeros the output until x reaches the right value.The slope of the unit, $w^{[1]}_2$, must be set so that the sum of unit 1 and 2 have the desired slope. The bias is again adjusted to keep the output negative until x has reached 2.</p> <p>The \"off\" or disable feature  of the ReLU activation enables models to stitch together linear segments to model complex non-linear functions.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/Relu/#optional-lab-relu-activation","title":"Optional Lab - ReLU activation\u00b6","text":""},{"location":"DeepLearning/part2/Relu/#2-relu-activation","title":"2 - ReLU Activation\u00b6","text":"<p>This week, a new activation was introduced, the Rectified Linear Unit (ReLU). $$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$</p>"},{"location":"DeepLearning/part2/Relu/#why-non-linear-activations","title":"Why Non-Linear Activations?\u00b6","text":"<p> The function shown is composed of linear pieces (piecewise linear). The slope is consistent during the linear portion and then changes abruptly at transition points. At transition points, a new linear function is added which, when added to the existing function, will produce the new slope. The new function is added at transition point but does not contribute to the output prior to that point. The non-linear activation function is responsible for disabling the input prior to and sometimes after the transition points. The following exercise provides a more tangible example.</p> <p>\u975e\u7ebf\u6027\u51fd\u6570\u6bd4\u5982ReLU\u53ef\u4ee5\u8ba9y-x\u66f2\u7ebf\u4ea7\u751f\u8f6c\u6298\u70b9\u3002\u7531\u4e8e\u6bcf\u4e2a\u795e\u7ecf\u5143\u6fc0\u6d3b\u51fd\u6570\u4e0d\u540c\uff0c\u8fd9\u4e2a\u8f6c\u6298\u70b9\u5c31\u4e0d\u4e00\u6837\u3002\u60f3\u8c61\uff1a\u662f\u5426\u5e94\u7528\u5f53\u524dReLU\u53d6\u51b3\u4e8e\u5f53\u524d\u8f93\u5165\u503c\u662f\u5426\u5927\u4e8e0.\u800c\u540c\u4e00\u5c42\u4e2d\uff0c\u4e0d\u540c\u795e\u7ecf\u5143\u6240\u8ba1\u7b97\u7684wx+b\u662f\u6839\u636e\u81ea\u5df1\u7684w\u548cb\u51b3\u5b9a\u4e86\uff0c\u8fd9\u5c31\u4f7f\u5f97\u540c\u4e00\u5c42\u4e0d\u540c\u795e\u7ecf\u5143\u9608\u503c\u4e0d\u4e00\u6837\u3002</p> <p>\u6ce8\u610f\uff0c\u82e5w\u4e3a\u8d1f\uff0cReLU(x)\u53ef\u4ee5\u662f\u51cf\u51fd\u6570\u3002</p>"},{"location":"DeepLearning/part2/Relu/#congratulations","title":"Congratulations!\u00b6","text":"<p>You are now more familiar with the ReLU and the importance of its non-linear behavior.</p>"},{"location":"DeepLearning/part2/SoftMax/","title":"Optional Lab - Softmax Function","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom IPython.display import display, Markdown, Latex\nfrom sklearn.datasets import make_blobs\n%matplotlib widget\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\nfrom lab_utils_softmax import plt_softmax\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from IPython.display import display, Markdown, Latex from sklearn.datasets import make_blobs %matplotlib widget from matplotlib.widgets import Slider from lab_utils_common import dlc from lab_utils_softmax import plt_softmax import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) <p>Note: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N.</p> <p>The softmax function can be written: $$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$ The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write: \\begin{align} \\mathbf{a}(x) = \\begin{bmatrix} P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\ \\vdots \\\\ P(y = N | \\mathbf{x}; \\mathbf{w},b) \\end{bmatrix} = \\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }} \\begin{bmatrix} e^{z_1} \\\\ \\vdots \\\\ e^{z_{N}} \\\\ \\end{bmatrix} \\tag{2} \\end{align}</p> <p>Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$. Let's create a NumPy implementation:</p> In\u00a0[2]: Copied! <pre>def my_softmax(z):\n    ez = np.exp(z)              #element-wise exponenial\n    sm = ez/np.sum(ez)\n    return(sm)\n</pre> def my_softmax(z):     ez = np.exp(z)              #element-wise exponenial     sm = ez/np.sum(ez)     return(sm) <p>Below, vary the values of the <code>z</code> inputs using the sliders.</p> In\u00a0[3]: Copied! <pre>plt.close(\"all\")\nplt_softmax(my_softmax)\n</pre> plt.close(\"all\") plt_softmax(my_softmax) <pre>Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous \u2026</pre> <p>As you are varying the values of the z's above, there are a few things to note:</p> <ul> <li>the exponential in the numerator of the softmax magnifies small differences in the values</li> <li>the output values sum to one</li> <li>the softmax spans all of the outputs. A change in <code>z0</code> for example will change the values of <code>a0</code>-<code>a3</code>. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.</li> </ul> <p>The loss function associated with Softmax, the cross-entropy loss, is: \\begin{equation}   L(\\mathbf{a},y)=\\begin{cases}     -log(a_1), &amp; \\text{if $y=1$}.\\\\         &amp;\\vdots\\\\      -log(a_N), &amp; \\text{if $y=N$}   \\end{cases} \\tag{3} \\end{equation}</p> <p>Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.</p> <p>Recall: In this course, Loss is for one example while Cost covers all examples.</p> <p>Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}     1, &amp; \\text{if $y==n$}.\\\\     0, &amp; \\text{otherwise}.   \\end{cases}$$ Now the cost is: \\begin{align} J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4} \\end{align}</p> <p>Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.</p> In\u00a0[4]: Copied! <pre># make  dataset for example\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nX_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n</pre> # make  dataset for example centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]] X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30) <p>The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the <code>compile</code> directive.</p> <p>The loss function is <code>SparseCategoricalCrossentropy</code>. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities.</p> In\u00a0[5]: Copied! <pre>model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'softmax')    # &lt; softmax activation here\n    ]\n)\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'softmax')    # &lt; softmax activation here     ] ) model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(),     optimizer=tf.keras.optimizers.Adam(0.001), )  model.fit(     X_train,y_train,     epochs=10 )          <pre>Epoch 1/10\n63/63 [==============================] - 0s 966us/step - loss: 0.8312\nEpoch 2/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.3203\nEpoch 3/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.1408\nEpoch 4/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0847\nEpoch 5/10\n63/63 [==============================] - 0s 944us/step - loss: 0.0626\nEpoch 6/10\n63/63 [==============================] - 0s 974us/step - loss: 0.0515\nEpoch 7/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0447\nEpoch 8/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0402\nEpoch 9/10\n63/63 [==============================] - 0s 922us/step - loss: 0.0361\nEpoch 10/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0332\n</pre> Out[5]: <pre>&lt;keras.callbacks.History at 0x7fd399415490&gt;</pre> <p>Because the softmax is integrated into the output layer, the output is a vector of probabilities.</p> In\u00a0[6]: Copied! <pre>p_nonpreferred = model.predict(X_train)\nprint(p_nonpreferred [:2])\nprint(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))\n</pre> p_nonpreferred = model.predict(X_train) print(p_nonpreferred [:2]) print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred)) <pre>[[3.39e-03 1.02e-02 9.73e-01 1.30e-02]\n [9.97e-01 3.46e-03 9.35e-06 9.02e-06]]\nlargest value 0.9999994 smallest value 4.1378247e-09\n</pre> Preferred  <p>Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.   This is enabled by the 'preferred' organization shown here.</p> <p>In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as logits. The loss function has an additional argument: <code>from_logits = True</code>. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation.</p> In\u00a0[7]: Copied! <pre>preferred_model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'linear')   #&lt;-- Note\n    ]\n)\npreferred_model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\npreferred_model.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> preferred_model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'linear')   #&lt;-- Note     ] ) preferred_model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note     optimizer=tf.keras.optimizers.Adam(0.001), )  preferred_model.fit(     X_train,y_train,     epochs=10 )          <pre>Epoch 1/10\n63/63 [==============================] - 0s 1ms/step - loss: 1.0936\nEpoch 2/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.4933\nEpoch 3/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.2809\nEpoch 4/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.1506\nEpoch 5/10\n63/63 [==============================] - 0s 966us/step - loss: 0.0916\nEpoch 6/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0694\nEpoch 7/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0592\nEpoch 8/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0527\nEpoch 9/10\n63/63 [==============================] - 0s 919us/step - loss: 0.0489\nEpoch 10/10\n63/63 [==============================] - 0s 1ms/step - loss: 0.0457\n</pre> Out[7]: <pre>&lt;keras.callbacks.History at 0x7fd23c552050&gt;</pre> In\u00a0[8]: Copied! <pre>p_preferred = preferred_model.predict(X_train)\nprint(f\"two example output vectors:\\n {p_preferred[:2]}\")\nprint(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))\n</pre> p_preferred = preferred_model.predict(X_train) print(f\"two example output vectors:\\n {p_preferred[:2]}\") print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred)) <pre>two example output vectors:\n [[-2.72 -4.45  2.9  -1.37]\n [ 6.42  1.32 -1.32 -6.74]]\nlargest value 12.410254 smallest value -13.030992\n</pre> <p>The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a softmax.</p> In\u00a0[9]: Copied! <pre>sm_preferred = tf.nn.softmax(p_preferred).numpy()\nprint(f\"two example output vectors:\\n {sm_preferred[:2]}\")\nprint(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))\n</pre> sm_preferred = tf.nn.softmax(p_preferred).numpy() print(f\"two example output vectors:\\n {sm_preferred[:2]}\") print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred)) <pre>two example output vectors:\n [[3.55e-03 6.34e-04 9.82e-01 1.38e-02]\n [9.94e-01 6.05e-03 4.35e-04 1.92e-06]]\nlargest value 0.9999995 smallest value 1.5196424e-11\n</pre> <p>To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax().</p> In\u00a0[10]: Copied! <pre>for i in range(5):\n    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")\n</pre> for i in range(5):     print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\") <pre>[-2.72 -4.45  2.9  -1.37], category: 2\n[ 6.42  1.32 -1.32 -6.74], category: 0\n[ 4.64  1.5  -1.31 -5.3 ], category: 0\n[-0.29  3.76 -3.11 -1.58], category: 1\n[-0.64 -6.05  5.23 -5.2 ], category: 2\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/SoftMax/#optional-lab-softmax-function","title":"Optional Lab - Softmax Function\u00b6","text":"<p>In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.</p>"},{"location":"DeepLearning/part2/SoftMax/#softmax-function","title":"Softmax Function\u00b6","text":"<p>In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.</p>"},{"location":"DeepLearning/part2/SoftMax/#cost","title":"Cost\u00b6","text":""},{"location":"DeepLearning/part2/SoftMax/#tensorflow","title":"Tensorflow\u00b6","text":"<p>This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.</p> <p>Let's start by creating a dataset to train a multiclass classification model.</p>"},{"location":"DeepLearning/part2/SoftMax/#the-obvious-organization","title":"The Obvious organization\u00b6","text":""},{"location":"DeepLearning/part2/SoftMax/#output-handling","title":"Output Handling\u00b6","text":"<p>Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. Let's look at the preferred model outputs:</p>"},{"location":"DeepLearning/part2/SoftMax/#sparsecategorialcrossentropy-or-categoricalcrossentropy","title":"SparseCategorialCrossentropy or CategoricalCrossEntropy\u00b6","text":"<p>Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.</p> <ul> <li>SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.</li> <li>CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].</li> </ul>"},{"location":"DeepLearning/part2/SoftMax/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you</p> <ul> <li>Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks.</li> <li>Learned the preferred model construction in Tensorflow:<ul> <li>No activation on the final layer (same as linear activation)</li> <li>SparseCategoricalCrossentropy loss function</li> <li>use from_logits=True</li> </ul> </li> <li>Recognized that unlike ReLU and Sigmoid, the softmax spans multiple outputs.</li> </ul>"},{"location":"DeepLearning/part2/autils/","title":"Autils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.activations import linear, relu, sigmoid\n</pre> import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.activations import linear, relu, sigmoid In\u00a0[\u00a0]: Copied! <pre>dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC', dlmedblue='#4285F4')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'; dlmedblue='#4285F4'\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\nplt.style.use('./deeplearning.mplstyle')\n</pre> dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC', dlmedblue='#4285F4') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'; dlmedblue='#4285F4' dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] plt.style.use('./deeplearning.mplstyle') In\u00a0[\u00a0]: Copied! <pre>def load_data():\n    X = np.load(\"data/X.npy\")\n    y = np.load(\"data/y.npy\")\n    return X, y\n</pre> def load_data():     X = np.load(\"data/X.npy\")     y = np.load(\"data/y.npy\")     return X, y In\u00a0[\u00a0]: Copied! <pre>def plt_act_trio():\n    X = np.linspace(-5,5,100)\n    fig,ax = plt.subplots(1,3, figsize=(6,2))\n    widgvis(fig)\n    ax[0].plot(X,tf.keras.activations.linear(X))\n    ax[0].axvline(0, lw=0.3, c=\"black\")\n    ax[0].axhline(0, lw=0.3, c=\"black\")\n    ax[0].set_title(\"Linear\")\n    ax[1].plot(X,tf.keras.activations.sigmoid(X))\n    ax[1].axvline(0, lw=0.3, c=\"black\")\n    ax[1].axhline(0, lw=0.3, c=\"black\")\n    ax[1].set_title(\"Sigmoid\")\n    ax[2].plot(X,tf.keras.activations.relu(X))\n    ax[2].axhline(0, lw=0.3, c=\"black\")\n    ax[2].axvline(0, lw=0.3, c=\"black\")\n    ax[2].set_title(\"ReLu\")\n    fig.suptitle(\"Common Activation Functions\", fontsize=14)\n    fig.tight_layout(pad=0.2)\n    plt.show()\n</pre> def plt_act_trio():     X = np.linspace(-5,5,100)     fig,ax = plt.subplots(1,3, figsize=(6,2))     widgvis(fig)     ax[0].plot(X,tf.keras.activations.linear(X))     ax[0].axvline(0, lw=0.3, c=\"black\")     ax[0].axhline(0, lw=0.3, c=\"black\")     ax[0].set_title(\"Linear\")     ax[1].plot(X,tf.keras.activations.sigmoid(X))     ax[1].axvline(0, lw=0.3, c=\"black\")     ax[1].axhline(0, lw=0.3, c=\"black\")     ax[1].set_title(\"Sigmoid\")     ax[2].plot(X,tf.keras.activations.relu(X))     ax[2].axhline(0, lw=0.3, c=\"black\")     ax[2].axvline(0, lw=0.3, c=\"black\")     ax[2].set_title(\"ReLu\")     fig.suptitle(\"Common Activation Functions\", fontsize=14)     fig.tight_layout(pad=0.2)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def widgvis(fig):\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n</pre> def widgvis(fig):     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False In\u00a0[\u00a0]: Copied! <pre>def plt_ex1():\n    X = np.linspace(0,2*np.pi, 100)\n    y = np.cos(X)+1\n    y[50:100]=0\n    fig,ax = plt.subplots(1,1, figsize=(2,2))\n    widgvis(fig)\n    ax.set_title(\"Target\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.plot(X,y)\n    fig.tight_layout(pad=0.1)\n    plt.show()\n    return(X,y)\n</pre> def plt_ex1():     X = np.linspace(0,2*np.pi, 100)     y = np.cos(X)+1     y[50:100]=0     fig,ax = plt.subplots(1,1, figsize=(2,2))     widgvis(fig)     ax.set_title(\"Target\")     ax.set_xlabel(\"x\")     ax.set_ylabel(\"y\")     ax.plot(X,y)     fig.tight_layout(pad=0.1)     plt.show()     return(X,y) In\u00a0[\u00a0]: Copied! <pre>def plt_ex2():\n    X = np.linspace(0,2*np.pi, 100)\n    y = np.cos(X)+1\n    y[0:49]=0\n    fig,ax = plt.subplots(1,1, figsize=(2,2))\n    widgvis(fig)\n    ax.set_title(\"Target\")\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.plot(X,y)\n    fig.tight_layout(pad=0.1)\n    plt.show()\n    return(X,y)\n</pre> def plt_ex2():     X = np.linspace(0,2*np.pi, 100)     y = np.cos(X)+1     y[0:49]=0     fig,ax = plt.subplots(1,1, figsize=(2,2))     widgvis(fig)     ax.set_title(\"Target\")     ax.set_xlabel(\"x\")     ax.set_ylabel(\"y\")     ax.plot(X,y)     fig.tight_layout(pad=0.1)     plt.show()     return(X,y) In\u00a0[\u00a0]: Copied! <pre>def gen_data():\n    X = np.linspace(0,2*np.pi, 100)\n    y = np.cos(X)+1\n    X=X.reshape(-1,1)\n    return(X,y)\n</pre> def gen_data():     X = np.linspace(0,2*np.pi, 100)     y = np.cos(X)+1     X=X.reshape(-1,1)     return(X,y) In\u00a0[\u00a0]: Copied! <pre>def plt_dual(X,y,yhat):\n    fig,ax = plt.subplots(1,2, figsize=(4,2))\n    widgvis(fig)\n    ax[0].set_title(\"Target\")\n    ax[0].set_xlabel(\"x\")\n    ax[0].set_ylabel(\"y\")\n    ax[0].plot(X,y)\n    ax[1].set_title(\"Prediction\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"y\")\n    ax[1].plot(X,y)\n    ax[1].plot(X,yhat)\n    fig.tight_layout(pad=0.1)\n    plt.show()\n</pre> def plt_dual(X,y,yhat):     fig,ax = plt.subplots(1,2, figsize=(4,2))     widgvis(fig)     ax[0].set_title(\"Target\")     ax[0].set_xlabel(\"x\")     ax[0].set_ylabel(\"y\")     ax[0].plot(X,y)     ax[1].set_title(\"Prediction\")     ax[1].set_xlabel(\"x\")     ax[1].set_ylabel(\"y\")     ax[1].plot(X,y)     ax[1].plot(X,yhat)     fig.tight_layout(pad=0.1)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_act1(X,y,z,a):\n    fig,ax = plt.subplots(1,3, figsize=(6,2.5))\n    widgvis(fig)\n    ax[0].plot(X,y,label=\"target\")\n    ax[0].axvline(0, lw=0.3, c=\"black\")\n    ax[0].axhline(0, lw=0.3, c=\"black\")\n    ax[0].set_title(\"y - target\")\n    ax[1].plot(X,y, label=\"target\")\n    ax[1].plot(X,z, c=dlc[\"dldarkred\"],label=\"z\")\n    ax[1].axvline(0, lw=0.3, c=\"black\")\n    ax[1].axhline(0, lw=0.3, c=\"black\")\n    ax[1].set_title(r\"$z = w \\cdot x+b$\")\n    ax[1].legend(loc=\"upper center\")\n    ax[2].plot(X,y, label=\"target\")\n    ax[2].plot(X,a, c=dlc[\"dldarkred\"],label=\"ReLu(z)\")\n    ax[2].axhline(0, lw=0.3, c=\"black\")\n    ax[2].axvline(0, lw=0.3, c=\"black\")\n    ax[2].set_title(\"max(0,z)\")\n    ax[2].legend()\n    fig.suptitle(\"Role of Non-Linear Activation\", fontsize=12)\n    fig.tight_layout(pad=0.22)\n    return(ax)\n</pre> def plt_act1(X,y,z,a):     fig,ax = plt.subplots(1,3, figsize=(6,2.5))     widgvis(fig)     ax[0].plot(X,y,label=\"target\")     ax[0].axvline(0, lw=0.3, c=\"black\")     ax[0].axhline(0, lw=0.3, c=\"black\")     ax[0].set_title(\"y - target\")     ax[1].plot(X,y, label=\"target\")     ax[1].plot(X,z, c=dlc[\"dldarkred\"],label=\"z\")     ax[1].axvline(0, lw=0.3, c=\"black\")     ax[1].axhline(0, lw=0.3, c=\"black\")     ax[1].set_title(r\"$z = w \\cdot x+b$\")     ax[1].legend(loc=\"upper center\")     ax[2].plot(X,y, label=\"target\")     ax[2].plot(X,a, c=dlc[\"dldarkred\"],label=\"ReLu(z)\")     ax[2].axhline(0, lw=0.3, c=\"black\")     ax[2].axvline(0, lw=0.3, c=\"black\")     ax[2].set_title(\"max(0,z)\")     ax[2].legend()     fig.suptitle(\"Role of Non-Linear Activation\", fontsize=12)     fig.tight_layout(pad=0.22)     return(ax) In\u00a0[\u00a0]: Copied! <pre>def plt_add_notation(ax):\n    ax[1].annotate(text = \"matches\\n here\", xy =(1.5,1.0), \n                   xytext = (0.1,-1.5), fontsize=9,\n                  arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8))\n    ax[1].annotate(text = \"but not\\n here\", xy =(5,-2.5), \n                   xytext = (1,-3), fontsize=9,\n                  arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8))\n    ax[2].annotate(text = \"ReLu\\n 'off'\", xy =(2.6,0), \n                   xytext = (0.1,0.1), fontsize=9,\n                  arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8))\n</pre> def plt_add_notation(ax):     ax[1].annotate(text = \"matches\\n here\", xy =(1.5,1.0),                     xytext = (0.1,-1.5), fontsize=9,                   arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8))     ax[1].annotate(text = \"but not\\n here\", xy =(5,-2.5),                     xytext = (1,-3), fontsize=9,                   arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8))     ax[2].annotate(text = \"ReLu\\n 'off'\", xy =(2.6,0),                     xytext = (0.1,0.1), fontsize=9,                   arrowprops=dict(facecolor=dlc[\"dlpurple\"],width=2, headwidth=8)) In\u00a0[\u00a0]: Copied! <pre>def compile_fit(model,X,y):\n    model.compile(\n        loss=tf.keras.losses.MeanSquaredError(),\n        optimizer=tf.keras.optimizers.Adam(0.01),\n    )\n\n    model.fit(\n        X,y,\n        epochs=100,\n        verbose = 0\n    )\n    l1=model.get_layer(\"l1\")\n    l2=model.get_layer(\"l2\")\n    w1,b1 = l1.get_weights()\n    w2,b2 = l2.get_weights()\n    return(w1,b1,w2,b2)\n</pre> def compile_fit(model,X,y):     model.compile(         loss=tf.keras.losses.MeanSquaredError(),         optimizer=tf.keras.optimizers.Adam(0.01),     )      model.fit(         X,y,         epochs=100,         verbose = 0     )     l1=model.get_layer(\"l1\")     l2=model.get_layer(\"l2\")     w1,b1 = l1.get_weights()     w2,b2 = l2.get_weights()     return(w1,b1,w2,b2) In\u00a0[\u00a0]: Copied! <pre>def plt_model(X,y,yhat_pre, yhat_post):\n    fig,ax = plt.subplots(1,3, figsize=(8,2))\n    widgvis(fig)\n    ax[0].set_title(\"Target\")\n    ax[0].set_xlabel(\"x\")\n    ax[0].set_ylabel(\"y\")\n    ax[0].plot(X,y)\n    ax[1].set_title(\"Prediction, pre-training\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"y\")\n    ax[1].plot(X,y)\n    ax[1].plot(X,yhat_pre)\n    ax[2].set_title(\"Prediction, post-training\")\n    ax[2].set_xlabel(\"x\")\n    ax[2].set_ylabel(\"y\")\n    ax[2].plot(X,y)\n    ax[2].plot(X,yhat_post)\n    fig.tight_layout(pad=0.1)\n    plt.show()\n</pre> def plt_model(X,y,yhat_pre, yhat_post):     fig,ax = plt.subplots(1,3, figsize=(8,2))     widgvis(fig)     ax[0].set_title(\"Target\")     ax[0].set_xlabel(\"x\")     ax[0].set_ylabel(\"y\")     ax[0].plot(X,y)     ax[1].set_title(\"Prediction, pre-training\")     ax[1].set_xlabel(\"x\")     ax[1].set_ylabel(\"y\")     ax[1].plot(X,y)     ax[1].plot(X,yhat_pre)     ax[2].set_title(\"Prediction, post-training\")     ax[2].set_xlabel(\"x\")     ax[2].set_ylabel(\"y\")     ax[2].plot(X,y)     ax[2].plot(X,yhat_post)     fig.tight_layout(pad=0.1)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def display_errors(model,X,y):\n    f = model.predict(X)\n    yhat = np.argmax(f, axis=1)\n    doo = yhat != y[:,0]\n    idxs = np.where(yhat != y[:,0])[0]\n    if len(idxs) == 0:\n        print(\"no errors found\")\n    else:\n        cnt = min(8, len(idxs))\n        fig, ax = plt.subplots(1,cnt, figsize=(5,1.2))\n        fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.80]) #[left, bottom, right, top]\n        widgvis(fig)\n\n        for i in range(cnt):\n            j = idxs[i]\n            X_reshaped = X[j].reshape((20,20)).T\n\n            # Display the image\n            ax[i].imshow(X_reshaped, cmap='gray')\n\n            # Predict using the Neural Network\n            prediction = model.predict(X[j].reshape(1,400))\n            prediction_p = tf.nn.softmax(prediction)\n            yhat = np.argmax(prediction_p)\n\n            # Display the label above the image\n            ax[i].set_title(f\"{y[j,0]},{yhat}\",fontsize=10)\n            ax[i].set_axis_off()\n            fig.suptitle(\"Label, yhat\", fontsize=12)\n    return(len(idxs))\n</pre> def display_errors(model,X,y):     f = model.predict(X)     yhat = np.argmax(f, axis=1)     doo = yhat != y[:,0]     idxs = np.where(yhat != y[:,0])[0]     if len(idxs) == 0:         print(\"no errors found\")     else:         cnt = min(8, len(idxs))         fig, ax = plt.subplots(1,cnt, figsize=(5,1.2))         fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.80]) #[left, bottom, right, top]         widgvis(fig)          for i in range(cnt):             j = idxs[i]             X_reshaped = X[j].reshape((20,20)).T              # Display the image             ax[i].imshow(X_reshaped, cmap='gray')              # Predict using the Neural Network             prediction = model.predict(X[j].reshape(1,400))             prediction_p = tf.nn.softmax(prediction)             yhat = np.argmax(prediction_p)              # Display the label above the image             ax[i].set_title(f\"{y[j,0]},{yhat}\",fontsize=10)             ax[i].set_axis_off()             fig.suptitle(\"Label, yhat\", fontsize=12)     return(len(idxs)) In\u00a0[\u00a0]: Copied! <pre>def display_digit(X):\n    \"\"\" display a single digit. The input is one digit (400,). \"\"\"\n    fig, ax = plt.subplots(1,1, figsize=(0.5,0.5))\n    widgvis(fig)\n    X_reshaped = X.reshape((20,20)).T\n    # Display the image\n    ax.imshow(X_reshaped, cmap='gray')\n    plt.show()\n</pre> def display_digit(X):     \"\"\" display a single digit. The input is one digit (400,). \"\"\"     fig, ax = plt.subplots(1,1, figsize=(0.5,0.5))     widgvis(fig)     X_reshaped = X.reshape((20,20)).T     # Display the image     ax.imshow(X_reshaped, cmap='gray')     plt.show()"},{"location":"DeepLearning/part2/lab_utils_common/","title":"Lab utils common","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>lab_utils_common contains common routines and variable definitions used by all the labs in this week. by contrast, specific, large plotting routines will be in separate files and are generally imported into the week where they are used. those files will import this file</p> In\u00a0[\u00a0]: Copied! <pre>import copy\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch\nfrom ipywidgets import Output\nfrom matplotlib.widgets import Button, CheckButtons\n\nnp.set_printoptions(precision=2)\n\ndlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC', dlmedblue='#4285F4')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'; dlmedblue='#4285F4'\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\nplt.style.use('./deeplearning.mplstyle')\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Parameters\n    ----------\n    z : array_like\n        A scalar or numpy array of any size.\n\n    Returns\n    -------\n     g : array_like\n         sigmoid(z)\n    \"\"\"\n    z = np.clip( z, -500, 500 )           # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n\n    return g\n</pre> import copy import math import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from ipywidgets import Output from matplotlib.widgets import Button, CheckButtons  np.set_printoptions(precision=2)  dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC', dlmedblue='#4285F4') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'; dlmedblue='#4285F4' dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] plt.style.use('./deeplearning.mplstyle')  def sigmoid(z):     \"\"\"     Compute the sigmoid of z      Parameters     ----------     z : array_like         A scalar or numpy array of any size.      Returns     -------      g : array_like          sigmoid(z)     \"\"\"     z = np.clip( z, -500, 500 )           # protect against overflow     g = 1.0/(1.0+np.exp(-z))      return g <p>Regression Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>def predict_logistic(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return sigmoid(X @ w + b)\n\ndef predict_linear(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return X @ w + b\n\ndef compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n    \"\"\"\n    Computes cost using logistic loss, non-matrix version\n\n    Args:\n      X (ndarray): Shape (m,n)  matrix of examples with n features\n      y (ndarray): Shape (m,)   target values\n      w (ndarray): Shape (n,)   parameters for prediction\n      b (scalar):               parameter  for prediction\n      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n      safe : (boolean)          True-selects under/overflow safe algorithm\n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m,n = X.shape\n    cost = 0.0\n    for i in range(m):\n        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n        if safe:  #avoids overflows\n            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n        else:\n            f_wb_i = sigmoid(z_i)                                                   #(n,)\n            cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n    cost = cost/m\n\n    reg_cost = 0\n    if lambda_ != 0:\n        for j in range(n):\n            reg_cost += (w[j]**2)                                               # scalar\n        reg_cost = (lambda_/(2*m))*reg_cost\n\n    return cost + reg_cost\n\n\ndef log_1pexp(x, maximum=20):\n    ''' approximate log(1+exp^x)\n        https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice\n    Args:\n    x   : (ndarray Shape (n,1) or (n,)  input\n    out : (ndarray Shape matches x      output ~= np.log(1+exp(x))\n    '''\n\n    out  = np.zeros_like(x,dtype=float)\n    i    = x &lt;= maximum\n    ni   = np.logical_not(i)\n\n    out[i]  = np.log(1 + np.exp(x[i]))\n    out[ni] = x[ni]\n    return out\n\n\ndef compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):\n    \"\"\"\n    Computes the cost using  using matrices\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model\n      b : (scalar )                       Values of parameter of the model\n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns:\n      total_cost: (scalar)                cost\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n    if logistic:\n        if safe:  #safe from overflow\n            z = X @ w + b                                                           #(m,n)(n,1)=(m,1)\n            cost = -(y * z) + log_1pexp(z)\n            cost = np.sum(cost)/m                                                   # (scalar)\n        else:\n            f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)\n            cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)\n            cost = cost[0,0]                                                        # scalar\n    else:\n        f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)\n        cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar\n\n    reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar\n\n    total_cost = cost + reg_cost                                                # scalar\n\n    return total_cost                                                           # scalar\n\ndef compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):\n    \"\"\"\n    Computes the gradient using matrices\n\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model\n      b : (scalar )                       Values of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n    Returns\n      dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w\n      dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n\n    f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)\n    err   = f_wb - y                                              # (m,1)\n    dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)\n    dj_db = (1/m) * np.sum(err)                                   # scalar\n\n    dj_dw += (lambda_/m) * w        # regularize                  # (n,1)\n\n    return dj_db, dj_dw                                           # scalar, (n,1)\n\ndef gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True, Trace=True):\n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking\n    num_iters gradient steps with learning rate alpha\n\n    Args:\n      X (ndarray):    Shape (m,n)         matrix of examples\n      y (ndarray):    Shape (m,) or (m,1) target value of each example\n      w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model\n      b_in (scalar):                      Initial value of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n      alpha (float):                      Learning rate\n      num_iters (int):                    number of iterations to run gradient descent\n\n    Returns:\n      w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape\n      b (scalar):                         Updated value of parameter\n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    w = w.reshape(-1,1)      #prep for matrix operations\n    y = y.reshape(-1,1)\n    last_cost = np.Inf\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        # Save cost J at each iteration\n        ccost = compute_cost_matrix(X, y, w, b, logistic, lambda_)\n        if Trace and i&lt;100000:      # prevent resource exhaustion\n            J_history.append( ccost )\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            if verbose: print(f\"Iteration {i:4d}: Cost {ccost}   \")\n            if verbose ==2: print(f\"dj_db, dj_dw = {dj_db: 0.3f}, {dj_dw.reshape(-1)}\")\n\n            if ccost == last_cost:\n                alpha = alpha/10\n                print(f\" alpha now {alpha}\")\n            last_cost = ccost\n\n    return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing\n\ndef zscore_normalize_features(X):\n    \"\"\"\n    computes  X, zcore normalized by column\n\n    Args:\n      X (ndarray): Shape (m,n) input data, m examples, n features\n\n    Returns:\n      X_norm (ndarray): Shape (m,n)  input normalized by column\n      mu (ndarray):     Shape (n,)   mean of each feature\n      sigma (ndarray):  Shape (n,)   standard deviation of each feature\n    \"\"\"\n    # find the mean of each column/feature\n    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n    # find the standard deviation of each column/feature\n    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n    # element-wise, subtract mu for that column from each example, divide by std for that column\n    X_norm = (X - mu) / sigma\n\n    return X_norm, mu, sigma\n\n#check our work\n#from sklearn.preprocessing import scale\n#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n</pre> def predict_logistic(X, w, b):     \"\"\" performs prediction \"\"\"     return sigmoid(X @ w + b)  def predict_linear(X, w, b):     \"\"\" performs prediction \"\"\"     return X @ w + b  def compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):     \"\"\"     Computes cost using logistic loss, non-matrix version      Args:       X (ndarray): Shape (m,n)  matrix of examples with n features       y (ndarray): Shape (m,)   target values       w (ndarray): Shape (n,)   parameters for prediction       b (scalar):               parameter  for prediction       lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization       safe : (boolean)          True-selects under/overflow safe algorithm     Returns:       cost (scalar): cost     \"\"\"      m,n = X.shape     cost = 0.0     for i in range(m):         z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()         if safe:  #avoids overflows             cost += -(y[i] * z_i ) + log_1pexp(z_i)         else:             f_wb_i = sigmoid(z_i)                                                   #(n,)             cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar     cost = cost/m      reg_cost = 0     if lambda_ != 0:         for j in range(n):             reg_cost += (w[j]**2)                                               # scalar         reg_cost = (lambda_/(2*m))*reg_cost      return cost + reg_cost   def log_1pexp(x, maximum=20):     ''' approximate log(1+exp^x)         https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice     Args:     x   : (ndarray Shape (n,1) or (n,)  input     out : (ndarray Shape matches x      output ~= np.log(1+exp(x))     '''      out  = np.zeros_like(x,dtype=float)     i    = x &lt;= maximum     ni   = np.logical_not(i)      out[i]  = np.log(1 + np.exp(x[i]))     out[ni] = x[ni]     return out   def compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):     \"\"\"     Computes the cost using  using matrices     Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model       b : (scalar )                       Values of parameter of the model       verbose : (Boolean) If true, print out intermediate value f_wb     Returns:       total_cost: (scalar)                cost     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D     if logistic:         if safe:  #safe from overflow             z = X @ w + b                                                           #(m,n)(n,1)=(m,1)             cost = -(y * z) + log_1pexp(z)             cost = np.sum(cost)/m                                                   # (scalar)         else:             f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)             cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)             cost = cost[0,0]                                                        # scalar     else:         f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)         cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar      reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar      total_cost = cost + reg_cost                                                # scalar      return total_cost                                                           # scalar  def compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):     \"\"\"     Computes the gradient using matrices      Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model       b : (scalar )                       Values of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero     Returns       dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w       dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D      f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)     err   = f_wb - y                                              # (m,1)     dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)     dj_db = (1/m) * np.sum(err)                                   # scalar      dj_dw += (lambda_/m) * w        # regularize                  # (n,1)      return dj_db, dj_dw                                           # scalar, (n,1)  def gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True, Trace=True):     \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking     num_iters gradient steps with learning rate alpha      Args:       X (ndarray):    Shape (m,n)         matrix of examples       y (ndarray):    Shape (m,) or (m,1) target value of each example       w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model       b_in (scalar):                      Initial value of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero       alpha (float):                      Learning rate       num_iters (int):                    number of iterations to run gradient descent      Returns:       w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape       b (scalar):                         Updated value of parameter     \"\"\"     # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in     w = w.reshape(-1,1)      #prep for matrix operations     y = y.reshape(-1,1)     last_cost = np.Inf      for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)          # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw         b = b - alpha * dj_db          # Save cost J at each iteration         ccost = compute_cost_matrix(X, y, w, b, logistic, lambda_)         if Trace and i&lt;100000:      # prevent resource exhaustion             J_history.append( ccost )          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters / 10) == 0:             if verbose: print(f\"Iteration {i:4d}: Cost {ccost}   \")             if verbose ==2: print(f\"dj_db, dj_dw = {dj_db: 0.3f}, {dj_dw.reshape(-1)}\")              if ccost == last_cost:                 alpha = alpha/10                 print(f\" alpha now {alpha}\")             last_cost = ccost      return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing  def zscore_normalize_features(X):     \"\"\"     computes  X, zcore normalized by column      Args:       X (ndarray): Shape (m,n) input data, m examples, n features      Returns:       X_norm (ndarray): Shape (m,n)  input normalized by column       mu (ndarray):     Shape (n,)   mean of each feature       sigma (ndarray):  Shape (n,)   standard deviation of each feature     \"\"\"     # find the mean of each column/feature     mu     = np.mean(X, axis=0)                 # mu will have shape (n,)     # find the standard deviation of each column/feature     sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)     # element-wise, subtract mu for that column from each example, divide by std for that column     X_norm = (X - mu) / sigma      return X_norm, mu, sigma  #check our work #from sklearn.preprocessing import scale #scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True) <p>Common Plotting Routines #####################################################</p> In\u00a0[\u00a0]: Copied! <pre>def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n    \"\"\" plots logistic data with two axis \"\"\"\n    # Find Indices of Positive and Negative Examples\n    pos = y == 1\n    neg = y == 0\n    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n    neg = neg.reshape(-1,)\n\n    # Plot examples\n    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n    ax.legend(loc=loc)\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\ndef plt_tumor_data(x, y, ax):\n    \"\"\" plots tumor data on one axis \"\"\"\n    pos = y == 1\n    neg = y == 0\n\n    ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n    ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n    ax.set_ylim(-0.175,1.1)\n    ax.set_ylabel('y')\n    ax.set_xlabel('Tumor Size')\n    ax.set_title(\"Logistic Regression on Categorical Data\")\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\n# Draws a threshold at 0.5\ndef draw_vthresh(ax,x):\n    \"\"\" draws a threshold \"\"\"\n    ylim = ax.get_ylim()\n    xlim = ax.get_xlim()\n    ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)\n    ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)\n    ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',\n                xytext=[30,5],textcoords='offset points')\n    d = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(d)\n    ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',\n                 xytext=[-50,5],textcoords='offset points', ha='left')\n    f = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(f)\n\n\n#-----------------------------------------------------\n# common interactive plotting routines\n#-----------------------------------------------------\n\nclass button_manager:\n    ''' Handles some missing features of matplotlib check buttons\n    on init:\n        creates button, links to button_click routine,\n        calls call_on_click with active index and firsttime=True\n    on click:\n        maintains single button on state, calls call_on_click\n    '''\n\n    #@output.capture()  # debug\n    def __init__(self,fig, dim, labels, init, call_on_click):\n        '''\n        dim: (list)     [leftbottom_x,bottom_y,width,height]\n        labels: (list)  for example ['1','2','3','4','5','6']\n        init: (list)    for example [True, False, False, False, False, False]\n        '''\n        self.fig = fig\n        self.ax = plt.axes(dim)  #lx,by,w,h\n        self.init_state = init\n        self.call_on_click = call_on_click\n        self.button  = CheckButtons(self.ax,labels,init)\n        self.button.on_clicked(self.button_click)\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True),firsttime=True)\n\n    #@output.capture()  # debug\n    def reinit(self):\n        self.status = self.init_state\n        self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status\n\n    #@output.capture()  # debug\n    def button_click(self, event):\n        ''' maintains one-on state. If on-button is clicked, will process correctly '''\n        #new_status = self.button.get_status()\n        #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]\n        #newidx = new.index(True)\n        self.button.eventson = False\n        self.button.set_active(self.status.index(True))  #turn off old or reenable if same\n        self.button.eventson = True\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True))\n</pre>  def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):     \"\"\" plots logistic data with two axis \"\"\"     # Find Indices of Positive and Negative Examples     pos = y == 1     neg = y == 0     pos = pos.reshape(-1,)  #work with 1D or 1D y vectors     neg = neg.reshape(-1,)      # Plot examples     ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)     ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)     ax.legend(loc=loc)      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  def plt_tumor_data(x, y, ax):     \"\"\" plots tumor data on one axis \"\"\"     pos = y == 1     neg = y == 0      ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")     ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)     ax.set_ylim(-0.175,1.1)     ax.set_ylabel('y')     ax.set_xlabel('Tumor Size')     ax.set_title(\"Logistic Regression on Categorical Data\")      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  # Draws a threshold at 0.5 def draw_vthresh(ax,x):     \"\"\" draws a threshold \"\"\"     ylim = ax.get_ylim()     xlim = ax.get_xlim()     ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)     ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)     ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',                 xytext=[30,5],textcoords='offset points')     d = FancyArrowPatch(         posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(d)     ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',                  xytext=[-50,5],textcoords='offset points', ha='left')     f = FancyArrowPatch(         posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(f)   #----------------------------------------------------- # common interactive plotting routines #-----------------------------------------------------  class button_manager:     ''' Handles some missing features of matplotlib check buttons     on init:         creates button, links to button_click routine,         calls call_on_click with active index and firsttime=True     on click:         maintains single button on state, calls call_on_click     '''      #@output.capture()  # debug     def __init__(self,fig, dim, labels, init, call_on_click):         '''         dim: (list)     [leftbottom_x,bottom_y,width,height]         labels: (list)  for example ['1','2','3','4','5','6']         init: (list)    for example [True, False, False, False, False, False]         '''         self.fig = fig         self.ax = plt.axes(dim)  #lx,by,w,h         self.init_state = init         self.call_on_click = call_on_click         self.button  = CheckButtons(self.ax,labels,init)         self.button.on_clicked(self.button_click)         self.status = self.button.get_status()         self.call_on_click(self.status.index(True),firsttime=True)      #@output.capture()  # debug     def reinit(self):         self.status = self.init_state         self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status      #@output.capture()  # debug     def button_click(self, event):         ''' maintains one-on state. If on-button is clicked, will process correctly '''         #new_status = self.button.get_status()         #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]         #newidx = new.index(True)         self.button.eventson = False         self.button.set_active(self.status.index(True))  #turn off old or reenable if same         self.button.eventson = True         self.status = self.button.get_status()         self.call_on_click(self.status.index(True))"},{"location":"DeepLearning/part2/lab_utils_multiclass/","title":"Lab utils multiclass","text":"In\u00a0[\u00a0]: Copied! <pre># C2_W1 Utilities\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n</pre> # C2_W1 Utilities import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs In\u00a0[\u00a0]: Copied! <pre>def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n</pre> def sigmoid(x):     return 1 / (1 + np.exp(-x)) In\u00a0[\u00a0]: Copied! <pre># Plot  multi-class training points\ndef plot_mc_data(X, y, class_labels=None, legend=False,size=40):\n    classes = np.unique(y)\n    for i in classes:\n        label = class_labels[i] if class_labels else \"class {}\".format(i)\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1],  cmap=plt.cm.Paired,\n                    edgecolor='black', s=size, label=label)\n    if legend: plt.legend()\n</pre> # Plot  multi-class training points def plot_mc_data(X, y, class_labels=None, legend=False,size=40):     classes = np.unique(y)     for i in classes:         label = class_labels[i] if class_labels else \"class {}\".format(i)         idx = np.where(y == i)         plt.scatter(X[idx, 0], X[idx, 1],  cmap=plt.cm.Paired,                     edgecolor='black', s=size, label=label)     if legend: plt.legend() In\u00a0[\u00a0]: Copied! <pre>#Plot a multi-class categorical decision boundary\n# This version handles a non-vector prediction (adds a for-loop over points)\ndef plot_cat_decision_boundary(X,predict , class_labels=None, legend=False, vector=True):\n\n    # create a mesh to points to plot\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = max(x_max-x_min, y_max-y_min)/200\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    points = np.c_[xx.ravel(), yy.ravel()]\n    print(\"points\", points.shape)\n    print(\"xx.shape\", xx.shape)\n\n    #make predictions for each point in mesh\n    if vector:\n        Z = predict(points)\n    else:\n        Z = np.zeros((len(points),))\n        for i in range(len(points)):\n            Z[i] = predict(points[i].reshape(1,2))\n    Z = Z.reshape(xx.shape)\n\n    #contour plot highlights boundaries between values - classes in this case\n    plt.figure()\n    plt.contour(xx, yy, Z, colors='g') \n    plt.axis('tight')\n</pre> #Plot a multi-class categorical decision boundary # This version handles a non-vector prediction (adds a for-loop over points) def plot_cat_decision_boundary(X,predict , class_labels=None, legend=False, vector=True):      # create a mesh to points to plot     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1     h = max(x_max-x_min, y_max-y_min)/200     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))     points = np.c_[xx.ravel(), yy.ravel()]     print(\"points\", points.shape)     print(\"xx.shape\", xx.shape)      #make predictions for each point in mesh     if vector:         Z = predict(points)     else:         Z = np.zeros((len(points),))         for i in range(len(points)):             Z[i] = predict(points[i].reshape(1,2))     Z = Z.reshape(xx.shape)      #contour plot highlights boundaries between values - classes in this case     plt.figure()     plt.contour(xx, yy, Z, colors='g')      plt.axis('tight')"},{"location":"DeepLearning/part2/lab_utils_multiclass_TF/","title":"lab utils multiclass TF","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl\nimport warnings\nfrom matplotlib import cm\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport matplotlib.colors as colors\nfrom lab_utils_common import dlc\nfrom matplotlib import cm\n</pre> import matplotlib.pyplot as plt import numpy as np import matplotlib as mpl import warnings from matplotlib import cm from matplotlib.patches import FancyArrowPatch from matplotlib.colors import ListedColormap, LinearSegmentedColormap import matplotlib.colors as colors from lab_utils_common import dlc from matplotlib import cm In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\nplt.style.use('./deeplearning.mplstyle')\n</pre> dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC' dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] plt.style.use('./deeplearning.mplstyle') In\u00a0[\u00a0]: Copied! <pre>dkcolors = plt.cm.Paired((1,3,7,9,5,11))\nltcolors = plt.cm.Paired((0,2,6,8,4,10))\ndkcolors_map = mpl.colors.ListedColormap(dkcolors)\nltcolors_map = mpl.colors.ListedColormap(ltcolors)\n</pre> dkcolors = plt.cm.Paired((1,3,7,9,5,11)) ltcolors = plt.cm.Paired((0,2,6,8,4,10)) dkcolors_map = mpl.colors.ListedColormap(dkcolors) ltcolors_map = mpl.colors.ListedColormap(ltcolors) In\u00a0[\u00a0]: Copied! <pre>#Plot a multi-class categorical decision boundary\n# This version handles a non-vector prediction (adds a for-loop over points)\ndef plot_cat_decision_boundary_mc(ax, X, predict , class_labels=None, legend=False, vector=True):\n\n    # create a mesh to points to plot\n    x_min, x_max = X[:, 0].min()- 0.5, X[:, 0].max()+0.5\n    y_min, y_max = X[:, 1].min()- 0.5, X[:, 1].max()+0.5\n    h = max(x_max-x_min, y_max-y_min)/100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    points = np.c_[xx.ravel(), yy.ravel()]\n    #print(\"points\", points.shape)\n    #print(\"xx.shape\", xx.shape)\n\n    #make predictions for each point in mesh\n    if vector:\n        Z = predict(points)\n    else:\n        Z = np.zeros((len(points),))\n        for i in range(len(points)):\n            Z[i] = predict(points[i].reshape(1,2))\n    Z = Z.reshape(xx.shape)\n\n    #contour plot highlights boundaries between values - classes in this case\n    ax.contour(xx, yy, Z, linewidths=1) \n    #ax.axis('tight')\n</pre> #Plot a multi-class categorical decision boundary # This version handles a non-vector prediction (adds a for-loop over points) def plot_cat_decision_boundary_mc(ax, X, predict , class_labels=None, legend=False, vector=True):      # create a mesh to points to plot     x_min, x_max = X[:, 0].min()- 0.5, X[:, 0].max()+0.5     y_min, y_max = X[:, 1].min()- 0.5, X[:, 1].max()+0.5     h = max(x_max-x_min, y_max-y_min)/100     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))     points = np.c_[xx.ravel(), yy.ravel()]     #print(\"points\", points.shape)     #print(\"xx.shape\", xx.shape)      #make predictions for each point in mesh     if vector:         Z = predict(points)     else:         Z = np.zeros((len(points),))         for i in range(len(points)):             Z[i] = predict(points[i].reshape(1,2))     Z = Z.reshape(xx.shape)      #contour plot highlights boundaries between values - classes in this case     ax.contour(xx, yy, Z, linewidths=1)      #ax.axis('tight') In\u00a0[\u00a0]: Copied! <pre>def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired, \n                legend=False, size=50, m='o', equal_xy = False):\n    \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"\n    for i in range(classes):\n        idx = np.where(y == i)\n        col = len(idx[0])*[i]\n        label = class_labels[i] if class_labels else \"c{}\".format(i)\n        # this didn't work on coursera but did in local version\n        #ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n        #            c=col, vmin=0, vmax=map.N, cmap=map,\n        #            s=size, label=label)\n        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n                    color=map(col), vmin=0, vmax=map.N, \n                    s=size, label=label)\n    if legend: ax.legend()\n    if equal_xy: ax.axis(\"equal\")\n</pre> def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired,                  legend=False, size=50, m='o', equal_xy = False):     \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"     for i in range(classes):         idx = np.where(y == i)         col = len(idx[0])*[i]         label = class_labels[i] if class_labels else \"c{}\".format(i)         # this didn't work on coursera but did in local version         #ax.scatter(X[idx, 0], X[idx, 1],  marker=m,         #            c=col, vmin=0, vmax=map.N, cmap=map,         #            s=size, label=label)         ax.scatter(X[idx, 0], X[idx, 1],  marker=m,                     color=map(col), vmin=0, vmax=map.N,                      s=size, label=label)     if legend: ax.legend()     if equal_xy: ax.axis(\"equal\") In\u00a0[\u00a0]: Copied! <pre>def plt_mc(X_train,y_train,classes, centers, std):\n    css = np.unique(y_train)\n    fig,ax = plt.subplots(1,1,figsize=(3,3))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n    plt_mc_data(ax, X_train,y_train,classes, map=dkcolors_map, legend=True, size=50, equal_xy = False)\n    ax.set_title(\"Multiclass Data\")\n    ax.set_xlabel(\"x0\")\n    ax.set_ylabel(\"x1\")\n    #for c in css:\n    #    circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n    #    ax.add_patch(circ)\n    plt.show()\n</pre> def plt_mc(X_train,y_train,classes, centers, std):     css = np.unique(y_train)     fig,ax = plt.subplots(1,1,figsize=(3,3))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False     plt_mc_data(ax, X_train,y_train,classes, map=dkcolors_map, legend=True, size=50, equal_xy = False)     ax.set_title(\"Multiclass Data\")     ax.set_xlabel(\"x0\")     ax.set_ylabel(\"x1\")     #for c in css:     #    circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)     #    ax.add_patch(circ)     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_cat_mc(X_train, y_train, model, classes):\n    #make a model for plotting routines to call\n    model_predict = lambda Xl: np.argmax(model.predict(Xl),axis=1)\n\n    fig,ax = plt.subplots(1,1, figsize=(3,3))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n \n    #add the original data to the decison boundary\n    plt_mc_data(ax, X_train,y_train, classes, map=dkcolors_map, legend=True)\n    #plot the decison boundary. \n    plot_cat_decision_boundary_mc(ax, X_train, model_predict, vector=True)\n    ax.set_title(\"model decision boundary\")\n\n    plt.xlabel(r'$x_0$');\n    plt.ylabel(r\"$x_1$\"); \n    plt.show()\n</pre> def plt_cat_mc(X_train, y_train, model, classes):     #make a model for plotting routines to call     model_predict = lambda Xl: np.argmax(model.predict(Xl),axis=1)      fig,ax = plt.subplots(1,1, figsize=(3,3))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False       #add the original data to the decison boundary     plt_mc_data(ax, X_train,y_train, classes, map=dkcolors_map, legend=True)     #plot the decison boundary.      plot_cat_decision_boundary_mc(ax, X_train, model_predict, vector=True)     ax.set_title(\"model decision boundary\")      plt.xlabel(r'$x_0$');     plt.ylabel(r\"$x_1$\");      plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_prob_z(ax,fwb, x0_rng=(-8,8), x1_rng=(-5,4)):\n    \"\"\" plots a decision boundary but include shading to indicate the probability\n        and adds a conouter to show where z=0\n    \"\"\"\n    #setup useful ranges and common linspaces\n    x0_space  = np.linspace(x0_rng[0], x0_rng[1], 40)\n    x1_space  = np.linspace(x1_rng[0], x1_rng[1], 40)\n\n    # get probability for x0,x1 ranges\n    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n    z = np.zeros_like(tmp_x0)\n    c = np.zeros_like(tmp_x0)\n    for i in range(tmp_x0.shape[0]):\n        for j in range(tmp_x1.shape[1]):\n            x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])\n            z[i,j] = fwb(x)\n            c[i,j] = 0. if z[i,j] == 0 else 1.\n    with warnings.catch_warnings():  # suppress no contour warning\n        warnings.simplefilter(\"ignore\")\n        #ax.contour(tmp_x0, tmp_x1, c, colors='b', linewidths=1) \n        ax.contour(tmp_x0, tmp_x1, c, linewidths=1) \n\n    cmap = plt.get_cmap('Blues')\n    new_cmap = truncate_colormap(cmap, 0.0, 0.7)\n\n    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n                   norm=cm.colors.Normalize(vmin=np.amin(z), vmax=np.amax(z)),\n                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n    ax.figure.colorbar(pcm, ax=ax)\n</pre> def plt_prob_z(ax,fwb, x0_rng=(-8,8), x1_rng=(-5,4)):     \"\"\" plots a decision boundary but include shading to indicate the probability         and adds a conouter to show where z=0     \"\"\"     #setup useful ranges and common linspaces     x0_space  = np.linspace(x0_rng[0], x0_rng[1], 40)     x1_space  = np.linspace(x1_rng[0], x1_rng[1], 40)      # get probability for x0,x1 ranges     tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)     z = np.zeros_like(tmp_x0)     c = np.zeros_like(tmp_x0)     for i in range(tmp_x0.shape[0]):         for j in range(tmp_x1.shape[1]):             x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])             z[i,j] = fwb(x)             c[i,j] = 0. if z[i,j] == 0 else 1.     with warnings.catch_warnings():  # suppress no contour warning         warnings.simplefilter(\"ignore\")         #ax.contour(tmp_x0, tmp_x1, c, colors='b', linewidths=1)          ax.contour(tmp_x0, tmp_x1, c, linewidths=1)       cmap = plt.get_cmap('Blues')     new_cmap = truncate_colormap(cmap, 0.0, 0.7)      pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,                    norm=cm.colors.Normalize(vmin=np.amin(z), vmax=np.amax(z)),                    cmap=new_cmap, shading='nearest', alpha = 0.9)     ax.figure.colorbar(pcm, ax=ax) In\u00a0[\u00a0]: Copied! <pre>def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n    \"\"\" truncates color map \"\"\"\n    new_cmap = colors.LinearSegmentedColormap.from_list(\n        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n        cmap(np.linspace(minval, maxval, n)))\n    return new_cmap\n</pre> def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):     \"\"\" truncates color map \"\"\"     new_cmap = colors.LinearSegmentedColormap.from_list(         'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),         cmap(np.linspace(minval, maxval, n)))     return new_cmap In\u00a0[\u00a0]: Copied! <pre>def plt_layer_relu(X, Y, W1, b1, classes):\n    nunits = (W1.shape[1])\n    Y = Y.reshape(-1,)\n    fig,ax = plt.subplots(1,W1.shape[1], figsize=(7,2.5))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n\n    for i in range(nunits):\n        layerf= lambda x : np.maximum(0,(np.dot(x,W1[:,i]) + b1[i]))\n        plt_prob_z(ax[i], layerf)\n        plt_mc_data(ax[i], X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n        ax[i].set_title(f\"Layer 1 Unit {i}\")\n        ax[i].set_ylabel(r\"$x_1$\",size=10)\n        ax[i].set_xlabel(r\"$x_0$\",size=10)\n    fig.tight_layout()\n    plt.show()\n</pre> def plt_layer_relu(X, Y, W1, b1, classes):     nunits = (W1.shape[1])     Y = Y.reshape(-1,)     fig,ax = plt.subplots(1,W1.shape[1], figsize=(7,2.5))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False      for i in range(nunits):         layerf= lambda x : np.maximum(0,(np.dot(x,W1[:,i]) + b1[i]))         plt_prob_z(ax[i], layerf)         plt_mc_data(ax[i], X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')         ax[i].set_title(f\"Layer 1 Unit {i}\")         ax[i].set_ylabel(r\"$x_1$\",size=10)         ax[i].set_xlabel(r\"$x_0$\",size=10)     fig.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_output_layer_linear(X, Y, W, b, classes, x0_rng=None, x1_rng=None):\n    nunits = (W.shape[1])\n    Y = Y.reshape(-1,)\n    fig,ax = plt.subplots(2,int(nunits/2), figsize=(7,5))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n    for i,axi in enumerate(ax.flat):\n        layerf = lambda x : np.dot(x,W[:,i]) + b[i]\n        plt_prob_z(axi, layerf, x0_rng=x0_rng, x1_rng=x1_rng)\n        plt_mc_data(axi, X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n        axi.set_ylabel(r\"$a^{[1]}_1$\",size=9)\n        axi.set_xlabel(r\"$a^{[1]}_0$\",size=9)\n        axi.set_xlim(x0_rng)\n        axi.set_ylim(x1_rng)\n        axi.set_title(f\"Linear Output Unit {i}\")\n    fig.tight_layout()\n    plt.show()\n</pre> def plt_output_layer_linear(X, Y, W, b, classes, x0_rng=None, x1_rng=None):     nunits = (W.shape[1])     Y = Y.reshape(-1,)     fig,ax = plt.subplots(2,int(nunits/2), figsize=(7,5))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False     for i,axi in enumerate(ax.flat):         layerf = lambda x : np.dot(x,W[:,i]) + b[i]         plt_prob_z(axi, layerf, x0_rng=x0_rng, x1_rng=x1_rng)         plt_mc_data(axi, X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')         axi.set_ylabel(r\"$a^{[1]}_1$\",size=9)         axi.set_xlabel(r\"$a^{[1]}_0$\",size=9)         axi.set_xlim(x0_rng)         axi.set_ylim(x1_rng)         axi.set_title(f\"Linear Output Unit {i}\")     fig.tight_layout()     plt.show()"},{"location":"DeepLearning/part2/lab_utils_relu/","title":"Lab utils relu","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nplt.style.use('./deeplearning.mplstyle')\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec plt.style.use('./deeplearning.mplstyle') from matplotlib.widgets import Slider from lab_utils_common import dlc In\u00a0[\u00a0]: Copied! <pre>def widgvis(fig):\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n</pre> def widgvis(fig):     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False In\u00a0[\u00a0]: Copied! <pre>def plt_base(ax):\n    X = np.linspace(0, 3, 3*100)\n    y = np.r_[ -2*X[0:100]+2, 1*X[100:200]-3+2, 3*X[200:300]-7+2 ]\n    w00 = -2\n    b00 =  2\n    w01 =  0  #  1\n    b01 =  0  # -1\n    w02 =  0  #  2\n    b02 =  0  # -4\n    ax[0].plot(X, y, color = dlc[\"dlblue\"], label=\"target\")\n    arts = []\n    arts.extend( plt_yhat(ax[0], X, w00, b00, w01, b01, w02, b02) )\n    _ = plt_unit(ax[1], X, w00, b00)   #Fixed\n    arts.extend( plt_unit(ax[2], X, w01, b01) )\n    arts.extend( plt_unit(ax[3], X, w02, b02) )\n    return(X, arts)\n</pre> def plt_base(ax):     X = np.linspace(0, 3, 3*100)     y = np.r_[ -2*X[0:100]+2, 1*X[100:200]-3+2, 3*X[200:300]-7+2 ]     w00 = -2     b00 =  2     w01 =  0  #  1     b01 =  0  # -1     w02 =  0  #  2     b02 =  0  # -4     ax[0].plot(X, y, color = dlc[\"dlblue\"], label=\"target\")     arts = []     arts.extend( plt_yhat(ax[0], X, w00, b00, w01, b01, w02, b02) )     _ = plt_unit(ax[1], X, w00, b00)   #Fixed     arts.extend( plt_unit(ax[2], X, w01, b01) )     arts.extend( plt_unit(ax[3], X, w02, b02) )     return(X, arts) In\u00a0[\u00a0]: Copied! <pre>def plt_yhat(ax, X, w00, b00, w01, b01, w02, b02):\n    yhat = np.maximum(0, np.dot(w00, X) + b00) + \\\n            np.maximum(0, np.dot(w01, X) + b01) + \\\n            np.maximum(0, np.dot(w02, X) + b02)\n    lp = ax.plot(X, yhat, lw=2, color = dlc[\"dlorange\"], label=\"a2\")\n    return(lp)\n</pre> def plt_yhat(ax, X, w00, b00, w01, b01, w02, b02):     yhat = np.maximum(0, np.dot(w00, X) + b00) + \\             np.maximum(0, np.dot(w01, X) + b01) + \\             np.maximum(0, np.dot(w02, X) + b02)     lp = ax.plot(X, yhat, lw=2, color = dlc[\"dlorange\"], label=\"a2\")     return(lp) In\u00a0[\u00a0]: Copied! <pre>def plt_unit(ax, X, w, b):\n    z = np.dot(w,X) + b\n    yhat = np.maximum(0,z)\n    lpa = ax.plot(X, z,    dlc[\"dlblue\"], label=\"z\")\n    lpb = ax.plot(X, yhat, dlc[\"dlmagenta\"], lw=1, label=\"a\")\n    return([lpa[0], lpb[0]])\n</pre> def plt_unit(ax, X, w, b):     z = np.dot(w,X) + b     yhat = np.maximum(0,z)     lpa = ax.plot(X, z,    dlc[\"dlblue\"], label=\"z\")     lpb = ax.plot(X, yhat, dlc[\"dlmagenta\"], lw=1, label=\"a\")     return([lpa[0], lpb[0]]) <p>if output is need for debug, put this in a cell and call ahead of time. Output will be below that cell. from ipywidgets import Output   #this line stays here output = Output()               #this line stays here display(output)                 #this line goes in notebook</p> In\u00a0[\u00a0]: Copied! <pre>def plt_relu_ex():\n    artists = []\n\n    fig = plt.figure()\n    fig.suptitle(\"Explore Non-Linear Activation\")\n\n    gs = GridSpec(3, 2, width_ratios=[2, 1], height_ratios=[1, 1, 1])\n    ax1 = fig.add_subplot(gs[0:2,0])\n    ax2 = fig.add_subplot(gs[0,1])\n    ax3 = fig.add_subplot(gs[1,1])\n    ax4 = fig.add_subplot(gs[2,1])\n    ax = [ax1,ax2,ax3,ax4]\n    \n    widgvis(fig)\n    #plt.subplots_adjust(bottom=0.35)\n\n    axb2 = fig.add_axes([0.15, 0.10, 0.30, 0.03]) # [left, bottom, width, height]\n    axw2 = fig.add_axes([0.15, 0.15, 0.30, 0.03])\n    axb1 = fig.add_axes([0.15, 0.20, 0.30, 0.03])\n    axw1 = fig.add_axes([0.15, 0.25, 0.30, 0.03])\n\n    sw1 = Slider(axw1, 'w1', -4.0, 4.0, valinit=0, valstep=0.1)\n    sb1 = Slider(axb1, 'b1', -4.0, 4.0, valinit=0, valstep=0.1)\n    sw2 = Slider(axw2, 'w2', -4.0, 4.0, valinit=0, valstep=0.1)\n    sb2 = Slider(axb2, 'b2', -4.0, 4.0, valinit=0, valstep=0.1)\n    \n    X,lp = plt_base(ax)\n    artists.extend( lp )\n    \n    #@output.capture()\n    def update(val):\n        #print(\"-----------\")\n        #print(f\"len artists {len(artists)}\", artists)\n        for i in range(len(artists)):\n            artist = artists[i]\n            #print(\"artist:\", artist)\n            artist.remove()\n        artists.clear()\n        #print(artists)\n        w00 = -2\n        b00 =  2\n        w01 =  sw1.val  #  1\n        b01 =  sb1.val  # -1\n        w02 =  sw2.val  #  2\n        b02 =  sb2.val  # -4\n        artists.extend(plt_yhat(ax[0], X, w00, b00, w01, b01, w02, b02))\n        artists.extend(plt_unit(ax[2], X, w01, b01) )\n        artists.extend(plt_unit(ax[3], X, w02, b02) )\n        #fig.canvas.draw_idle()\n        \n    sw1.on_changed(update)\n    sb1.on_changed(update)\n    sw2.on_changed(update)\n    sb2.on_changed(update)\n\n    ax[0].set_title(\" Match Target \")\n    ax[0].legend()\n    ax[0].set_xlabel(\"x\")\n    ax[1].set_title(\"Unit 0 (fixed) \")\n    ax[1].legend()\n    ax[2].set_title(\"Unit 1\")\n    ax[2].legend() \n    ax[3].set_title(\"Unit 2\")\n    ax[3].legend()\n    plt.tight_layout()\n\n    plt.show()\n    return([sw1,sw2,sb1,sb2,artists]) # returned to keep a live reference to sliders\n</pre> def plt_relu_ex():     artists = []      fig = plt.figure()     fig.suptitle(\"Explore Non-Linear Activation\")      gs = GridSpec(3, 2, width_ratios=[2, 1], height_ratios=[1, 1, 1])     ax1 = fig.add_subplot(gs[0:2,0])     ax2 = fig.add_subplot(gs[0,1])     ax3 = fig.add_subplot(gs[1,1])     ax4 = fig.add_subplot(gs[2,1])     ax = [ax1,ax2,ax3,ax4]          widgvis(fig)     #plt.subplots_adjust(bottom=0.35)      axb2 = fig.add_axes([0.15, 0.10, 0.30, 0.03]) # [left, bottom, width, height]     axw2 = fig.add_axes([0.15, 0.15, 0.30, 0.03])     axb1 = fig.add_axes([0.15, 0.20, 0.30, 0.03])     axw1 = fig.add_axes([0.15, 0.25, 0.30, 0.03])      sw1 = Slider(axw1, 'w1', -4.0, 4.0, valinit=0, valstep=0.1)     sb1 = Slider(axb1, 'b1', -4.0, 4.0, valinit=0, valstep=0.1)     sw2 = Slider(axw2, 'w2', -4.0, 4.0, valinit=0, valstep=0.1)     sb2 = Slider(axb2, 'b2', -4.0, 4.0, valinit=0, valstep=0.1)          X,lp = plt_base(ax)     artists.extend( lp )          #@output.capture()     def update(val):         #print(\"-----------\")         #print(f\"len artists {len(artists)}\", artists)         for i in range(len(artists)):             artist = artists[i]             #print(\"artist:\", artist)             artist.remove()         artists.clear()         #print(artists)         w00 = -2         b00 =  2         w01 =  sw1.val  #  1         b01 =  sb1.val  # -1         w02 =  sw2.val  #  2         b02 =  sb2.val  # -4         artists.extend(plt_yhat(ax[0], X, w00, b00, w01, b01, w02, b02))         artists.extend(plt_unit(ax[2], X, w01, b01) )         artists.extend(plt_unit(ax[3], X, w02, b02) )         #fig.canvas.draw_idle()              sw1.on_changed(update)     sb1.on_changed(update)     sw2.on_changed(update)     sb2.on_changed(update)      ax[0].set_title(\" Match Target \")     ax[0].legend()     ax[0].set_xlabel(\"x\")     ax[1].set_title(\"Unit 0 (fixed) \")     ax[1].legend()     ax[2].set_title(\"Unit 1\")     ax[2].legend()      ax[3].set_title(\"Unit 2\")     ax[3].legend()     plt.tight_layout()      plt.show()     return([sw1,sw2,sb1,sb2,artists]) # returned to keep a live reference to sliders"},{"location":"DeepLearning/part2/lab_utils_softmax/","title":"Lab utils softmax","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom IPython.display import display, Markdown, Latex\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from IPython.display import display, Markdown, Latex from matplotlib.widgets import Slider from lab_utils_common import dlc In\u00a0[\u00a0]: Copied! <pre>def plt_softmax(my_softmax):\n    fig, ax = plt.subplots(1,2,figsize=(8,4))\n    plt.subplots_adjust(bottom=0.35)\n\n    axz0 = fig.add_axes([0.15, 0.10, 0.30, 0.03]) # [left, bottom, width, height]\n    axz1 = fig.add_axes([0.15, 0.15, 0.30, 0.03])\n    axz2 = fig.add_axes([0.15, 0.20, 0.30, 0.03])\n    axz3 = fig.add_axes([0.15, 0.25, 0.30, 0.03])\n\n    z3 = Slider(axz3, 'z3', 0.1, 10.0, valinit=4, valstep=0.1)\n    z2 = Slider(axz2, 'z2', 0.1, 10.0, valinit=3, valstep=0.1)\n    z1 = Slider(axz1, 'z1', 0.1, 10.0, valinit=2, valstep=0.1)\n    z0 = Slider(axz0, 'z0', 0.1, 10.0, valinit=1, valstep=0.1)\n\n    z = np.array(['z0','z1','z2','z3'])\n    bar = ax[0].barh(z, height=0.6, width=[z0.val,z1.val,z2.val,z3.val], left=None, align='center')\n    bars = bar.get_children()\n    ax[0].set_xlim([0,10])\n    ax[0].set_title(\"z input to softmax\")\n\n    a = my_softmax(np.array([z0.val,z1.val,z2.val,z3.val]))\n    anames = np.array(['a0','a1','a2','a3'])\n    sbar = ax[1].barh(anames, height=0.6, width=a, left=None, align='center',color=dlc[\"dldarkred\"])\n    sbars = sbar.get_children()\n    ax[1].set_xlim([0,1])\n    ax[1].set_title(\"softmax(z)\")\n\n    def update(val):\n        bars[0].set_width(z0.val)\n        bars[1].set_width(z1.val)\n        bars[2].set_width(z2.val)\n        bars[3].set_width(z3.val)\n        a = my_softmax(np.array([z0.val,z1.val,z2.val,z3.val]))\n        sbars[0].set_width(a[0])\n        sbars[1].set_width(a[1])\n        sbars[2].set_width(a[2])\n        sbars[3].set_width(a[3])\n\n        fig.canvas.draw_idle()\n\n    z0.on_changed(update)\n    z1.on_changed(update)\n    z2.on_changed(update)\n    z3.on_changed(update)\n\n    plt.show()\n</pre> def plt_softmax(my_softmax):     fig, ax = plt.subplots(1,2,figsize=(8,4))     plt.subplots_adjust(bottom=0.35)      axz0 = fig.add_axes([0.15, 0.10, 0.30, 0.03]) # [left, bottom, width, height]     axz1 = fig.add_axes([0.15, 0.15, 0.30, 0.03])     axz2 = fig.add_axes([0.15, 0.20, 0.30, 0.03])     axz3 = fig.add_axes([0.15, 0.25, 0.30, 0.03])      z3 = Slider(axz3, 'z3', 0.1, 10.0, valinit=4, valstep=0.1)     z2 = Slider(axz2, 'z2', 0.1, 10.0, valinit=3, valstep=0.1)     z1 = Slider(axz1, 'z1', 0.1, 10.0, valinit=2, valstep=0.1)     z0 = Slider(axz0, 'z0', 0.1, 10.0, valinit=1, valstep=0.1)      z = np.array(['z0','z1','z2','z3'])     bar = ax[0].barh(z, height=0.6, width=[z0.val,z1.val,z2.val,z3.val], left=None, align='center')     bars = bar.get_children()     ax[0].set_xlim([0,10])     ax[0].set_title(\"z input to softmax\")      a = my_softmax(np.array([z0.val,z1.val,z2.val,z3.val]))     anames = np.array(['a0','a1','a2','a3'])     sbar = ax[1].barh(anames, height=0.6, width=a, left=None, align='center',color=dlc[\"dldarkred\"])     sbars = sbar.get_children()     ax[1].set_xlim([0,1])     ax[1].set_title(\"softmax(z)\")      def update(val):         bars[0].set_width(z0.val)         bars[1].set_width(z1.val)         bars[2].set_width(z2.val)         bars[3].set_width(z3.val)         a = my_softmax(np.array([z0.val,z1.val,z2.val,z3.val]))         sbars[0].set_width(a[0])         sbars[1].set_width(a[1])         sbars[2].set_width(a[2])         sbars[3].set_width(a[3])          fig.canvas.draw_idle()      z0.on_changed(update)     z1.on_changed(update)     z2.on_changed(update)     z3.on_changed(update)      plt.show()"},{"location":"DeepLearning/part2/public_tests/","title":"Public tests","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.activations import linear, sigmoid, relu\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.activations import linear, sigmoid, relu In\u00a0[\u00a0]: Copied! <pre>def test_my_softmax(target):\n    z = np.array([1., 2., 3., 4.])\n    a = target(z)\n    atf = tf.nn.softmax(z)\n    \n    assert np.allclose(a, atf, atol=1e-10), f\"Wrong values. Expected {atf}, got {a}\"\n    \n    z = np.array([np.log(0.1)] * 10)\n    a = target(z)\n    atf = tf.nn.softmax(z)\n    \n    assert np.allclose(a, atf, atol=1e-10), f\"Wrong values. Expected {atf}, got {a}\"\n    \n    print(\"\\033[92m All tests passed.\")\n</pre> def test_my_softmax(target):     z = np.array([1., 2., 3., 4.])     a = target(z)     atf = tf.nn.softmax(z)          assert np.allclose(a, atf, atol=1e-10), f\"Wrong values. Expected {atf}, got {a}\"          z = np.array([np.log(0.1)] * 10)     a = target(z)     atf = tf.nn.softmax(z)          assert np.allclose(a, atf, atol=1e-10), f\"Wrong values. Expected {atf}, got {a}\"          print(\"\\033[92m All tests passed.\") In\u00a0[\u00a0]: Copied! <pre>def test_model(target, classes, input_size):\n    target.build(input_shape=(None,input_size))\n    \n    assert len(target.layers) == 3, \\\n        f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n    assert target.input.shape.as_list() == [None, input_size], \\\n        f\"Wrong input shape. Expected [None,  {input_size}] but got {target.input.shape.as_list()}\"\n    i = 0\n    expected = [[Dense, [None, 25], relu],\n                [Dense, [None, 15], relu],\n                [Dense, [None, classes], linear]]\n\n    for layer in target.layers:\n        assert type(layer) == expected[i][0], \\\n            f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"\n        assert layer.output.shape.as_list() == expected[i][1], \\\n            f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"\n        assert layer.activation == expected[i][2], \\\n            f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"\n        i = i + 1\n\n    print(\"\\033[92mAll tests passed!\")\n</pre> def test_model(target, classes, input_size):     target.build(input_shape=(None,input_size))          assert len(target.layers) == 3, \\         f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"     assert target.input.shape.as_list() == [None, input_size], \\         f\"Wrong input shape. Expected [None,  {input_size}] but got {target.input.shape.as_list()}\"     i = 0     expected = [[Dense, [None, 25], relu],                 [Dense, [None, 15], relu],                 [Dense, [None, classes], linear]]      for layer in target.layers:         assert type(layer) == expected[i][0], \\             f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"         assert layer.output.shape.as_list() == expected[i][1], \\             f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"         assert layer.activation == expected[i][2], \\             f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"         i = i + 1      print(\"\\033[92mAll tests passed!\")"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/","title":"Optional Lab - Softmax Function","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom IPython.display import display, Markdown, Latex\nfrom sklearn.datasets import make_blobs\n%matplotlib widget\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\nfrom lab_utils_softmax import plt_softmax\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from IPython.display import display, Markdown, Latex from sklearn.datasets import make_blobs %matplotlib widget from matplotlib.widgets import Slider from lab_utils_common import dlc from lab_utils_softmax import plt_softmax import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) <p>Note: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N.</p> <p>The softmax function can be written: $$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$ The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write: \\begin{align} \\mathbf{a}(x) = \\begin{bmatrix} P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\ \\vdots \\\\ P(y = N | \\mathbf{x}; \\mathbf{w},b) \\end{bmatrix} = \\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }} \\begin{bmatrix} e^{z_1} \\\\ \\vdots \\\\ e^{z_{N}} \\\\ \\end{bmatrix} \\tag{2} \\end{align}</p> <p>Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$. Let's create a NumPy implementation:</p> In\u00a0[\u00a0]: Copied! <pre>def my_softmax(z):\n    ez = np.exp(z)              #element-wise exponenial\n    sm = ez/np.sum(ez)\n    return(sm)\n</pre> def my_softmax(z):     ez = np.exp(z)              #element-wise exponenial     sm = ez/np.sum(ez)     return(sm) <p>Below, vary the values of the <code>z</code> inputs using the sliders.</p> In\u00a0[\u00a0]: Copied! <pre>plt.close(\"all\")\nplt_softmax(my_softmax)\n</pre> plt.close(\"all\") plt_softmax(my_softmax) <p>As you are varying the values of the z's above, there are a few things to note:</p> <ul> <li>the exponential in the numerator of the softmax magnifies small differences in the values</li> <li>the output values sum to one</li> <li>the softmax spans all of the outputs. A change in <code>z0</code> for example will change the values of <code>a0</code>-<code>a3</code>. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.</li> </ul> <p>The loss function associated with Softmax, the cross-entropy loss, is: \\begin{equation}   L(\\mathbf{a},y)=\\begin{cases}     -log(a_1), &amp; \\text{if $y=1$}.\\\\         &amp;\\vdots\\\\      -log(a_N), &amp; \\text{if $y=N$}   \\end{cases} \\tag{3} \\end{equation}</p> <p>Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.</p> <p>Recall: In this course, Loss is for one example while Cost covers all examples.</p> <p>Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}     1, &amp; \\text{if $y==n$}.\\\\     0, &amp; \\text{otherwise}.   \\end{cases}$$ Now the cost is: \\begin{align} J(\\mathbf{w},b) = - \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4} \\end{align}</p> <p>Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.</p> In\u00a0[\u00a0]: Copied! <pre># make  dataset for example\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nX_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n</pre> # make  dataset for example centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]] X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30) <p>The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the <code>compile</code> directive.</p> <p>The loss function <code>SparseCategoricalCrossentropy</code>. The loss described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities.</p> In\u00a0[\u00a0]: Copied! <pre>model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'softmax')    # &lt; softmax activation here\n    ]\n)\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'softmax')    # &lt; softmax activation here     ] ) model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(),     optimizer=tf.keras.optimizers.Adam(0.001), )  model.fit(     X_train,y_train,     epochs=10 )          <p>Because the softmax is integrated into the output layer, the output is a vector of probabilities.</p> In\u00a0[\u00a0]: Copied! <pre>p_nonpreferred = model.predict(X_train)\nprint(p_nonpreferred [:2])\nprint(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))\n</pre> p_nonpreferred = model.predict(X_train) print(p_nonpreferred [:2]) print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred)) Preferred  <p>Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.   This is enabled by the 'preferred' organization shown here.</p> <p>In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as logits. The loss function has an additional argument: <code>from_logits = True</code>. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation.</p> In\u00a0[\u00a0]: Copied! <pre>preferred_model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'linear')   #&lt;-- Note\n    ]\n)\npreferred_model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\npreferred_model.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> preferred_model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'linear')   #&lt;-- Note     ] ) preferred_model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note     optimizer=tf.keras.optimizers.Adam(0.001), )  preferred_model.fit(     X_train,y_train,     epochs=10 )          In\u00a0[\u00a0]: Copied! <pre>p_preferred = preferred_model.predict(X_train)\nprint(f\"two example output vectors:\\n {p_preferred[:2]}\")\nprint(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))\n</pre> p_preferred = preferred_model.predict(X_train) print(f\"two example output vectors:\\n {p_preferred[:2]}\") print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred)) <p>The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a softmax.</p> In\u00a0[\u00a0]: Copied! <pre>sm_preferred = tf.nn.softmax(p_preferred).numpy()\nprint(f\"two example output vectors:\\n {sm_preferred[:2]}\")\nprint(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))\n</pre> sm_preferred = tf.nn.softmax(p_preferred).numpy() print(f\"two example output vectors:\\n {sm_preferred[:2]}\") print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred)) <p>To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax().</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")\n</pre> for i in range(5):     print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#optional-lab-softmax-function","title":"Optional Lab - Softmax Function\u00b6","text":"<p>In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#softmax-function","title":"Softmax Function\u00b6","text":"<p>In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#cost","title":"Cost\u00b6","text":""},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#tensorflow","title":"Tensorflow\u00b6","text":"<p>This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.</p> <p>Let's start by creating a dataset to train a multiclass classification model.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#the-obvious-organization","title":"The Obvious organization\u00b6","text":""},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#output-handling","title":"Output Handling\u00b6","text":"<p>Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. Let's look at the preferred model outputs:</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#sparsecategorialcrossentropy-or-categoricalcrossentropy","title":"SparseCategorialCrossentropy or CategoricalCrossEntropy\u00b6","text":"<p>Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.</p> <ul> <li>SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.</li> <li>CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].</li> </ul>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy1/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you</p> <ul> <li>Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks.</li> <li>Learned the preferred model construction in Tensorflow:<ul> <li>No activation on the final layer (same as linear activation)</li> <li>SparseCategoricalCrossentropy loss function</li> <li>use from_logits=True</li> </ul> </li> <li>Recognized that unlike ReLU and Sigmoid, the softmax spans multiple outputs.</li> </ul>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/","title":"Optional Lab - Softmax Function","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom IPython.display import display, Markdown, Latex\nfrom sklearn.datasets import make_blobs\n%matplotlib widget\nfrom matplotlib.widgets import Slider\nfrom lab_utils_common import dlc\nfrom lab_utils_softmax import plt_softmax\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from IPython.display import display, Markdown, Latex from sklearn.datasets import make_blobs %matplotlib widget from matplotlib.widgets import Slider from lab_utils_common import dlc from lab_utils_softmax import plt_softmax import logging logging.getLogger(\"tensorflow\").setLevel(logging.ERROR) tf.autograph.set_verbosity(0) <p>Note: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N.</p> <p>The softmax function can be written: $$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$ The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write: \\begin{align} \\mathbf{a}(x) = \\begin{bmatrix} P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\ \\vdots \\\\ P(y = N | \\mathbf{x}; \\mathbf{w},b) \\end{bmatrix} = \\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }} \\begin{bmatrix} e^{z_1} \\\\ \\vdots \\\\ e^{z_{N}} \\\\ \\end{bmatrix} \\tag{2} \\end{align}</p> <p>Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$. Let's create a NumPy implementation:</p> In\u00a0[\u00a0]: Copied! <pre>def my_softmax(z):\n    ez = np.exp(z)              #element-wise exponenial\n    sm = ez/np.sum(ez)\n    return(sm)\n</pre> def my_softmax(z):     ez = np.exp(z)              #element-wise exponenial     sm = ez/np.sum(ez)     return(sm) <p>Below, vary the values of the <code>z</code> inputs using the sliders.</p> In\u00a0[\u00a0]: Copied! <pre>plt.close(\"all\")\nplt_softmax(my_softmax)\n</pre> plt.close(\"all\") plt_softmax(my_softmax) <p>As you are varying the values of the z's above, there are a few things to note:</p> <ul> <li>the exponential in the numerator of the softmax magnifies small differences in the values</li> <li>the output values sum to one</li> <li>the softmax spans all of the outputs. A change in <code>z0</code> for example will change the values of <code>a0</code>-<code>a3</code>. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.</li> </ul> <p>The loss function associated with Softmax, the cross-entropy loss, is: \\begin{equation}   L(\\mathbf{a},y)=\\begin{cases}     -log(a_1), &amp; \\text{if $y=1$}.\\\\         &amp;\\vdots\\\\      -log(a_N), &amp; \\text{if $y=N$}   \\end{cases} \\tag{3} \\end{equation}</p> <p>Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.</p> <p>Recall: In this course, Loss is for one example while Cost covers all examples.</p> <p>Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}     1, &amp; \\text{if $y==n$}.\\\\     0, &amp; \\text{otherwise}.   \\end{cases}$$ Now the cost is: \\begin{align} J(\\mathbf{w},b) = - \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4} \\end{align}</p> <p>Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.</p> In\u00a0[\u00a0]: Copied! <pre># make  dataset for example\ncenters = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\nX_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n</pre> # make  dataset for example centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]] X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30) <p>The model below is implemented with the softmax as an activation in the final Dense layer. The loss function is separately specified in the <code>compile</code> directive.</p> <p>The loss function is <code>SparseCategoricalCrossentropy</code>. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities.</p> In\u00a0[\u00a0]: Copied! <pre>model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'softmax')    # &lt; softmax activation here\n    ]\n)\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'softmax')    # &lt; softmax activation here     ] ) model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(),     optimizer=tf.keras.optimizers.Adam(0.001), )  model.fit(     X_train,y_train,     epochs=10 )          <p>Because the softmax is integrated into the output layer, the output is a vector of probabilities.</p> In\u00a0[\u00a0]: Copied! <pre>p_nonpreferred = model.predict(X_train)\nprint(p_nonpreferred [:2])\nprint(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))\n</pre> p_nonpreferred = model.predict(X_train) print(p_nonpreferred [:2]) print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred)) Preferred  <p>Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.   This is enabled by the 'preferred' organization shown here.</p> <p>In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as logits. The loss function has an additional argument: <code>from_logits = True</code>. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation.</p> In\u00a0[\u00a0]: Copied! <pre>preferred_model = Sequential(\n    [ \n        Dense(25, activation = 'relu'),\n        Dense(15, activation = 'relu'),\n        Dense(4, activation = 'linear')   #&lt;-- Note\n    ]\n)\npreferred_model.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\npreferred_model.fit(\n    X_train,y_train,\n    epochs=10\n)\n        \n</pre> preferred_model = Sequential(     [          Dense(25, activation = 'relu'),         Dense(15, activation = 'relu'),         Dense(4, activation = 'linear')   #&lt;-- Note     ] ) preferred_model.compile(     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #&lt;-- Note     optimizer=tf.keras.optimizers.Adam(0.001), )  preferred_model.fit(     X_train,y_train,     epochs=10 )          In\u00a0[\u00a0]: Copied! <pre>p_preferred = preferred_model.predict(X_train)\nprint(f\"two example output vectors:\\n {p_preferred[:2]}\")\nprint(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))\n</pre> p_preferred = preferred_model.predict(X_train) print(f\"two example output vectors:\\n {p_preferred[:2]}\") print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred)) <p>The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a softmax.</p> In\u00a0[\u00a0]: Copied! <pre>sm_preferred = tf.nn.softmax(p_preferred).numpy()\nprint(f\"two example output vectors:\\n {sm_preferred[:2]}\")\nprint(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))\n</pre> sm_preferred = tf.nn.softmax(p_preferred).numpy() print(f\"two example output vectors:\\n {sm_preferred[:2]}\") print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred)) <p>To select the most likely category, the softmax is not required. One can find the index of the largest output using np.argmax().</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")\n</pre> for i in range(5):     print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#optional-lab-softmax-function","title":"Optional Lab - Softmax Function\u00b6","text":"<p>In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#softmax-function","title":"Softmax Function\u00b6","text":"<p>In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#cost","title":"Cost\u00b6","text":""},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#tensorflow","title":"Tensorflow\u00b6","text":"<p>This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.</p> <p>Let's start by creating a dataset to train a multiclass classification model.</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#the-obvious-organization","title":"The Obvious organization\u00b6","text":""},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#output-handling","title":"Output Handling\u00b6","text":"<p>Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. Let's look at the preferred model outputs:</p>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#sparsecategorialcrossentropy-or-categoricalcrossentropy","title":"SparseCategorialCrossentropy or CategoricalCrossEntropy\u00b6","text":"<p>Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.</p> <ul> <li>SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.</li> <li>CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].</li> </ul>"},{"location":"DeepLearning/part2/archive/C2_W2_SoftMax-Copy2/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you</p> <ul> <li>Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks.</li> <li>Learned the preferred model construction in Tensorflow:<ul> <li>No activation on the final layer (same as linear activation)</li> <li>SparseCategoricalCrossentropy loss function</li> <li>use from_logits=True</li> </ul> </li> <li>Recognized that unlike ReLU and Sigmoid, the softmax spans multiple outputs.</li> </ul>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/","title":"Optional Lab: Back propagation using a computation graph","text":"In\u00a0[1]: Copied! <pre>from sympy import *\nimport numpy as np\nimport re\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import TextBox\nfrom matplotlib.widgets import Button\nimport ipywidgets as widgets\nfrom lab_utils_backprop import *\n</pre> from sympy import * import numpy as np import re %matplotlib widget import matplotlib.pyplot as plt from matplotlib.widgets import TextBox from matplotlib.widgets import Button import ipywidgets as widgets from lab_utils_backprop import * In\u00a0[32]: Copied! <pre>plt.close(\"all\")\nplt_network(config_nw0, \"./images/C2_W2_BP_network0.PNG\")\n</pre> plt.close(\"all\") plt_network(config_nw0, \"./images/C2_W2_BP_network0.PNG\") Out[32]: <pre>&lt;lab_utils_backprop.plt_network at 0x1eaac20e4d0&gt;</pre>                      Figure                  <p>Above, you can see we broke the expression into two nodes which we can work on independently. If you already have a good understanding of the process from the lecture, you can go ahead and fill in the boxes in the diagram above. You will want to first fill in the blue boxes going left to right and then fill in the green boxes starting on the right and moving to the left. If you have the correct values, the values will show as green or blue. If the value is incorrect, it will be red. Note, the interactive graphic is not particularly robust. If you run into trouble with the interface, run the cell above again to restart.</p> <p>If you are unsure of the process, we will work this example step by step below.</p> In\u00a0[3]: Copied! <pre>w = 3\na = 2+3*w\nJ = a**2\nprint(f\"a = {a}, J = {J}\")\n</pre> w = 3 a = 2+3*w J = a**2 print(f\"a = {a}, J = {J}\") <pre>a = 11, J = 121\n</pre> <p>You can fill these values in the blue boxes above.</p> In\u00a0[4]: Copied! <pre>a_epsilon = a + 0.001       # a epsilon\nJ_epsilon = a_epsilon**2    # J_epsilon\nk = (J_epsilon - J)/0.001   # difference divided by epsilon\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \")\n</pre> a_epsilon = a + 0.001       # a epsilon J_epsilon = a_epsilon**2    # J_epsilon k = (J_epsilon - J)/0.001   # difference divided by epsilon print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \") <pre>J = 121, J_epsilon = 121.02200099999999, dJ_da ~= k = 22.000999999988835 \n</pre> <p>$\\frac{\\partial J}{\\partial a}$ is 22 which is $2\\times a$. Our result is not exactly $2 \\times a$ because our epsilon value is not infinitesimally small.</p> In\u00a0[5]: Copied! <pre>sw,sJ,sa = symbols('w,J,a')\nsJ = sa**2\nsJ\n</pre> sw,sJ,sa = symbols('w,J,a') sJ = sa**2 sJ Out[5]:  $\\displaystyle a^{2}$  In\u00a0[6]: Copied! <pre>sJ.subs([(sa,a)])\n</pre> sJ.subs([(sa,a)]) Out[6]:  $\\displaystyle 121$  In\u00a0[7]: Copied! <pre>dJ_da = diff(sJ, sa)\ndJ_da\n</pre> dJ_da = diff(sJ, sa) dJ_da Out[7]:  $\\displaystyle 2 a$  <p>So, $\\frac{\\partial J}{\\partial a} = 2a$. When $a=11$, $\\frac{\\partial J}{\\partial a} = 22$. This matches our arithmetic calculation above. If you have not already done so, you can go back to the diagram above and fill in the value for $\\frac{\\partial J}{\\partial a}$.</p> In\u00a0[8]: Copied! <pre>w_epsilon = w + 0.001       # a  plus a small value, epsilon\na_epsilon = 2 + 3*w_epsilon\nk = (a_epsilon - a)/0.001   # difference divided by epsilon\nprint(f\"a = {a}, a_epsilon = {a_epsilon}, da_dw ~= k = {k} \")\n</pre> w_epsilon = w + 0.001       # a  plus a small value, epsilon a_epsilon = 2 + 3*w_epsilon k = (a_epsilon - a)/0.001   # difference divided by epsilon print(f\"a = {a}, a_epsilon = {a_epsilon}, da_dw ~= k = {k} \") <pre>a = 11, a_epsilon = 11.003, da_dw ~= k = 3.0000000000001137 \n</pre> <p>Calculated arithmetically,  $\\frac{\\partial a}{\\partial w} \\approx 3$. Let's try it with SymPy.</p> In\u00a0[9]: Copied! <pre>sa = 2 + 3*sw\nsa\n</pre> sa = 2 + 3*sw sa Out[9]:  $\\displaystyle 3 w + 2$  In\u00a0[10]: Copied! <pre>da_dw = diff(sa,sw)\nda_dw\n</pre> da_dw = diff(sa,sw) da_dw Out[10]:  $\\displaystyle 3$  <p>The next step is the interesting part:</p> <ul> <li>We know that a small change in $w$ will cause $a$ to change by 3 times that amount.</li> <li>We know that a small change in $a$ will cause $J$ to change by $2\\times a$ times that amount. (a=11 in this example) so, putting these together,</li> <li>We  know that a small change in $w$ will cause $J$ to change by $3 \\times 2\\times a$ times that amount.</li> </ul> <p>These cascading changes go by the name of the chain rule.  It can be written like this: $$\\frac{\\partial J}{\\partial w} = \\frac{\\partial a}{\\partial w} \\frac{\\partial J}{\\partial a} $$</p> <p>It's worth spending some time thinking this through if it is not clear. This is a key take-away.</p> <p>Let's try calculating it:</p> In\u00a0[11]: Copied! <pre>dJ_dw = da_dw * dJ_da\ndJ_dw\n</pre> dJ_dw = da_dw * dJ_da dJ_dw Out[11]:  $\\displaystyle 6 a$  <p>And $a$ is 11 in this example so $\\frac{\\partial J}{\\partial w} = 66$. We can check this arithmetically:</p> In\u00a0[12]: Copied! <pre>w_epsilon = w + 0.001\na_epsilon = 2 + 3*w_epsilon\nJ_epsilon = a_epsilon**2\nk = (J_epsilon - J)/0.001   # difference divided by epsilon\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> w_epsilon = w + 0.001 a_epsilon = 2 + 3*w_epsilon J_epsilon = a_epsilon**2 k = (J_epsilon - J)/0.001   # difference divided by epsilon print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 121, J_epsilon = 121.06600900000001, dJ_dw ~= k = 66.0090000000082 \n</pre> <p>OK! You can now fill the values for  $\\frac{\\partial a}{\\partial w}$ and $\\frac{\\partial J}{\\partial w}$ in  the diagram if you have not already done so.</p> <p>Another view One could visualize these cascading changes this way:  A small change in $w$ is multiplied by $\\frac{\\partial a}{\\partial w}$ resulting in a change that is 3 times as large. This larger change is then multiplied by $\\frac{\\partial J}{\\partial a}$ resulting in a change that is now $3 \\times 22 = 66$ times larger.</p> In\u00a0[13]: Copied! <pre>plt.close(\"all\")\nplt_network(config_nw1, \"./images/C2_W2_BP_network1.PNG\")\n</pre> plt.close(\"all\") plt_network(config_nw1, \"./images/C2_W2_BP_network1.PNG\") Out[13]: <pre>&lt;lab_utils_backprop.plt_network at 0x1eaaaf5e5f0&gt;</pre>                      Figure                  <p>Below, we will go through the computations required to fill in the above computation graph in detail. We start with the forward path.</p> In\u00a0[14]: Copied! <pre># Inputs and parameters\nx = 2\nw = -2\nb = 8\ny = 1\n# calculate per step values   \nc = w * x\na = c + b\nd = a - y\nJ = d**2/2\nprint(f\"J={J}, d={d}, a={a}, c={c}\")\n</pre> # Inputs and parameters x = 2 w = -2 b = 8 y = 1 # calculate per step values    c = w * x a = c + b d = a - y J = d**2/2 print(f\"J={J}, d={d}, a={a}, c={c}\") <pre>J=4.5, d=3, a=4, c=-4\n</pre> In\u00a0[15]: Copied! <pre>d_epsilon = d + 0.001\nJ_epsilon = d_epsilon**2/2\nk = (J_epsilon - J)/0.001   # difference divided by epsilon\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dd ~= k = {k} \")\n</pre> d_epsilon = d + 0.001 J_epsilon = d_epsilon**2/2 k = (J_epsilon - J)/0.001   # difference divided by epsilon print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dd ~= k = {k} \") <pre>J = 4.5, J_epsilon = 4.5030005, dJ_dd ~= k = 3.0004999999997395 \n</pre> <p>$\\frac{\\partial J}{\\partial d}$ is 3, which is the value of $d$. Our result is not exactly $d$ because our epsilon value is not infinitesimally small.</p> In\u00a0[16]: Copied! <pre>sx,sw,sb,sy,sJ = symbols('x,w,b,y,J')\nsa, sc, sd = symbols('a,c,d')\nsJ = sd**2/2\nsJ\n</pre> sx,sw,sb,sy,sJ = symbols('x,w,b,y,J') sa, sc, sd = symbols('a,c,d') sJ = sd**2/2 sJ Out[16]:  $\\displaystyle \\frac{d^{2}}{2}$  In\u00a0[17]: Copied! <pre>sJ.subs([(sd,d)])\n</pre> sJ.subs([(sd,d)]) Out[17]:  $\\displaystyle \\frac{9}{2}$  In\u00a0[18]: Copied! <pre>dJ_dd = diff(sJ, sd)\ndJ_dd\n</pre> dJ_dd = diff(sJ, sd) dJ_dd Out[18]:  $\\displaystyle d$  <p>So, $\\frac{\\partial J}{\\partial d}$ = d. When $d=3$, $\\frac{\\partial J}{\\partial d}$ = 3. This matches our arithmetic calculation above. If you have not already done so, you can go back to the diagram above and fill in the value for $\\frac{\\partial J}{\\partial d}$.</p> In\u00a0[19]: Copied! <pre>a_epsilon = a + 0.001         # a  plus a small value\nd_epsilon = a_epsilon - y\nk = (d_epsilon - d)/0.001   # difference divided by epsilon\nprint(f\"d = {d}, d_epsilon = {d_epsilon}, dd_da ~= k = {k} \")\n</pre> a_epsilon = a + 0.001         # a  plus a small value d_epsilon = a_epsilon - y k = (d_epsilon - d)/0.001   # difference divided by epsilon print(f\"d = {d}, d_epsilon = {d_epsilon}, dd_da ~= k = {k} \") <pre>d = 3, d_epsilon = 3.0010000000000003, dd_da ~= k = 1.000000000000334 \n</pre> <p>Calculated arithmetically,  $\\frac{\\partial d}{\\partial a} \\approx 1$. Let's try it with SymPy.</p> In\u00a0[20]: Copied! <pre>sd = sa - sy\nsd\n</pre> sd = sa - sy sd Out[20]:  $\\displaystyle a - y$  In\u00a0[21]: Copied! <pre>dd_da = diff(sd,sa)\ndd_da\n</pre> dd_da = diff(sd,sa) dd_da Out[21]:  $\\displaystyle 1$  <p>Calculated arithmetically,  $\\frac{\\partial d}{\\partial a}$ also equals 1.</p> <p>The next step is the interesting part, repeated again in this example:</p> <ul> <li>We know that a small change in $a$ will cause $d$ to change by 1 times that amount.</li> <li>We know that a small change in $d$ will cause $J$ to change by $d$ times that amount. (d=3 in this example) so, putting these together,</li> <li>We  know that a small change in $a$ will cause $J$ to change by $1\\times d$ times that amount.</li> </ul> <p>This is again the chain rule.  It can be written like this: $$\\frac{\\partial J}{\\partial a} = \\frac{\\partial d}{\\partial a} \\frac{\\partial J}{\\partial d} $$</p> <p>Let's try calculating it:</p> In\u00a0[22]: Copied! <pre>dJ_da = dd_da * dJ_dd\ndJ_da\n</pre> dJ_da = dd_da * dJ_dd dJ_da Out[22]:  $\\displaystyle d$  <p>And $d$ is 3 in this example so $\\frac{\\partial J}{\\partial a} = 3$. We can check this arithmetically:</p> In\u00a0[23]: Copied! <pre>a_epsilon = a + 0.001\nd_epsilon = a_epsilon - y\nJ_epsilon = d_epsilon**2/2\nk = (J_epsilon - J)/0.001   \nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \")\n</pre> a_epsilon = a + 0.001 d_epsilon = a_epsilon - y J_epsilon = d_epsilon**2/2 k = (J_epsilon - J)/0.001    print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \") <pre>J = 4.5, J_epsilon = 4.503000500000001, dJ_da ~= k = 3.0005000000006277 \n</pre> <p>OK, they match! You can now fill the values for  $\\frac{\\partial d}{\\partial a}$ and $\\frac{\\partial J}{\\partial a}$ in the diagram if you have not already done so.</p> <p>The steps in backprop Now that you have worked through several nodes, we can write down the basic method: working right to left, for each node:</p> <ul> <li>calculate the local derivative(s) of the node</li> <li>using the chain rule, combine with the derivative of the cost with respect to the node to the right.</li> </ul> <p>The 'local derivative(s)' are the derivative(s) of the output of the current node with respect to all inputs or parameters.</p> <p>Let's continue the job. We'll be a bit less verbose now that you are familiar with the method.</p> In\u00a0[24]: Copied! <pre># calculate the local derivatives da_dc, da_db\nsa = sc + sb\nsa\n</pre> # calculate the local derivatives da_dc, da_db sa = sc + sb sa Out[24]:  $\\displaystyle b + c$  In\u00a0[25]: Copied! <pre>da_dc = diff(sa,sc)\nda_db = diff(sa,sb)\nprint(da_dc, da_db)\n</pre> da_dc = diff(sa,sc) da_db = diff(sa,sb) print(da_dc, da_db) <pre>1 1\n</pre> In\u00a0[26]: Copied! <pre>dJ_dc = da_dc * dJ_da\ndJ_db = da_db * dJ_da\nprint(f\"dJ_dc = {dJ_dc},  dJ_db = {dJ_db}\")\n</pre> dJ_dc = da_dc * dJ_da dJ_db = da_db * dJ_da print(f\"dJ_dc = {dJ_dc},  dJ_db = {dJ_db}\") <pre>dJ_dc = d,  dJ_db = d\n</pre> <p>And in our example, d = 3</p> In\u00a0[27]: Copied! <pre># calculate the local derivative\nsc = sw * sx\nsc\n</pre> # calculate the local derivative sc = sw * sx sc Out[27]:  $\\displaystyle w x$  In\u00a0[28]: Copied! <pre>dc_dw = diff(sc,sw)\ndc_dw\n</pre> dc_dw = diff(sc,sw) dc_dw Out[28]:  $\\displaystyle x$  <p>This derivative is a bit more exciting than the last one. This will vary depending on the value of $x$. This is 2 in our example.</p> <p>Combine this with $\\frac{\\partial J}{\\partial c}$ to find $\\frac{\\partial J}{\\partial w}$.</p> In\u00a0[29]: Copied! <pre>dJ_dw = dc_dw * dJ_dc\ndJ_dw\n</pre> dJ_dw = dc_dw * dJ_dc dJ_dw Out[29]:  $\\displaystyle d x$  In\u00a0[30]: Copied! <pre>print(f\"dJ_dw = {dJ_dw.subs([(sd,d),(sx,x)])}\")\n</pre> print(f\"dJ_dw = {dJ_dw.subs([(sd,d),(sx,x)])}\") <pre>dJ_dw = 2*d\n</pre> <p>$d=3$,  so $\\frac{\\partial J}{\\partial w} = 6$ for our example. Let's test this arithmetically:</p> In\u00a0[31]: Copied! <pre>J_epsilon = ((w+0.001)*x+b - y)**2/2\nk = (J_epsilon - J)/0.001  \nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J_epsilon = ((w+0.001)*x+b - y)**2/2 k = (J_epsilon - J)/0.001   print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 4.5, J_epsilon = 4.506002, dJ_dw ~= k = 6.001999999999619 \n</pre> <p>They match! Great. You can add $\\frac{\\partial J}{\\partial w}$ to the diagram above and our analysis is complete.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#optional-lab-back-propagation-using-a-computation-graph","title":"Optional Lab: Back propagation using a computation graph\u00b6","text":"<p>Working through this lab will give you insight into a key algorithm used by most machine learning frameworks. Gradient descent requires the derivative of the cost with respect to each parameter in the network.  Neural networks can have millions or even billions of parameters. The back propagation algorithm is used to compute those derivatives. Computation graphs are used to simplify the operation. Let's dig into this below.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#computation-graph","title":"Computation Graph\u00b6","text":"<p>A computation graph simplifies the computation of complex derivatives by breaking them into smaller steps. Let's see how this works.</p> <p>Let's calculate the derivative of this slightly complex expression, $J = (2+3w)^2$. We would like to find the derivative of $J$ with respect to $w$ or $\\frac{\\partial J}{\\partial w}$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#forward-propagation","title":"Forward Propagation\u00b6","text":"<p>Let's calculate the values in the forward direction.</p> <p>Just a note about this section. It uses global variables and reuses them as the calculation progresses. If you run cells out of order, you may get funny results. If you do, go back to this point and run them in order.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#backprop","title":"Backprop\u00b6","text":"<p> Backprop is the algorithm we use to calculate derivatives. As described in the lectures, backprop starts at the right and moves to the left. The first node to consider is $J = a^2 $ and the first step is to find $\\frac{\\partial J}{\\partial a}$</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-a","title":"$\\frac{\\partial J}{\\partial a}$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#arithmetically","title":"Arithmetically\u00b6","text":"<p>Find $\\frac{\\partial J}{\\partial a}$ by finding how $J$ changes as a result of a little change in $a$. This is described in detail in the derivatives optional lab.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#symbolically","title":"Symbolically\u00b6","text":"<p>Now, let's use SymPy to calculate derivatives symbolically as we did in the derivatives optional lab. We will prefix the name of the variable with an 's' to indicate this is a symbolic variable.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-w","title":"$\\frac{\\partial J}{\\partial w}$\u00b6","text":"<p>  Moving from right to left, the next value we would like to compute is $\\frac{\\partial J}{\\partial w}$. To do this, we first need to calculate $\\frac{\\partial a}{\\partial w}$ which describes how the output of this node, $a$, changes when the input $w$ changes a little bit.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#arithmetically","title":"Arithmetically\u00b6","text":"<p>Find $\\frac{\\partial a}{\\partial w}$ by finding how $a$ changes as a result of a little change in $w$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#computation-graph-of-a-simple-neural-network","title":"Computation Graph of a Simple Neural Network\u00b6","text":"<p>Below is a graph of the neural network used in the lecture with different values. Try and fill in the values in the boxes. Note, the interactive graphic is not particularly robust. If you run into trouble with the interface, run the cell below again to restart.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#forward-propagation","title":"Forward propagation\u00b6","text":"<p>The calculations in the forward path are the ones you have recently learned for neural networks. You can compare the values below to those you calculated for the diagram above.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#backward-propagation-backprop","title":"Backward propagation (Backprop)\u00b6","text":"<p> As described in the lectures, backprop starts at the right and moves to the left. The first node to consider is $J = \\frac{1}{2}d^2 $ and the first step is to find $\\frac{\\partial J}{\\partial d}$</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-d","title":"$\\frac{\\partial J}{\\partial d}$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#arithmetically","title":"Arithmetically\u00b6","text":"<p>Find $\\frac{\\partial J}{\\partial d}$ by finding how $J$ changes as a result of a little change in $d$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#symbolically","title":"Symbolically\u00b6","text":"<p>Now, let's use SymPy to calculate derivatives symbolically, as we did in the derivatives optional lab. We will prefix the name of the variable with an 's' to indicate this is a symbolic variable.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-a","title":"$\\frac{\\partial J}{\\partial a}$\u00b6","text":"<p>  Moving from right to left, the next value we would like to compute is $\\frac{\\partial J}{\\partial a}$. To do this, we first need to calculate $\\frac{\\partial d}{\\partial a}$ which describes how the output of this node changes when the input $a$ changes a little bit. (Note, we are not interested in how the output changes when $y$ changes since $y$ is not a parameter.)</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#arithmetically","title":"Arithmetically\u00b6","text":"<p>Find $\\frac{\\partial d}{\\partial a}$ by finding how $d$ changes as a result of a little change in $a$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#symbolically","title":"Symbolically\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-c-fracpartial-jpartial-b","title":"$\\frac{\\partial J}{\\partial c}$,  $\\frac{\\partial J}{\\partial b}$\u00b6","text":"<p>The next node has two derivatives of interest. We need to calculate  $\\frac{\\partial J}{\\partial c}$ so we can propagate to the left. We also want to calculate   $\\frac{\\partial J}{\\partial b}$. Finding the derivative of the cost with respect to the parameters $w$ and $b$ is the object of backprop. We will find the local derivatives,  $\\frac{\\partial a}{\\partial c}$ and  $\\frac{\\partial a}{\\partial b}$ first and then combine those with the derivative coming from the right, $\\frac{\\partial J}{\\partial a}$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#fracpartial-jpartial-w","title":"$\\frac{\\partial J}{\\partial w}$\u00b6","text":"<p> The last node in this example calculates <code>c</code>. Here, we are interested in how J changes with respect to the parameter w. We will not back propagate to the input $x$, so we are not interested in $\\frac{\\partial J}{\\partial x}$. Let's start by calculating $\\frac{\\partial c}{\\partial w}$.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Backprop/#congratulations","title":"Congratulations!\u00b6","text":"<p>You've worked through an example of back propagation using a computation graph. You can apply this to larger examples by following the same node by node approach.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/","title":"Optional Lab - Derivatives","text":"In\u00a0[2]: Copied! <pre>from sympy import symbols, diff\n</pre> from sympy import symbols, diff <p>The formal definition of derivatives can be a bit daunting with limits and values 'going to zero'. The idea is really much simpler.</p> <p>The derivative of a function describes how the output of a function changes when there is a small change in an input variable.</p> <p>Let's use the cost function $J(w)$ as an example. The cost $J$ is the output and $w$ is the input variable. Let's give a 'small change' a name epsilon or $\\epsilon$. We use these Greek letters because it is traditional in mathematics to use epsilon($\\epsilon$) or delta ($\\Delta$) to represent a small value. You can think of it as representing 0.001 or some other small value.</p> <p>$$ \\begin{equation} \\text{if } w \\uparrow \\epsilon \\text{ causes }J(w) \\uparrow \\text{by }k \\times \\epsilon \\text{ then}  \\\\ \\frac{\\partial J(w)}{\\partial w} = k \\tag{1} \\end{equation} $$</p> <p>This just says if you change the input to the function $J(w)$ by a little bit and the output changes by $k$ times that little bit, then the derivative of $J(w)$ is equal to $k$.</p> <p>Let's try this out.  Let's look at the derivative of the function $J(w) = w^2$ at the point $w=3$ and $\\epsilon = 0.001$</p> In\u00a0[3]: Copied! <pre>J = (3)**2\nJ_epsilon = (3 + 0.001)**2\nk = (J_epsilon - J)/0.001    # difference divided by epsilon\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k:0.6f} \")\n</pre> J = (3)**2 J_epsilon = (3 + 0.001)**2 k = (J_epsilon - J)/0.001    # difference divided by epsilon print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k:0.6f} \") <pre>J = 9, J_epsilon = 9.006001, dJ_dw ~= k = 6.001000 \n</pre> <p>We have increased the input value a little bit (0.001), causing the output to change from 9 to 9.006001, an increase of 6 times the input increase. Referencing (1) above, this says that $k=6$, so $\\frac{\\partial J(w)}{\\partial w} \\approx 6$. If you are familiar with calculus, you know, written symbolically,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$. With $w=3$ this is 6. Our calculation above is not exactly 6 because to be exactly correct $\\epsilon$ would need to be infinitesimally small or really, really small. That is why we use the symbols $\\approx$ or ~= rather than =. Let's see what happens if we make $\\epsilon$ smaller.</p> In\u00a0[4]: Copied! <pre>J = (3)**2\nJ_epsilon = (3 + 0.000000001)**2\nk = (J_epsilon - J)/0.000000001\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J = (3)**2 J_epsilon = (3 + 0.000000001)**2 k = (J_epsilon - J)/0.000000001 print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 9, J_epsilon = 9.000000006, dJ_dw ~= k = 6.000000496442226 \n</pre> <p>The value gets close to exactly 6 as we reduce the size of $\\epsilon$. Feel free to try reducing the value further.</p> In\u00a0[5]: Copied! <pre>J, w = symbols('J, w')\n</pre> J, w = symbols('J, w') <p>Define and print the expression. Note SymPy produces a latex string which generates a nicely readable equation.</p> In\u00a0[6]: Copied! <pre>J=w**2\nJ\n</pre> J=w**2 J Out[6]:  $\\displaystyle w^{2}$  <p>Use SymPy's <code>diff</code> to differentiate the expression for $J$ with respect to $w$. Note the result matches our earlier example.</p> In\u00a0[7]: Copied! <pre>dJ_dw = diff(J,w)\ndJ_dw\n</pre> dJ_dw = diff(J,w) dJ_dw Out[7]:  $\\displaystyle 2 w$  <p>Evaluate the derivative at a few points by 'substituting' numeric values for the symbolic values. In the first example, $w$ is replaced by $2$.</p> In\u00a0[8]: Copied! <pre>dJ_dw.subs([(w,2)])    # derivative at the point w = 2\n</pre> dJ_dw.subs([(w,2)])    # derivative at the point w = 2 Out[8]:  $\\displaystyle 4$  In\u00a0[9]: Copied! <pre>dJ_dw.subs([(w,3)])    # derivative at the point w = 3\n</pre> dJ_dw.subs([(w,3)])    # derivative at the point w = 3 Out[9]:  $\\displaystyle 6$  In\u00a0[10]: Copied! <pre>dJ_dw.subs([(w,-3)])    # derivative at the point w = -3\n</pre> dJ_dw.subs([(w,-3)])    # derivative at the point w = -3 Out[10]:  $\\displaystyle -6$  In\u00a0[11]: Copied! <pre>w, J = symbols('w, J')\n</pre> w, J = symbols('w, J') In\u00a0[12]: Copied! <pre>J = 2 * w\nJ\n</pre> J = 2 * w J Out[12]:  $\\displaystyle 2 w$  In\u00a0[13]: Copied! <pre>dJ_dw = diff(J,w)\ndJ_dw\n</pre> dJ_dw = diff(J,w) dJ_dw Out[13]:  $\\displaystyle 2$  In\u00a0[14]: Copied! <pre>dJ_dw.subs([(w,-3)])    # derivative at the point w = -3\n</pre> dJ_dw.subs([(w,-3)])    # derivative at the point w = -3 Out[14]:  $\\displaystyle 2$  <p>Compare this with the arithmetic calculation</p> In\u00a0[15]: Copied! <pre>J = 2*3\nJ_epsilon = 2*(3 + 0.001)\nk = (J_epsilon - J)/0.001\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J = 2*3 J_epsilon = 2*(3 + 0.001) k = (J_epsilon - J)/0.001 print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 6, J_epsilon = 6.002, dJ_dw ~= k = 1.9999999999997797 \n</pre> <p>For the function $J=2w$, it is easy to see that any change in $w$ will result in 2 times that amount of change in the output $J$, regardless of the starting value of $w$. Our NumPy and arithmetic results confirm this.</p> In\u00a0[16]: Copied! <pre>J, w = symbols('J, w')\n</pre> J, w = symbols('J, w') In\u00a0[17]: Copied! <pre>J=w**3\nJ\n</pre> J=w**3 J Out[17]:  $\\displaystyle w^{3}$  In\u00a0[18]: Copied! <pre>dJ_dw = diff(J,w)\ndJ_dw\n</pre> dJ_dw = diff(J,w) dJ_dw Out[18]:  $\\displaystyle 3 w^{2}$  In\u00a0[19]: Copied! <pre>dJ_dw.subs([(w,2)])   # derivative at the point w=2\n</pre> dJ_dw.subs([(w,2)])   # derivative at the point w=2 Out[19]:  $\\displaystyle 12$  <p>Compare this with the arithmetic calculation</p> In\u00a0[20]: Copied! <pre>J = (2)**3\nJ_epsilon = (2+0.001)**3\nk = (J_epsilon - J)/0.001\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J = (2)**3 J_epsilon = (2+0.001)**3 k = (J_epsilon - J)/0.001 print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 8, J_epsilon = 8.012006000999998, dJ_dw ~= k = 12.006000999997823 \n</pre> In\u00a0[21]: Copied! <pre>J, w = symbols('J, w')\n</pre> J, w = symbols('J, w') In\u00a0[22]: Copied! <pre>J= 1/w\nJ\n</pre> J= 1/w J Out[22]:  $\\displaystyle \\frac{1}{w}$  In\u00a0[23]: Copied! <pre>dJ_dw = diff(J,w)\ndJ_dw\n</pre> dJ_dw = diff(J,w) dJ_dw Out[23]:  $\\displaystyle - \\frac{1}{w^{2}}$  In\u00a0[24]: Copied! <pre>dJ_dw.subs([(w,2)])\n</pre> dJ_dw.subs([(w,2)]) Out[24]:  $\\displaystyle - \\frac{1}{4}$  <p>Compare this with the arithmetic calculation</p> In\u00a0[25]: Copied! <pre>J = 1/2\nJ_epsilon = 1/(2+0.001)\nk = (J_epsilon - J)/0.001\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J = 1/2 J_epsilon = 1/(2+0.001) k = (J_epsilon - J)/0.001 print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 0.5, J_epsilon = 0.49975012493753124, dJ_dw ~= k = -0.2498750624687629 \n</pre> In\u00a0[26]: Copied! <pre>J, w = symbols('J, w')\n</pre> J, w = symbols('J, w') <p>If you have time, try to repeat the above steps on the function  $J = \\frac{1}{w^2}$ and evaluate at w=4</p> In\u00a0[27]: Copied! <pre>J, w = symbols('J, w')\n</pre> J, w = symbols('J, w') In\u00a0[28]: Copied! <pre>J= 1/(w**2)\nJ\n</pre> J= 1/(w**2) J Out[28]:  $\\displaystyle \\frac{1}{w^{2}}$  In\u00a0[29]: Copied! <pre>dJ_dw = diff(J,w)\ndJ_dw\n</pre> dJ_dw = diff(J,w) dJ_dw Out[29]:  $\\displaystyle - \\frac{2}{w^{3}}$  In\u00a0[30]: Copied! <pre>dJ_dw.subs([(w,4)])\n</pre> dJ_dw.subs([(w,4)]) Out[30]:  $\\displaystyle - \\frac{1}{32}$  <p>Compare this with the arithmetic calculation</p> In\u00a0[31]: Copied! <pre>J = 1/4**2\nJ_epsilon = 1/(4+0.001)**2\nk = (J_epsilon - J)/0.001\nprint(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")\n</pre> J = 1/4**2 J_epsilon = 1/(4+0.001)**2 k = (J_epsilon - J)/0.001 print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \") <pre>J = 0.0625, J_epsilon = 0.06246876171484496, dJ_dw ~= k = -0.031238285155041345 \n</pre> Click for hints <pre>J= 1/w**2\ndJ_dw = diff(J,w)\ndJ_dw.subs([(w,4)])\n</pre>"},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#optional-lab-derivatives","title":"Optional Lab - Derivatives\u00b6","text":"<p>This lab will give you a more intuitive understanding of derivatives. It will show you a simple way of calculating derivatives arithmetically. It will also introduce you to a handy Python library that allows you to calculate derivatives symbolically.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#informal-definition-of-derivatives","title":"Informal definition of derivatives\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#finding-symbolic-derivatives","title":"Finding symbolic derivatives\u00b6","text":"<p>In backprop it is useful to know the derivative of simple functions at any input value. Put another way, we would like to know the 'symbolic' derivative rather than the 'arithmetic' derivative. An example of a symbolic derivative is,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$, the derivative of $J(w) = w^2$ above.  With the symbolic derivative you can find the value of the derivative at any input value $w$.</p> <p>If you have taken a calculus course, you are familiar with the many differentiation rules that mathematicians have developed to solve for a derivative given an expression. Well, it turns out this process has been automated with symbolic differentiation programs. An example of this in python is the SymPy library. Let's take a look at how to use this.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#j-w2","title":"$J = w^2$\u00b6","text":"<p>Define the python variables and their symbolic names.</p>"},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#j-2w","title":"$J = 2w$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#j-w3","title":"$J = w^3$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#j-frac1w","title":"$J = \\frac{1}{w}$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#j-frac1w2","title":"$J = \\frac{1}{w^2}$\u00b6","text":""},{"location":"DeepLearning/part2/backprop/C2_W2_Derivatives/#congratulations","title":"Congratulations!\u00b6","text":"<p>If you have run through the above examples, you understand a derivative describes the change in the output of a function that is a result of a small change in an input to that function. You also can use SymPy in python to find the symbolic derivative of functions.</p>"},{"location":"DeepLearning/part2/backprop/lab_utils_backprop/","title":"For debug, put this in the notebook being debugged and be sure to set the out=out parameter","text":"In\u00a0[\u00a0]: Copied! <pre>from sympy import *\nimport numpy as np\nimport re\n</pre> from sympy import * import numpy as np import re In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib.widgets import TextBox\nfrom matplotlib.widgets import Button\nimport ipywidgets as widgets\n</pre> import matplotlib.pyplot as plt from matplotlib.widgets import TextBox from matplotlib.widgets import Button import ipywidgets as widgets In\u00a0[\u00a0]: Copied! <pre>def widgvis(fig):\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n</pre> def widgvis(fig):     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False In\u00a0[\u00a0]: Copied! <pre>def between(a, b, x):\n    ''' determine if a point x is between a and b. a may be greater or less than b '''\n    if a &gt; b:\n        return b &lt;= x &lt;= a\n    if b &gt; a:\n        return a &lt;= x &lt;= b\n</pre> def between(a, b, x):     ''' determine if a point x is between a and b. a may be greater or less than b '''     if a &gt; b:         return b &lt;= x &lt;= a     if b &gt; a:         return a &lt;= x &lt;= b In\u00a0[\u00a0]: Copied! <pre>def near(pt, alist, dist=15):\n    for a in alist:\n        x, y = a.ao.get_position()  #(bot left, bot right) data coords, not relative\n        x = x - 5  \n        y = y + 2.5\n        if 0 &lt; (pt[0] - x) &lt; 25 and 0 &lt; (y - pt[1]) &lt; 25:\n            return(True, a)\n    return(False,None)\n</pre> def near(pt, alist, dist=15):     for a in alist:         x, y = a.ao.get_position()  #(bot left, bot right) data coords, not relative         x = x - 5           y = y + 2.5         if 0 &lt; (pt[0] - x) &lt; 25 and 0 &lt; (y - pt[1]) &lt; 25:             return(True, a)     return(False,None) In\u00a0[\u00a0]: Copied! <pre>def inboxes(pt, boxlist):\n    ''' returns true if pt is within one of the boxes in boxlist '''\n    #with out:\n    #    print(f\" inboxes:{boxlist}, {pt}\")\n    for b in boxlist:\n        if b.inbox(pt):\n            return(True, b)\n    return(False, None)\n</pre> def inboxes(pt, boxlist):     ''' returns true if pt is within one of the boxes in boxlist '''     #with out:     #    print(f\" inboxes:{boxlist}, {pt}\")     for b in boxlist:         if b.inbox(pt):             return(True, b)     return(False, None) In\u00a0[\u00a0]: Copied! <pre>class avalue():\n    ''' one of the values on the figure that can be filled in '''\n    def __init__(self, value, pt, cl):\n        self.value = value\n        self.cl = cl   # color\n        self.pt = pt   # point\n    \n    def add_anote(self, ax):\n        self.ax = ax\n        self.ao = self.ax.annotate(\"?\", self.pt, c=self.cl, fontsize='x-small')\n</pre> class avalue():     ''' one of the values on the figure that can be filled in '''     def __init__(self, value, pt, cl):         self.value = value         self.cl = cl   # color         self.pt = pt   # point          def add_anote(self, ax):         self.ax = ax         self.ao = self.ax.annotate(\"?\", self.pt, c=self.cl, fontsize='x-small') In\u00a0[\u00a0]: Copied! <pre>class astring():\n    ''' a string that can be set visible or invisible '''\n    def __init__(self, ax, string, pt, cl):\n        self.string = string\n        self.cl = cl   # color\n        self.pt = pt   # point\n        self.ax = ax\n        self.ao = self.ax.annotate(self.string, self.pt, c=\"white\", fontsize='x-small')\n    \n    def astring_visible(self):\n        self.ao.set_color(self.cl)\n\n    def astring_invisible(self):\n        self.ao.set_color(\"white\")\n</pre> class astring():     ''' a string that can be set visible or invisible '''     def __init__(self, ax, string, pt, cl):         self.string = string         self.cl = cl   # color         self.pt = pt   # point         self.ax = ax         self.ao = self.ax.annotate(self.string, self.pt, c=\"white\", fontsize='x-small')          def astring_visible(self):         self.ao.set_color(self.cl)      def astring_invisible(self):         self.ao.set_color(\"white\") In\u00a0[\u00a0]: Copied! <pre>class abox():\n    ''' one of the boxes in the graph that has a value '''\n    def __init__(self, ax, value, left, bottom, right, top, anpt, cl, adj_anote_obj):\n        self.ax = ax\n        self.value = value  # correct value for annotation\n        self.left = left\n        self.right = right \n        self.bottom = bottom\n        self.top = top\n        self.anpt= anpt # x,y where expression should be listed\n        self.cl = cl\n        self.ao = self.ax.annotate(\"?\", self.anpt, c=self.cl, fontsize='x-small')\n        self.astr = adj_anote_obj   # 2ndary text for marking edges or none\n            \n    def inbox(self, pt):\n        ''' true if point is within the box '''\n        #with out:   #debug\n        #    print(f\" b.inbox: {pt}\")\n        x, y = pt  \n        isbetween =  between(self.top, self.bottom, y) and between(self.left, self.right, x)\n        return isbetween\n    \n    def update_val(self, value, cl=None):\n        self.ao.set_text(value)\n        if cl:\n            self.ao.set_c(cl)\n        else:\n            self.ao.set_c(self.cl)\n            \n    def show_secondary(self):\n        if self.astr:  # if there is a 2ndary set of text\n            self.astr.ao.set_c(\"green\")\n\n    def clear_secondary(self):\n        if self.astr:  # if there is a 2ndary set of text\n            self.astr.ao.set_c(\"white\")\n</pre> class abox():     ''' one of the boxes in the graph that has a value '''     def __init__(self, ax, value, left, bottom, right, top, anpt, cl, adj_anote_obj):         self.ax = ax         self.value = value  # correct value for annotation         self.left = left         self.right = right          self.bottom = bottom         self.top = top         self.anpt= anpt # x,y where expression should be listed         self.cl = cl         self.ao = self.ax.annotate(\"?\", self.anpt, c=self.cl, fontsize='x-small')         self.astr = adj_anote_obj   # 2ndary text for marking edges or none                  def inbox(self, pt):         ''' true if point is within the box '''         #with out:   #debug         #    print(f\" b.inbox: {pt}\")         x, y = pt           isbetween =  between(self.top, self.bottom, y) and between(self.left, self.right, x)         return isbetween          def update_val(self, value, cl=None):         self.ao.set_text(value)         if cl:             self.ao.set_c(cl)         else:             self.ao.set_c(self.cl)                  def show_secondary(self):         if self.astr:  # if there is a 2ndary set of text             self.astr.ao.set_c(\"green\")      def clear_secondary(self):         if self.astr:  # if there is a 2ndary set of text             self.astr.ao.set_c(\"white\") In\u00a0[\u00a0]: Copied! <pre>            \n</pre> In\u00a0[\u00a0]: Copied! <pre>class plt_network():\n    \n    def __init__(self, fn, image, out=None):\n        self.out = out # debug\n        #with self.out:\n        #    print(\"hello world\")\n        img = plt.imread(image)\n        self.fig, self.ax = plt.subplots(figsize=self.sizefig(img))\n        boxes = fn(self.ax)\n        self.boxes = boxes\n        widgvis(self.fig)\n        self.ax.xaxis.set_visible(False)\n        self.ax.yaxis.set_visible(False)\n        self.ax.imshow(img)\n        self.fig.text(0.1,0.9, \"Click in boxes to fill in values.\")\n        self.glist = []  # place to stash global things \n        self.san = []    # selected annotation\n        \n        self.cid = self.fig.canvas.mpl_connect('button_press_event', self.onclick)\n        self.axreveal = plt.axes([0.55, 0.02, 0.15, 0.075]) #[left, bottom, width, height]\n        self.axhide   = plt.axes([0.76, 0.02, 0.15, 0.075])\n        self.breveal  = Button(self.axreveal, 'Reveal All')\n        self.breveal.on_clicked(self.reveal_values)\n        self.bhide    = Button(self.axhide, 'Hide All')\n        self.bhide.on_clicked(self.hide_values)\n        #plt.show()\n\n    def sizefig(self,img):\n        iy,ix,iz = np.shape(img)\n        if 10/5 &lt; ix/iy:   # if x is the limiting size\n            figx = 10\n            figy = figx*iy/ix\n        else:\n            figy = 5\n            figx = figy*ix/iy\n        return(figx,figy)\n       \n    def updateval(self, event):\n        #with self.out:  #debug\n        #    print(event)\n        box = self.san[0]\n        num_format = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")\n        isnumber = re.match(num_format,event)\n        if not isnumber:\n            box.update_val('?','red')\n        else:\n            #with self.out:\n            #    print(event)\n            newval = int(float(event)) if int(float(event)) == float(event) else float(event)\n            newval = round(newval,2)\n            #with self.out:\n            #    print(newval, box.value, type(newval), type(box.value))\n            if newval == box.value:\n                box.show_secondary()\n                box.update_val(round(newval,2))\n            else:\n                box.update_val(round(newval,2), 'red')\n                box.clear_secondary()\n        self.glist[0].remove()\n        self.glist.clear()\n        self.san.clear()\n\n    # collects all clicks within diagram and dispatches\n    def onclick(self, event):\n        #with self.out:\n        #    print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %\n        #          ('double' if event.dblclick else 'single', event.button,\n        #           event.x, event.y, event.xdata, event.ydata))\n        if len(self.san) != 0: # already waiting for new value\n            return\n        inbox, box = inboxes((event.xdata, event.ydata), self.boxes)\n        #with self.out:\n        #    print(f\" in box: {inbox, box}\")\n        if inbox:\n            self.san.append(box) \n            #an.set_text(an.get_text() + \"1\") # debug\n            graphBox = self.fig.add_axes([0.225, 0.02, 0.2, 0.075])  # [left, bottom, width, height]\n            txtBox = TextBox(graphBox, \"newvalue: \")\n            txtBox.on_submit(self.updateval)\n            self.glist.append(graphBox)\n            self.glist.append(txtBox)\n        return\n\n    def reveal_values(self, event):\n        for b in self.boxes:\n            b.update_val(b.value)\n            b.show_secondary()\n        plt.draw()\n\n    def hide_values(self, event):\n        for b in self.boxes:\n            b.update_val(\"?\")\n            b.clear_secondary()\n        plt.draw()\n</pre> class plt_network():          def __init__(self, fn, image, out=None):         self.out = out # debug         #with self.out:         #    print(\"hello world\")         img = plt.imread(image)         self.fig, self.ax = plt.subplots(figsize=self.sizefig(img))         boxes = fn(self.ax)         self.boxes = boxes         widgvis(self.fig)         self.ax.xaxis.set_visible(False)         self.ax.yaxis.set_visible(False)         self.ax.imshow(img)         self.fig.text(0.1,0.9, \"Click in boxes to fill in values.\")         self.glist = []  # place to stash global things          self.san = []    # selected annotation                  self.cid = self.fig.canvas.mpl_connect('button_press_event', self.onclick)         self.axreveal = plt.axes([0.55, 0.02, 0.15, 0.075]) #[left, bottom, width, height]         self.axhide   = plt.axes([0.76, 0.02, 0.15, 0.075])         self.breveal  = Button(self.axreveal, 'Reveal All')         self.breveal.on_clicked(self.reveal_values)         self.bhide    = Button(self.axhide, 'Hide All')         self.bhide.on_clicked(self.hide_values)         #plt.show()      def sizefig(self,img):         iy,ix,iz = np.shape(img)         if 10/5 &lt; ix/iy:   # if x is the limiting size             figx = 10             figy = figx*iy/ix         else:             figy = 5             figx = figy*ix/iy         return(figx,figy)             def updateval(self, event):         #with self.out:  #debug         #    print(event)         box = self.san[0]         num_format = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\")         isnumber = re.match(num_format,event)         if not isnumber:             box.update_val('?','red')         else:             #with self.out:             #    print(event)             newval = int(float(event)) if int(float(event)) == float(event) else float(event)             newval = round(newval,2)             #with self.out:             #    print(newval, box.value, type(newval), type(box.value))             if newval == box.value:                 box.show_secondary()                 box.update_val(round(newval,2))             else:                 box.update_val(round(newval,2), 'red')                 box.clear_secondary()         self.glist[0].remove()         self.glist.clear()         self.san.clear()      # collects all clicks within diagram and dispatches     def onclick(self, event):         #with self.out:         #    print('%s click: button=%d, x=%d, y=%d, xdata=%f, ydata=%f' %         #          ('double' if event.dblclick else 'single', event.button,         #           event.x, event.y, event.xdata, event.ydata))         if len(self.san) != 0: # already waiting for new value             return         inbox, box = inboxes((event.xdata, event.ydata), self.boxes)         #with self.out:         #    print(f\" in box: {inbox, box}\")         if inbox:             self.san.append(box)              #an.set_text(an.get_text() + \"1\") # debug             graphBox = self.fig.add_axes([0.225, 0.02, 0.2, 0.075])  # [left, bottom, width, height]             txtBox = TextBox(graphBox, \"newvalue: \")             txtBox.on_submit(self.updateval)             self.glist.append(graphBox)             self.glist.append(txtBox)         return      def reveal_values(self, event):         for b in self.boxes:             b.update_val(b.value)             b.show_secondary()         plt.draw()      def hide_values(self, event):         for b in self.boxes:             b.update_val(\"?\")             b.clear_secondary()         plt.draw() In\u00a0[\u00a0]: Copied! <pre>def config_nw0(ax):\n    #\"./images/C2_W2_BP_network0.PNG\"\n\n    w = 3\n    a = 2+3*w\n    J = a**2\n    \n    pass             ; dJ_dJ  = 1\n    dJ_da = 2*a      ; dJ_da = dJ_dJ * dJ_da\n    da_dw = 3        ; dJ_dw = dJ_da * da_dw\n\n    box1 = abox(ax, round(a,2), 307,  140,  352, 100, (315, 128),'blue', None) # left, bottom, right, top, \n    box2 = abox(ax, round(J,2), 581,  138,  624, 100, (589, 128),'blue', None) \n\n    dJ_da_a = astring(ax, r\"$\\frac{\\partial J}{\\partial a}=$\"+f\"{dJ_da}\", (291,186), \"green\")\n    box3 = abox(ax, round(dJ_da,2), 545, 417, 588, 380, (553,407), 'green', dJ_da_a) \n\n    dJ_dw_a = astring(ax, r\"$\\frac{\\partial J}{\\partial w}=$\"+f\"{dJ_dw}\", (60,186), \"green\")\n    box4 = abox(ax, round(da_dw,2), 195, 421, 237, 380, (203,411), 'green', None)   \n    box5 = abox(ax, round(dJ_dw,2), 265, 515, 310, 475, (273,505), 'green', dJ_dw_a)   \n\n    boxes = [box1, box2, box3, box4, box5]\n    \n    return boxes   \n</pre> def config_nw0(ax):     #\"./images/C2_W2_BP_network0.PNG\"      w = 3     a = 2+3*w     J = a**2          pass             ; dJ_dJ  = 1     dJ_da = 2*a      ; dJ_da = dJ_dJ * dJ_da     da_dw = 3        ; dJ_dw = dJ_da * da_dw      box1 = abox(ax, round(a,2), 307,  140,  352, 100, (315, 128),'blue', None) # left, bottom, right, top,      box2 = abox(ax, round(J,2), 581,  138,  624, 100, (589, 128),'blue', None)       dJ_da_a = astring(ax, r\"$\\frac{\\partial J}{\\partial a}=$\"+f\"{dJ_da}\", (291,186), \"green\")     box3 = abox(ax, round(dJ_da,2), 545, 417, 588, 380, (553,407), 'green', dJ_da_a)       dJ_dw_a = astring(ax, r\"$\\frac{\\partial J}{\\partial w}=$\"+f\"{dJ_dw}\", (60,186), \"green\")     box4 = abox(ax, round(da_dw,2), 195, 421, 237, 380, (203,411), 'green', None)        box5 = abox(ax, round(dJ_dw,2), 265, 515, 310, 475, (273,505), 'green', dJ_dw_a)         boxes = [box1, box2, box3, box4, box5]          return boxes    In\u00a0[\u00a0]: Copied! <pre>def config_nw1(ax):\n    # \"./images/C2_W2_BP_Network1.PNG\"\n\n    x = 2\n    w = -2\n    b = 8\n    y = 1\n    \n    c = w * x\n    a = c + b\n    d = a - y\n    J = d**2/2\n    \n    pass             ; dJ_dJ = 1\n    dJ_dd = 2*d/2    ; dJ_dd = dJ_dJ * dJ_dd\n    dd_da = 1        ; dJ_da = dJ_dd * dd_da\n    da_db = 1        ; dJ_db = dJ_da * da_db\n    da_dc = 1        ; dJ_dc = dJ_da * da_dc\n    dc_dw = x        ; dJ_dw = dJ_dc * dc_dw\n    \n    box1 = abox(ax, round(c,2), 330,  162,  382, 114, (338, 150),'blue', None) # left, bottom, right, top, \n    box2 = abox(ax, round(a,2), 636,  162,  688, 114, (644, 150),'blue', None) \n    box3 = abox(ax, round(d,2), 964,  162, 1015, 114, (972, 150),'blue', None) \n    box4 = abox(ax, round(J,2), 1266, 162, 1315, 114, (1274,150),'blue', None) \n\n    dJ_dd_a = astring(ax, r\"$\\frac{\\partial J}{\\partial d}=$\"+f\"{dJ_dd}\", (967,208), \"green\")\n    box5 = abox(ax, round(dJ_dd,2), 1222, 488, 1275, 441, (1230,478), 'green', dJ_dd_a) \n\n    dJ_da_a = astring(ax, r\"$\\frac{\\partial J}{\\partial a}=$\"+f\"{dJ_da}\", (615,208), \"green\")\n    box6 = abox(ax, round(dd_da,2), 900, 383, 951,  333, (908,373), 'green', None)   \n    box7 = abox(ax, round(dJ_da,2), 988, 483, 1037, 441, (996,473), 'green', dJ_da_a)   \n\n    dJ_dc_a = astring(ax, r\"$\\frac{\\partial J}{\\partial c}=$\"+f\"{dJ_dc}\", (337,208), \"green\")\n    box8 = abox(ax, round(da_dc,2),  570, 380, 620, 333, (578,370), 'green', None)   \n    box9 = abox(ax, round(dJ_dc,2),  638, 467, 688, 419, (646,457), 'green', dJ_dc_a)   \n\n    dJ_db_a = astring(ax, r\"$\\frac{\\partial J}{\\partial b}=$\"+f\"{dJ_dc}\", (474,252), \"green\")\n    box10 = abox(ax, round(da_db,2), 563, 582, 615, 533, (571,572), 'green', None)   \n    box11 = abox(ax, round(dJ_db,2), 630, 677, 684, 630, (638,667), 'green', dJ_db_a)   \n\n    dJ_dw_a = astring(ax, r\"$\\frac{\\partial J}{\\partial w}=$\"+f\"{dJ_dw}\", (60,208), \"green\")\n    box12 = abox(ax, round(dc_dw,2), 191, 379, 341, 332, (199,369), 'green', None)   \n    box13 = abox(ax, round(dJ_dw,2), 266, 495, 319, 448, (274,485), 'green', dJ_dw_a)   \n\n    boxes = [box1, box2, box3, box4, box5, box6, box7, box8, box9, box10, box11, box12, box13]\n\n    return boxes   \n</pre> def config_nw1(ax):     # \"./images/C2_W2_BP_Network1.PNG\"      x = 2     w = -2     b = 8     y = 1          c = w * x     a = c + b     d = a - y     J = d**2/2          pass             ; dJ_dJ = 1     dJ_dd = 2*d/2    ; dJ_dd = dJ_dJ * dJ_dd     dd_da = 1        ; dJ_da = dJ_dd * dd_da     da_db = 1        ; dJ_db = dJ_da * da_db     da_dc = 1        ; dJ_dc = dJ_da * da_dc     dc_dw = x        ; dJ_dw = dJ_dc * dc_dw          box1 = abox(ax, round(c,2), 330,  162,  382, 114, (338, 150),'blue', None) # left, bottom, right, top,      box2 = abox(ax, round(a,2), 636,  162,  688, 114, (644, 150),'blue', None)      box3 = abox(ax, round(d,2), 964,  162, 1015, 114, (972, 150),'blue', None)      box4 = abox(ax, round(J,2), 1266, 162, 1315, 114, (1274,150),'blue', None)       dJ_dd_a = astring(ax, r\"$\\frac{\\partial J}{\\partial d}=$\"+f\"{dJ_dd}\", (967,208), \"green\")     box5 = abox(ax, round(dJ_dd,2), 1222, 488, 1275, 441, (1230,478), 'green', dJ_dd_a)       dJ_da_a = astring(ax, r\"$\\frac{\\partial J}{\\partial a}=$\"+f\"{dJ_da}\", (615,208), \"green\")     box6 = abox(ax, round(dd_da,2), 900, 383, 951,  333, (908,373), 'green', None)        box7 = abox(ax, round(dJ_da,2), 988, 483, 1037, 441, (996,473), 'green', dJ_da_a)         dJ_dc_a = astring(ax, r\"$\\frac{\\partial J}{\\partial c}=$\"+f\"{dJ_dc}\", (337,208), \"green\")     box8 = abox(ax, round(da_dc,2),  570, 380, 620, 333, (578,370), 'green', None)        box9 = abox(ax, round(dJ_dc,2),  638, 467, 688, 419, (646,457), 'green', dJ_dc_a)         dJ_db_a = astring(ax, r\"$\\frac{\\partial J}{\\partial b}=$\"+f\"{dJ_dc}\", (474,252), \"green\")     box10 = abox(ax, round(da_db,2), 563, 582, 615, 533, (571,572), 'green', None)        box11 = abox(ax, round(dJ_db,2), 630, 677, 684, 630, (638,667), 'green', dJ_db_a)         dJ_dw_a = astring(ax, r\"$\\frac{\\partial J}{\\partial w}=$\"+f\"{dJ_dw}\", (60,208), \"green\")     box12 = abox(ax, round(dc_dw,2), 191, 379, 341, 332, (199,369), 'green', None)        box13 = abox(ax, round(dJ_dw,2), 266, 495, 319, 448, (274,485), 'green', dJ_dw_a)         boxes = [box1, box2, box3, box4, box5, box6, box7, box8, box9, box10, box11, box12, box13]      return boxes    In\u00a0[\u00a0]: Copied! <pre>#not used\ndef config_nw2():\n    x0 = 1\n    x1 = 2\n    w0 = -2\n    w1 = 3\n    b  = -4\n    y  = 1\n    d  = x0 * w0\n    e  = x1 * w1\n    f  = d+e+b\n    g  = -f\n    h  = np.exp(g)\n    i  = h+1\n    a  = 1/i\n    k  = y-a\n    L  = k**2\n\n    pass             ; dL_dL  = 1\n    dL_dk = 2*k      ; dL_dk  = dL_dL * dL_dk\n    dk_da = -1       ; dL_da  = dL_dk * dk_da\n    da_di = -1/i**2  ; dL_di  = dL_da * da_di\n    di_dh = 1        ; dL_dh  = dL_di * di_dh\n    dh_dg = exp(g)   ; dL_dg  = dL_dh * dh_dg\n    dg_df = -1       ; dL_df  = dL_dg * dg_df\n    df_dd = 1        ; dL_dd  = dL_df * df_dd\n    df_de = 2        ; dL_de  = dL_df * df_de\n    df_db = 1        ; dL_db  = dL_df * df_db\n    dd_dw0 = 1       ; dL_dw0 = dL_dd * dd_dw0\n    de_dw1 = 2       ; dL_dw1 = dL_de * de_dw1\n\n    an1 = avalue(round(d,2), (270,265), 'blue')\n    an2 = avalue(round(e,2), (270,350), 'blue')\n    an3 = avalue(round(f,2), (400,315), 'blue')\n    an4 = avalue(round(g,2), (540,315), 'blue')\n    an5 = avalue(round(h,2), (650,315), 'blue')\n    an6 = avalue(round(i,2), (760,315), 'blue')\n    an7 = avalue(round(a,2), (890,315), 'blue')\n    an8 = avalue(round(k,2), (1015,315), 'blue')\n    an9 = avalue(round(L,2), (1120,315), 'blue')\n    bn1 = avalue(round(dL_dd,2), (260,300),  'green')   #d\n    bn2 = avalue(round(dL_de,2), (270,385),  'green')   #e\n    bn3 = avalue(round(dL_df,2), (408,350),  'green')   #f\n    bn4 = avalue(round(dL_dg,2), (540,350),  'green')   #g\n    bn5 = avalue(round(dL_dh,2), (650,350),  'green')   #h\n    bn6 = avalue(round(dL_di,2), (760,350),  'green')   #i\n    bn7 = avalue(round(dL_da,2), (890,350),  'green')   #a\n    bn8 = avalue(round(dL_dk,2), (1015,350), 'green')   #k\n    bn9 = avalue(round(dL_dw0,2), (210,300), 'green')   #w0\n    bn10 = avalue(round(dL_dw1,2), (205,440),'green')   #w1\n    bn11 = avalue(round(dL_db,2), (345,385), 'green')   #b\n\n    anotes = [an1, an2, an3, an4, an5, an6, an7, an8, an9,\n              bn1, bn2, bn3, bn4, bn5, bn6, bn7, bn8, bn9, bn10, bn11]\n\n    box1 = abox(r\"$\\frac{\\partial v}{\\partial t}$\", 943, 347, 980, 310, (980,300))\n    boxes = [box1]\n\n    fn = \"./images/C2_W2_BP_bkground.PNG\"\n    return fn, anotes, boxes\n</pre> #not used def config_nw2():     x0 = 1     x1 = 2     w0 = -2     w1 = 3     b  = -4     y  = 1     d  = x0 * w0     e  = x1 * w1     f  = d+e+b     g  = -f     h  = np.exp(g)     i  = h+1     a  = 1/i     k  = y-a     L  = k**2      pass             ; dL_dL  = 1     dL_dk = 2*k      ; dL_dk  = dL_dL * dL_dk     dk_da = -1       ; dL_da  = dL_dk * dk_da     da_di = -1/i**2  ; dL_di  = dL_da * da_di     di_dh = 1        ; dL_dh  = dL_di * di_dh     dh_dg = exp(g)   ; dL_dg  = dL_dh * dh_dg     dg_df = -1       ; dL_df  = dL_dg * dg_df     df_dd = 1        ; dL_dd  = dL_df * df_dd     df_de = 2        ; dL_de  = dL_df * df_de     df_db = 1        ; dL_db  = dL_df * df_db     dd_dw0 = 1       ; dL_dw0 = dL_dd * dd_dw0     de_dw1 = 2       ; dL_dw1 = dL_de * de_dw1      an1 = avalue(round(d,2), (270,265), 'blue')     an2 = avalue(round(e,2), (270,350), 'blue')     an3 = avalue(round(f,2), (400,315), 'blue')     an4 = avalue(round(g,2), (540,315), 'blue')     an5 = avalue(round(h,2), (650,315), 'blue')     an6 = avalue(round(i,2), (760,315), 'blue')     an7 = avalue(round(a,2), (890,315), 'blue')     an8 = avalue(round(k,2), (1015,315), 'blue')     an9 = avalue(round(L,2), (1120,315), 'blue')     bn1 = avalue(round(dL_dd,2), (260,300),  'green')   #d     bn2 = avalue(round(dL_de,2), (270,385),  'green')   #e     bn3 = avalue(round(dL_df,2), (408,350),  'green')   #f     bn4 = avalue(round(dL_dg,2), (540,350),  'green')   #g     bn5 = avalue(round(dL_dh,2), (650,350),  'green')   #h     bn6 = avalue(round(dL_di,2), (760,350),  'green')   #i     bn7 = avalue(round(dL_da,2), (890,350),  'green')   #a     bn8 = avalue(round(dL_dk,2), (1015,350), 'green')   #k     bn9 = avalue(round(dL_dw0,2), (210,300), 'green')   #w0     bn10 = avalue(round(dL_dw1,2), (205,440),'green')   #w1     bn11 = avalue(round(dL_db,2), (345,385), 'green')   #b      anotes = [an1, an2, an3, an4, an5, an6, an7, an8, an9,               bn1, bn2, bn3, bn4, bn5, bn6, bn7, bn8, bn9, bn10, bn11]      box1 = abox(r\"$\\frac{\\partial v}{\\partial t}$\", 943, 347, 980, 310, (980,300))     boxes = [box1]      fn = \"./images/C2_W2_BP_bkground.PNG\"     return fn, anotes, boxes"},{"location":"DeepLearning/part2/backprop/lab_utils_backprop/#for-debug-put-this-in-the-notebook-being-debugged-and-be-sure-to-set-the-outout-parameter","title":"For debug, put this in the notebook being debugged and be sure to set the out=out parameter\u00b6","text":"<p>out = widgets.Output(layout={'border': '1px solid black'}) out</p>"},{"location":"EmbeddedSoft/EmbeddedSoft/","title":"EmbeddedSoft","text":"<p>\u672c\u7b14\u8bb0\u8bb0\u5f55\u5d4c\u5165\u5f0f\u5f00\u53d1\u65b9\u5411\u7684\u76f8\u5173\u5185\u5bb9\uff0c\u5927\u6982\u662f\u8f6f\u4ef6\u65b9\u5411\u5427\u3002\u5982\u679c\u4ee5\u540e\u6211\u6709\u5b66\u4e60\u786c\u4ef6\u518d\u6dfb\u52a0\u8fdb\u6765\u3002</p>"},{"location":"EmbeddedSoft/EmbeddedSoft/#_1","title":"\u76ee\u5f55","text":"<ul> <li>ARM_Arch</li> <li>STM32_HAL</li> <li>\u7535\u63a7\u7b97\u6cd5/\u7535\u63a7\u7b97\u6cd5</li> <li>\u901a\u4fe1\u534f\u8bae/\u901a\u4fe1\u534f\u8bae</li> <li>\u89c6\u89c9/\u89c6\u89c9</li> <li>Assembly_Intro</li> <li>RTOS</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/","title":"Cortex-M \u5904\u7406\u5668\u6838\u5fc3\u67b6\u6784","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_1","title":"\u6574\u4f53\u67b6\u6784\u6846\u56fe","text":"<pre><code>ARM Cortex-M \u5904\u7406\u5668\u6838\u5fc3\u67b6\u6784\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Cortex-M Processor Core                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Register Bank  \u2502     Pipeline Stages      \u2502  Memory Interface \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502    R0-R12   \u2502 \u2502 \u2502 Fetch\u2502\u2502Decode\u2502\u2502Execute\u2502 \u2502    AHB-Lite \u2502 \u2502\n\u2502 \u2502   General   \u2502 \u2502 \u2502     \u2502 \u2502     \u2502 \u2502     \u2502 \u2502      Bus IF   \u2502 \u2502\n\u2502 \u2502  Purpose    \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2502               \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502 \u2502    R13      \u2502 \u2502 \u2502    NVIC             \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502   (SP)      \u2502 \u2502 \u2502  Nested Vectored    \u2502 \u2502 \u2502   MPU       \u2502 \u2502\n\u2502 \u2502  MSP/PSP    \u2502 \u2502 \u2502  Interrupt Ctrl     \u2502 \u2502 \u2502(Memory Prot \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502  Unit)      \u2502 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2502    R14      \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                 \u2502\n\u2502 \u2502    (LR)     \u2502 \u2502 \u2502     SysTick Timer   \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502  Debug      \u2502 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                         \u2502 \u2502  Interface  \u2502 \u2502\n\u2502 \u2502    R15      \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2502    (PC)     \u2502 \u2502 \u2502     FPU             \u2502 \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 (Cortex-M4/M7 etc.) \u2502 \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                 \u2502\n\u2502 \u2502   xPSR      \u2502 \u2502                         \u2502                 \u2502\n\u2502 \u2502(APSR+IPSR+  \u2502 \u2502                         \u2502                 \u2502\n\u2502 \u2502 EPSR)       \u2502 \u2502                         \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_2","title":"\u5bc4\u5b58\u5668\u7ec4\u8be6\u7ec6\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_3","title":"\u5b8c\u6574\u7684\u5bc4\u5b58\u5668\u4f53\u7cfb","text":"<pre><code>Cortex-M \u5b8c\u6574\u5bc4\u5b58\u5668\u96c6\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     \u5bc4\u5b58\u5668\u7ec4 (Register Bank)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    \u901a\u7528\u5bc4\u5b58\u5668           \u2502        \u7279\u6b8a\u529f\u80fd\u5bc4\u5b58\u5668               \u2502\n\u2502   (16 \u00d7 32-bit)         \u2502      (Special Registers)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 R0  - \u53c2\u6570/\u7ed3\u679c/\u4e34\u65f6    \u2502 xPSR - \u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668              \u2502\n\u2502 R1  - \u53c2\u6570/\u7ed3\u679c/\u4e34\u65f6    \u2502   \u251c\u2500 APSR (\u5e94\u7528\u72b6\u6001)              \u2502\n\u2502 R2  - \u53c2\u6570/\u7ed3\u679c/\u4e34\u65f6    \u2502   \u251c\u2500 IPSR (\u4e2d\u65ad\u72b6\u6001)              \u2502\n\u2502 R3  - \u53c2\u6570/\u7ed3\u679c/\u4e34\u65f6    \u2502   \u2514\u2500 EPSR (\u6267\u884c\u72b6\u6001)              \u2502\n\u2502 R4  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502                                   \u2502\n\u2502 R5  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 PRIMASK   - \u4e2d\u65ad\u5c4f\u853d              \u2502\n\u2502 R6  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 FAULTMASK - \u6545\u969c\u5c4f\u853d              \u2502\n\u2502 R7  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 BASEPRI   - \u57fa\u672c\u4f18\u5148\u7ea7            \u2502\n\u2502 R8  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 CONTROL   - \u63a7\u5236\u5bc4\u5b58\u5668            \u2502\n\u2502 R9  - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502                                   \u2502\n\u2502 R10 - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 MSP       - \u4e3b\u6808\u6307\u9488              \u2502\n\u2502 R11 - \u53d8\u91cf\u5bc4\u5b58\u5668        \u2502 PSP       - \u8fdb\u7a0b\u6808\u6307\u9488            \u2502\n\u2502 R12 - \u4e34\u65f6\u5bc4\u5b58\u5668        \u2502                                   \u2502\n\u2502 R13 - \u6808\u6307\u9488 (SP)       \u2502 NVIC\u5bc4\u5b58\u5668 - \u4e2d\u65ad\u63a7\u5236              \u2502\n\u2502 R14 - \u94fe\u63a5\u5bc4\u5b58\u5668 (LR)   \u2502 SCB\u5bc4\u5b58\u5668 - \u7cfb\u7edf\u63a7\u5236               \u2502\n\u2502 R15 - \u7a0b\u5e8f\u8ba1\u6570\u5668 (PC)   \u2502 MPU\u5bc4\u5b58\u5668 - \u5185\u5b58\u4fdd\u62a4               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_4","title":"\u5904\u7406\u5668\u6d41\u6c34\u7ebf\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_5","title":"\u4e09\u7ea7\u6d41\u6c34\u7ebf\u7ec6\u8282","text":"<pre><code>Cortex-M \u4e09\u7ea7\u6d41\u6c34\u7ebf\u8be6\u7ec6\u7ed3\u6784\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       3-Stage Pipeline                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u53d6\u6307\u9636\u6bb5   \u2502   \u8bd1\u7801\u9636\u6bb5   \u2502   \u6267\u884c\u9636\u6bb5   \u2502     \u5199\u56de\u9636\u6bb5     \u2502\n\u2502  (Fetch)    \u2502   (Decode)   \u2502  (Execute)   \u2502    (Write-back)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                  \u2502\n\u2502 \u2502Instruction\u2502 \u2502 \u2502Register \u2502 \u2502 \u2502  ALU    \u2502 \u2502 \u5bc4\u5b58\u5668\u5199\u5165       \u2502\n\u2502 \u2502  Memory  \u2502 \u2502 \u2502  Read   \u2502 \u2502 \u2502(Arithmetic\u2502 \u2502 \u5185\u5b58\u5199\u5165        \u2502\n\u2502 \u2502          \u2502 \u2502 \u2502         \u2502 \u2502 \u2502 Logic Unit)\u2502 \u2502 \u5206\u652f\u89e3\u6790        \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                  \u2502\n\u2502             \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                  \u2502\n\u2502 PC \u2192 \u5730\u5740   \u2502 \u6307\u4ee4\u89e3\u7801     \u2502 \u2502 Address  \u2502 \u2502 \u7ed3\u679c\u5199\u56de\u5bc4\u5b58\u5668   \u2502\n\u2502 \u6307\u4ee4\u8bfb\u53d6    \u2502 \u5bc4\u5b58\u5668\u8bfb\u53d6   \u2502 \u2502Calculation\u2502 \u2502 \u6216\u5185\u5b58          \u2502\n\u2502 \u9884\u53d6\u6307\u7f13\u51b2  \u2502 \u4ea7\u751f\u63a7\u5236\u4fe1\u53f7 \u2502 \u2502  \u5355\u5143     \u2502 \u2502                  \u2502\n\u2502             \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                  \u2502\n\u2502 \u66f4\u65b0PC      \u2502              \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                  \u2502\n\u2502             \u2502              \u2502 \u2502 Memory   \u2502 \u2502                  \u2502\n\u2502             \u2502              \u2502 \u2502 Access   \u2502 \u2502                  \u2502\n\u2502             \u2502              \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_6","title":"\u5185\u5b58\u7cfb\u7edf\u67b6\u6784","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_7","title":"\u603b\u7ebf\u77e9\u9635\u548c\u5185\u5b58\u6620\u5c04","text":"<pre><code>Cortex-M \u5185\u5b58\u7cfb\u7edf\u67b6\u6784\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Cortex-M Memory System                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Code Memory  \u2502      SRAM/Data Memory    \u2502   Peripherals   \u2502\n\u2502   (Flash/ROM)   \u2502        (Internal)        \u2502    (AHB/APB)    \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502  Vector     \u2502 \u2502 \u2502  Stack      \u2502         \u2502 \u2502 GPIO        \u2502 \u2502\n\u2502 \u2502  Table      \u2502 \u2502 \u2502  Area       \u2502         \u2502 \u2502 UART        \u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502 \u2502 SPI         \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2502 I2C         \u2502 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u2502 Timer       \u2502 \u2502\n\u2502 \u2502  Program    \u2502 \u2502 \u2502  Heap       \u2502         \u2502 \u2502 ADC/DAC     \u2502 \u2502\n\u2502 \u2502  Code       \u2502 \u2502 \u2502             \u2502         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                 \u2502 \u2502  .data      \u2502         \u2502 \u2502 System      \u2502 \u2502\n\u2502                 \u2502 \u2502  Section    \u2502         \u2502 \u2502 Peripherals \u2502 \u2502\n\u2502                 \u2502 \u2502             \u2502         \u2502 \u2502 NVIC, SCB,  \u2502 \u2502\n\u2502                 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502         \u2502 \u2502 SysTick,    \u2502 \u2502\n\u2502                 \u2502 \u2502 \u2502 .bss    \u2502 \u2502         \u2502 \u2502 MPU, etc.   \u2502 \u2502\n\u2502                 \u2502 \u2502 \u2502 Section \u2502 \u2502         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502         \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   AHB-Lite      \u2502\n                    \u2502   Bus Matrix    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Cortex-M      \u2502\n                    \u2502   Processor     \u2502\n                    \u2502     Core        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#nvic","title":"\u5d4c\u5957\u5411\u91cf\u4e2d\u65ad\u63a7\u5236\u5668 (NVIC)","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#nvic_1","title":"NVIC \u67b6\u6784","text":"<pre><code>NVIC (Nested Vectored Interrupt Controller) \u67b6\u6784\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         NVIC \u7ed3\u6784                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u4e2d\u65ad\u4f18\u5148\u7ea7\u7ba1\u7406  \u2502     \u4e2d\u65ad\u60ac\u6302\u4e0e\u63a7\u5236       \u2502   \u5f02\u5e38\u5904\u7406       \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 \u4f18\u5148\u7ea7\u5bc4\u5b58\u5668 \u2502 \u2502 \u2502 \u60ac\u6302\u5bc4\u5b58\u5668  \u2502         \u2502 \u2502 \u5411\u91cf\u8868\u504f\u79fb  \u2502 \u2502\n\u2502 \u2502 PRIO0-PRIOx \u2502 \u2502 \u2502 PEND0-PENDx \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2502   (8-bit    \u2502 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2502  per IRQ)   \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u4f7f\u80fd\u5bc4\u5b58\u5668  \u2502         \u2502 \u2502 \u5f53\u524d\u4f18\u5148\u7ea7  \u2502 \u2502\n\u2502 \u2502 \u4f18\u5148\u7ea7\u5206\u7ec4  \u2502 \u2502 \u2502 EN0-ENx     \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2502  \u5bc4\u5b58\u5668      \u2502 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                 \u2502\n\u2502                 \u2502 \u2502 \u6d3b\u52a8\u5bc4\u5b58\u5668  \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2502 ACT0-ACTx   \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2502             \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#scb-systick","title":"\u7cfb\u7edf\u63a7\u5236\u5757 (SCB) \u548c SysTick","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_8","title":"\u7cfb\u7edf\u63a7\u5236\u67b6\u6784","text":"<pre><code>\u7cfb\u7edf\u63a7\u5236\u5757 (SCB) \u548c SysTick \u5b9a\u65f6\u5668\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   System Control Block                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   CPUID\u5bc4\u5b58\u5668   \u2502     \u5f02\u5e38\u63a7\u5236\u5bc4\u5b58\u5668        \u2502   SysTick\u5b9a\u65f6\u5668  \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 \u5904\u7406\u5668\u8bc6\u522b   \u2502 \u2502 \u2502  SHPR2-3    \u2502         \u2502 \u2502 \u63a7\u5236\u548c\u72b6\u6001  \u2502 \u2502\n\u2502 \u2502  \u4fe1\u606f        \u2502 \u2502 \u2502 (\u7cfb\u7edf\u5904\u7406\u7a0b \u2502         \u2502 \u2502   \u5bc4\u5b58\u5668    \u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502  \u5e8f\u4f18\u5148\u7ea7)  \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502   ICSR      \u2502 \u2502 \u2502  CCR        \u2502         \u2502 \u2502  \u91cd\u8f7d\u503c     \u2502 \u2502\n\u2502 \u2502(\u4e2d\u65ad\u63a7\u5236\u72b6\u6001)\u2502 \u2502 \u2502(\u914d\u7f6e\u548c\u63a7\u5236) \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502  VTOR       \u2502 \u2502 \u2502  SHCSR      \u2502         \u2502 \u2502  \u5f53\u524d\u503c     \u2502 \u2502\n\u2502 \u2502(\u5411\u91cf\u8868\u504f\u79fb) \u2502 \u2502 \u2502(\u7cfb\u7edf\u5904\u7406\u7a0b \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502 \u5e8f\u63a7\u5236\u72b6\u6001) \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#mpu","title":"\u5185\u5b58\u4fdd\u62a4\u5355\u5143 (MPU)","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#mpu_1","title":"MPU \u67b6\u6784","text":"<pre><code>MPU (Memory Protection Unit) \u5185\u5b58\u4fdd\u62a4\u67b6\u6784\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      MPU \u7ed3\u6784                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   \u533a\u57df\u5bc4\u5b58\u5668    \u2502      \u5c5e\u6027\u63a7\u5236            \u2502   \u6545\u969c\u68c0\u6d4b       \u2502\n\u2502   (8-16 regions)\u2502                         \u2502                 \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502  RBAR0-15   \u2502 \u2502 \u2502  MPU_CTRL   \u2502         \u2502 \u2502  MMAR       \u2502 \u2502\n\u2502 \u2502 (\u533a\u57df\u57fa\u5730\u5740) \u2502 \u2502 \u2502  (\u63a7\u5236)     \u2502         \u2502 \u2502(\u5185\u5b58\u7ba1\u7406\u5730\u5740)\u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502  RLAR0-15   \u2502 \u2502 \u2502  MPU_RNR    \u2502         \u2502 \u2502  BFAR       \u2502 \u2502\n\u2502 \u2502 (\u533a\u57df\u5c5e\u6027)   \u2502 \u2502 \u2502 (\u533a\u57df\u7f16\u53f7)  \u2502         \u2502 \u2502(\u603b\u7ebf\u6545\u969c\u5730\u5740)\u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                 \u2502\n\u2502 \u2502 \u533a\u57df\u5927\u5c0f     \u2502 \u2502 \u2502  MPU_RASR  \u2502         \u2502                 \u2502\n\u2502 \u2502  \u548c\u5c5e\u6027      \u2502 \u2502 \u2502 (\u533a\u57df\u5c5e\u6027)  \u2502         \u2502                 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_9","title":"\u8c03\u8bd5\u7cfb\u7edf\u67b6\u6784","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#_10","title":"\u8c03\u8bd5\u63a5\u53e3\u548c\u7ec4\u4ef6","text":"<pre><code>Cortex-M \u8c03\u8bd5\u7cfb\u7edf\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Debug System                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   \u8c03\u8bd5\u63a5\u53e3       \u2502      \u8c03\u8bd5\u7ec4\u4ef6            \u2502   \u8ddf\u8e2a\u5355\u5143       \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502   SWD       \u2502 \u2502 \u2502 \u65ad\u70b9\u5355\u5143     \u2502         \u2502 \u2502   ITM       \u2502 \u2502\n\u2502 \u2502 (Serial Wire\u2502 \u2502 \u2502 (FPB)       \u2502         \u2502 \u2502(\u6307\u4ee4\u8ddf\u8e2a)    \u2502 \u2502\n\u2502 \u2502   Debug)    \u2502 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502   JTAG      \u2502 \u2502 \u2502 \u6570\u636e\u89c2\u5bdf\u70b9  \u2502         \u2502 \u2502   DWT       \u2502 \u2502\n\u2502 \u2502 (\u53ef\u9009)      \u2502 \u2502 \u2502  (DWT)      \u2502         \u2502 \u2502(\u6570\u636e\u8ddf\u8e2a)    \u2502 \u2502\n\u2502 \u2502             \u2502 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502                 \u2502 \u2502 ROM\u8868       \u2502         \u2502 \u2502   ETM       \u2502 \u2502\n\u2502                 \u2502 \u2502 (\u8c03\u8bd5\u8bc6\u522b)  \u2502         \u2502 \u2502(\u5d4c\u5165\u5f0f\u8ddf\u8e2a)  \u2502 \u2502\n\u2502                 \u2502 \u2502             \u2502         \u2502 \u2502             \u2502 \u2502\n\u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#fpu-cortex-m4m7","title":"\u6d6e\u70b9\u5355\u5143 (FPU) - Cortex-M4/M7","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM%20Cortex-M%20CPU/#fpu","title":"FPU \u67b6\u6784","text":"<pre><code>\u6d6e\u70b9\u5355\u5143 (FPU) - \u53ef\u9009\u7ec4\u4ef6\uff1a\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Floating Point Unit (FPU)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u6d6e\u70b9\u5bc4\u5b58\u5668\u6587\u4ef6  \u2502     \u6d6e\u70b9\u8fd0\u7b97\u5355\u5143          \u2502  \u6d6e\u70b9\u72b6\u6001\u548c\u63a7\u5236   \u2502\n\u2502                 \u2502                         \u2502                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502   S0-S31    \u2502 \u2502 \u2502  \u6d6e\u70b9\u52a0\u6cd5\u5668  \u2502         \u2502 \u2502  FPSCR      \u2502 \u2502\n\u2502 \u2502 (32-bit     \u2502 \u2502 \u2502             \u2502         \u2502 \u2502(\u6d6e\u70b9\u72b6\u6001\u63a7\u5236)\u2502 \u2502\n\u2502 \u2502 \u5355\u7cbe\u5ea6)      \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502 \u2502  \u5bc4\u5b58\u5668     \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502  \u6d6e\u70b9\u4e58\u6cd5\u5668  \u2502         \u2502                 \u2502\n\u2502 \u2502  D0-D15     \u2502 \u2502 \u2502             \u2502         \u2502                 \u2502\n\u2502 \u2502 (64-bit     \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                 \u2502\n\u2502 \u2502 \u53cc\u7cbe\u5ea6)      \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502                 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u6d6e\u70b9\u9664\u6cd5/    \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2502 \u5e73\u65b9\u6839\u5355\u5143   \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2502             \u2502         \u2502                 \u2502\n\u2502                 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/","title":"ARM Arch","text":"<ul> <li>\u4ec0\u4e48\u662fARM\uff1f<ul> <li>\u6838\u5fc3\u7279\u70b9</li> </ul> </li> <li>RISC\u67b6\u6784\u7b80\u4ecb<ul> <li>\u4ec0\u4e48\u662fRISC\uff1f</li> <li>RISC\u7684\u6838\u5fc3\u601d\u60f3</li> <li>RISC vs CISC \u5bf9\u6bd4</li> </ul> </li> <li>\u6838\u5fc3\u90e8\u4ef6\uff1a\u4e3b\u63a7\u3001\u5185\u5b58\u3001FLASH<ul> <li>1. \u4e3b\u63a7\u82af\u7247\uff08CPU\uff09 - \"\u5927\u8111\"</li> <li>2. \u5185\u5b58\uff08RAM\uff09 - \"\u5de5\u4f5c\u53f0\"</li> <li>3. FLASH\u5b58\u50a8 - \"\u6587\u4ef6\u67dc\"</li> </ul> </li> <li>\u4e09\u8005\u5982\u4f55\u534f\u540c\u5de5\u4f5c\uff1f<ul> <li>\u7b80\u5355\u6bd4\u55bb</li> <li>\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b</li> </ul> </li> <li>\u8fd0\u7b97\u7684\u57fa\u672c\u539f\u7406<ul> <li>1. \u4e8c\u8fdb\u5236\u4e16\u754c</li> <li>2. \u57fa\u672c\u8fd0\u7b97\u8fc7\u7a0b</li> <li>3. \u6307\u4ee4\u6267\u884c\u6d41\u7a0b</li> <li>\u6307\u5411\u6307\u4ee4</li> </ul> </li> <li>\u4eceC\u4ee3\u7801\u5230CPU\u6267\u884c\u7684\u5b8c\u6574\u6d41\u7a0b<ul> <li>\u5b9e\u9645\u5e94\u7528\u4e3e\u4f8b<ul> <li>\u624b\u673a\u62cd\u7167\u8fc7\u7a0b</li> <li>\u6e38\u620f\u8fd0\u884c</li> </ul> </li> </ul> </li> <li>\u91cd\u8981\u6982\u5ff5\u603b\u7ed3</li> <li>\u5b66\u4e60</li> </ul> <p>\u53c2\u8003\uff1aCortex-M3\u6743\u5a01\u6307\u5357.pdf</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#arm","title":"\u4ec0\u4e48\u662fARM\uff1f","text":"<p>ARM\u662f\u4e00\u79cd\u5904\u7406\u5668\u67b6\u6784\uff0c\u5c31\u50cf\u6c7d\u8f66\u7684\u53d1\u52a8\u673a\u8bbe\u8ba1\u56fe\u3002ARM\u516c\u53f8\u53ea\u8bbe\u8ba1\"\u84dd\u56fe\"\uff0c\u5176\u4ed6\u516c\u53f8\uff08\u5982\u82f9\u679c\u3001\u9ad8\u901a\u3001\u4e09\u661f\uff09\u6839\u636e\u8fd9\u4e2a\u84dd\u56fe\u5236\u9020\u5b9e\u9645\u7684\u82af\u7247\u3002</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_1","title":"\u6838\u5fc3\u7279\u70b9","text":"<ul> <li>\u4f4e\u529f\u8017\uff1a\u975e\u5e38\u7701\u7535\uff0c\u9002\u5408\u624b\u673a\u3001\u5e73\u677f\u7b49\u79fb\u52a8\u8bbe\u5907</li> <li>\u9ad8\u6027\u4ef7\u6bd4\uff1a\u6027\u80fd\u8db3\u591f\u5f3a\uff0c\u6210\u672c\u76f8\u5bf9\u8f83\u4f4e</li> <li>\u5e7f\u6cdb\u5e94\u7528\uff1a\u4ece\u667a\u80fd\u624b\u8868\u5230\u670d\u52a1\u5668\u90fd\u5728\u4f7f\u7528</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#risc","title":"RISC\u67b6\u6784\u7b80\u4ecb","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#risc_1","title":"\u4ec0\u4e48\u662fRISC\uff1f","text":"<p>RISC\uff08\u7cbe\u7b80\u6307\u4ee4\u96c6\u8ba1\u7b97\u673a\uff09\u662f\u4e00\u79cd\u5904\u7406\u5668\u8bbe\u8ba1\u54f2\u5b66\uff0c\u4e0eCISC\uff08\u590d\u6742\u6307\u4ee4\u96c6\uff09\u76f8\u5bf9\u3002</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#risc_2","title":"RISC\u7684\u6838\u5fc3\u601d\u60f3","text":"<ul> <li>\u7b80\u5355\u6307\u4ee4\uff1a\u6bcf\u6761\u6307\u4ee4\u53ea\u5b8c\u6210\u4e00\u4e2a\u57fa\u672c\u64cd\u4f5c</li> <li>\u56fa\u5b9a\u957f\u5ea6\uff1a\u6307\u4ee4\u957f\u5ea6\u4e00\u81f4\uff0c\u89e3\u7801\u7b80\u5355</li> <li>Load-Store\u67b6\u6784\uff1a\u53ea\u6709\u4e13\u95e8\u7684\u52a0\u8f7d/\u5b58\u50a8\u6307\u4ee4\u53ef\u4ee5\u8bbf\u95ee\u5185\u5b58</li> <li>\u591a\u7528\u5bc4\u5b58\u5668\uff1a\u5927\u91cf\u901a\u7528\u5bc4\u5b58\u5668\u63d0\u9ad8\u6548\u7387</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#risc-vs-cisc","title":"RISC vs CISC \u5bf9\u6bd4","text":"\u7279\u6027 RISC (ARM) CISC (x86) \u6307\u4ee4\u6570\u91cf \u8f83\u5c11\uff0c\u7b80\u5355 \u5f88\u591a\uff0c\u590d\u6742 \u6307\u4ee4\u957f\u5ea6 \u56fa\u5b9a \u53ef\u53d8 \u5185\u5b58\u8bbf\u95ee \u53ea\u80fd\u901a\u8fc7Load/Store \u6307\u4ee4\u53ef\u76f4\u63a5\u64cd\u4f5c\u5185\u5b58 \u8bbe\u8ba1\u54f2\u5b66 \u786c\u4ef6\u7b80\u5355\uff0c\u7f16\u8bd1\u5668\u590d\u6742 \u786c\u4ef6\u590d\u6742\uff0c\u7f16\u8bd1\u5668\u7b80\u5355 \u529f\u8017 \u4f4e \u9ad8"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#flash","title":"\u6838\u5fc3\u90e8\u4ef6\uff1a\u4e3b\u63a7\u3001\u5185\u5b58\u3001FLASH","text":"<p>\u8be6\u89c1\uff1a - ARM Cortex-M CPU</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#1-cpu-","title":"1. \u4e3b\u63a7\u82af\u7247\uff08CPU\uff09 - \"\u5927\u8111\"","text":"<p>\u4e3b\u63a7\u82af\u7247\u662f\u8bbe\u5907\u7684\"\u5927\u8111\"\uff0c\u8d1f\u8d23\u6240\u6709\u7684\u8ba1\u7b97\u548c\u63a7\u5236\u5de5\u4f5c\u3002</p> <p>\u4e3b\u8981\u529f\u80fd\uff1a - \u6267\u884c\u6570\u5b66\u8fd0\u7b97\uff081+1=2\uff09 - \u903b\u8f91\u5224\u65ad\uff08\u5982\u679c...\u90a3\u4e48...\uff09 - \u63a7\u5236\u5176\u4ed6\u90e8\u4ef6\uff08\u8ba9\u5c4f\u5e55\u663e\u793a\u3001\u8ba9\u5587\u53ed\u53d1\u58f0\uff09</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#2-ram-","title":"2. \u5185\u5b58\uff08RAM\uff09 - \"\u5de5\u4f5c\u53f0\"","text":"<p>\u5185\u5b58\u5c31\u50cf\u662f\u5de5\u4f5c\u65f6\u7684\"\u4e34\u65f6\u5de5\u4f5c\u53f0\"\u3002</p> <p>\u7279\u70b9\uff1a - \u901f\u5ea6\u5feb\uff1aCPU\u53ef\u4ee5\u5feb\u901f\u8bfb\u5199 - \u4e34\u65f6\u5b58\u50a8\uff1a\u65ad\u7535\u540e\u6570\u636e\u6d88\u5931 - \u7a7a\u95f4\u6709\u9650\uff1a\u540c\u65f6\u53ea\u80fd\u5904\u7406\u4e00\u5b9a\u91cf\u7684\u6570\u636e</p> <p>\u5de5\u4f5c\u65b9\u5f0f\uff1a</p> <pre><code>CPU \u4ece FLASH \u52a0\u8f7d\u7a0b\u5e8f \u2192 \u5728 \u5185\u5b58 \u4e2d\u8fd0\u884c\u7a0b\u5e8f \u2192 \u5904\u7406\u5b8c\u6210\u540e\u4fdd\u5b58\u7ed3\u679c\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#3-flash-","title":"3. FLASH\u5b58\u50a8 - \"\u6587\u4ef6\u67dc\"","text":"<p>FLASH\u5c31\u50cf\u662f\"\u6587\u4ef6\u67dc\"\uff0c\u7528\u4e8e\u957f\u671f\u4fdd\u5b58\u6570\u636e\u3002</p> <p>\u7279\u70b9\uff1a - \u6c38\u4e45\u5b58\u50a8\uff1a\u65ad\u7535\u540e\u6570\u636e\u4e0d\u4e22\u5931 - \u5bb9\u91cf\u8f83\u5927\uff1a\u53ef\u4ee5\u5b58\u50a8\u5f88\u591a\u7a0b\u5e8f\u548c\u6570\u636e - \u901f\u5ea6\u8f83\u6162\uff1a\u6bd4\u5185\u5b58\u8bfb\u5199\u6162</p> <p>\u5b58\u50a8\u5185\u5bb9\uff1a - \u64cd\u4f5c\u7cfb\u7edf\uff08\u5982Android\u3001iOS\uff09 - \u5e94\u7528\u7a0b\u5e8f\uff08\u5fae\u4fe1\u3001\u6e38\u620f\uff09 - \u7528\u6237\u6570\u636e\uff08\u7167\u7247\u3001\u6587\u6863\uff09</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_2","title":"\u4e09\u8005\u5982\u4f55\u534f\u540c\u5de5\u4f5c\uff1f","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_3","title":"\u7b80\u5355\u6bd4\u55bb","text":"<p>\u60f3\u8c61\u4f60\u5728\u529e\u516c\u5ba4\u5de5\u4f5c\uff1a - \u4e3b\u63a7\uff08CPU\uff09 = \u4f60\u672c\u4eba\uff08\u601d\u8003\u548c\u5de5\u4f5c\uff09 - \u5185\u5b58\uff08RAM\uff09 = \u529e\u516c\u684c\uff08\u4e34\u65f6\u653e\u6b63\u5728\u5904\u7406\u7684\u6587\u4ef6\uff09 - FLASH = \u6587\u4ef6\u67dc\uff08\u957f\u671f\u5b58\u653e\u6240\u6709\u6587\u4ef6\uff09</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_4","title":"\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b","text":"<ol> <li>\u5f00\u673a\u542f\u52a8\uff1a\u4eceFLASH\u52a0\u8f7d\u64cd\u4f5c\u7cfb\u7edf\u5230\u5185\u5b58</li> <li>\u8fd0\u884c\u7a0b\u5e8f\uff1a\u4eceFLASH\u52a0\u8f7dApp\u5230\u5185\u5b58\u8fd0\u884c</li> <li>\u6570\u636e\u5904\u7406\uff1aCPU\u5728\u5185\u5b58\u4e2d\u8ba1\u7b97\u548c\u5904\u7406</li> <li>\u4fdd\u5b58\u7ed3\u679c\uff1a\u5c06\u91cd\u8981\u6570\u636e\u5199\u56deFLASH\u4fdd\u5b58</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_5","title":"\u8fd0\u7b97\u7684\u57fa\u672c\u539f\u7406","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#1","title":"1. \u4e8c\u8fdb\u5236\u4e16\u754c","text":"<p>\u8ba1\u7b97\u673a\u53ea\u8ba4\u8bc60\u548c1\uff0c\u6240\u6709\u6570\u636e\u90fd\u7528\u4e8c\u8fdb\u5236\u8868\u793a\uff1a - \u6570\u5b57\uff1a5 = \u4e8c\u8fdb\u5236 101 - \u6587\u5b57\uff1aA = \u4e8c\u8fdb\u5236 01000001 - \u56fe\u7247\uff1a\u7531\u65e0\u6570\u4e2a\u5f69\u8272\u70b9\u7ec4\u6210\uff0c\u6bcf\u4e2a\u70b9\u7528\u6570\u5b57\u8868\u793a</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#2","title":"2. \u57fa\u672c\u8fd0\u7b97\u8fc7\u7a0b","text":"<p>\u52a0\u6cd5\u793a\u4f8b\uff1a</p> <pre><code>\u8ba1\u7b97\uff1a3 + 2\n\n\u4e8c\u8fdb\u5236\uff1a011 + 010\n\u6b65\u9aa4\uff1a\n  011\n+ 010\n------\n  101  \uff08\u5341\u8fdb\u52365\uff09\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#3","title":"3. \u6307\u4ee4\u6267\u884c\u6d41\u7a0b","text":"<p>CPU\u7684\u5de5\u4f5c\u5c31\u662f\u4e0d\u65ad\u91cd\u590d\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ol> <li>\u53d6\u6307\u4ee4\uff1a\u4ece\u5185\u5b58\u8bfb\u53d6\u4e0b\u4e00\u6761\u8981\u6267\u884c\u7684\u547d\u4ee4</li> <li>\u89e3\u7801\uff1a\u7406\u89e3\u8fd9\u4e2a\u547d\u4ee4\u8981\u505a\u4ec0\u4e48</li> <li>\u6267\u884c\uff1a\u5b9e\u9645\u8fdb\u884c\u8fd0\u7b97</li> <li>\u5b58\u50a8\uff1a\u5c06\u7ed3\u679c\u4fdd\u5b58\u5230\u5185\u5b58\u6216\u5bc4\u5b58\u5668</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_6","title":"\u6307\u5411\u6307\u4ee4","text":"<p>CPU\u662f\u9760\u673a\u5668\u7801\u548c\u5bc4\u5b58\u5668\u64cd\u4f5c\u6765\u8fd0\u884c\u7684\uff0c\u89c1\uff1a - GeneralReg -  - SpecialReg_xPSR</p>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#ccpu","title":"\u4eceC\u4ee3\u7801\u5230CPU\u6267\u884c\u7684\u5b8c\u6574\u6d41\u7a0b","text":"<pre><code>C\u6e90\u4ee3\u7801 \u2192 \u7f16\u8bd1\u5668 \u2192 \u6c47\u7f16\u4ee3\u7801 \u2192 \u6c47\u7f16\u5668 \u2192 \u673a\u5668\u7801 \u2192 \u5b58\u50a8\u5728FLASH\n                                              \u2193\n                                       \u5f00\u673a\u52a0\u8f7d\u5230\u5185\u5b58\n                                              \u2193\n                                        CPU\u9010\u6761\u6267\u884c\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_7","title":"\u5b9e\u9645\u5e94\u7528\u4e3e\u4f8b","text":""},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_8","title":"\u624b\u673a\u62cd\u7167\u8fc7\u7a0b","text":"<ol> <li>\u542f\u52a8\u76f8\u673a\uff1a\u4eceFLASH\u52a0\u8f7d\u76f8\u673a\u7a0b\u5e8f\u5230\u5185\u5b58</li> <li>\u53d6\u666f\u9884\u89c8\uff1aCPU\u5904\u7406\u6444\u50cf\u5934\u6570\u636e\uff0c\u5728\u5185\u5b58\u4e2d\u751f\u6210\u9884\u89c8\u753b\u9762</li> <li>\u6309\u4e0b\u5feb\u95e8\uff1aCPU\u8fdb\u884c\u56fe\u50cf\u5904\u7406\u3001\u538b\u7f29</li> <li>\u4fdd\u5b58\u7167\u7247\uff1a\u5c06\u5904\u7406\u597d\u7684\u56fe\u7247\u4ece\u5185\u5b58\u5199\u5165FLASH\u5b58\u50a8</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_9","title":"\u6e38\u620f\u8fd0\u884c","text":"<ol> <li>\u6e38\u620f\u7a0b\u5e8f\u5b58\u50a8\u5728FLASH\u4e2d</li> <li>\u8fd0\u884c\u65f6\u52a0\u8f7d\u5230\u5185\u5b58</li> <li>CPU\u548cGPU\uff08\u56fe\u5f62\u5904\u7406\u5668\uff09\u5728\u5185\u5b58\u4e2d\u8fdb\u884c\u8ba1\u7b97</li> <li>\u751f\u6210\u753b\u9762\u8f93\u51fa\u5230\u5c4f\u5e55</li> <li>\u6e38\u620f\u8fdb\u5ea6\u4fdd\u5b58\u5230FLASH</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_10","title":"\u91cd\u8981\u6982\u5ff5\u603b\u7ed3","text":"\u90e8\u4ef6 \u6bd4\u55bb \u7279\u70b9 \u4f5c\u7528 \u4e3b\u63a7(CPU) \u5927\u8111 \u8fd0\u7b97\u3001\u63a7\u5236 \u6267\u884c\u6240\u6709\u8ba1\u7b97\u4efb\u52a1 \u5185\u5b58(RAM) \u5de5\u4f5c\u53f0 \u901f\u5ea6\u5feb\u3001\u4e34\u65f6\u6027 \u63d0\u4f9b\u8fd0\u884c\u7a7a\u95f4 FLASH \u6587\u4ef6\u67dc \u6c38\u4e45\u5b58\u50a8\u3001\u5bb9\u91cf\u5927 \u957f\u671f\u4fdd\u5b58\u6570\u636e"},{"location":"EmbeddedSoft/ARM_Arch/ARM_Arch/#_11","title":"\u5b66\u4e60","text":"<ol> <li>\u7406\u89e3\u5c42\u6b21\u5173\u7cfb\uff1aC\u8bed\u8a00 \u2192 \u6c47\u7f16 \u2192 \u673a\u5668\u7801 \u2192 \u786c\u4ef6\u6267\u884c</li> <li>\u52a8\u624b\u5b9e\u9a8c\uff1a\u5c1d\u8bd5\u5199\u7b80\u5355C\u7a0b\u5e8f\uff0c\u67e5\u770b\u751f\u6210\u7684\u6c47\u7f16\u4ee3\u7801</li> <li>\u5173\u6ce8\u6570\u636e\u6d41\uff1a\u89c2\u5bdf\u6570\u636e\u5982\u4f55\u5728\u5bc4\u5b58\u5668\u3001\u5185\u5b58\u3001FLASH\u4e4b\u95f4\u6d41\u52a8</li> <li>\u7406\u89e3RISC\u4f18\u52bf\uff1a\u7b80\u5355\u6307\u4ee4\u5982\u4f55\u901a\u8fc7\u6d41\u6c34\u7ebf\u63d0\u9ad8\u6548\u7387</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/","title":"GeneralReg","text":"<ul> <li>\u5165\u95e8\u6c47\u7f16\u6307\u4ee4</li> <li>ARM64\u5bc4\u5b58\u5668<ul> <li>\u4ec0\u4e48\u662f\u5bc4\u5b58\u5668\uff1f</li> <li>ARM64\u5bc4\u5b58\u5668\u5206\u7c7b\uff08AArch64\uff09<ul> <li>1. \u901a\u7528\u5bc4\u5b58\u5668\uff08X0-X30\uff09</li> <li>2. \u7279\u6b8a\u529f\u80fd\u5bc4\u5b58\u5668</li> </ul> </li> <li>\u5bc4\u5b58\u5668\u5927\u5c0f\u8bbf\u95ee</li> <li>\u5173\u952e\u5bc4\u5b58\u5668\u8be6\u89e3<ul> <li>1. \u53c2\u6570\u548c\u8fd4\u56de\u503c\u5bc4\u5b58\u5668\uff08X0-X7\uff09</li> <li>2. \u94fe\u63a5\u5bc4\u5b58\u5668 LR\uff08X30\uff09</li> <li>3. \u6808\u6307\u9488 SP</li> <li>4. \u96f6\u5bc4\u5b58\u5668 XZR</li> </ul> </li> <li>\u5b9e\u9645\u7f16\u7a0b\u4e2d\u7684\u5bc4\u5b58\u5668\u4f7f\u7528<ul> <li>\u51fd\u6570\u8c03\u7528\u7ea6\u5b9a</li> <li>\u7b80\u5355\u8ba1\u7b97\u793a\u4f8b</li> </ul> </li> <li>\u5bc4\u5b58\u5668\u4f7f\u7528\u89c4\u5219<ul> <li>\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\uff08\u4e34\u65f6\u5bc4\u5b58\u5668\uff09</li> <li>\u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668</li> </ul> </li> <li>\u4e0eC\u8bed\u8a00\u7684\u5bf9\u5e94\u5173\u7cfb<ul> <li>C\u51fd\u6570 \u2194 \u6c47\u7f16\u5bc4\u5b58\u5668</li> </ul> </li> <li>\u91cd\u8981\u63d0\u793a</li> </ul> </li> <li>ARM32\u5bc4\u5b58\u5668<ul> <li>\u5bc4\u5b58\u5668\u5206\u914d\u7ed9\u51fd\u6570\u7684\u89c4\u5219\uff08AAPCS\u8c03\u7528\u7ea6\u5b9a\uff09<ul> <li>\u6240\u6709\u51fd\u6570\u5171\u7528\u5bc4\u5b58\u5668\uff0c\u4f46\u901a\u8fc7\u660e\u786e\u89c4\u5219\u534f\u4f5c\uff1a</li> </ul> </li> <li>\u53c2\u6570\u6570\u91cf\u4e0d\u540c\u7684\u5904\u7406\u65b9\u5f0f<ul> <li>\u60c5\u51b51\uff1a4\u4e2a\u53ca\u4ee5\u4e0b\u53c2\u6570\uff08\u5168\u90e8\u7528\u5bc4\u5b58\u5668\uff09</li> <li>\u60c5\u51b52\uff1a\u8d85\u8fc74\u4e2a\u53c2\u6570\uff08\u6df7\u5408\u4f7f\u7528\u5bc4\u5b58\u5668\u548c\u6808\uff09</li> </ul> </li> <li>\u51fd\u6570\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u5171\u4eab\u5bc4\u5b58\u5668<ul> <li>\u51fd\u6570\u5f00\u573a\u5178\u578b\u6a21\u5f0f\uff1a</li> <li>\u5bc4\u5b58\u5668\u4f7f\u7528\u7b56\u7565\uff1a</li> </ul> </li> <li>\u5404\u4e2a\u51fd\u6570\u5982\u4f55\u901a\u8fc7\u5171\u4eab\u5bc4\u5b58\u5668\u8054\u7cfb<ul> <li>1. \u53c2\u6570\u548c\u8fd4\u56de\u503c\u7684\u4f20\u9012\uff08\u8003\u8651\u591a\u53c2\u6570\u60c5\u51b5\uff09</li> <li>2. \u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668\uff08R0-R3, R12\uff09\u7684\u534f\u4f5c</li> <li>3. \u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668\uff08R4-R11\uff09\u7684\u534f\u4f5c</li> </ul> </li> <li>\u5b8c\u6574\u7684\u534f\u4f5c\u793a\u4f8b\uff08\u5305\u542b\u591a\u53c2\u6570\uff09</li> <li>\u5173\u952e\u7406\u89e3\u70b9</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_1","title":"\u5165\u95e8\u6c47\u7f16\u6307\u4ee4","text":"<p>\u89c1Assembly_Intro</p>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#arm64","title":"ARM64\u5bc4\u5b58\u5668","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_2","title":"\u4ec0\u4e48\u662f\u5bc4\u5b58\u5668\uff1f","text":"<p>\u5bc4\u5b58\u5668\u662fCPU\u5185\u90e8\u7684\u5c0f\u578b\u3001\u8d85\u9ad8\u901f\u5b58\u50a8\u5355\u5143\uff0c\u7528\u4e8e\u4e34\u65f6\u5b58\u653e\u6570\u636e\u548c\u6307\u4ee4\u3002\u53ef\u4ee5\u628a\u5b83\u60f3\u8c61\u6210\u5de5\u4f5c\u53f0\u2014\u2014CPU\u76f4\u63a5\u5728\u5bc4\u5b58\u5668\u4e0a\u8fdb\u884c\u8ba1\u7b97\uff0c\u6bd4\u8bbf\u95ee\u5185\u5b58\u5feb\u5f97\u591a\u3002</p>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#arm64aarch64","title":"ARM64\u5bc4\u5b58\u5668\u5206\u7c7b\uff08AArch64\uff09","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#1-x0-x30","title":"1. \u901a\u7528\u5bc4\u5b58\u5668\uff08X0-X30\uff09","text":"<p>ARM64\u670931\u4e2a64\u4f4d\u901a\u7528\u5bc4\u5b58\u5668\uff0c\u7f16\u53f7X0\u5230X30\uff1a</p> \u5bc4\u5b58\u5668 \u522b\u540d \u4e3b\u8981\u7528\u9014 \u8bf4\u660e X0 - \u51fd\u6570\u53c2\u65701 / \u8fd4\u56de\u503c \u7b2c\u4e00\u4e2a\u53c2\u6570\u548c\u51fd\u6570\u8fd4\u56de\u503c X1-X7 - \u51fd\u6570\u53c2\u65702-8 \u989d\u5916\u7684\u51fd\u6570\u53c2\u6570 X8 - \u95f4\u63a5\u7ed3\u679c\u5bc4\u5b58\u5668 \u7279\u6b8a\u7528\u9014 X9-X15 - \u4e34\u65f6\u5bc4\u5b58\u5668 \u8c03\u7528\u65f6\u4e0d\u4fdd\u5b58\uff0c\u968f\u610f\u4f7f\u7528 X16-X17 - \u5185\u90e8\u4e34\u65f6\u5bc4\u5b58\u5668 \u7cfb\u7edf\u4f7f\u7528\uff0c\u907f\u514d\u4f7f\u7528 X18 - \u5e73\u53f0\u5bc4\u5b58\u5668 \u4fdd\u7559\u7ed9\u5e73\u53f0 X19-X28 - \u88ab\u4fdd\u5b58\u5bc4\u5b58\u5668 \u8c03\u7528\u65f6\u5fc5\u987b\u4fdd\u5b58\u539f\u503c X29 FP \u5e27\u6307\u9488 \u6307\u5411\u5f53\u524d\u6808\u5e27\u5e95\u90e8 X30 LR \u94fe\u63a5\u5bc4\u5b58\u5668 \u5b58\u50a8\u51fd\u6570\u8fd4\u56de\u5730\u5740\uff08\u4fdd\u62a4\u73b0\u573a\uff09"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#2","title":"2. \u7279\u6b8a\u529f\u80fd\u5bc4\u5b58\u5668","text":"\u5bc4\u5b58\u5668 \u540d\u79f0 \u4f5c\u7528 XZR \u96f6\u5bc4\u5b58\u5668 \u8bfb\u53d6\u603b\u662f0\uff0c\u5199\u5165\u65e0\u6548\u679c SP \u6808\u6307\u9488 \u6307\u5411\u5f53\u524d\u6808\u9876 PC \u7a0b\u5e8f\u8ba1\u6570\u5668 \u6307\u5411\u4e0b\u4e00\u6761\u8981\u6267\u884c\u7684\u6307\u4ee4"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_3","title":"\u5bc4\u5b58\u5668\u5927\u5c0f\u8bbf\u95ee","text":"<p>\u6bcf\u4e2a64\u4f4d\u5bc4\u5b58\u5668\u90fd\u53ef\u4ee5\u6309\u4e0d\u540c\u5927\u5c0f\u8bbf\u95ee\uff1a</p> <pre><code>X0  // \u8bbf\u95ee\u5b8c\u6574\u768464\u4f4d\u5bc4\u5b58\u5668\nW0  // \u53ea\u8bbf\u95ee\u4f4e32\u4f4d\uff08\u9ad832\u4f4d\u6e05\u96f6\uff09\n</code></pre> <p>\u793a\u4f8b\uff1a</p> <pre><code>MOV X0, 0x123456789ABCDEF0  @ X0 = 0x123456789ABCDEF0\nMOV W1, 0x12345678          @ W1 = 0x0000000012345678\nADD W0, W1, W1              @ X0 = 0x000000002468ACF0\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_4","title":"\u5173\u952e\u5bc4\u5b58\u5668\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#1-x0-x7","title":"1. \u53c2\u6570\u548c\u8fd4\u56de\u503c\u5bc4\u5b58\u5668\uff08X0-X7\uff09","text":"<p>\u51fd\u6570\u8c03\u7528\u793a\u4f8b\uff1a</p> <pre><code>// C\u4ee3\u7801\nint result = add(10, 20, 30);\n\n// \u5bf9\u5e94\u7684\u5bc4\u5b58\u5668\u4f7f\u7528\uff1a\n// X0 = 10 (\u7b2c\u4e00\u4e2a\u53c2\u6570)\n// X1 = 20 (\u7b2c\u4e8c\u4e2a\u53c2\u6570) \n// X2 = 30 (\u7b2c\u4e09\u4e2a\u53c2\u6570)\n// \u8fd4\u56de\u503c\u901a\u8fc7 X0 \u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#2-lrx30","title":"2. \u94fe\u63a5\u5bc4\u5b58\u5668 LR\uff08X30\uff09","text":"<p>\u5b58\u50a8\u51fd\u6570\u8c03\u7528\u7684\u8fd4\u56de\u5730\u5740\uff1a</p> <pre><code>BL my_function    @ \u8df3\u8f6c\u5230my_function\uff0c\u540c\u65f6\u5c06\u8fd4\u56de\u5730\u5740\u4fdd\u5b58\u5230LR\n                  @ \u5728my_function\u4e2d\uff1a\nRET               @ \u7b49\u540c\u4e8e MOV PC, LR\uff0c\u8df3\u56de\u5230\u8c03\u7528\u5904\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#3-sp","title":"3. \u6808\u6307\u9488 SP","text":"<p>\u7ba1\u7406\u51fd\u6570\u8c03\u7528\u7684\u6808\u7a7a\u95f4\uff1a</p> <pre><code>SUB SP, SP, #16   @ \u5728\u6808\u4e0a\u5206\u914d16\u5b57\u8282\u7a7a\u95f4\nADD SP, SP, #16   @ \u91ca\u653e\u6808\u7a7a\u95f4\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#4-xzr","title":"4. \u96f6\u5bc4\u5b58\u5668 XZR","text":"<p>\u63d0\u4f9b\u5e38\u6570\u503c0\uff0c\u5f88\u6709\u7528\uff1a</p> <pre><code>MOV X0, XZR       @ X0 = 0\nCMP X1, XZR       @ \u6bd4\u8f83X1\u662f\u5426\u7b49\u4e8e0\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_5","title":"\u5b9e\u9645\u7f16\u7a0b\u4e2d\u7684\u5bc4\u5b58\u5668\u4f7f\u7528","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_6","title":"\u51fd\u6570\u8c03\u7528\u7ea6\u5b9a","text":"<pre><code>@ \u51fd\u6570\u5b9a\u4e49\nmy_function:\n    SUB SP, SP, #16       @ 1. \u5728\u6808\u4e0a\u5206\u914d\u7a7a\u95f4\n    STR X30, [SP, #8]     @ 2. \u4fdd\u5b58\u8fd4\u56de\u5730\u5740(LR)\n    STR X29, [SP, #0]     @ 3. \u4fdd\u5b58\u5e27\u6307\u9488(FP)\n\n    @ \u51fd\u6570\u4f53...\n    ADD X0, X0, X1        @ \u4f7f\u7528X0,X1(\u53c2\u6570\u5bc4\u5b58\u5668)\n\n    LDR X29, [SP, #0]     @ \u6062\u590dFP\n    LDR X30, [SP, #8]     @ \u6062\u590dLR  \n    ADD SP, SP, #16       @ \u91ca\u653e\u6808\u7a7a\u95f4\n    RET                   @ \u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_7","title":"\u7b80\u5355\u8ba1\u7b97\u793a\u4f8b","text":"<pre><code>@ \u8ba1\u7b97 (a + b) * c\n@ \u5047\u8bbe: a\u5728X0, b\u5728X1, c\u5728X2\n\nADD X3, X0, X1    @ X3 = a + b (\u4f7f\u7528\u4e34\u65f6\u5bc4\u5b58\u5668X3)\nMUL X0, X3, X2    @ X0 = (a+b) * c\uff0c\u7ed3\u679c\u653e\u5728X0\u8fd4\u56de\nRET\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_8","title":"\u5bc4\u5b58\u5668\u4f7f\u7528\u89c4\u5219","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_9","title":"\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\uff08\u4e34\u65f6\u5bc4\u5b58\u5668\uff09","text":"<ul> <li>X0-X18\uff1a\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u53ef\u80fd\u88ab\u4fee\u6539\uff0c\u8c03\u7528\u8005\u9700\u8981\u4fdd\u5b58\u91cd\u8981\u6570\u636e</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_10","title":"\u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668","text":"<ul> <li>X19-X28\uff1a\u5982\u679c\u51fd\u6570\u8981\u4f7f\u7528\u8fd9\u4e9b\u5bc4\u5b58\u5668\uff0c\u5fc5\u987b\u5148\u5728\u6808\u4e0a\u4fdd\u5b58\u539f\u503c\uff0c\u8fd4\u56de\u524d\u6062\u590d</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#c","title":"\u4e0eC\u8bed\u8a00\u7684\u5bf9\u5e94\u5173\u7cfb","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#c_1","title":"C\u51fd\u6570 \u2194 \u6c47\u7f16\u5bc4\u5b58\u5668","text":"<pre><code>// C\u4ee3\u7801\nint calculate(int a, int b, int c) {\n    int temp = a + b;\n    return temp * c;\n}\n</code></pre> <p>\u5bf9\u5e94\u7684\u6c47\u7f16\u5bc4\u5b58\u5668\u4f7f\u7528\uff1a - <code>a</code> \u2192 X0 - <code>b</code> \u2192 X1  - <code>c</code> \u2192 X2 - <code>temp</code> \u2192 X3\uff08\u4e34\u65f6\u5bc4\u5b58\u5668\uff09 - \u8fd4\u56de\u503c \u2192 X0</p>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_11","title":"\u91cd\u8981\u63d0\u793a","text":"<ol> <li>\u5bc4\u5b58\u5668\u662f\u7a00\u7f3a\u8d44\u6e90\uff1a\u53ea\u670931\u4e2a\u901a\u7528\u5bc4\u5b58\u5668\uff0c\u8981\u9ad8\u6548\u4f7f\u7528</li> <li>\u7406\u89e3\u6570\u636e\u6d41\uff1a\u89c2\u5bdf\u6570\u636e\u5982\u4f55\u5728\u5bc4\u5b58\u5668\u95f4\u6d41\u52a8</li> <li>\u6808\u7684\u91cd\u8981\u6027\uff1a\u5f53\u5bc4\u5b58\u5668\u4e0d\u591f\u65f6\uff0c\u4f7f\u7528\u6808\u6765\u5b58\u50a8\u4e34\u65f6\u6570\u636e</li> <li>\u8c03\u7528\u7ea6\u5b9a\uff1a\u9075\u5faa\u5bc4\u5b58\u5668\u4f7f\u7528\u89c4\u5219\uff0c\u786e\u4fdd\u51fd\u6570\u95f4\u6b63\u786e\u534f\u4f5c</li> </ol> <p>\u8bb0\u4f4f\uff1a\u5bc4\u5b58\u5668\u662fCPU\u7684\u5de5\u4f5c\u53f0\uff0c\u6240\u6709\u8ba1\u7b97\u90fd\u5728\u8fd9\u91cc\u53d1\u751f\uff01</p>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#arm32","title":"ARM32\u5bc4\u5b58\u5668","text":"<p>ARM32\u67b6\u6784\u670916\u4e2a32\u4f4d\u6838\u5fc3\u5bc4\u5b58\u5668\uff0c\u6240\u6709\u51fd\u6570\u90fd\u5171\u4eab\u8fd9\u540c\u4e00\u7ec4\u5bc4\u5b58\u5668\uff1a</p> <p>\u901a\u7528\u5bc4\u5b58\u5668\uff1a</p> <ul> <li>R0-R3\uff1a\u53c2\u6570/\u7ed3\u679c\u5bc4\u5b58\u5668\uff08\u8c03\u7528\u8005\u4fdd\u5b58\uff09- \u50cf\"\u4e34\u65f6\u5de5\u4f5c\u53f0\"\uff0c\u7528\u4e8e\u4f20\u9012\u524d4\u4e2a\u53c2\u6570</li> <li>R4-R11\uff1a\u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668 - \u50cf\"\u4e2a\u4eba\u5de5\u4f5c\u53f0\"\uff0c\u7528\u5b8c\u540e\u5fc5\u987b\u6062\u590d\u539f\u6837</li> <li>R12\uff1a\u5185\u90e8\u8c03\u7528\u6682\u5b58\u5bc4\u5b58\u5668</li> </ul> <p>\u7279\u6b8a\u529f\u80fd\u5bc4\u5b58\u5668\uff1a</p> <ul> <li>R13 (SP)\uff1a\u6808\u6307\u9488 - \u603b\u7ba1\u7406\u5458\u5ea7\u4f4d\uff0c\u4e5f\u7528\u4e8e\u4f20\u9012\u7b2c5\u4e2a\u53ca\u4ee5\u540e\u7684\u53c2\u6570</li> <li>R14 (LR)\uff1a\u94fe\u63a5\u5bc4\u5b58\u5668 - \u7535\u8bdd\u673a\uff0c\u8bb0\u5f55\u8fd4\u56de\u5730\u5740</li> <li>R15 (PC)\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668 - \u5f53\u524d\u4efb\u52a1\u6307\u793a\u724c</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#aapcs","title":"\u5bc4\u5b58\u5668\u5206\u914d\u7ed9\u51fd\u6570\u7684\u89c4\u5219\uff08AAPCS\u8c03\u7528\u7ea6\u5b9a\uff09","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_12","title":"\u6240\u6709\u51fd\u6570\u5171\u7528\u5bc4\u5b58\u5668\uff0c\u4f46\u901a\u8fc7\u660e\u786e\u89c4\u5219\u534f\u4f5c\uff1a","text":"<p>\u53c2\u6570\u4f20\u9012\u89c4\u5219\uff1a</p> <pre><code>// \u6240\u6709\u51fd\u6570\u90fd\u9075\u5b88\u7edf\u4e00\u7684\u53c2\u6570\u4f20\u9012\u89c4\u5219\nvoid func(int a, int b, int c, int d, int e, int f);\n// \u8c03\u7528\u65f6\uff1a\n// R0 = a, R1 = b, R2 = c, R3 = d (\u524d4\u4e2a\u53c2\u6570)\n// [SP+0] = e, [SP+4] = f (\u7b2c5\u30016\u4e2a\u53c2\u6570\u901a\u8fc7\u6808\u4f20\u9012)\n</code></pre> <p>\u8fd4\u56de\u503c\u89c4\u5219\uff1a - \u6240\u6709\u51fd\u6570\u90fd\u901a\u8fc7R0\u8fd4\u56de\u7ed3\u679c</p>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_13","title":"\u53c2\u6570\u6570\u91cf\u4e0d\u540c\u7684\u5904\u7406\u65b9\u5f0f","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#14","title":"\u60c5\u51b51\uff1a4\u4e2a\u53ca\u4ee5\u4e0b\u53c2\u6570\uff08\u5168\u90e8\u7528\u5bc4\u5b58\u5668\uff09","text":"<pre><code>; \u8c03\u7528 func(1, 2, 3, 4)\nMOV R0, #1      ; \u53c2\u65701 \u2192 R0\nMOV R1, #2      ; \u53c2\u65702 \u2192 R1  \nMOV R2, #3      ; \u53c2\u65703 \u2192 R2\nMOV R3, #4      ; \u53c2\u65704 \u2192 R3\nBL func         ; \u8c03\u7528\u51fd\u6570\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#24","title":"\u60c5\u51b52\uff1a\u8d85\u8fc74\u4e2a\u53c2\u6570\uff08\u6df7\u5408\u4f7f\u7528\u5bc4\u5b58\u5668\u548c\u6808\uff09","text":"<pre><code>; \u8c03\u7528 func(1, 2, 3, 4, 5, 6)\nMOV R0, #1      ; \u53c2\u65701 \u2192 R0\nMOV R1, #2      ; \u53c2\u65702 \u2192 R1\nMOV R2, #3      ; \u53c2\u65703 \u2192 R2  \nMOV R3, #4      ; \u53c2\u65704 \u2192 R3\nSUB SP, SP, #8  ; \u4e3a\u989d\u5916\u53c2\u6570\u5206\u914d\u6808\u7a7a\u95f4\nMOV R4, #5\nSTR R4, [SP, #0] ; \u53c2\u65705 \u2192 [SP+0]\nMOV R4, #6\nSTR R4, [SP, #4] ; \u53c2\u65706 \u2192 [SP+4]\nBL func         ; \u8c03\u7528\u51fd\u6570\nADD SP, SP, #8  ; \u6e05\u7406\u6808\u7a7a\u95f4\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_14","title":"\u51fd\u6570\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u5171\u4eab\u5bc4\u5b58\u5668","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_15","title":"\u51fd\u6570\u5f00\u573a\u5178\u578b\u6a21\u5f0f\uff1a","text":"<pre><code>func:\n    PUSH {R4-R6, LR}    ; \u5982\u679c\u8981\u4f7f\u7528R4-R11\uff0c\u5fc5\u987b\u5148\u4fdd\u5b58\n\n    ; \u8bbf\u95ee\u53c2\u6570\uff1a\n    ; \u524d4\u4e2a\u53c2\u6570\u76f4\u63a5\u5728R0-R3\u4e2d\n    MOV R4, R0          ; \u53c2\u65701\u4fdd\u5b58\u5230R4\n    MOV R5, R1          ; \u53c2\u65702\u4fdd\u5b58\u5230R5\n\n    ; \u8bbf\u95ee\u7b2c5\u4e2a\u53ca\u4ee5\u540e\u7684\u53c2\u6570\uff08\u9700\u8981\u8ba1\u7b97\u6808\u504f\u79fb\uff09\n    LDR R6, [SP, #16]   ; \u53c2\u65705\u5728[SP+16]\uff08\u8df3\u8fc7PUSH\u7684\u5bc4\u5b58\u5668\uff09\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_16","title":"\u5bc4\u5b58\u5668\u4f7f\u7528\u7b56\u7565\uff1a","text":"<ul> <li>R0-R3\uff1a\u4e34\u65f6\u8ba1\u7b97\uff0c\u4efb\u4f55\u51fd\u6570\u90fd\u53ef\u4ee5\u76f4\u63a5\u7528\uff0c\u4e5f\u7528\u4e8e\u53c2\u6570\u4f20\u9012</li> <li>R4-R11\uff1a\u4efb\u4f55\u51fd\u6570\u60f3\u7528\u5c31\u5fc5\u987b\u5148\u4fdd\u5b58\u518d\u6062\u590d</li> <li>\u6808\uff1a\u7528\u4e8e\u4f20\u9012\u989d\u5916\u53c2\u6570\u548c\u4fdd\u5b58\u5c40\u90e8\u53d8\u91cf</li> <li>\u6240\u6709\u51fd\u6570\u90fd\u9075\u5b88\u540c\u6837\u7684\u89c4\u5219</li> </ul>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_17","title":"\u5404\u4e2a\u51fd\u6570\u5982\u4f55\u901a\u8fc7\u5171\u4eab\u5bc4\u5b58\u5668\u8054\u7cfb","text":""},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#1","title":"1. \u53c2\u6570\u548c\u8fd4\u56de\u503c\u7684\u4f20\u9012\uff08\u8003\u8651\u591a\u53c2\u6570\u60c5\u51b5\uff09","text":"<pre><code>; main\u51fd\u6570\u51c6\u5907\u53c2\u6570\uff086\u4e2a\u53c2\u6570\uff09\nmain:\n    MOV R0, #10         ; \u53c2\u65701 \u2192 R0\n    MOV R1, #20         ; \u53c2\u65702 \u2192 R1\n    MOV R2, #30         ; \u53c2\u65703 \u2192 R2\n    MOV R3, #40         ; \u53c2\u65704 \u2192 R3\n    SUB SP, SP, #8      ; \u4e3a\u989d\u5916\u53c2\u6570\u5206\u914d\u7a7a\u95f4\n    MOV R4, #50\n    STR R4, [SP, #0]    ; \u53c2\u65705 \u2192 [SP+0]\n    MOV R4, #60         \n    STR R4, [SP, #4]    ; \u53c2\u65706 \u2192 [SP+4]\n    BL complex_calc     ; \u8c03\u7528\u51fd\u6570\n\n; complex_calc\u51fd\u6570\u63a5\u6536\u6240\u6709\u53c2\u6570\ncomplex_calc:\n    PUSH {R4-R6, LR}\n    ; \u63a5\u6536\u5bc4\u5b58\u5668\u4e2d\u7684\u53c2\u6570\n    MOV R4, R0          ; \u53c2\u65701 = 10\n    MOV R5, R1          ; \u53c2\u65702 = 20\n    ; \u63a5\u6536\u6808\u4e2d\u7684\u53c2\u6570\n    LDR R6, [SP, #16]   ; \u53c2\u65705 = 50\n    ; ... \u8ba1\u7b97 ...\n    MOV R0, R4          ; \u7ed3\u679c\u901a\u8fc7R0\u8fd4\u56de\n    POP {R4-R6, PC}\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#2-r0-r3-r12","title":"2. \u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668\uff08R0-R3, R12\uff09\u7684\u534f\u4f5c","text":"<pre><code>main:\n    MOV R4, R0              ; \u5982\u679cR0\u6709\u91cd\u8981\u6570\u636e\uff0c\u5148\u4fdd\u5b58\u5230R4\n    BL other_func           ; \u8c03\u7528\u5176\u4ed6\u51fd\u6570\n    ; \u6211\u77e5\u9053other_func\u53ef\u80fd\u4f1a\u6539\u52a8R0-R3\uff0c\u6240\u4ee5\u4e0d\u4f9d\u8d56\u5b83\u4eec\n    MOV R0, R4              ; \u6062\u590d\u6211\u539f\u6765\u7684\u6570\u636e\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#3-r4-r11","title":"3. \u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668\uff08R4-R11\uff09\u7684\u534f\u4f5c","text":"<pre><code>; \u4efb\u4f55\u51fd\u6570\u5982\u679c\u60f3\u7528R4-R11\uff0c\u90fd\u5fc5\u987b\uff1a\nmy_func:\n    PUSH {R4-R6}        ; \u5148\u4fdd\u5b58\u539f\u6765\u7684\u503c\n    ; ... \u4f7f\u7528R4-R6 ...  ; \u968f\u4fbf\u4f7f\u7528\n    POP {R4-R6}         ; \u6062\u590d\u539f\u6837\uff0c\u4e0d\u5f71\u54cd\u5176\u4ed6\u51fd\u6570\n    BX LR\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_18","title":"\u5b8c\u6574\u7684\u534f\u4f5c\u793a\u4f8b\uff08\u5305\u542b\u591a\u53c2\u6570\uff09","text":"<pre><code>// \u6240\u6709\u51fd\u6570\u5171\u4eab\u540c\u4e00\u5957\u5bc4\u5b58\u5668\uff0c\u4f46\u901a\u8fc7\u89c4\u5219\u534f\u4f5c\nint add(int a, int b) {\n    return a + b;       // \u7528R0,R1\u63a5\u6536\u53c2\u6570\uff0cR0\u8fd4\u56de\u7ed3\u679c\n}\n\nint complex_calc(int a, int b, int c, int d, int e, int f) {\n    // \u524d4\u4e2a\u53c2\u6570\u5728\u5bc4\u5b58\u5668\uff0c\u540e2\u4e2a\u5728\u6808\u4e2d\n    int sum = a + b + c + d + e + f;\n    return add(sum, 100); // \u7ee7\u7eed\u8c03\u7528\u5176\u4ed6\u51fd\u6570\n}\n</code></pre> <pre><code>; \u6240\u6709\u51fd\u6570\u5171\u7528\u5bc4\u5b58\u5668\uff0c\u4f46\u5404\u53f8\u5176\u804c\nadd:\n    ; \u6211\u77e5\u9053\uff1aR0=a, R1=b\u6765\u81ea\u8c03\u7528\u8005\n    ADD R0, R0, R1      ; \u8ba1\u7b97a+b\n    ; \u6211\u77e5\u9053\uff1a\u8c03\u7528\u8005\u671f\u671b\u7ed3\u679c\u5728R0\n    BX LR               ; \u8fd4\u56de\n\ncomplex_calc:\n    PUSH {R4-R6, LR}    ; \u6211\u8981\u7528R4-R6\uff0c\u6240\u4ee5\u5148\u4fdd\u5b58\n    ; \u63a5\u6536\u524d4\u4e2a\u53c2\u6570\uff1aR0=a, R1=b, R2=c, R3=d\n    MOV R4, R0\n    MOV R5, R1\n    ADD R4, R4, R5      ; a + b\n    ADD R4, R4, R2      ; + c\n    ADD R4, R4, R3      ; + d\n\n    ; \u63a5\u6536\u6808\u4e2d\u7684\u7b2c5\u30016\u4e2a\u53c2\u6570\n    LDR R5, [SP, #16]   ; \u53c2\u6570e (\u8df3\u8fc7PUSH\u76844\u4e2a\u5bc4\u5b58\u5668)\n    LDR R6, [SP, #20]   ; \u53c2\u6570f\n    ADD R4, R4, R5      ; + e\n    ADD R4, R4, R6      ; + f\n\n    ; \u8c03\u7528add\u51fd\u6570\n    MOV R0, R4          ; \u7b2c\u4e00\u4e2a\u53c2\u6570 = sum\n    MOV R1, #100        ; \u7b2c\u4e8c\u4e2a\u53c2\u6570 = 100\n    BL add\n\n    ; \u6211\u77e5\u9053\uff1aadd\u7684\u7ed3\u679c\u5728R0\u4e2d\n    POP {R4-R6, PC}     ; \u6062\u590d\u5bc4\u5b58\u5668\uff0c\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/GeneralReg/#_19","title":"\u5173\u952e\u7406\u89e3\u70b9","text":"<ol> <li>\u4e0d\u662f\u6bcf\u4e2a\u51fd\u6570\u6709\u4e13\u5c5e\u5bc4\u5b58\u5668\uff0c\u800c\u662f\u6240\u6709\u51fd\u6570\u5171\u752816\u4e2a\u5bc4\u5b58\u5668</li> <li>\u901a\u8fc7\u660e\u786e\u7684\u4f7f\u7528\u89c4\u5219\u6765\u907f\u514d\u51b2\u7a81</li> <li>\u53c2\u6570\u4f20\u9012\uff1a</li> <li>\u524d4\u4e2a\uff1aR0-R3\u5bc4\u5b58\u5668\uff08\u6700\u5feb\uff09</li> <li>\u7b2c5\u4e2a\u53ca\u4ee5\u540e\uff1a\u6808\u7a7a\u95f4\uff08\u7a0d\u6162\uff09</li> <li>\u8fd4\u56de\u503c\uff1a\u59cb\u7ec8\u901a\u8fc7R0\u8fd4\u56de</li> <li>\u957f\u671f\u6570\u636e\u8981\u4e48\u7528R4-R11\uff08\u4f46\u8981\u4fdd\u5b58\u6062\u590d\uff09\uff0c\u8981\u4e48\u7528\u6808</li> </ol> <p>\u8fd9\u79cd\u8bbe\u8ba1\u65e2\u9ad8\u6548\uff08\u5927\u591a\u6570\u8c03\u7528\u7528\u5bc4\u5b58\u5668\uff09\u53c8\u7075\u6d3b\uff08\u652f\u6301\u4efb\u610f\u591a\u53c2\u6570\uff09\uff0c\u5728\u6027\u80fd\u548c\u5b9e\u7528\u6027\u95f4\u5b8c\u7f8e\u5e73\u8861\uff01</p>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/","title":"xPSR\u7684\u57fa\u672c\u6982\u5ff5","text":"<p>xPSR\u662fARM Cortex-M\u5904\u7406\u5668\u4e2d\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668\uff08Program Status Register\uff09\u7684\u7ec4\u5408\u540d\u79f0\uff0c\u5b83\u5b9e\u9645\u4e0a\u5305\u542b\u4e86\u4e09\u4e2a\u72ec\u7acb\u7684\u72b6\u6001\u5bc4\u5b58\u5668\uff1a</p> <ul> <li>APSR (Application PSR)\uff1a\u5e94\u7528\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668</li> <li>IPSR (Interrupt PSR)\uff1a\u4e2d\u65ad\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668  </li> <li>EPSR (Execution PSR)\uff1a\u6267\u884c\u72b6\u6001\u5bc4\u5b58\u5668</li> </ul> <p>\u5728\u5f02\u5e38\u5904\u7406\u65f6\uff0c\u8fd9\u4e09\u4e2a\u5bc4\u5b58\u5668\u88ab\u6253\u5305\u6210\u4e00\u4e2a32\u4f4d\u7684xPSR\u503c\u4fdd\u5b58\u5230\u6808\u4e2d\u3002</p>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_1","title":"xPSR\u7684\u4f4d\u57df\u7ed3\u6784","text":"<p>xPSR\u7684\u5b8c\u657432\u4f4d\u7ed3\u6784\u5982\u4e0b\uff1a</p> <pre><code>xPSR\u5bc4\u5b58\u5668\u4f4d\u57df\u5e03\u5c40\uff1a\n31 30 29 28 27 26 25 24 23 20 19 16 15 10 9     8     7     6     5     0\n+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-----+-----+-----+-----+-----+\n|N |Z |C |V |Q |  |  |  |  |  |  |  |  |  |  |ICI/IT|T |  |  |  |  |Exception|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |     |   |  |  |  |  | Number  |\n+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-----+-----+-----+-----+-----+\n\n\u4f4d\u57df\u8bf4\u660e\uff1a\n[31:27] \u6761\u4ef6\u6807\u5fd7\u4f4d\n[26:25] \u4fdd\u7559\n[24]    Q\u6807\u5fd7 (\u9971\u548c\u8fd0\u7b97\u6807\u5fd7)\n[23:20] \u4fdd\u7559  \n[19:16] GE[3:0] (\u5728\u67d0\u4e9bCortex-M\u5904\u7406\u5668\u4e2d\u7528\u4e8eSIMD)\n[15:10] ICI/IT (\u4e2d\u65ad\u8fde\u7eed\u6307\u4ee4/IF-THEN\u6307\u4ee4\u72b6\u6001)\n[9]     T\u6807\u5fd7 (\u59cb\u7ec8\u4e3a1\uff0c\u8868\u793aThumb\u72b6\u6001)\n[8:0]   \u5f02\u5e38\u7f16\u53f7 (IPSR\u90e8\u5206)\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#_1","title":"\u5404\u72b6\u6001\u5bc4\u5b58\u5668\u7684\u5177\u4f53\u5185\u5bb9","text":""},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#apsr-","title":"APSR - \u5e94\u7528\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668","text":"<p>APSR\u5305\u542b\u7b97\u672f\u548c\u903b\u8f91\u8fd0\u7b97\u7684\u6761\u4ef6\u6807\u5fd7\uff1a</p> <pre><code>APSR\u4f4d\u57df (xPSR[31:27] + xPSR[24]):\n31 30 29 28 27 24\n+--+--+--+--+--+\n|N |Z |C |V |Q |\n+--+--+--+--+--+\n\n\u6807\u5fd7\u4f4d\u542b\u4e49\uff1a\n- N (Negative): \u7ed3\u679c\u4e3a\u8d1f\u65f6\u7f6e1\n- Z (Zero):    \u7ed3\u679c\u4e3a\u96f6\u65f6\u7f6e1  \n- C (Carry):   \u8fdb\u4f4d\u6216\u501f\u4f4d\u65f6\u7f6e1\n- V (Overflow): \u6709\u7b26\u53f7\u6ea2\u51fa\u65f6\u7f6e1\n- Q (Saturation): \u9971\u548c\u8fd0\u7b97\u53d1\u751f\u65f6\u7f6e1\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#ipsr-","title":"IPSR - \u4e2d\u65ad\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668","text":"<p>IPSR\u5305\u542b\u5f53\u524d\u6b63\u5728\u5904\u7406\u7684\u5f02\u5e38\u7f16\u53f7\uff1a</p> <pre><code>IPSR\u4f4d\u57df (xPSR[8:0]):\n8     0\n+-----+\n|Exception|\n| Number  |\n+-----+\n\n\u5e38\u89c1\u5f02\u5e38\u7f16\u53f7\uff1a\n0: \u7ebf\u7a0b\u6a21\u5f0f (\u65e0\u5f02\u5e38)\n1: \u590d\u4f4d\n2: NMI (\u4e0d\u53ef\u5c4f\u853d\u4e2d\u65ad)\n3: \u786c\u6545\u969c\n4: \u5185\u5b58\u7ba1\u7406\u6545\u969c\n5: \u603b\u7ebf\u6545\u969c\n6: \u4f7f\u7528\u6545\u969c\n7-10: \u4fdd\u7559\n11: SVCall\n12: \u8c03\u8bd5\u76d1\u63a7\n13: \u4fdd\u7559\n14: PendSV\n15: SysTick\n16-255: \u5916\u90e8\u4e2d\u65ad\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#epsr-","title":"EPSR - \u6267\u884c\u72b6\u6001\u5bc4\u5b58\u5668","text":"<p>EPSR\u5305\u542b\u5904\u7406\u5668\u7684\u6267\u884c\u72b6\u6001\u4fe1\u606f\uff1a</p> <pre><code>EPSR\u4f4d\u57df (xPSR[15:10] + xPSR[9]):\n15     10 9\n+-----+--+\n|ICI/IT |T|\n+-----+--+\n\n\u4f4d\u57df\u542b\u4e49\uff1a\n- T (Thumb state): \u59cb\u7ec8\u4e3a1\uff0cCortex-M\u53ea\u652f\u6301Thumb\u6307\u4ee4\u96c6\n- ICI/IT: \n  * \u5728\u4e2d\u65ad\u8fde\u7eed\u6307\u4ee4\u65f6\u5305\u542b\u540e\u7eed\u6307\u4ee4\u4fe1\u606f\n  * \u5728IT (If-Then) \u5757\u4e2d\u5305\u542b\u6761\u4ef6\u6267\u884c\u72b6\u6001\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_2","title":"xPSR\u5728\u4efb\u52a1\u5207\u6362\u4e2d\u7684\u4f5c\u7528","text":""},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_3","title":"\u4e0a\u4e0b\u6587\u4fdd\u5b58\u65f6\u7684xPSR","text":"<p>\u5f53\u5f02\u5e38\u53d1\u751f\u65f6\uff0c\u786c\u4ef6\u81ea\u52a8\u5c06xPSR\u4fdd\u5b58\u5230\u6808\u4e2d\uff1a</p> <pre><code>\u5f02\u5e38\u5165\u53e3\u65f6\u7684\u6808\u5e27\uff1a\n+------------------+\n|      xPSR        | \u2190 \u5305\u542b\u5b8c\u6574\u7684\u5904\u7406\u5668\u72b6\u6001\n+------------------+\n|       PC         | \u2190 \u8fd4\u56de\u5730\u5740\n+------------------+\n|       LR         | \u2190 \u5f02\u5e38\u8fd4\u56de\u4fe1\u606f\n+------------------+\n|       R12        |\n+------------------+\n|       R3         |\n+------------------+\n|       R2         |\n+------------------+\n|       R1         |\n+------------------+\n|       R0         |\n+------------------+ \u2190 \u5f02\u5e38\u540e\u7684\u6808\u9876\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_4","title":"xPSR\u5728\u4efb\u52a1\u6062\u590d\u4e2d\u7684\u91cd\u8981\u6027","text":"<p>\u4efb\u52a1\u6062\u590d\u65f6\uff0cxPSR\u786e\u4fdd\u5904\u7406\u5668\u72b6\u6001\u5b8c\u5168\u8fd8\u539f\uff1a</p> <ol> <li>\u6761\u4ef6\u6807\u5fd7\u6062\u590d\uff1aAPSR\u90e8\u5206\u786e\u4fdd\u7b97\u672f\u8fd0\u7b97\u72b6\u6001\u4e00\u81f4</li> <li>\u6267\u884c\u72b6\u6001\u6062\u590d\uff1aEPSR\u90e8\u5206\u7ef4\u6301\u6b63\u786e\u7684\u6307\u4ee4\u6267\u884c\u72b6\u6001</li> <li>\u5f02\u5e38\u72b6\u6001\u6e05\u7406\uff1aIPSR\u90e8\u5206\u5728\u5f02\u5e38\u8fd4\u56de\u65f6\u88ab\u6e05\u9664\uff08\u56de\u5230\u7ebf\u7a0b\u6a21\u5f0f\uff09</li> </ol>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsrrtos","title":"xPSR\u5728RTOS\u4e2d\u7684\u5177\u4f53\u5e94\u7528","text":""},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#_2","title":"\u4efb\u52a1\u7b2c\u4e00\u6b21\u542f\u52a8\u7684\u7279\u6b8a\u5904\u7406","text":"<p>\u5bf9\u4e8e\u65b0\u521b\u5efa\u7684\u4efb\u52a1\uff0c\u9700\u8981\u624b\u52a8\u6784\u9020\u521d\u59cbxPSR\uff1a</p> <pre><code>// \u65b0\u4efb\u52a1\u6808\u5e27\u521d\u59cb\u5316\nvoid init_task_stack(uint32_t *stack_top, void (*task_entry)(void *), void *arg) {\n    // \u6784\u9020\u521d\u59cb\u4e0a\u4e0b\u6587\u6808\u5e27\n    *(--stack_top) = 0x01000000;    // xPSR: Thumb\u72b6\u6001\uff0c\u9ed8\u8ba4\u6761\u4ef6\u6807\u5fd7\n    *(--stack_top) = (uint32_t)task_entry;  // PC: \u4efb\u52a1\u5165\u53e3\u70b9\n    *(--stack_top) = 0xFFFFFFFD;    // LR: \u5f02\u5e38\u8fd4\u56de\u4f7f\u7528PSP\n    // ... \u5176\u4ed6\u5bc4\u5b58\u5668\u521d\u59cb\u5316\n}\n</code></pre> <p>\u521d\u59cbxPSR\u503c <code>0x01000000</code> \u7684\u542b\u4e49\uff1a - T\u4f4d(bit24) = 1\uff1aThumb\u72b6\u6001 - \u9ed8\u8ba4\u6761\u4ef6\u6807\u5fd7\uff1aN=0, Z=0, C=0, V=0</p>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_5","title":"\u5f02\u5e38\u8fd4\u56de\u65f6\u7684xPSR\u5904\u7406","text":"<p>\u5f02\u5e38\u8fd4\u56de\u6307\u4ee4\u81ea\u52a8\u4ece\u6808\u4e2d\u6062\u590dxPSR\uff1a</p> <pre><code>; \u5f02\u5e38\u8fd4\u56de\u65f6\uff0c\u786c\u4ef6\u81ea\u52a8\u6267\u884c\uff1a\n; - \u4ece\u6808\u4e2d\u5f39\u51faxPSR, PC, LR, R12, R0-R3\n; - \u6839\u636exPSR\u6062\u590d\u5904\u7406\u5668\u72b6\u6001\n; - \u6839\u636eLR\u7684\u503c\u786e\u5b9a\u8fd4\u56de\u6a21\u5f0f\u548c\u6808\u6307\u9488\n\nBX LR    ; \u5f02\u5e38\u8fd4\u56de\uff0cLR\u901a\u5e38\u4e3a0xFFFFFFFD\uff08\u8fd4\u56de\u5230\u7ebf\u7a0b\u6a21\u5f0f\u4f7f\u7528PSP\uff09\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_6","title":"xPSR\u4e0e\u4efb\u52a1\u8c03\u8bd5","text":""},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_7","title":"\u8c03\u8bd5\u4fe1\u606f\u4e2d\u7684xPSR","text":"<p>\u5728\u8c03\u8bd5\u5668\u4e2d\u53ef\u4ee5\u67e5\u770bxPSR\u7684\u503c\u6765\u5206\u6790\u4efb\u52a1\u72b6\u6001\uff1a</p> <pre><code>\u8c03\u8bd5\u5668\u4e2d\u7684xPSR\u663e\u793a\u793a\u4f8b\uff1a\nxPSR = 0x61000013\n\u5206\u89e3\uff1a\n- N=0, Z=1, C=0, V=0, Q=0  (APSR: 0x60000000)\n- T=1, ICI/IT=0            (EPSR: 0x01000000)  \n- \u5f02\u5e38\u53f7=19                (IPSR: 0x00000013)\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_8","title":"\u5e38\u89c1xPSR\u72b6\u6001\u5206\u6790","text":"xPSR\u503c \u72b6\u6001\u8bf4\u660e \u5e38\u89c1\u573a\u666f 0x01000000 \u6b63\u5e38\u7ebf\u7a0b\u6a21\u5f0f \u4efb\u52a1\u6b63\u5e38\u8fd0\u884c 0x01000001 \u5904\u7406\u5668\u590d\u4f4d \u7cfb\u7edf\u542f\u52a8 0x01000003 \u786c\u6545\u969c\u5904\u7406 \u4e25\u91cd\u9519\u8bef 0x0100000B SVCall\u5904\u7406 \u7cfb\u7edf\u8c03\u7528\u6267\u884c\u4e2d 0x0100000E PendSV\u5904\u7406 \u4efb\u52a1\u5207\u6362\u4e2d"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_9","title":"xPSR\u76f8\u5173\u7684\u5e38\u89c1\u95ee\u9898","text":""},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#xpsr_10","title":"\u6808\u5e27\u635f\u574f\u5bfc\u81f4\u7684xPSR\u9519\u8bef","text":"<p>\u5982\u679c\u4efb\u52a1\u6808\u88ab\u7834\u574f\uff0c\u6062\u590d\u7684xPSR\u503c\u53ef\u80fd\u65e0\u6548\uff1a</p> <pre><code>\u6808\u635f\u574f\u5bfc\u81f4\u7684\u95ee\u9898\uff1a\n+------------------+\n|  \u635f\u574f\u7684xPSR\u503c    | \u2190 \u65e0\u6548\u7684\u5904\u7406\u5668\u72b6\u6001\n+------------------+\n|  \u65e0\u6548\u7684PC\u5730\u5740    | \u2190 \u53ef\u80fd\u5bfc\u81f4\u786c\u6545\u969c\n+------------------+\n</code></pre>"},{"location":"EmbeddedSoft/ARM_Arch/SpecialReg_xPSR/#_3","title":"\u4efb\u52a1\u8bbe\u8ba1\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u907f\u514d\u4fee\u6539xPSR\uff1a\u5e94\u7528\u7a0b\u5e8f\u4e0d\u5e94\u76f4\u63a5\u4fee\u6539xPSR</li> <li>\u6b63\u786e\u6784\u9020\u521d\u59cb\u6808\uff1a\u65b0\u4efb\u52a1\u7684\u521d\u59cbxPSR\u5fc5\u987b\u6b63\u786e\u8bbe\u7f6e</li> <li>\u6808\u6ea2\u51fa\u4fdd\u62a4\uff1a\u9632\u6b62\u6808\u635f\u574f\u5bfc\u81f4\u7684xPSR\u7834\u574f</li> </ol> <p>xPSR\u5728RTOS\u4efb\u52a1\u5207\u6362\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5b83\u786e\u4fdd\u4e86\u4efb\u52a1\u72b6\u6001\u7684\u5b8c\u6574\u4fdd\u5b58\u548c\u7cbe\u786e\u6062\u590d\uff0c\u662f\u4efb\u52a1\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/","title":"\u6c47\u7f16\u57fa\u7840","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_2","title":"\u57fa\u672c\u6307\u4ee4\u683c\u5f0f","text":"<pre><code>\u64cd\u4f5c\u7801 \u76ee\u6807\u5bc4\u5b58\u5668, \u6e90\u5bc4\u5b58\u56681, \u6e90\u5bc4\u5b58\u56682/\u7acb\u5373\u6570\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#1","title":"1. \u6570\u636e\u79fb\u52a8\u6307\u4ee4","text":"<pre><code>MOV R0, R1          @ \u628aR1\u7684\u503c\u590d\u5236\u5230R0\nMOV R0, #100        @ \u628a\u6570\u5b57100\u653e\u5230R0\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#2","title":"2. \u7b97\u672f\u8fd0\u7b97\u6307\u4ee4","text":"<pre><code>ADD R0, R1, R2      @ R0 = R1 + R2\nSUB R0, R1, R2      @ R0 = R1 - R2\nMUL R0, R1, R2      @ R0 = R1 \u00d7 R2\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#3","title":"3. \u5185\u5b58\u8bbf\u95ee\u6307\u4ee4","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_3","title":"\u5bc4\u5b58\u5668\u5730\u5740\u7ed3\u6784","text":"<pre><code>[\u57fa\u5740\u5bc4\u5b58\u5668, \u504f\u79fb\u91cf]\n</code></pre> <ul> <li>\u56fa\u5b9a\u7684\u504f\u79fb\u91cf\uff08\u5355\u4f4d\uff1a4\u5b57\u8282\uff09</li> </ul> <pre><code>LDR R0, [R1,#4]     @ \u4eceR1+4\u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u52a0\u8f7d4\u4e2a\u5b57\u8282\u7684\u6570\u636e\u5230R0;Load Register\nLDRH R0, [R1,#4]     @ \u4eceR1+4\u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u7684\u52a0\u8f7d2\u4e2a\u5b57\u8282(half)\u7684\u6570\u636e\u5230R0;\nLDRD R0, [R1,#4]     @ \u4eceR1+4\u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u7684\u52a0\u8f7d8\u4e2a\u5b57\u8282\uff08double\uff09\u7684\u6570\u636e\u5230R0;\nLDRB R0, [R1,#4]     @ \u4eceR1+4\u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u7684\u52a0\u8f7d1\u4e2a\u5b57\u8282\u7684\u6570\u636e\u5230R0;\nSTR R0, [R1]        @ \u628aR0\u7684\u6570\u636e\u5b58\u50a8\u5230R1\u6307\u5411\u7684\u5185\u5b58\u5730\u5740,Store Register\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#4","title":"4. \u6bd4\u8f83\u548c\u8df3\u8f6c\u6307\u4ee4","text":"<pre><code>CMP R0, R1          @ \u6bd4\u8f83R0\u548cR1\uff0c\u8bbe\u7f6e\u6807\u5fd7\u4f4d\nB   label           @ \u65e0\u6761\u4ef6\u8df3\u8f6c\u5230label\nBL  label           @ Branch and Link,\u5148\u628a\u8fd4\u56de\u5730\u5740\u4fdd\u5b58\u5728LR\u5bc4\u5b58\u5668\u518d\u8df3\u8f6c\nBEQ label           @ \u5982\u679c\u76f8\u7b49\u5219\u8df3\u8f6c\nBNE label           @ \u5982\u679c\u4e0d\u76f8\u7b49\u5219\u8df3\u8f6c\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_4","title":"\u7b80\u5355\u6c47\u7f16\u793a\u4f8b","text":"<pre><code>@ \u8ba1\u7b97 10 + 20\nMOV R0, #10         @ R0 = 10\nMOV R1, #20         @ R1 = 20\nADD R2, R0, R1      @ R2 = R0 + R1 = 30\n</code></pre> <p>## \u66f4\u591a\u6307\u4ee4\u53ca\u8be6\u89e3  - PUSH -  - POP -  - B&amp;BX</p>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#c","title":"C\u8bed\u8a00\u4e0e\u53cd\u6c47\u7f16","text":"<p>\u5148\u4e86\u89e3</p>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#-mem_mgmt","title":"- Mem_Mgmt","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#-label","title":"- label","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#c_1","title":"\u7b80\u5355\u7684C\u7a0b\u5e8f","text":"<pre><code>// simple.c\nint add_numbers(int a, int b) {\n    int result = a + b;\n    return result;\n}\n\nint main() {\n    int x = 5;\n    int y = 3;\n    int sum = add_numbers(x, y);\n    return 0;\n}\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_5","title":"\u7f16\u8bd1\u4e3a\u6c47\u7f16","text":"<p>\u4f7f\u7528GCC\u7f16\u8bd1\u5668\u53ef\u4ee5\u67e5\u770b\u751f\u6210\u7684\u6c47\u7f16\u4ee3\u7801\uff1a</p> <pre><code>arm-none-eabi-gcc -S simple.c -o simple.s\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#c_2","title":"C\u4ee3\u7801\u4e0e\u6c47\u7f16\u4ee3\u7801\u5206\u6790","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#c_3","title":"C\u6e90\u4ee3\u7801","text":"<pre><code>int add_numbers(int a, int b) {\n    int result = a + b;\n    return result;\n}\n\nint main() {\n    int x = 5;\n    int y = 3;\n    int sum = add_numbers(x, y);\n    return 0;\n}\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#arm","title":"\u5bf9\u5e94\u7684ARM\u6c47\u7f16\u4ee3\u7801\u53ca\u5206\u6790","text":"<p>\u9700\u8981\u5148\u4e86\u89e3\uff1a</p> <ul> <li>ARM_Arch</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#add_numbers","title":"add_numbers\u51fd\u6570\u6c47\u7f16\uff08\u7b80\u5355\u7248\u672c\uff09","text":"<pre><code>add_numbers:\n    @ \u7b80\u5355\u51fd\u6570 - \u76f4\u63a5\u8ba1\u7b97\u5e76\u8fd4\u56de\n    ADD     R0, R0, R1      @ result = a + b (\u53c2\u6570a\u5728R0, b\u5728R1, \u7ed3\u679c\u76f4\u63a5\u653e\u56deR0)\n    BX      LR              @ \u8fd4\u56de\u8c03\u7528\u8005\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#add_numbers_1","title":"add_numbers\u51fd\u6570\u6c47\u7f16\uff08\u4fdd\u5b58\u5bc4\u5b58\u5668\u7248\u672c\uff09","text":"<pre><code>add_numbers:\n    @ \u51fd\u6570\u5f00\u59cb - \u4fdd\u5b58\u5bc4\u5b58\u5668\n    PUSH    {R4, R5, LR}    @ \u4fdd\u5b58\u9700\u8981\u4fdd\u62a4\u7684\u5bc4\u5b58\u5668(R4,R5)\u548c\u8fd4\u56de\u5730\u5740(LR)\u5230\u6808\u4e2d\n\n    @ \u51fd\u6570\u4f53\n    MOV     R4, R0          @ \u5c06\u53c2\u6570a\u4fdd\u5b58\u5230R4\n    MOV     R5, R1          @ \u5c06\u53c2\u6570b\u4fdd\u5b58\u5230R5\n    ADD     R0, R4, R5      @ result = a + b\uff0c\u7ed3\u679c\u653e\u5728R0\n\n    @ \u51fd\u6570\u7ed3\u675f\n    POP     {R4, R5, PC}    @ \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u7528PC\u66ff\u4ee3LR\u5b9e\u73b0\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#main","title":"main\u51fd\u6570\u6c47\u7f16","text":"<pre><code>main:\n    @ \u51fd\u6570\u5f00\u59cb - \u5206\u914d\u6808\u7a7a\u95f4\n    PUSH    {R4, R5, LR}    @ \u4fdd\u5b58\u9700\u8981\u4fdd\u62a4\u7684\u5bc4\u5b58\u5668\u548c\u8fd4\u56de\u5730\u5740\u5230\u6808\u4e2d\n                            @ \u8fd9\u91ccR5\u5176\u5b9e\u7528\u4e0d\u5230\uff0c\u8fd9\u91cc\u4ec5\u4f5c\u793a\u4f8b\u7ed9\u51fa\n    SUB     SP, SP, #12     @ \u4e3a\u5c40\u90e8\u53d8\u91cfx,y,sum\u5206\u914d\u6808\u7a7a\u95f4\n\n    @ \u521d\u59cb\u5316\u5c40\u90e8\u53d8\u91cf\n    MOV     R4, #5          @ x = 5\n    STR     R4, [SP, #8]    @ \u5c06x\u5b58\u50a8\u5230\u6808\u4e2d[SP+8]\n    MOV     R4, #3          @ y = 3\n    STR     R4, [SP, #4]    @ \u5c06y\u5b58\u50a8\u5230\u6808\u4e2d[SP+4]\n\n    @ \u51fd\u6570\u8c03\u7528\uff0c\u5c40\u90e8\u53d8\u91cf\u4f20\u5165\n    LDR     R0, [SP, #8]    @ \u51c6\u5907\u7b2c\u4e00\u4e2a\u53c2\u6570x\u5230R0\n    LDR     R1, [SP, #4]    @ \u51c6\u5907\u7b2c\u4e8c\u4e2a\u53c2\u6570y\u5230R1\n    BL      add_numbers     @ \u8c03\u7528add_numbers\u51fd\u6570\n    STR     R0, [SP, #0]    @ \u5c06\u8fd4\u56de\u503c\u5b58\u50a8\u5230sum [SP+0]\n\n    @ \u51fd\u6570\u8fd4\u56de\n    MOV     R0, #0          @ \u8bbe\u7f6e\u8fd4\u56de\u503c0\n    ADD     SP, SP, #12     @ \u91ca\u653e\u5c40\u90e8\u53d8\u91cf\u7a7a\u95f4\n    POP     {R4, R5, PC}    @ \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u8fd4\u56de\n</code></pre> <p>\u7591\u95ee\uff1a\u4e3a\u4ec0\u4e48\u9700\u8981\u501f\u52a9\u5bc4\u5b58\u5668\u6765\u8bbf\u95ee\u5185\u5b58\uff1f\u56e0\u4e3a ARM\u91c7\u7528Load-Store\u67b6\u6784\uff0c\u8fd9\u610f\u5473\u7740\uff1a - \u53ea\u6709LOAD/STORE\u6307\u4ee4\u53ef\u4ee5\u8bbf\u95ee\u5185\u5b58 - \u6240\u6709\u6570\u636e\u5904\u7406\u6307\u4ee4\u53ea\u80fd\u5728\u5bc4\u5b58\u5668\u95f4\u64cd\u4f5c - \u6ca1\u6709\"\u76f4\u63a5\u5185\u5b58\u5230\u5185\u5b58\"\u7684\u64cd\u4f5c</p>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_6","title":"\u6808\u5e27\u5e03\u5c40\u5206\u6790","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#main_1","title":"main\u51fd\u6570\u6808\u5e27","text":"<pre><code>\u9ad8\u5730\u5740 \u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2190 \u65e7SP\n         \u2502    \u65e7LR     \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502     R5      \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502     R4      \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2190 \u5f53\u524dSP (\u51fd\u6570\u5f00\u59cb\u540e)\n         \u2502    x (5)    \u2502 [SP+8]\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502    y (3)    \u2502 [SP+4]\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502   sum(8)    \u2502 [SP+0]\n\u4f4e\u5730\u5740 \u2192 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2190 \u65b0SP (SP-12\u540e)\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_7","title":"\u5173\u952e\u70b9\u5206\u6790","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#1_1","title":"1. \u53c2\u6570\u4f20\u9012\u89c4\u5219","text":"<ul> <li>\u524d4\u4e2a\u53c2\u6570\uff1a\u901a\u8fc7R0-R3\u4f20\u9012</li> <li>\u8fd4\u56de\u503c\uff1a\u901a\u8fc7R0\u8fd4\u56de</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#2_1","title":"2. \u5bc4\u5b58\u5668\u4f7f\u7528\u7ea6\u5b9a","text":"<ul> <li>R0-R3\uff1a\u8c03\u7528\u8005\u4fdd\u5b58\uff0c\u53ef\u7528\u4e8e\u53c2\u6570\u4f20\u9012</li> <li>R4-R11\uff1a\u88ab\u8c03\u7528\u8005\u4fdd\u5b58\uff0c\u4f7f\u7528\u524d\u5fc5\u987b\u538b\u6808\u4fdd\u5b58</li> <li>LR (R14)\uff1a\u94fe\u63a5\u5bc4\u5b58\u5668\uff0c\u5b58\u50a8\u8fd4\u56de\u5730\u5740</li> <li>SP (R13)\uff1a\u6808\u6307\u9488</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#3_1","title":"3. \u51fd\u6570\u8c03\u7528\u8fc7\u7a0b","text":"<ol> <li>\u53c2\u6570\u51c6\u5907\uff1a\u5c06\u53c2\u6570\u653e\u5165R0-R3</li> <li>BL\u6307\u4ee4\uff1a\u8df3\u8f6c\u5230\u51fd\u6570\uff0c\u540c\u65f6\u5c06\u8fd4\u56de\u5730\u5740\u4fdd\u5b58\u5230LR</li> <li>\u51fd\u6570\u6267\u884c\uff1a\u88ab\u8c03\u7528\u51fd\u6570\u5de5\u4f5c</li> <li>BX LR\uff1a\u8fd4\u56de\u5230\u8c03\u7528\u70b9</li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#4_1","title":"4. \u6808\u5e73\u8861","text":"<ul> <li>PUSH/POP \u5fc5\u987b\u6210\u5bf9\u51fa\u73b0</li> <li>SUB/ADD SP \u5fc5\u987b\u6210\u5bf9\u51fa\u73b0</li> <li>\u786e\u4fdd\u51fd\u6570\u8fd4\u56de\u65f6SP\u6062\u590d\u5230\u539f\u59cb\u503c</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_8","title":"\u53cd\u6c47\u7f16\u4ee3\u7801\u7ed3\u6784\u89e3\u6790","text":"<p>\u53cd\u6c47\u7f16\u4ee3\u7801\u901a\u5e38\u663e\u793a\u4e3a\u4ee5\u4e0b\u683c\u5f0f\uff1a</p> <pre><code>\u5185\u5b58\u5730\u5740    \u673a\u5668\u7801    ASCII\u663e\u793a    \u6307\u4ee4    \u64cd\u4f5c\u6570\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_9","title":"\u5177\u4f53\u793a\u4f8b\u5206\u6790","text":"<pre><code>0x08002F34  B503    ..    PUSH    {R0, R1, LR}\n</code></pre> <p>\u9010\u6bb5\u89e3\u91ca\uff1a</p> \u90e8\u5206 \u5185\u5bb9 \u542b\u4e49 <code>0x08002F34</code> \u5185\u5b58\u5730\u5740 \u8fd9\u6761\u6307\u4ee4\u5728\u5185\u5b58\u4e2d\u7684\u4f4d\u7f6e <code>B503</code> \u673a\u5668\u7801 CPU\u5b9e\u9645\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\uff0816\u8fdb\u5236\u663e\u793a\uff09 <code>..</code> ASCII\u663e\u793a \u673a\u5668\u7801\u5bf9\u5e94\u7684ASCII\u5b57\u7b26\uff08\u4e0d\u53ef\u6253\u5370\u5b57\u7b26\u663e\u793a\u4e3a\u70b9\uff09 <code>PUSH</code> \u6307\u4ee4\u52a9\u8bb0\u7b26 \u4eba\u7c7b\u53ef\u8bfb\u7684\u6307\u4ee4\u540d\u79f0 <code>{R0, R1, LR}</code> \u64cd\u4f5c\u6570 \u6307\u4ee4\u8981\u64cd\u4f5c\u7684\u6570\u636e\u6216\u5bc4\u5b58\u5668"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_10","title":"\u5404\u90e8\u5206\u8be6\u7ec6\u8bf4\u660e","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#1_2","title":"1. \u5185\u5b58\u5730\u5740","text":"<ul> <li>\u6307\u4ee4\u5728\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u4f4d\u7f6e</li> <li>16\u8fdb\u5236\u683c\u5f0f\uff0c\u5982 <code>0x08002F34</code></li> <li>\u4ee3\u7801\u6267\u884c\u65f6\uff0cPC\uff08\u7a0b\u5e8f\u8ba1\u6570\u5668\uff09\u5c31\u6307\u5411\u8fd9\u4e9b\u5730\u5740</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#2_2","title":"2. \u673a\u5668\u7801","text":"<ul> <li>CPU\u771f\u6b63\u7406\u89e3\u548c\u6267\u884c\u7684\u4e8c\u8fdb\u5236\u6307\u4ee4</li> <li>\u4ee516\u8fdb\u5236\u663e\u793a\uff0c\u5982 <code>B503</code></li> <li>ARM\u67b6\u6784\u4e2d\u901a\u5e38\u662f2\u5b57\u8282\u62164\u5b57\u8282</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#3-ascii","title":"3. ASCII\u663e\u793a\u5217","text":"<ul> <li>\u7528\u9014\uff1a\u5c06\u673a\u5668\u7801\u5b57\u8282\u5f53\u4f5cASCII\u5b57\u7b26\u663e\u793a</li> <li>\u89c4\u5219\uff1a\u53ef\u6253\u5370\u5b57\u7b26\u663e\u793a\u539f\u5b57\u7b26\uff0c\u4e0d\u53ef\u6253\u5370\u5b57\u7b26\u663e\u793a\u4e3a <code>.</code></li> </ul> <p>\u5e38\u89c1\u60c5\u51b5\uff1a</p> <pre><code>0x08002F36  4408    .D    ADD     R0, R1, R0    ; 44='D', 08=\u4e0d\u53ef\u6253\u5370\u2192'.'\n0x08002F38  4142    AB    MOV     R2, R1        ; 41='A', 42='B'\u2192\"AB\"\n0x08002F3C  0000    ..    MOVS    R0, R0        ; 00,00\u90fd\u4e0d\u53ef\u6253\u5370\u2192\"..\"\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#4_2","title":"4. \u6307\u4ee4\u548c\u64cd\u4f5c\u6570","text":"<ul> <li>\u6307\u4ee4\uff1a\u4eba\u7c7b\u53ef\u8bfb\u7684\u64cd\u4f5c\u540d\u79f0\uff0c\u5982 <code>ADD</code>, <code>PUSH</code>, <code>LDR</code></li> <li>\u64cd\u4f5c\u6570\uff1a\u6307\u4ee4\u64cd\u4f5c\u7684\u5bf9\u8c61\uff0c\u5982\u5bc4\u5b58\u5668\u3001\u5185\u5b58\u5730\u5740\u3001\u7acb\u5373\u6570</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_11","title":"\u66f4\u591a\u5b8c\u6574\u793a\u4f8b","text":"<pre><code>; \u793a\u4f8b1\uff1a\u51fd\u6570\u5f00\u573a\n0x08002F00  B570    p.    PUSH    {R4-R6,LR}    ; \u4fdd\u5b58\u5bc4\u5b58\u5668\u5230\u6808\n; \u2191\u5730\u5740      \u2191\u673a\u5668\u7801 \u2191ASCII \u2191\u6307\u4ee4    \u2191\u64cd\u4f5c\u6570\n\n; \u793a\u4f8b2\uff1a\u7b97\u672f\u8fd0\u7b97\n0x08002F36  4408    .D    ADD     R0, R1, R0    ; R0 = R1 + R0\n; 44='D', 08=\u4e0d\u53ef\u6253\u5370\u2192\".D\"\n\n; \u793a\u4f8b3\uff1a\u5185\u5b58\u8bbf\u95ee  \n0x08002F38  6801    .h    LDR     R1, [R0, #0]  ; \u4eceR0\u5730\u5740\u52a0\u8f7d\u6570\u636e\u5230R1\n; 68='h', 01=\u4e0d\u53ef\u6253\u5370\u2192\".h\"\n\n; \u793a\u4f8b4\uff1a\u6761\u4ef6\u8df3\u8f6c\n0x08002F3A  D101    ..    BNE     0x08002F40    ; \u5982\u679c\u4e0d\u76f8\u7b49\u5219\u8df3\u8f6c\n; D1,01\u90fd\u4e0d\u53ef\u6253\u5370\u2192\"..\"\n\n; \u793a\u4f8b5\uff1a\u957f\u6307\u4ee4\uff084\u5b57\u8282\uff09\n0x08002F3C  F8DFE004 ....  LDR     LR, [PC, #4]   ; \u957f\u6307\u4ee4\uff0c4\u5b57\u8282\u673a\u5668\u7801\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#ascii","title":"ASCII\u663e\u793a\u5217\u7684\u5b9e\u7528\u4ef7\u503c","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#1_3","title":"1. \u8bc6\u522b\u533a\u57df\u7c7b\u578b","text":"<ul> <li>\u4ee3\u7801\u533a\u57df\uff1a\u591a\u4e3a <code>..</code> \u6216\u89c4\u5f8b\u7684\u6a21\u5f0f</li> <li>\u6570\u636e\u533a\u57df\uff1a\u53ef\u80fd\u51fa\u73b0\u53ef\u8bfb\u5b57\u7b26\u4e32</li> <li>\u6df7\u5408\u533a\u57df\uff1a\u6307\u4ee4\u4e2d\u5076\u7136\u51fa\u73b0\u7684\u53ef\u6253\u5370\u5b57\u7b26</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#2_3","title":"2. \u5b9e\u9645\u5e94\u7528\u793a\u4f8b","text":"<pre><code>; \u4ee3\u7801\u6bb5 - \u591a\u4e3a\"..\"\n0x08002F40  B401    ..    PUSH    {R0}\n0x08002F42  4602    .F    MOV     R2, R0\n0x08002F44  E002    ..    B       0x08002F4C\n\n; \u6570\u636e\u6bb5 - \u53ef\u80fd\u5305\u542b\u6587\u672c\n0x08002F50  6C6C6548  Hell    ; \u5b57\u7b26\u4e32\"Hello\"\u7684\u5f00\u59cb\n0x08002F54  006F6F72  roo.    ; \u53ef\u80fd\u7684\u6570\u636e\u533a\u57df\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_12","title":"\u5982\u4f55\u9605\u8bfb\u53cd\u6c47\u7f16","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#1_4","title":"1. \u5173\u6ce8\u6307\u4ee4\u6d41","text":"<pre><code>0x08002F34  B503      PUSH    {R0,R1,LR}    ; \u4fdd\u5b58\u5bc4\u5b58\u5668\u5230\u6808\n0x08002F36  4408      ADD     R0, R1, R0    ; R0 = R1 + R0  \n0x08002F38  6801      LDR     R1, [R0, #0]  ; \u4ece\u5185\u5b58\u52a0\u8f7d\u6570\u636e\n0x08002F3A  D101      BNE     0x08002F40    ; \u6761\u4ef6\u8df3\u8f6c\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#2_4","title":"2. \u7406\u89e3\u6570\u636e\u6d41\u5411","text":"<ul> <li> <p>\u5bc4\u5b58\u5668\u95f4\uff1a<code>ADD R0, R1, R2</code>\u00a0\u2192 \u6570\u636e\u4eceR1,R2\u6d41\u5411R0</p> </li> <li> <p>\u5185\u5b58\u5230\u5bc4\u5b58\u5668\uff1a<code>LDR R0, [R1]</code>\u00a0\u2192 \u6570\u636e\u4ece\u5185\u5b58\u6d41\u5411\u5bc4\u5b58\u5668</p> </li> <li> <p>\u5bc4\u5b58\u5668\u5230\u5185\u5b58\uff1a<code>STR R0, [R1]</code>\u00a0\u2192 \u6570\u636e\u4ece\u5bc4\u5b58\u5668\u6d41\u5411\u5185\u5b58</p> </li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#3_2","title":"3. \u8bc6\u522b\u51fd\u6570\u7ed3\u6784","text":"<pre><code>; \u51fd\u6570\u5f00\u59cb\n0x08002F00  B570      PUSH    {R4-R6,LR}    ; \u4fdd\u5b58\u73b0\u573a\n0x08002F02  4604      MOV     R4, R0        ; \u5904\u7406\u53c2\u6570\n; ... \u51fd\u6570\u4f53 ...\n0x08002F10  BD70      POP     {R4-R6,PC}    ; \u6062\u590d\u73b0\u573a\u5e76\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/Assembly_Intro/#_13","title":"\u5feb\u901f\u8bc6\u522b\u5e38\u89c1\u6a21\u5f0f","text":"\u6a21\u5f0f \u793a\u4f8b \u542b\u4e49 \u51fd\u6570\u5f00\u5934 <code>PUSH {..., LR}</code> \u4fdd\u5b58\u8fd4\u56de\u5730\u5740\uff0c\u5f00\u59cb\u65b0\u51fd\u6570 \u51fd\u6570\u7ed3\u5c3e <code>POP {..., PC}</code> \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u8fd4\u56de \u51fd\u6570\u8c03\u7528 <code>BL 0x0800xxxx</code> \u8c03\u7528\u5b50\u51fd\u6570 \u6761\u4ef6\u5224\u65ad <code>CMP R0, #0</code>\u00a0+\u00a0<code>BEQ/BNE</code> if-else\u903b\u8f91 \u5faa\u73af \u6807\u7b7e + \u6bd4\u8f83 + \u6761\u4ef6\u8df3\u8f6c for/while\u5faa\u73af ## \u5feb\u901f\u9605\u8bfb\u6280\u5de7 <ol> <li>\u91cd\u70b9\u770b\u6307\u4ee4\u5217\uff1a\u7406\u89e3\u7a0b\u5e8f\u903b\u8f91\u7684\u4e3b\u8981\u4f9d\u636e</li> <li>ASCII\u5217\u4e3a\u8f85\u52a9\uff1a\u5e2e\u52a9\u8bc6\u522b\u533a\u57df\u7c7b\u578b\u548c\u6570\u636e\u5185\u5bb9  </li> <li>\u5173\u6ce8\u6570\u636e\u6d41\uff1a\u89c2\u5bdf\u5bc4\u5b58\u5668\u95f4\u7684\u6570\u636e\u4f20\u9012</li> <li>\u8bc6\u522b\u6a21\u5f0f\uff1a\u51fd\u6570\u8c03\u7528\u3001\u5faa\u73af\u3001\u6761\u4ef6\u5224\u65ad\u7684\u56fa\u5b9a\u6a21\u5f0f</li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/","title":"B \u6307\u4ee4","text":""},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_1","title":"\u57fa\u672c\u6982\u5ff5","text":"<p>B\u6307\u4ee4\u7528\u4e8e\u65e0\u6761\u4ef6\u8df3\u8f6c\u5230\u6307\u5b9a\u7684\u6807\u7b7e\u6216\u5730\u5740\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_2","title":"\u8bed\u6cd5\u683c\u5f0f","text":"<pre><code>B   \u76ee\u6807\u5730\u5740/\u6807\u7b7e\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_3","title":"\u793a\u4f8b","text":"<pre><code>B   loop_start      @ \u65e0\u6761\u4ef6\u8df3\u8f6c\u5230loop_start\u6807\u7b7e\nB   0x08001000      @ \u65e0\u6761\u4ef6\u8df3\u8f6c\u5230\u5730\u57400x08001000\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_4","title":"\u7279\u70b9","text":"<ul> <li>\u4e0d\u4fdd\u5b58\u8fd4\u56de\u5730\u5740</li> <li>\u7528\u4e8e\u5faa\u73af\u3001\u6761\u4ef6\u5206\u652f\u7b49</li> <li>\u8df3\u8f6c\u8303\u56f4\u6709\u9650\u5236\uff08\u76f8\u5bf9\u504f\u79fb\uff09</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#bx","title":"BX \u6307\u4ee4\uff08\u5e26\u72b6\u6001\u5207\u6362\u7684\u8df3\u8f6c\uff09","text":""},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_5","title":"\u57fa\u672c\u6982\u5ff5","text":"<p>BX\u6307\u4ee4\u7528\u4e8e\u8df3\u8f6c\u5230\u5bc4\u5b58\u5668\u6307\u5b9a\u7684\u5730\u5740\uff0c\u5e76\u53ef\u5207\u6362ARM/Thumb\u72b6\u6001\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_6","title":"\u8bed\u6cd5\u683c\u5f0f","text":"<pre><code>BX  \u76ee\u6807\u5bc4\u5b58\u5668\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_7","title":"\u793a\u4f8b","text":"<pre><code>BX  LR              @ \u8df3\u8f6c\u5230LR\u5bc4\u5b58\u5668\u5730\u5740\uff0c\u7528\u4e8e\u51fd\u6570\u8fd4\u56de\nBX  R0              @ \u8df3\u8f6c\u5230R0\u5bc4\u5b58\u5668\u6307\u5b9a\u7684\u5730\u5740\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_8","title":"\u7279\u70b9","text":"<ul> <li>\u6839\u636e\u76ee\u6807\u5730\u5740\u6700\u4f4e\u4f4d\u81ea\u52a8\u5207\u6362\u72b6\u6001\uff0c\u4e0d\u540c\u72b6\u6001\u6027\u80fd\u5f00\u9500\u4e0d\u4e00\u6837</li> <li>\u6700\u4f4e\u4f4d=0\uff1a\u5207\u6362\u5230ARM\uff0832\u4f4d\u6307\u4ee4\uff09\u72b6\u6001</li> <li>\u6700\u4f4e\u4f4d=1\uff1a\u5207\u6362\u5230Thumb\uff0816\u4f4d\u6307\u4ee4\uff09\u72b6\u6001</li> <li>\u5e38\u7528\u4e8e\u51fd\u6570\u8fd4\u56de</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_9","title":"\u5173\u952e\u533a\u522b","text":"\u7279\u6027 B \u6307\u4ee4 BX \u6307\u4ee4 \u64cd\u4f5c\u6570 \u7acb\u5373\u6570/\u6807\u7b7e \u5bc4\u5b58\u5668 \u72b6\u6001\u5207\u6362 \u65e0 \u6839\u636e\u5730\u5740\u6700\u4f4e\u4f4d\u81ea\u52a8\u5207\u6362 \u4e3b\u8981\u7528\u9014 \u5faa\u73af\u3001\u6761\u4ef6\u8df3\u8f6c \u51fd\u6570\u8fd4\u56de\u3001\u52a8\u6001\u8df3\u8f6c \u8fd4\u56de\u5730\u5740 \u4e0d\u4fdd\u5b58 \u901a\u5e38\u4e0eBL\u914d\u5408\u4f7f\u7528"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_10","title":"\u5b9e\u9645\u5e94\u7528\u573a\u666f","text":""},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#b-","title":"B \u6307\u4ee4 - \u5faa\u73af\u548c\u6761\u4ef6\u5224\u65ad","text":"<pre><code>loop:\n    @ \u5faa\u73af\u4f53\u4ee3\u7801\n    SUBS R0, R0, #1      @ \u8ba1\u6570\u5668\u51cf1\u5e76\u8bbe\u7f6e\u6807\u5fd7\n    BNE  loop            @ \u5982\u679c\u4e0d\u4e3a0\u5219\u7ee7\u7eed\u5faa\u73af\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#bx-","title":"BX \u6307\u4ee4 - \u51fd\u6570\u8fd4\u56de","text":"<pre><code>my_function:\n    PUSH {R4, LR}        @ \u4fdd\u5b58\u5bc4\u5b58\u5668\n    @ \u51fd\u6570\u4f53\u4ee3\u7801...\n    POP {R4, LR}         @ \u6062\u590d\u5bc4\u5b58\u5668\n    BX  LR               @ \u8fd4\u56de\u5230\u8c03\u7528\u8005\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#bl-bx-","title":"BL + BX \u7ec4\u5408 - \u5b8c\u6574\u7684\u51fd\u6570\u8c03\u7528","text":"<pre><code>main:\n    BL   my_function     @ \u8c03\u7528\u51fd\u6570\uff0cLR\u81ea\u52a8\u4fdd\u5b58\u8fd4\u56de\u5730\u5740\n    @ ... \u7ee7\u7eed\u6267\u884c ...\n\nmy_function:\n    @ \u51fd\u6570\u4f53...\n    BX   LR              @ \u8fd4\u56de\u5230main\u51fd\u6570\u7684\u8c03\u7528\u70b9\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_11","title":"\u72b6\u6001\u5207\u6362\u673a\u5236","text":""},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#thumb","title":"Thumb\u72b6\u6001\u8bc6\u522b","text":"<pre><code>@ \u5047\u8bbe\u8981\u8df3\u8f6c\u5230Thumb\u4ee3\u7801\uff08\u5730\u5740\u6700\u4f4e\u4f4d=1\uff09\nLDR  R0, =thumb_code+1   @ \u5730\u5740\u52a01\u8868\u793aThumb\u72b6\u6001\nBX   R0                  @ \u8df3\u8f6c\u5e76\u5207\u6362\u5230Thumb\u72b6\u6001\n\nthumb_code:\n    .thumb               @ Thumb\u6307\u4ee4\u96c6\u4ee3\u7801\n    @ ...\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/B%26BX/#_12","title":"\u91cd\u8981\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>BX LR \u662f\u51fd\u6570\u8fd4\u56de\u7684\u6807\u51c6\u65b9\u5f0f</li> <li>B \u6307\u4ee4\u7528\u4e8e\u76f8\u5bf9\u8df3\u8f6c\uff0c\u8303\u56f4\u53d7\u9650</li> <li>BX \u53ef\u4ee5\u8df3\u8f6c\u5230\u4efb\u610f\u5bc4\u5b58\u5668\u6307\u5b9a\u7684\u5730\u5740</li> <li>\u72b6\u6001\u5207\u6362\u7531\u786c\u4ef6\u81ea\u52a8\u5904\u7406\uff0c\u5bf9\u7a0b\u5e8f\u5458\u900f\u660e</li> </ol> <p>\u8fd9\u4e24\u79cd\u8df3\u8f6c\u6307\u4ee4\u5171\u540c\u6784\u6210\u4e86ARM\u67b6\u6784\u4e2d\u6d41\u7a0b\u63a7\u5236\u7684\u57fa\u7840\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/Mem_Mgmt/","title":"\u5185\u5b58\u5e03\u5c40","text":"<p>\u4e00\u4e2a\u5178\u578b\u7684C\u7a0b\u5e8f\u5728\u5185\u5b58\u4e2d\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u533a\u57df\uff1a</p> <ol> <li> <p>\u4ee3\u7801\u533a\uff08Text Segment\uff09</p> <ul> <li>\u5b58\u653e\u7a0b\u5e8f\u7684\u673a\u5668\u6307\u4ee4\uff0c\u662f\u53ea\u8bfb\u7684\u3002</li> </ul> </li> <li> <p>\u9759\u6001\u5b58\u50a8\u533a\uff08Static Storage\uff09</p> <ul> <li> <p>\u6570\u636e\u6bb5\uff08Data Segment\uff09\uff1a\u5b58\u653e\u5df2\u521d\u59cb\u5316\u7684\u5168\u5c40\u53d8\u91cf\u548c\u9759\u6001\u5c40\u90e8\u53d8\u91cf\uff08\u7528<code>static</code>\u5173\u952e\u5b57\u4fee\u9970\uff09\u3002</p> </li> <li> <p>BSS\u6bb5\uff08BSS Segment\uff09\uff1a\u5b58\u653e\u672a\u521d\u59cb\u5316\u7684\u5168\u5c40\u53d8\u91cf\u548c\u9759\u6001\u5c40\u90e8\u53d8\u91cf\u3002\u5728\u7a0b\u5e8f\u5f00\u59cb\u6267\u884c\u524d\uff0c\u7cfb\u7edf\u4f1a\u81ea\u52a8\u5c06\u8fd9\u4e2a\u533a\u57df\u7684\u6570\u636e\u521d\u59cb\u5316\u4e3a0\u6216NULL\u3002</p> </li> </ul> </li> <li> <p>\u5806\u533a\uff08Heap\uff09</p> <ul> <li>\u7528\u4e8e\u52a8\u6001\u5185\u5b58\u5206\u914d\uff0c\u6bd4\u5982<code>malloc</code>,\u00a0<code>calloc</code>,\u00a0<code>realloc</code>\u7533\u8bf7\u7684\u5185\u5b58\u5c31\u5728\u5806\u4e0a\u3002</li> </ul> </li> <li> <p>\u6808\u533a\uff08Stack\uff09</p> <ul> <li>\u7528\u4e8e\u5b58\u653e\u51fd\u6570\u7684\u5c40\u90e8\u53d8\u91cf\u3001\u51fd\u6570\u53c2\u6570\u3001\u8fd4\u56de\u5730\u5740\u7b49\u3002\u5b83\u7684\u7ba1\u7406\u662f\u81ea\u52a8\u7684\uff0c\u7531\u7f16\u8bd1\u5668\u5b8c\u6210\u3002</li> </ul> </li> <li> <p>heap</p> </li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/Mem_Mgmt/#_2","title":"\u5806\u7ba1\u7406","text":""},{"location":"EmbeddedSoft/Assembly_Intro/Mem_Mgmt/#_3","title":"\u6808\u7ba1\u7406","text":"<ul> <li>stack</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/","title":"POP\u57fa\u672c\u6982\u5ff5","text":"<p>POP\u6307\u4ee4\u7528\u4e8e\u4ece\u6808\u4e2d\u6062\u590d\u6570\u636e\u5230\u5bc4\u5b58\u5668\uff0c\u662fPUSH\u7684\u9006\u64cd\u4f5c\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_1","title":"\u8bed\u6cd5\u683c\u5f0f","text":"<pre><code>POP {\u5bc4\u5b58\u5668\u5217\u8868}\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_2","title":"\u5de5\u4f5c\u539f\u7406","text":""},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_3","title":"\u6267\u884c\u8fc7\u7a0b","text":"<pre><code>POP {R0, R1, PC}\n</code></pre> <p>\u6267\u884c\u6b65\u9aa4\uff1a 1. \u4ece\u5f53\u524dSP\u5730\u5740\u52a0\u8f7d\u6570\u636e\u5230\u7b2c\u4e00\u4e2a\u5bc4\u5b58\u5668 2. SP = SP + 4 3. \u4ece\u65b0SP\u5730\u5740\u52a0\u8f7d\u6570\u636e\u5230\u4e0b\u4e00\u4e2a\u5bc4\u5b58\u5668 4. SP = SP + 4 5. \u91cd\u590d\u76f4\u5230\u6240\u6709\u5bc4\u5b58\u5668\u6062\u590d</p>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_4","title":"\u5177\u4f53\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_5","title":"\u57fa\u7840\u7528\u6cd5","text":"<pre><code>0x08002F44  BD03    ..    POP     {R0, R1, PC}\n</code></pre> <p>\u6267\u884c\u524d\uff1a</p> <pre><code>SP = 0x20000FF4\n\u6808\u5185\u5bb9\uff1a\n0x20000FF4: R0\u7684\u539f\u59cb\u503c\n0x20000FF8: R1\u7684\u539f\u59cb\u503c  \n0x20000FFC: LR\u7684\u539f\u59cb\u503c\n</code></pre> <p>\u6267\u884c\u540e\uff1a</p> <pre><code>SP = 0x20001000  (SP\u589e\u52a012\u5b57\u8282)\nR0, R1\u6062\u590d\u539f\u503c\nPC = LR\u7684\u539f\u59cb\u503c\uff08\u5b9e\u73b0\u51fd\u6570\u8fd4\u56de\uff09\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_6","title":"\u51fd\u6570\u8fd4\u56de\u5178\u578b\u7528\u6cd5","text":"<pre><code>POP {R4, R5, PC}    ; \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u7528PC\u66ff\u4ee3LR\u5b9e\u73b0\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#push","title":"\u4e0ePUSH\u7684\u914d\u5bf9\u4f7f\u7528","text":""},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_7","title":"\u6b63\u786e\u7684\u914d\u5bf9","text":"<pre><code>; \u51fd\u6570\u5f00\u59cb\nPUSH {R4, R5, LR}   ; \u4fdd\u5b58\u5bc4\u5b58\u5668\n\n; \u51fd\u6570\u4f53...\n\n; \u51fd\u6570\u7ed3\u675f  \nPOP {R4, R5, PC}    ; \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_8","title":"\u6808\u5e73\u8861","text":"<ul> <li>PUSH\u548cPOP\u5fc5\u987b\u6210\u5bf9\u51fa\u73b0</li> <li>\u64cd\u4f5c\u5bc4\u5b58\u5668\u6570\u91cf\u5fc5\u987b\u5339\u914d</li> <li>\u5426\u5219\u4f1a\u5bfc\u81f4\u6808\u6307\u9488\u9519\u4e71</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_9","title":"\u7279\u6b8a\u7528\u6cd5","text":""},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_10","title":"\u51fd\u6570\u8fd4\u56de\u6280\u5de7","text":"<pre><code>POP {PC}            ; \u76f4\u63a5\u8fd4\u56de\u5230\u8c03\u7528\u8005\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_11","title":"\u6062\u590d\u90e8\u5206\u5bc4\u5b58\u5668","text":"<pre><code>POP {R4-R6}         ; \u53ea\u6062\u590d\u90e8\u5206\u5bc4\u5b58\u5668\nADD SP, SP, #4      ; \u624b\u52a8\u8c03\u6574SP\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/POP/#_12","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u6808\u5e73\u8861\uff1a\u5fc5\u987b\u4e0ePUSH\u914d\u5bf9\u4f7f\u7528</li> <li>\u5bc4\u5b58\u5668\u987a\u5e8f\uff1a\u6c47\u7f16\u5668\u4f1a\u81ea\u52a8\u6392\u5e8f</li> <li>\u8fd4\u56de\u5730\u5740\uff1a\u901a\u5e38\u7528PC\u66ff\u4ee3LR\u5b9e\u73b0\u8fd4\u56de</li> <li>\u6808\u5bf9\u9f50\uff1a\u5fc5\u987b\u4fdd\u6301\u6808\u6307\u9488\u5bf9\u9f50</li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/","title":"PUSH","text":""},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#push","title":"\u4ec0\u4e48\u662f PUSH\uff1f","text":"<p>PUSH \u6307\u4ee4\u7528\u4e8e\u5c06\u6570\u636e\u4fdd\u5b58\u5230\u6808\u5185\u5b58\u4e2d\u3002\u6808\u662f\u4e00\u79cd\"\u540e\u8fdb\u5148\u51fa\"\uff08LIFO\uff09\u7684\u5185\u5b58\u533a\u57df\uff0c\u4e3b\u8981\u7528\u4e8e\u51fd\u6570\u8c03\u7528\u65f6\u4fdd\u5b58\u73b0\u573a\uff0c\u540c\u65f6\u4e5f\u53ef\u80fd\u662f\u8981\u5f00\u8f9f\u7a7a\u95f4\u7ed9\u540e\u9762\u7528\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_1","title":"\u4f5c\u7528","text":"<ul> <li>\u4fdd\u5b58\u4e0a\u5c42\u51fd\u6570\u73b0\u573a\uff0c\u672c\u5c42\u51fd\u6570\u6267\u884c\u5b8c\u540e\u6062\u590d</li> <li>\u5f00\u8f9f\u7a7a\u95f4\u7ed9\u540e\u9762\u7528</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_2","title":"\u57fa\u672c\u8bed\u6cd5","text":"<pre><code>PUSH {\u5bc4\u5b58\u5668\u5217\u8868}\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_3","title":"\u5de5\u4f5c\u539f\u7406","text":""},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_4","title":"\u6808\u7684\u751f\u957f\u65b9\u5411","text":"<p>\u5728ARM\u67b6\u6784\u4e2d\uff0c\u6808\u901a\u5e38\u662f\u5411\u4e0b\u751f\u957f\u7684\uff1a - \u6808\u6307\u9488\uff08SP\uff09 \u6307\u5411\u6808\u9876 - PUSH\u65f6\uff0cSP\u51cf\u5c0f\uff0c\u6570\u636e\u5b58\u5165\u6808\u4e2d - POP\u65f6\uff0cSP\u589e\u52a0\uff0c\u6570\u636e\u4ece\u6808\u4e2d\u6062\u590d</p>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_5","title":"\u6267\u884c\u8fc7\u7a0b","text":"<pre><code>PUSH {R0, R1, LR}\n</code></pre> <p>\u5b9e\u9645\u6267\u884c\u6b65\u9aa4\uff1a         1. SP = SP - 12\uff08\u4e3a3\u4e2a\u5bc4\u5b58\u5668\u5206\u914d\u7a7a\u95f4\uff0c\u6bcf\u4e2a4\u5b57\u8282\uff0c\u56e0\u4e3aARM\u67b6\u6784\u4e3a32\u4f4d\uff09 1. \u5c06LR\u5b58\u5165 <code>[SP+8]</code> 2. \u5c06R1\u5b58\u5165 <code>[SP+4]</code>  3. \u5c06R0\u5b58\u5165 <code>[SP+0]</code></p>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_6","title":"\u793a\u4f8b\u5206\u6790","text":""},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#1","title":"\u793a\u4f8b1\uff1a\u57fa\u672c\u7528\u6cd5","text":"<pre><code>0x08002F34  B503    ..    PUSH    {R0, R1, LR}\n</code></pre> <p>\u6267\u884c\u524d\uff1a</p> <pre><code>SP = 0x20001000\n\u6808\u5185\u5b58\uff1a\u672a\u77e5\n</code></pre> <p>\u6267\u884c\u540e\uff1a</p> <pre><code>SP = 0x20000FF4  (SP\u51cf\u5c11\u4e8612\u5b57\u8282)\n\u6808\u5185\u5b58\uff1a\n0x20000FF4: R0\u7684\u503c\n0x20000FF8: R1\u7684\u503c  \n0x20000FFC: LR\u7684\u503c\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#2","title":"\u793a\u4f8b2\uff1a\u51fd\u6570\u5f00\u5934\u7684\u5178\u578b\u7528\u6cd5","text":"<pre><code>0x08002F00  B570    p.    PUSH    {R4-R6, LR}\n</code></pre> <ul> <li>\u4fdd\u5b58R4\u3001R5\u3001R6\u548cLR\u56db\u4e2a\u5bc4\u5b58\u5668</li> <li>\u4e3a\u6bcf\u4e2a\u5bc4\u5b58\u5668\u5206\u914d4\u5b57\u8282\uff0c\u517116\u5b57\u8282\u6808\u7a7a\u95f4</li> <li>SP\u51cf\u5c1116</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#3","title":"\u793a\u4f8b3\uff1a\u4fdd\u5b58\u591a\u4e2a\u5bc4\u5b58\u5668","text":"<pre><code>0x08002F10  E92D4FF0  -O..    PUSH    {R4-R11, LR}\n</code></pre> <ul> <li>\u4fdd\u5b58R4\u5230R11\u51718\u4e2a\u5bc4\u5b58\u5668\uff0c\u52a0\u4e0aLR\u51719\u4e2a</li> <li>\u5206\u914d36\u5b57\u8282\u6808\u7a7a\u95f4\uff089\u00d74\uff09</li> <li>SP\u51cf\u5c1136</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#push_1","title":"PUSH \u7684\u4f5c\u7528","text":""},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#1_1","title":"1. \u51fd\u6570\u8c03\u7528\u4fdd\u5b58\u73b0\u573a","text":"<pre><code>my_function:\n    PUSH    {R4-R6, LR}    ; \u4fdd\u5b58\u8981\u7528\u5230\u7684\u5bc4\u5b58\u5668\u548c\u8fd4\u56de\u5730\u5740\n    ; ... \u51fd\u6570\u4f53 ...\n    POP     {R4-R6, PC}    ; \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#2_1","title":"2. \u4e34\u65f6\u4fdd\u5b58\u5bc4\u5b58\u5668\u503c","text":"<p>\u5f53\u5bc4\u5b58\u5668\u4e0d\u591f\u7528\u65f6\uff0c\u628a\u4e0d\u5e38\u7528\u7684\u5bc4\u5b58\u5668\u6682\u65f6\u4fdd\u5b58\u5230\u6808\u4e0a\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#3_1","title":"3. \u4e2d\u65ad\u5904\u7406","text":"<p>\u53d1\u751f\u4e2d\u65ad\u65f6\uff0c\u81ea\u52a8\u4fdd\u5b58\u5173\u952e\u5bc4\u5b58\u5668\u5230\u6808\u4e2d\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#pop","title":"\u4e0e POP \u6307\u4ee4\u914d\u5bf9\u4f7f\u7528","text":"<p>PUSH \u548c POP \u5fc5\u987b\u6210\u5bf9\u4f7f\u7528\uff0c\u786e\u4fdd\u6808\u5e73\u8861\uff1a</p> <pre><code>; \u6b63\u786e\u7684\u7528\u6cd5\nPUSH    {R0, R1, LR}    ; \u8fdb\u5165\u65f6\u4fdd\u5b58\n; ... \u4e00\u4e9b\u64cd\u4f5c ...\nPOP     {R0, R1, PC}    ; \u9000\u51fa\u65f6\u6062\u590d\uff0c\u7528PC\u66ff\u4ee3LR\u5b9e\u73b0\u8fd4\u56de\n\n; \u9519\u8bef\u7684\u7528\u6cd5\u4f1a\u5bfc\u81f4\u6808\u5d29\u6e83\uff01\nPUSH    {R0, R1, LR}\n; ... \u5fd8\u8bb0POP ...\n; \u7ed3\u679c\uff1a\u6808\u6307\u9488\u9519\u4e71\uff0c\u7a0b\u5e8f\u5d29\u6e83\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_7","title":"\u4ee3\u7801\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_8","title":"\u5b8c\u6574\u7684\u51fd\u6570\u6a21\u677f","text":"<pre><code>my_function:\n    PUSH    {R4, R5, LR}       ; 1. \u4fdd\u5b58\u5bc4\u5b58\u5668\n    SUB     SP, SP, #8         ; 2. \u4e3a\u5c40\u90e8\u53d8\u91cf\u5206\u914d\u7a7a\u95f4\n\n    ; \u51fd\u6570\u4f53\u4ee3\u7801\n    MOV     R4, R0             ; \u4f7f\u7528\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\n    MOV     R5, #100\n    ADD     R0, R4, R5\n\n    ADD     SP, SP, #8         ; 3. \u91ca\u653e\u5c40\u90e8\u53d8\u91cf\u7a7a\u95f4\n    POP     {R4, R5, PC}       ; 4. \u6062\u590d\u5bc4\u5b58\u5668\u5e76\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_9","title":"\u91cd\u8981\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u6808\u5bf9\u9f50\uff1aSP\u5fc5\u987b\u4fdd\u63014\u5b57\u8282\u62168\u5b57\u8282\u5bf9\u9f50</li> <li>\u5bc4\u5b58\u5668\u987a\u5e8f\uff1aPUSH/POP\u7684\u5bc4\u5b58\u5668\u5217\u8868\u4f1a\u81ea\u52a8\u6309\u7f16\u53f7\u6392\u5e8f</li> <li>LR\u5904\u7406\uff1aPUSH\u4fdd\u5b58LR\uff0cPOP\u65f6\u901a\u5e38\u7528PC\u6765\u540c\u65f6\u6062\u590d\u548c\u8fd4\u56de</li> <li>\u6808\u5e73\u8861\uff1aPUSH\u548cPOP\u5fc5\u987b\u6570\u91cf\u5339\u914d\uff0c\u5426\u5219\u6808\u4f1a\u635f\u574f</li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/PUSH/#_10","title":"\u603b\u7ed3","text":"<p>PUSH \u662fARM\u6c47\u7f16\u4e2d\u6700\u91cd\u8981\u7684\u6307\u4ee4\u4e4b\u4e00\uff0c\u5b83\uff1a - \u4fdd\u5b58\u5bc4\u5b58\u5668\u5230\u6808\u5185\u5b58 - \u4e3a\u51fd\u6570\u8c03\u7528\u5efa\u7acb\u5de5\u4f5c\u73af\u5883 - \u4fdd\u62a4\u73b0\u573a\u4e0d\u88ab\u7834\u574f - \u5fc5\u987b\u4e0ePOP\u914d\u5bf9\u4f7f\u7528</p>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/","title":"\u5806\u7684\u6982\u5ff5","text":"<p>\u5806\u662f\u7a0b\u5e8f\u8fd0\u884c\u65f6\u52a8\u6001\u7533\u8bf7\u5185\u5b58\u7684\u5730\u65b9\uff0c\u5c31\u50cf\u4e00\u4e2a\u5927\u4ed3\u5e93\uff0c\u53ef\u4ee5\u968f\u65f6\u501f\u5730\u65b9\u5b58\u653e\u6570\u636e\uff0c\u7528\u5b8c\u540e\u9700\u8981\u624b\u52a8\u5f52\u8fd8\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \u5806\u7a7a\u95f4      \u2502\n\u2502                 \u2502\n\u2502  [\u7a7a\u95f2\u533a\u57df]     \u2502\n\u2502  [\u5df2\u4f7f\u7528\u5757]     \u2502\n\u2502  [\u7a7a\u95f2\u533a\u57df]     \u2502\n\u2502  [\u5df2\u4f7f\u7528\u5757]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_2","title":"\u5806\u4e0e\u6808\u7684\u533a\u522b","text":"<p>\u5185\u5b58\u5e03\u5c40\u5bf9\u6bd4\uff1a</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      \u6808         \u2502 \u2190 \u81ea\u52a8\u7ba1\u7406\uff0c\u50cf\u6574\u9f50\u4e66\u67b6\n\u2502  \u540e\u8fdb\u5148\u51fa       \u2502   \u7f16\u8bd1\u5668\u81ea\u52a8\u5206\u914d\u91ca\u653e\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      \u5806         \u2502 \u2190 \u624b\u52a8\u7ba1\u7406\uff0c\u50cf\u6742\u8d27\u4ed3\u5e93\n\u2502  \u968f\u610f\u5b58\u53d6       \u2502   \u9700\u8981\u81ea\u5df1\u7533\u8bf7\u548c\u91ca\u653e\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\u6808\u7684\u7279\u70b9\uff1a - \u81ea\u52a8\u5206\u914d\u548c\u91ca\u653e - \u5927\u5c0f\u56fa\u5b9a - \u8bbf\u95ee\u901f\u5ea6\u5feb</p> <p>\u5806\u7684\u7279\u70b9\uff1a - \u624b\u52a8\u7533\u8bf7\u548c\u91ca\u653e - \u7a7a\u95f4\u7075\u6d3b - \u8bbf\u95ee\u901f\u5ea6\u8f83\u6162</p>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_3","title":"\u57fa\u672c\u5806\u7ba1\u7406\u673a\u5236","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_4","title":"\u7a7a\u95f2\u5757\u94fe\u8868","text":"<p>\u7cfb\u7edf\u7ef4\u62a4\u4e00\u4e2a\u7a7a\u95f2\u5185\u5b58\u5757\u7684\u94fe\u8868\uff0c\u7533\u8bf7\u5185\u5b58\u65f6\u4ece\u8fd9\u4e2a\u94fe\u8868\u4e2d\u67e5\u627e\u5408\u9002\u7684\u5757\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u7a7a\u95f2\u94fe\u8868\uff1a\u5934 \u2192 [\u5757A:32\u5b57\u8282] \u2192 [\u5757B:16\u5b57\u8282] \u2192 [\u5757C:24\u5b57\u8282] \u2192 NULL\n\n\u5806\u5185\u5b58\uff1a\n[\u5df2\u7528][\u5757A-\u7a7a\u95f2][\u5df2\u7528][\u5757B-\u7a7a\u95f2][\u5df2\u7528][\u5757C-\u7a7a\u95f2]\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_5","title":"\u5806\u5185\u5b58\u5206\u914d\u548c\u91ca\u653e\u7684\u8fc7\u7a0b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_6","title":"\u5185\u5b58\u5206\u914d\u8fc7\u7a0b","text":"<p>\u5f53\u7a0b\u5e8f\u7533\u8bf7\u5185\u5b58\u65f6\uff0c\u5206\u914d\u5668\u5728\u7a7a\u95f2\u94fe\u8868\u4e2d\u5bfb\u627e\u8db3\u591f\u5927\u7684\u5757\u3002</p> <p>\u5206\u914d\u6b65\u9aa4\uff1a 1. \u5728\u7a7a\u95f2\u94fe\u8868\u4e2d\u641c\u7d22\u5408\u9002\u5927\u5c0f\u7684\u5757 2. \u627e\u5230\u540e\uff0c\u4ece\u8be5\u5757\u4e2d\u5206\u5272\u51fa\u9700\u8981\u7684\u5927\u5c0f 3. \u5269\u4f59\u90e8\u5206\u5f62\u6210\u65b0\u7684\u7a7a\u95f2\u5757 4. \u66f4\u65b0\u7a7a\u95f2\u94fe\u8868</p> <p>\u5206\u914d\u793a\u610f\u56fe\uff1a</p> <pre><code>\u521d\u59cb\u72b6\u6001\uff1a\n\u7a7a\u95f2\u94fe\u8868\uff1a[\u5757A:32\u5b57\u8282] \u2192 [\u5757B:16\u5b57\u8282]\n\n\u7533\u8bf720\u5b57\u8282\uff1a\n1. \u627e\u5230\u5757A(32\u5b57\u8282)\n2. \u5206\u5272\uff1a20\u5b57\u8282(\u5df2\u7528) + 12\u5b57\u8282(\u65b0\u7a7a\u95f2\u5757)\n3. \u7ed3\u679c\uff1a[\u5df2\u752820B] [\u7a7a\u95f212B] [\u7a7a\u95f216B]\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_7","title":"\u5185\u5b58\u91ca\u653e\u8fc7\u7a0b","text":"<p>\u5f53\u7a0b\u5e8f\u91ca\u653e\u5185\u5b58\u65f6\uff0c\u9700\u8981\u5c06\u5185\u5b58\u5757\u91cd\u65b0\u52a0\u5165\u7a7a\u95f2\u94fe\u8868\u3002</p> <p>\u91ca\u653e\u6b65\u9aa4\uff1a 1. \u5c06\u91ca\u653e\u7684\u5757\u6807\u8bb0\u4e3a\u7a7a\u95f2 2. \u68c0\u67e5\u76f8\u90bb\u5757\u662f\u5426\u4e5f\u662f\u7a7a\u95f2 3. \u5982\u679c\u76f8\u90bb\u7a7a\u95f2\uff0c\u8fdb\u884c\u5408\u5e76\u64cd\u4f5c 4. \u5c06\u5408\u5e76\u540e\u7684\u5757\u52a0\u5165\u7a7a\u95f2\u94fe\u8868</p> <p>\u91ca\u653e\u793a\u610f\u56fe\uff1a</p> <pre><code>\u521d\u59cb\u72b6\u6001\uff1a\n[\u5df2\u7528A][\u7a7a\u95f2B][\u5df2\u7528C][\u7a7a\u95f2D]\n\n\u91ca\u653e\u5757A\uff1a\n[\u7a7a\u95f2A][\u7a7a\u95f2B][\u5df2\u7528C][\u7a7a\u95f2D]\n\u5408\u5e76A\u548cB\uff1a\n[\u5927\u7a7a\u95f2AB][\u5df2\u7528C][\u7a7a\u95f2D]\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_8","title":"\u5185\u5b58\u788e\u7247\u5316\u7684\u8fc7\u7a0b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_9","title":"\u788e\u7247\u5316\u5982\u4f55\u4ea7\u751f","text":"<p>\u968f\u7740\u591a\u6b21\u5206\u914d\u548c\u91ca\u653e\uff0c\u5185\u5b58\u9010\u6e10\u88ab\u5206\u5272\u6210\u8bb8\u591a\u5c0f\u5757\u3002</p> <p>\u788e\u7247\u5316\u8fc7\u7a0b\u793a\u610f\u56fe\uff1a</p> <pre><code>\u521d\u59cb\uff1a [\u5927\u7a7a\u95f2\u5757128\u5b57\u8282]\n\n\u5206\u914d32B\uff1a [\u5df2\u752832][\u7a7a\u95f296]\n\u5206\u914d24B\uff1a [\u5df2\u752832][\u5df2\u752824][\u7a7a\u95f272]  \n\u5206\u914d16B\uff1a [\u5df2\u752832][\u5df2\u752824][\u5df2\u752816][\u7a7a\u95f256]\n\n\u91ca\u653e24B\uff1a [\u5df2\u752832][\u7a7a\u95f224][\u5df2\u752816][\u7a7a\u95f256]\n\u5206\u914d20B\uff1a [\u5df2\u752832][\u5df2\u752820][\u788e\u72474][\u5df2\u752816][\u7a7a\u95f256]\n\n\u73b0\u5728\u6709\uff1a2\u4e2a\u7a7a\u95f2\u5757(4B + 56B)\uff0c\u4f46\u65e0\u6cd5\u5206\u914d60B\u7684\u8bf7\u6c42\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_10","title":"\u5185\u5b58\u788e\u7247\u95ee\u9898","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_11","title":"\u5185\u90e8\u788e\u7247","text":"<p>\u5206\u914d\u7684\u5185\u5b58\u6bd4\u5b9e\u9645\u9700\u8981\u7684\u5927\uff0c\u591a\u51fa\u6765\u7684\u90e8\u5206\u88ab\u6d6a\u8d39\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u7533\u8bf710\u5b57\u8282\uff0c\u5206\u914d16\u5b57\u8282\uff1a\n[\u5757\u5934|10\u5b57\u8282\u6570\u636e|6\u5b57\u8282\u6d6a\u8d39]\n         \u2191\n     \u5185\u90e8\u788e\u7247\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_12","title":"\u5916\u90e8\u788e\u7247","text":"<p>\u7a7a\u95f2\u5185\u5b58\u88ab\u5206\u5272\u6210\u5f88\u591a\u5c0f\u5757\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u5185\u5b58\u7533\u8bf7\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u5806\u72b6\u6001\uff1a\n[\u5df2\u7528][\u7a7a\u95f28B][\u5df2\u7528][\u7a7a\u95f212B][\u5df2\u7528][\u7a7a\u95f26B]\n\n\u7533\u8bf720\u5b57\u8282\uff1a\u5931\u8d25\uff01\n\u603b\u7a7a\u95f2=26\u5b57\u8282\uff0c\u4f46\u90fd\u4e0d\u8fde\u7eed\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_13","title":"\u5e38\u89c1\u5206\u914d\u7b97\u6cd5","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_14","title":"\u9996\u6b21\u9002\u5e94\u7b97\u6cd5","text":"<p>\u4ece\u94fe\u8868\u5f00\u5934\u5f00\u59cb\u67e5\u627e\uff0c\u627e\u5230\u7b2c\u4e00\u4e2a\u8db3\u591f\u5927\u7684\u5757\u5c31\u5206\u914d\u3002</p> <p>\u5de5\u4f5c\u8fc7\u7a0b\uff1a</p> <pre><code>\u7a7a\u95f2\u94fe\u8868\uff1a[25B] \u2192 [18B] \u2192 [32B] \u2192 [12B]\n\u7533\u8bf720\u5b57\u8282\uff1a\n\u68c0\u67e525B \u2192 \u8db3\u591f\u5927\uff0c\u5206\u914d\n\u7ed3\u679c\uff1a[\u5df2\u752820B|\u788e\u72475B] \u2192 [18B] \u2192 [32B] \u2192 [12B]\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_15","title":"\u6700\u4f73\u9002\u5e94\u7b97\u6cd5","text":"<p>\u67e5\u627e\u4e0e\u7533\u8bf7\u5927\u5c0f\u6700\u63a5\u8fd1\u7684\u7a7a\u95f2\u5757\uff0c\u51cf\u5c11\u6d6a\u8d39\u3002</p> <p>\u5de5\u4f5c\u8fc7\u7a0b\uff1a</p> <pre><code>\u7a7a\u95f2\u5757\uff1a8B, 25B, 15B, 30B\n\u7533\u8bf712\u5b57\u8282\uff1a\n\u627e\u5230\u6700\u5339\u914d\u768415B\u5757\n\u7ed3\u679c\uff1a8B, [\u5df2\u752812B|\u788e\u72473B], 30B\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#rtos","title":"RTOS\u4e2d\u7684\u5806\u7ba1\u7406","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_16","title":"\u5355\u4e00\u5806\u7ba1\u7406","text":"<p>\u6574\u4e2a\u7cfb\u7edf\u5171\u7528\u4e00\u4e2a\u5806\u7a7a\u95f4\uff0c\u6240\u6709\u4efb\u52a1\u90fd\u4ece\u8fd9\u91cc\u7533\u8bf7\u5185\u5b58\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>   \u4efb\u52a1A   \u4efb\u52a1B   \u4efb\u52a1C\n     \u2193      \u2193      \u2193\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502   \u7cfb\u7edf\u5806      \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_17","title":"\u56fa\u5b9a\u5927\u5c0f\u5185\u5b58\u6c60","text":"<p>\u9884\u5206\u914d\u5f88\u591a\u76f8\u540c\u5927\u5c0f\u7684\u5185\u5b58\u5757\uff0c\u5206\u914d\u65f6\u76f4\u63a5\u7ed9\u51fa\u4e00\u4e2a\u7a7a\u95f2\u5757\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u5185\u5b58\u6c60\uff1a\n[\u57571][\u57572][\u57573][\u57574][\u57575][\u57576]\n \u2502    \u2502    \u2502    \u2502    \u2502    \u2502\n32B  32B  32B  32B  32B  32B\n\n\u4f18\u70b9\uff1a\u5206\u914d\u901f\u5ea6\u5feb\uff0c\u65e0\u5916\u90e8\u788e\u7247\n\u7f3a\u70b9\uff1a\u53ef\u80fd\u5185\u90e8\u788e\u7247\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_18","title":"\u5e38\u89c1\u9519\u8bef\u7c7b\u578b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_19","title":"\u5185\u5b58\u6cc4\u6f0f","text":"<p>\u7533\u8bf7\u5185\u5b58\u540e\u5fd8\u8bb0\u91ca\u653e\uff0c\u5bfc\u81f4\u53ef\u7528\u5185\u5b58\u8d8a\u6765\u8d8a\u5c11\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u7533\u8bf7 \u2192 [\u5185\u5b58\u5757] \u2192 \u5fd8\u8bb0\u91ca\u653e\n\u7533\u8bf7 \u2192 [\u5185\u5b58\u5757] \u2192 \u5fd8\u8bb0\u91ca\u653e\n...\n\u6700\u7ec8\uff1a\u5185\u5b58\u8017\u5c3d\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_20","title":"\u4f7f\u7528\u5df2\u91ca\u653e\u5185\u5b58","text":"<p>\u91ca\u653e\u5185\u5b58\u540e\u7ee7\u7eed\u4f7f\u7528\uff0c\u5bfc\u81f4\u6570\u636e\u635f\u574f\u6216\u7a0b\u5e8f\u5d29\u6e83\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u91ca\u653e \u2192 [\u5185\u5b58\u5757] \u2705\n\u4f7f\u7528 \u2192 [\u540c\u4e00\u5185\u5b58\u5757] \u274c \u5371\u9669\uff01\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/heap/#_21","title":"\u91cd\u590d\u91ca\u653e","text":"<p>\u5bf9\u540c\u4e00\u5757\u5185\u5b58\u91ca\u653e\u4e24\u6b21\uff0c\u901a\u5e38\u5bfc\u81f4\u7a0b\u5e8f\u5d29\u6e83\u3002</p> <p>\u793a\u610f\u56fe\uff1a</p> <pre><code>\u91ca\u653e \u2192 [\u5757A] \u2705\n\u518d\u6b21\u91ca\u653e \u2192 [\u5757A] \u274c \u7a0b\u5e8f\u5d29\u6e83\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/","title":"Label","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_1","title":"\u57fa\u672c\u6982\u5ff5","text":"<p>\u6807\u7b7e\u662f\u6c47\u7f16\u8bed\u8a00\u4e2d\u7684\u7b26\u53f7\u540d\u79f0\uff0c\u7528\u4e8e\u6807\u8bb0\u4ee3\u7801\u6216\u6570\u636e\u5728\u5185\u5b58\u4e2d\u7684\u4f4d\u7f6e\u3002\u5b83\u4ee3\u8868\u4e00\u4e2a\u5730\u5740\uff0c\u8ba9\u7a0b\u5e8f\u5458\u53ef\u4ee5\u901a\u8fc7\u540d\u79f0\u800c\u4e0d\u662f\u786c\u7f16\u7801\u7684\u5730\u5740\u6765\u5f15\u7528\u4f4d\u7f6e\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_2","title":"\u8bed\u6cd5\u683c\u5f0f","text":"<pre><code>\u6807\u7b7e\u540d\u79f0:\n    \u6307\u4ee4\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_3","title":"\u6807\u7b7e\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_4","title":"\u4ee3\u7801\u6807\u7b7e","text":"<pre><code>main:\n    MOV R0, #10\n    B   exit        @ \u8df3\u8f6c\u5230exit\u6807\u7b7e\n\nloop:\n    ADD R0, R0, #1\n    CMP R0, #20\n    BNE loop        @ \u8df3\u8f6c\u56deloop\u6807\u7b7e\n\nexit:\n    BX LR\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_5","title":"\u6570\u636e\u6807\u7b7e","text":"<pre><code>data_area:\n    .word 0x12345678    @ \u5b9a\u4e49\u4e00\u4e2a\u5b57\u6570\u636e\n    .byte 0xAA          @ \u5b9a\u4e49\u4e00\u4e2a\u5b57\u8282\u6570\u636e\n\ntext_string:\n    .asciz \"Hello\"      @ \u5b9a\u4e49\u5b57\u7b26\u4e32\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_6","title":"\u6807\u7b7e\u7684\u7279\u70b9","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_7","title":"\u5730\u5740\u66ff\u4ee3","text":"<pre><code>@ \u7f16\u8bd1\u5668\u4f1a\u5c06\u6807\u7b7e\u8f6c\u6362\u4e3a\u5b9e\u9645\u5730\u5740\nstart:              @ \u5047\u8bbe\u5730\u5740\u4e3a0x08001000\n    MOV R0, #1\n    B   target      @ \u5b9e\u9645\u7f16\u8bd1\u4e3a B 0x08001008\n\ntarget:             @ \u5730\u57400x08001008\n    MOV R1, #2\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_8","title":"\u4f5c\u7528\u57df","text":"<ul> <li>\u5c40\u90e8\u6807\u7b7e\uff1a\u901a\u5e38\u5728\u5f53\u524d\u6587\u4ef6\u5185\u53ef\u89c1</li> <li>\u5168\u5c40\u6807\u7b7e\uff1a\u4f7f\u7528<code>.global</code>\u58f0\u660e\uff0c\u5bf9\u5176\u4ed6\u6587\u4ef6\u53ef\u89c1</li> </ul> <pre><code>.global main        @ \u58f0\u660e\u4e3a\u5168\u5c40\u6807\u7b7e\uff0c\u94fe\u63a5\u5668\u53ef\u89c1\n\nmain:\n    @ \u7a0b\u5e8f\u5165\u53e3\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_9","title":"\u6807\u7b7e\u7684\u5b9e\u9645\u7528\u9014","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_10","title":"\u6d41\u7a0b\u63a7\u5236","text":"<pre><code>    CMP R0, #0\n    BEQ zero_case   @ \u5982\u679c\u7b49\u4e8e0\u8df3\u8f6c\u5230zero_case\u6807\u7b7e\n    B   non_zero    @ \u5426\u5219\u8df3\u8f6c\u5230non_zero\u6807\u7b7e\n\nzero_case:\n    MOV R1, #0\n    B   end\n\nnon_zero:\n    MOV R1, #1\n\nend:\n    @ \u7ee7\u7eed\u6267\u884c...\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_11","title":"\u5faa\u73af\u7ed3\u6784","text":"<pre><code>    MOV R0, #0          @ \u8ba1\u6570\u5668\nloop_start:             @ \u5faa\u73af\u5f00\u59cb\u6807\u7b7e\n    ADD R0, R0, #1\n    CMP R0, #10\n    BLT loop_start      @ \u5982\u679cR0 &lt; 10\uff0c\u7ee7\u7eed\u5faa\u73af\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_12","title":"\u51fd\u6570\u5b9a\u4e49","text":"<pre><code>calculate_sum:          @ \u51fd\u6570\u6807\u7b7e\n    ADD R0, R0, R1\n    BX  LR\n\nmain:\n    MOV R0, #5\n    MOV R1, #3\n    BL  calculate_sum   @ \u8c03\u7528\u51fd\u6570\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_13","title":"\u6570\u636e\u8bbf\u95ee","text":"<pre><code>    LDR R0, =data_table @ \u83b7\u53d6\u6570\u636e\u8868\u5730\u5740\n    LDR R1, [R0]        @ \u52a0\u8f7d\u7b2c\u4e00\u4e2a\u6570\u636e\n\ndata_table:\n    .word 100, 200, 300 @ \u6570\u636e\u5b9a\u4e49\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_14","title":"\u6807\u7b7e\u7684\u7c7b\u578b","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_15","title":"\u4ee3\u7801\u6807\u7b7e","text":"<p>\u6807\u8bb0\u53ef\u6267\u884c\u4ee3\u7801\u7684\u4f4d\u7f6e\uff0c\u7528\u4e8e\u8df3\u8f6c\u548c\u8c03\u7528\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_16","title":"\u6570\u636e\u6807\u7b7e","text":"<p>\u6807\u8bb0\u6570\u636e\u5b58\u50a8\u7684\u4f4d\u7f6e\uff0c\u7528\u4e8e\u52a0\u8f7d\u548c\u5b58\u50a8\u64cd\u4f5c\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_17","title":"\u5c40\u90e8\u6807\u7b7e","text":"<p>\u67d0\u4e9b\u6c47\u7f16\u5668\u652f\u6301\u6570\u5b57\u5c40\u90e8\u6807\u7b7e\uff1a</p> <pre><code>1:\n    @ \u4ee3\u7801...\n    B   1b      @ \u5411\u540e\u8df3\u8f6c\u5230\u6700\u8fd1\u76841\u6807\u7b7e\n    B   1f      @ \u5411\u524d\u8df3\u8f6c\u5230\u4e0b\u4e00\u4e2a1\u6807\u7b7e\n1:\n    @ \u53e6\u4e00\u4e2a1\u6807\u7b7e\n    B   1b      @ \u8df3\u8f6c\u5230\u524d\u4e00\u4e2a1\u6807\u7b7e\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_18","title":"\u6807\u7b7e\u7684\u547d\u540d\u89c4\u5219","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_19","title":"\u6709\u6548\u6807\u7b7e\u540d","text":"<pre><code>main:\nloop1:\n_data_start:\n_function_123:\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_20","title":"\u65e0\u6548\u6807\u7b7e\u540d","text":"<pre><code>1label:        @ \u4e0d\u80fd\u4ee5\u6570\u5b57\u5f00\u5934\nmy-label:      @ \u4e0d\u80fd\u5305\u542b\u8fde\u5b57\u7b26\nmy.label:      @ \u4e0d\u80fd\u5305\u542b\u70b9\u53f7\uff08\u9664\u975e\u7279\u6b8a\u7528\u9014\uff09\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_21","title":"\u6807\u7b7e\u5728\u53cd\u6c47\u7f16\u4e2d\u7684\u8868\u73b0","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_22","title":"\u53cd\u6c47\u7f16\u663e\u793a","text":"<pre><code>; \u6709\u6807\u7b7e\u7684\u60c5\u51b5\n0x08001000 main:\n0x08001000 200A          MOVS    R0, #10\n0x08001002 E002          B       exit\n\n0x08001004 loop:\n0x08001004 3001          ADDS    R0, #1\n\n0x08001006 exit:\n0x08001006 4770          BX      LR\n\n; \u65e0\u6807\u7b7e\u7684\u60c5\u51b5\n0x08001000 200A          MOVS    R0, #10\n0x08001002 E002          B       0x08001006\n0x08001004 3001          ADDS    R0, #1\n0x08001006 4770          BX      LR\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_23","title":"\u7279\u6b8a\u7528\u9014\u6807\u7b7e","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_24","title":"\u6bb5\u6807\u7b7e","text":"<pre><code>.text                   @ \u4ee3\u7801\u6bb5\u5f00\u59cb\n.global _start\n_start:\n    @ \u4ee3\u7801...\n\n.data                   @ \u6570\u636e\u6bb5\u5f00\u59cb\nvariables:\n    .word 0, 0, 0\n\n.bss                    @ \u672a\u521d\u59cb\u5316\u6570\u636e\u6bb5\nbuffer:\n    .space 256\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_25","title":"\u5bf9\u9f50\u6807\u7b7e","text":"<pre><code>    .align 2            @ 4\u5b57\u8282\u5bf9\u9f50\naligned_data:\n    .word 0x12345678\n\n    .align 3            @ 8\u5b57\u8282\u5bf9\u9f50\ndouble_aligned:\n    .dword 0x123456789ABCDEF0\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_26","title":"\u6807\u7b7e\u4e0e\u5730\u5740\u8ba1\u7b97","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_27","title":"\u5730\u5740\u5dee\u8ba1\u7b97","text":"<pre><code>start:\n    @ \u4e00\u4e9b\u4ee3\u7801...\nend:\n    @ \u8ba1\u7b97\u4ee3\u7801\u5927\u5c0f\n    LDR R0, =end\n    LDR R1, =start\n    SUB R2, R0, R1      @ R2 = \u4ee3\u7801\u5927\u5c0f\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_28","title":"\u76f8\u5bf9\u5730\u5740\u5f15\u7528","text":"<pre><code>    ADR R0, data_table  @ \u83b7\u53d6\u76f8\u5bf9\u5730\u5740\n    LDR R1, [R0]        @ \u52a0\u8f7d\u6570\u636e\n\ndata_table:\n    .word 0x12345678\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_29","title":"\u5e38\u89c1\u6807\u7b7e\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_30","title":"\u6761\u4ef6\u5206\u652f","text":"<pre><code>    CMP R0, #100\n    BGT greater_than    @ \u5982\u679c\u5927\u4e8e\u8df3\u8f6c\n    BLT less_than       @ \u5982\u679c\u5c0f\u4e8e\u8df3\u8f6c\n    BEQ equal           @ \u5982\u679c\u7b49\u4e8e\u8df3\u8f6c\n\ngreater_than:\n    MOV R1, #1\n    B   end_compare\n\nless_than:\n    MOV R1, #-1\n    B   end_compare\n\nequal:\n    MOV R1, #0\n\nend_compare:\n    @ \u7ee7\u7eed\u6267\u884c...\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_31","title":"\u8df3\u8f6c\u8868\u5b9e\u73b0","text":"<pre><code>    CMP R0, #3\n    BHS default_case    @ \u5982\u679c&gt;=3\u8df3\u8f6c\u5230\u9ed8\u8ba4\u60c5\u51b5\n    LDR PC, [PC, R0, LSL #2]  @ \u8df3\u8f6c\u5230\u5bf9\u5e94\u5904\u7406\u7a0b\u5e8f\n    B   end_switch\n\njump_table:\n    .word case0, case1, case2\n\ncase0:\n    @ \u60c5\u51b50\u5904\u7406\n    B   end_switch\n\ncase1:\n    @ \u60c5\u51b51\u5904\u7406\n    B   end_switch\n\ncase2:\n    @ \u60c5\u51b52\u5904\u7406\n    B   end_switch\n\ndefault_case:\n    @ \u9ed8\u8ba4\u5904\u7406\n\nend_switch:\n    @ \u7ee7\u7eed\u6267\u884c...\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/label/#_32","title":"\u8c03\u8bd5\u4fe1\u606f\u4e2d\u7684\u6807\u7b7e","text":""},{"location":"EmbeddedSoft/Assembly_Intro/label/#_33","title":"\u5e26\u8c03\u8bd5\u7b26\u53f7","text":"<pre><code>.LFB0:                  @ \u51fd\u6570\u5f00\u59cb\u6807\u7b7e\n    .loc 1 10 0         @ \u6587\u4ef61\u7b2c10\u884c\n    push    {r7, lr}\n\n.LBB2:                  @ \u57fa\u672c\u5757\u5f00\u59cb\n    .loc 1 11 0\n    movs    r0, #10\n\n.LBE2:                  @ \u57fa\u672c\u5757\u7ed3\u675f\n    .loc 1 12 0\n    pop     {r7, pc}\n.LFE0:                  @ \u51fd\u6570\u7ed3\u675f\u6807\u7b7e\n</code></pre> <p>\u6807\u7b7e\u662f\u6c47\u7f16\u7f16\u7a0b\u7684\u57fa\u7840\u6784\u5efa\u5757\uff0c\u5b83\u4eec\u4f7f\u4ee3\u7801\u66f4\u6613\u8bfb\u3001\u6613\u7ef4\u62a4\uff0c\u5e76\u63d0\u4f9b\u4e86\u5730\u5740\u5f15\u7528\u7684\u62bd\u8c61\u5c42\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/","title":"\u6808","text":"<p>\u6808\u662f\u5185\u5b58\u4e2d\u7684\u4e00\u5757\u7279\u6b8a\u533a\u57df\uff0c\u91c7\u7528\u540e\u8fdb\u5148\u51fa\u7684\u539f\u5219\u7ba1\u7406\u6570\u636e\uff0c\u4e3b\u8981\u7528\u4e8e\u51fd\u6570\u8c03\u7528\u3001\u5c40\u90e8\u53d8\u91cf\u5b58\u50a8\u548c\u73b0\u573a\u4fdd\u62a4\u3002</p>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_2","title":"\u6808\u7684\u6838\u5fc3\u7279\u6027","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_3","title":"\u751f\u957f\u65b9\u5411","text":"<ul> <li>\u5411\u4e0b\u751f\u957f\uff1a\u6808\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u6269\u5c55</li> <li>\u6808\u6307\u9488(SP)\uff1a\u59cb\u7ec8\u6307\u5411\u6808\u9876\u4f4d\u7f6e</li> </ul> <pre><code>\u9ad8\u5730\u5740 \u2192 \u6808\u5e95 (\u521d\u59cbSP)\n        \u2502\n        \u2502 \u5df2\u4f7f\u7528\u6808\u7a7a\u95f4\n        \u2502\n        \u2193 \u6808\u9876 (\u5f53\u524dSP) \n\u4f4e\u5730\u5740 \u2192 \u672a\u4f7f\u7528\u7a7a\u95f4\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_4","title":"\u6808\u7684\u57fa\u672c\u64cd\u4f5c","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#push-","title":"PUSH - \u538b\u6808","text":"<pre><code>PUSH {R0, R1, LR}    ; SP\u51cf\u5c11\uff0c\u6570\u636e\u5b58\u5165\u6808\u4e2d\n</code></pre> <p>\u6267\u884c\u8fc7\u7a0b\uff1a 1. SP = SP - 12 (3\u4e2a\u5bc4\u5b58\u5668 \u00d7 4\u5b57\u8282) 2. \u6570\u636e\u6309\u987a\u5e8f\u5b58\u5165\u6808\u4e2d</p>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#pop-","title":"POP - \u51fa\u6808","text":"<pre><code>POP {R0, R1, PC}     ; \u6570\u636e\u4ece\u6808\u4e2d\u6062\u590d\uff0cSP\u589e\u52a0\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_5","title":"\u6808\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u7684\u4f5c\u7528","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_6","title":"\u51fd\u6570\u5f00\u573a","text":"<pre><code>my_function:\n    PUSH {R4, R5, LR}    ; \u4fdd\u5b58\u73b0\u573a\n    SUB SP, SP, #8       ; \u5206\u914d\u5c40\u90e8\u53d8\u91cf\u7a7a\u95f4\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_7","title":"\u6808\u5e27\u5e03\u5c40","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2190 \u65e7SP\n\u2502    \u65e7LR     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     R5      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     R4      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2190 \u5f53\u524dSP\n\u2502 \u5c40\u90e8\u53d8\u91cf2   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u5c40\u90e8\u53d8\u91cf1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2190 \u65b0SP\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_8","title":"\u51fd\u6570\u9000\u573a","text":"<pre><code>    ADD SP, SP, #8       ; \u91ca\u653e\u5c40\u90e8\u53d8\u91cf\u7a7a\u95f4\n    POP {R4, R5, PC}     ; \u6062\u590d\u73b0\u573a\u5e76\u8fd4\u56de\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_9","title":"\u6808\u7684\u7528\u9014","text":"<ol> <li>\u4fdd\u5b58\u8fd4\u56de\u5730\u5740\uff1aLR\u5bc4\u5b58\u5668\u538b\u6808</li> <li>\u4fdd\u62a4\u5bc4\u5b58\u5668\uff1a\u9632\u6b62\u88ab\u8c03\u7528\u51fd\u6570\u7834\u574f\u8c03\u7528\u8005\u7684\u6570\u636e</li> <li>\u5c40\u90e8\u53d8\u91cf\u5b58\u50a8\uff1a\u51fd\u6570\u5185\u7684\u81ea\u52a8\u53d8\u91cf</li> <li>\u53c2\u6570\u4f20\u9012\uff1a\u5f53\u5bc4\u5b58\u5668\u4e0d\u591f\u65f6\u901a\u8fc7\u6808\u4f20\u9012</li> <li>\u4e2d\u65ad\u5904\u7406\uff1a\u4fdd\u5b58\u5904\u7406\u5668\u72b6\u6001</li> </ol>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_10","title":"\u6808\u6307\u9488\u64cd\u4f5c","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_11","title":"\u663e\u5f0f\u8c03\u6574","text":"<pre><code>SUB SP, SP, #16     ; \u5206\u914d16\u5b57\u8282\u6808\u7a7a\u95f4\nADD SP, SP, #16     ; \u91ca\u653e\u6808\u7a7a\u95f4\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_12","title":"\u9690\u5f0f\u8c03\u6574","text":"<pre><code>PUSH {R0-R3}        ; \u81ea\u52a8 SP = SP - 16\nPOP {R0-R3}         ; \u81ea\u52a8 SP = SP + 16\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_13","title":"\u91cd\u8981\u7279\u70b9","text":"<ul> <li>\u81ea\u52a8\u7ba1\u7406\uff1a\u7f16\u8bd1\u5668\u81ea\u52a8\u751f\u6210\u6808\u64cd\u4f5c\u6307\u4ee4</li> <li>\u540e\u8fdb\u5148\u51fa\uff1a\u6700\u540e\u538b\u5165\u7684\u6570\u636e\u6700\u5148\u5f39\u51fa</li> <li>\u7ebf\u7a0b\u79c1\u6709\uff1a\u6bcf\u4e2a\u7ebf\u7a0b\u6709\u81ea\u5df1\u7684\u6808\u7a7a\u95f4</li> <li>\u5927\u5c0f\u6709\u9650\uff1a\u6808\u6ea2\u51fa\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u5d29\u6e83</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#vs","title":"\u6808 vs \u6808\u5e27","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_14","title":"\u6838\u5fc3\u533a\u522b","text":"\u7279\u6027 \u6808 (Stack) \u6808\u5e27 (Stack Frame) \u6570\u91cf \u6bcf\u4e2a\u7ebf\u7a0b\u4e00\u4e2a\u6808 \u6bcf\u4e2a\u51fd\u6570\u8c03\u7528\u4e00\u4e2a\u6808\u5e27 \u751f\u547d\u5468\u671f \u7ebf\u7a0b\u6574\u4e2a\u751f\u547d\u5468\u671f \u51fd\u6570\u8c03\u7528\u671f\u95f4 \u5927\u5c0f \u8f83\u5927 (\u901a\u5e38\u51e0MB) \u8f83\u5c0f (\u51e0\u5341\u5230\u51e0\u767e\u5b57\u8282) \u7528\u9014 \u6240\u6709\u51fd\u6570\u8c03\u7528\u7684\u603b\u5bb9\u5668 \u5355\u4e2a\u51fd\u6570\u7684\u5c40\u90e8\u5de5\u4f5c\u533a"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_15","title":"\u8be6\u7ec6\u89e3\u91ca","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#stack-","title":"\u6808 (Stack) - \"\u6574\u680b\u5927\u697c\"","text":"<ul> <li>\u4e00\u4e2a\u5b8c\u6574\u7684\u8bb0\u5fc6\u533a\u57df\uff0c\u5c31\u50cf\u4e00\u680b\u529e\u516c\u5927\u697c</li> <li>\u6bcf\u4e2a\u7ebf\u7a0b\u6709\u81ea\u5df1\u7684\u6808</li> <li>\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u751f\u957f</li> <li>\u5305\u542b\u6240\u6709\u51fd\u6570\u8c03\u7528\u7684\u6808\u5e27</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#stack-frame-","title":"\u6808\u5e27 (Stack Frame) - \"\u5355\u4e2a\u529e\u516c\u5ba4\"","text":"<ul> <li>\u51fd\u6570\u8c03\u7528\u65f6\u521b\u5efa\u7684\u4e34\u65f6\u5de5\u4f5c\u533a\uff0c\u5c31\u50cf\u5927\u697c\u91cc\u7684\u4e00\u4e2a\u529e\u516c\u5ba4</li> <li>\u6bcf\u4e2a\u51fd\u6570\u8c03\u7528\u90fd\u6709\u72ec\u7acb\u7684\u6808\u5e27</li> <li>\u5305\u542b\u8be5\u51fd\u6570\u7684\u5c40\u90e8\u53d8\u91cf\u3001\u53c2\u6570\u3001\u8fd4\u56de\u5730\u5740\u7b49</li> <li>\u51fd\u6570\u8fd4\u56de\u65f6\"\u9000\u623f\"\uff0c\u6808\u5e27\u88ab\u91ca\u653e</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_16","title":"\u5b9e\u9645\u5185\u5b58\u5e03\u5c40","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_17","title":"\u591a\u4e2a\u51fd\u6570\u8c03\u7528\u65f6\u7684\u6808\u7ed3\u6784","text":"<pre><code>\u9ad8\u5730\u5740 \u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2190 \u6808\u5e95\n         \u2502  main\u51fd\u6570   \u2502 \u2190 main\u7684\u6808\u5e27\n         \u2502   \u6808\u5e27      \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502  funcA\u51fd\u6570  \u2502 \u2190 funcA\u7684\u6808\u5e27  \n         \u2502   \u6808\u5e27      \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502  funcB\u51fd\u6570  \u2502 \u2190 funcB\u7684\u6808\u5e27\n         \u2502   \u6808\u5e27      \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u2502  funcC\u51fd\u6570  \u2502 \u2190 funcC\u7684\u6808\u5e27\n         \u2502   \u6808\u5e27      \u2502\n\u4f4e\u5730\u5740 \u2192 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2190 \u5f53\u524dSP (\u6808\u9876)\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_18","title":"\u5177\u4f53\u793a\u4f8b\u5206\u6790","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#c","title":"C\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>int funcC(int x) {\n    int local_c = x + 5;\n    return local_c;\n}\n\nint funcB(int a) {\n    int local_b = a * 2;\n    return funcC(local_b);\n}\n\nint funcA() {\n    int local_a = 10;\n    return funcB(local_a);\n}\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_19","title":"\u6808\u5e27\u521b\u5efa\u8fc7\u7a0b","text":"<pre><code>main:\n    BL funcA           ; \u8c03\u7528funcA\n\nfuncA:\n    PUSH {R4, LR}      ; \u521b\u5efafuncA\u7684\u6808\u5e27\n    SUB SP, SP, #8     ; \u4e3a\u5c40\u90e8\u53d8\u91cf\u5206\u914d\u7a7a\u95f4\n    MOV R4, #10\n    STR R4, [SP, #4]   ; local_a = 10\n    BL funcB           ; \u8c03\u7528funcB\n    ADD SP, SP, #8     ; \u91ca\u653efuncA\u6808\u5e27\u7684\u5c40\u90e8\u53d8\u91cf\n    POP {R4, PC}       ; \u91ca\u653efuncA\u6808\u5e27\n\nfuncB:\n    PUSH {R4, LR}      ; \u5728funcA\u6808\u5e27\u4e4b\u4e0a\u521b\u5efafuncB\u6808\u5e27\n    SUB SP, SP, #8\n    ; ... funcB\u7684\u4ee3\u7801 ...\n    BL funcC           ; \u8c03\u7528funcC\n    ADD SP, SP, #8\n    POP {R4, PC}\n\nfuncC:\n    PUSH {R4, LR}      ; \u5728funcB\u6808\u5e27\u4e4b\u4e0a\u521b\u5efafuncC\u6808\u5e27\n    SUB SP, SP, #8\n    ; ... funcC\u7684\u4ee3\u7801 ...\n    ADD SP, SP, #8\n    POP {R4, PC}       ; \u91ca\u653efuncC\u6808\u5e27\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_20","title":"\u6808\u5e27\u7684\u5178\u578b\u5185\u5bb9","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_21","title":"\u5355\u4e2a\u6808\u5e27\u7684\u5185\u90e8\u7ed3\u6784","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2190 \u5e27\u6307\u9488 (FP/R11)\n\u2502  \u8c03\u7528\u8005FP   \u2502 \u2190 \u524d\u4e00\u4e2a\u6808\u5e27\u7684\u5e27\u6307\u9488\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u8fd4\u56de\u5730\u5740   \u2502 \u2190 \u51fd\u6570\u8fd4\u56de\u540e\u7ee7\u7eed\u6267\u884c\u7684\u4f4d\u7f6e\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u4fdd\u5b58\u7684R4   \u2502 \u2190 \u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\n\u2502  \u4fdd\u5b58\u7684R5   \u2502\n\u2502  \u4fdd\u5b58\u7684LR   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2190 \u6808\u6307\u9488 (SP) \u8fdb\u5165\u65f6\n\u2502  \u53c2\u65705+     \u2502 \u2190 \u591a\u4f59\u7684\u53c2\u6570\uff08\u5982\u679c\u6709\uff09\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u5c40\u90e8\u53d8\u91cf1   \u2502 \u2190 \u51fd\u6570\u7684\u81ea\u52a8\u53d8\u91cf\n\u2502 \u5c40\u90e8\u53d8\u91cf2   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2190 \u5f53\u524dSP (\u51fd\u6570\u6267\u884c\u4e2d)\n\u2502    \u7a7a\u4f59     \u2502 \u2190 \u53ef\u80fd\u7528\u4e8e\u4e34\u65f6\u5b58\u50a8\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_22","title":"\u5173\u952e\u7279\u70b9","text":""},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_23","title":"\u6808\u7684\u7279\u70b9","text":"<ul> <li>\u540e\u8fdb\u5148\u51fa (LIFO)\uff1a\u6700\u540e\u8c03\u7528\u7684\u51fd\u6570\u6700\u5148\u8fd4\u56de</li> <li>\u81ea\u52a8\u7ba1\u7406\uff1a\u7f16\u8bd1\u5668\u751f\u6210\u6808\u64cd\u4f5c\u4ee3\u7801</li> <li>\u5927\u5c0f\u56fa\u5b9a\uff1a\u6808\u6ea2\u51fa\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u5d29\u6e83</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_24","title":"\u6808\u5e27\u7684\u7279\u70b9","text":"<ul> <li>\u5d4c\u5957\u7ed3\u6784\uff1a\u6808\u5e27\u5728\u6808\u5185\u5d4c\u5957\u6392\u5217</li> <li>\u52a8\u6001\u521b\u5efa/\u9500\u6bc1\uff1a\u51fd\u6570\u8c03\u7528\u65f6\u521b\u5efa\uff0c\u8fd4\u56de\u65f6\u9500\u6bc1</li> <li>\u72ec\u7acb\u7a7a\u95f4\uff1a\u6bcf\u4e2a\u6808\u5e27\u5185\u7684\u53d8\u91cf\u76f8\u4e92\u9694\u79bb</li> </ul>"},{"location":"EmbeddedSoft/Assembly_Intro/stack/#_25","title":"\u603b\u7ed3","text":"<p>\u7b80\u5355\u6bd4\u55bb\uff1a - \u6808 = \u4e00\u6574\u680b\u529e\u516c\u5927\u697c - \u6808\u5e27 = \u5927\u697c\u91cc\u7684\u5355\u4e2a\u529e\u516c\u5ba4</p> <p>\u6240\u6709\u51fd\u6570\u5171\u4eab\u540c\u4e00\u4e2a\u6808\uff0c\u4f46\u6bcf\u4e2a\u51fd\u6570\u8c03\u7528\u90fd\u6709\u81ea\u5df1\u72ec\u7acb\u7684\u6808\u5e27\uff01</p>"},{"location":"EmbeddedSoft/CV/CV/","title":"CV","text":"<p>\u672c\u7b14\u8bb0\u8bb0\u5f55CV\u5373Computer Science\u76f8\u5173\u5185\u5bb9\uff0c\u5206\u7c7b\u4e3b\u8981\u662f\u770b\u82af\u7247\u7c7b\u578b\u3002</p>"},{"location":"EmbeddedSoft/CV/CV/#_1","title":"\u76ee\u5f55","text":"<ul> <li>K230</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/K230/","title":"CanMV-K230 \u4ecb\u7ecd","text":"<p>CanMV-K230 \u5f00\u53d1\u677f\u57fa\u4e8e\u5609\u6960\u79d1\u6280 Kendryte \u00ae \u7cfb\u5217 AIoT \u82af\u7247\u4e2d\u7684\u6700\u65b0\u4e00\u4ee3 AIoT SoC K230 \u7cfb\u5217\u82af\u7247\u3002\u8be5\u82af\u7247\u91c7\u7528\u5168\u65b0\u7684\u591a\u5f02\u6784\u5355\u5143\u52a0\u901f\u8ba1\u7b97\u67b6\u6784\uff0c\u96c6\u6210\u4e86 2 \u4e2a RISC-V \u9ad8\u80fd\u6548\u8ba1\u7b97\u6838\u5fc3\uff0c\u5185\u7f6e\u65b0\u4e00\u4ee3 KPU\uff08 Knowledge Process Unit\uff09\u667a\u80fd\u8ba1\u7b97\u5355\u5143\uff0c\u5177\u5907\u591a\u7cbe\u5ea6 AI \u7b97\u529b\uff0c\u5e7f\u6cdb\u652f\u6301\u901a\u7528\u7684 AI \u8ba1\u7b97\u6846\u67b6\uff0c\u90e8\u5206\u5178\u578b\u7f51\u7edc\u7684\u5229\u7528\u7387\u8d85\u8fc7\u4e86 70%\u3002</p> <p>\u8be5\u82af\u7247\u8fd8\u652f\u6301\u4e30\u5bcc\u7684\u5916\u8bbe\u63a5\u53e3\uff0c\u5e76\u96c6\u6210\u4e86 2D\u3001 2.5D \u7b49\u591a\u4e2a\u79cd\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5355\u5143\uff0c\u80fd\u591f\u5bf9\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u3001 AI \u7b49\u591a\u7c7b\u578b\u4efb\u52a1\u8fdb\u884c\u52a0\u901f\uff0c\u5177\u5907\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017\u3001\u5feb\u901f\u542f\u52a8\u7b49\u591a\u9879\u7279\u6027\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/K230/#_1","title":"\u5f00\u53d1\u677f\u793a\u610f","text":""},{"location":"EmbeddedSoft/CV/K230/K230/#_2","title":"\u529f\u80fd\u8bf4\u660e","text":"<ul> <li> <p>\u57fa\u672c\u5916\u8bbe\u5f00\u53d1</p> </li> <li> <p>\u591a\u5a92\u4f53\u4f7f\u7528</p> </li> <li> <p>AI\u90e8\u7f72</p> </li> <li> <p>Openmv\u7b97\u6cd5\u5e93</p> </li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Media/Display/","title":"Display","text":"<p>\u53c2\u89c1\uff1aDisplay \u793a\u4f8b\u8bb2\u89e3 \u2014 CanMV K230</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#_1","title":"\u6982\u8ff0","text":"<p>K230 \u914d\u5907 1 \u8def MIPI-DSI\uff081x4 lane\uff09\uff0c\u53ef\u9a71\u52a8 MIPI \u5c4f\u5e55\u6216\u901a\u8fc7\u63a5\u53e3\u82af\u7247\u8f6c\u6362\u9a71\u52a8 HDMI \u663e\u793a\u5668\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u65b9\u4fbf\u8c03\u8bd5\uff0c\u6211\u4eec\u8fd8\u652f\u6301\u865a\u62df\u663e\u793a\u5668\uff0c\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u00a0<code>VIRT</code>\u00a0\u8f93\u51fa\u8bbe\u5907\uff0c\u5373\u4f7f\u6ca1\u6709 HDMI \u663e\u793a\u5668\u6216 LCD \u5c4f\u5e55, \u4e5f\u53ef\u5728 CanMV-IDE \u4e2d\u8fdb\u884c\u56fe\u50cf\u9884\u89c8\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#display-api","title":"<code>Display</code> \u6a21\u5757 API","text":"<p>[!Note]</p> <p>\u8be5\u6a21\u5757\u81ea\u56fa\u4ef6\u7248\u672c V0.7 \u8d77\u6709\u663e\u8457\u66f4\u6539\uff0c\u82e5\u60a8\u4f7f\u7528\u7684\u662f V0.7 \u4e4b\u524d\u7684\u56fa\u4ef6\uff0c\u8bf7\u53c2\u8003\u65e7\u7248\u672c\u6587\u6863\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#_2","title":"\u6982\u8ff0","text":"<p>\u672c\u624b\u518c\u65e8\u5728\u6307\u5bfc\u5f00\u53d1\u4eba\u5458\u4f7f\u7528 Micro Python API \u8c03\u7528 CanMV Display \u6a21\u5757\uff0c\u5b9e\u73b0\u56fe\u50cf\u663e\u793a\u529f\u80fd\u3002</p> <p>\u5982\u9700\u589e\u52a0\u81ea\u5b9a\u4e49\u5c4f\u5e55\uff0c\u53ef\u53c2\u8003Display Debugger</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#api","title":"API \u4ecb\u7ecd","text":""},{"location":"EmbeddedSoft/CV/K230/Media/Display/#init","title":"<code>init</code>","text":"<p>\u63cf\u8ff0</p> <p>\u521d\u59cb\u5316 Display \u901a\u8def\uff0c\u5305\u62ec VO \u6a21\u5757\u3001 DSI \u6a21\u5757\u548c LCD/HDMI\u3002 <code>\u5fc5\u987b\u5728 MediaManager.init()\u4e4b\u524d\u8c03\u7528</code></p> <p>\u8bed\u6cd5</p> <pre><code>init(type=None, width=None, height=None, osd_num=1, to_ide=False, flag=None, fps=None, quality=90)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165 / \u8f93\u51fa \u8bf4\u660e type \u663e\u793a\u5668\u7c7b\u578b \u8f93\u5165 \u5fc5\u9009 width \u5206\u8fa8\u7387\u5bbd\u5ea6 \u8f93\u5165 \u53ef\u9009\u53c2\u6570,\u9ed8\u8ba4\u503c\u6839\u636e <code>type</code> \u51b3\u5b9a height \u5206\u8fa8\u7387\u9ad8\u5ea6 \u8f93\u5165 \u53ef\u9009\u53c2\u6570,\u9ed8\u8ba4\u503c\u6839\u636e <code>type</code> \u51b3\u5b9a osd_num \u5728<code>show_image</code>\u65f6\u652f\u6301\u7684 LAYER \u6570\u91cf \u8f93\u5165 \u8d8a\u5927\u5360\u7528\u5185\u5b58\u8d8a\u591a to_ide \u662f\u5426\u5c06\u5c4f\u5e55\u663e\u793a\u4f20\u8f93\u5230 IDE \u663e\u793a \u8f93\u5165 \u5f00\u542f\u65f6\u5360\u7528\u66f4\u591a\u5185\u5b58 flag \u663e\u793a \u6807\u5fd7 \u8f93\u5165 fps \u663e\u793a\u5e27\u7387 \u8f93\u5165 \u4ec5\u652f\u6301 <code>VIRT</code> \u7c7b\u578b quality \u8bbe\u7f6e <code>Jpeg</code> \u538b\u7f29\u8d28\u91cf \u8f93\u5165 \u4ec5\u5728 <code>to_ide=True</code> \u65f6\u6709\u6548\uff0c\u8303\u56f4 [10-100] <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#show_image","title":"<code>show_image</code>","text":"<p>\u63cf\u8ff0</p> <p>\u5728\u5c4f\u5e55\u4e0a\u663e\u793a\u56fe\u50cf\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>show_image(img, x=0, y=0, layer=None, alpha=255, flag=None)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165 / \u8f93\u51fa \u8bf4\u660e img \u663e\u793a\u7684\u56fe\u50cf \u8f93\u5165 x \u8d77\u59cb\u5750\u6807\u7684 x \u503c \u8f93\u5165 y \u8d77\u59cb\u5750\u6807\u7684 y \u503c \u8f93\u5165 layer \u663e\u793a\u5230 \u6307\u5b9a\u5c42 \u8f93\u5165 \u4ec5\u652f\u6301 <code>OSD</code> \u5c42\uff0c\u82e5\u9700\u8981\u591a\u5c42\u8bf7\u5728init \u4e2d\u8bbe\u7f6e <code>osd_num</code> alpha \u56fe\u5c42\u6df7\u5408 alpha \u8f93\u5165 flag \u663e\u793a \u6807\u5fd7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#deinit","title":"<code>deinit</code>","text":"<p>\u63cf\u8ff0</p> <p>\u6267\u884c\u53cd\u521d\u59cb\u5316\uff0c deinit \u65b9\u6cd5\u4f1a\u5173\u95ed\u6574\u4e2a Display \u901a\u8def\uff0c\u5305\u62ec VO \u6a21\u5757\u3001 DSI \u6a21\u5757\u548c LCD/HDMI\u3002 <code>\u5fc5\u987b\u5728 MediaManager.deinit()\u4e4b\u524d\u8c03\u7528</code> <code>\u5fc5\u987b\u5728 sensor.stop()\u4e4b\u540e\u8c03\u7528</code></p> <p>\u8bed\u6cd5</p> <pre><code>deinit()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#bind_layer","title":"<code>bind_layer</code>","text":"<p>\u63cf\u8ff0</p> <p>\u5c06 <code>sensor</code> \u6216 <code>vdec</code> \u6a21\u5757\u7684\u8f93\u51fa\u7ed1\u5b9a\u5230\u5c4f\u5e55\u663e\u793a\u3002\u65e0\u9700\u7528\u6237\u624b\u52a8\u5e72\u9884\u5373\u53ef\u6301\u7eed\u663e\u793a\u56fe\u50cf\u3002 <code>\u5fc5\u987b\u5728 init \u4e4b\u524d\u8c03\u7528</code></p> <p>\u8bed\u6cd5</p> <pre><code>bind_layer(src=(mod, dev, layer), dstlayer, rect=(x, y, w, h), pix_format, alpha, flag)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165 / \u8f93\u51fa \u8bf4\u660e src <code>sensor</code> \u6216 <code>vdec</code> \u7684\u8f93\u51fa\u4fe1\u606f \u8f93\u5165 \u53ef\u901a\u8fc7 <code>sensor.bind_info()</code> \u83b7\u53d6 dstlayer \u7ed1\u5b9a\u5230 Display \u7684 \u663e\u793a\u5c42 \u8f93\u5165 \u53ef\u7ed1\u5b9a\u5230 <code>video</code> \u6216 <code>osd</code> \u5c42 rect \u663e\u793a\u533a\u57df \u8f93\u5165 \u53ef\u901a\u8fc7 <code>sensor.bind_info()</code> \u83b7\u53d6 pix_format \u56fe\u50cf\u50cf\u7d20\u683c\u5f0f \u8f93\u5165 \u53ef\u901a\u8fc7 <code>sensor.bind_info()</code> \u83b7\u53d6 alpha \u56fe\u5c42\u6df7\u5408 alpha \u8f93\u5165 flag \u663e\u793a  \u6807\u5fd7 \u65e0"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#width","title":"<code>width</code>","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u5c4f\u5e55\u6216\u67d0\u4e00\u56fe\u5c42\u7684\u663e\u793a\u5bbd\u5ea6</p> <p>\u8bed\u6cd5</p> <pre><code>width(layer = None):\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165 / \u8f93\u51fa \u8bf4\u660e layer \u6307\u5b9a\u83b7\u53d6layer \u7684\u5bbd\u5ea6,\u5982\u679c\u4e0d\u4f20\u5219\u8868\u793a\u83b7\u53d6\u5c4f\u5e55\u7684\u5206\u8fa8\u7387\u5bbd\u5ea6 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 width \u5c4f\u5e55\u6216\u663e\u793a\u5c42\u7684\u5bbd\u5ea6\u4fe1\u606f"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#height","title":"<code>height</code>","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u5c4f\u5e55\u6216\u67d0\u4e00\u56fe\u5c42\u7684\u663e\u793a\u9ad8\u5ea6</p> <p>\u8bed\u6cd5</p> <pre><code>height(layer = None):\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165 / \u8f93\u51fa \u8bf4\u660e layer \u6307\u5b9a\u83b7\u53d6layer \u7684\u9ad8\u5ea6,\u5982\u679c\u4e0d\u4f20\u5219\u8868\u793a\u83b7\u53d6\u5c4f\u5e55\u7684\u5206\u8fa8\u7387\u9ad8\u5ea6 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 height \u5c4f\u5e55\u6216\u663e\u793a\u5c42\u7684\u9ad8\u5ea6\u4fe1\u606f"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#_3","title":"\u6570\u636e\u7ed3\u6784\u63cf\u8ff0","text":""},{"location":"EmbeddedSoft/CV/K230/Media/Display/#type","title":"type","text":"\u7c7b\u578b \u53c2\u6570\u53d6\u503c \u5907\u6ce8 VIRT 640x480@90 \u9ed8\u8ba4\u503c <code>IDE</code> \u8c03\u8bd5\u4e13\u7528\uff0c\u4e0d\u5728\u5916\u63a5\u5c4f\u5e55\u4e0a\u663e\u793a\u5185\u5bb9 \u7528\u6237\u53ef\u81ea\u5b9a\u4e49\u8bbe\u7f6e\u5206\u8fa8\u7387 (64x64)-(4096x4096) \u548c\u5e27\u7387 (1-200) DEBUGGER \u8c03\u8bd5\u5c4f\u5e55\u4e13\u7528 ST7701 Display.init(Display.ST7701, width = 800, height = 480) \u9ed8\u8ba4\u503c 800x480 Display.init(Display.ST7701, width = 480, height = 800) 480x800 Display.init(Display.ST7701, width = 854, height = 480) 854x480 Display.init(Display.ST7701, width = 480, height = 854) 480x854 Display.init(Display.ST7701, width = 640, height = 480) 640x480 Display.init(Display.ST7701, width = 480, height = 640) 480x640 Display.init(Display.ST7701, width = 368, height = 552) 368x552 Display.init(Display.ST7701, width = 552, height = 368) 552x368 HX8399 Display.init(Display.HX8399, width = 1920, height = 1080) \u9ed8\u8ba4\u503c 1920x1080 Display.init(Display.HX8399, width = 1080, height = 1920) 1920x1080 ILI9806 Display.init(Display.ILI9806, width = 800, height = 480) \u9ed8\u8ba4\u503c 800x480 Display.init(Display.ILI9806, width = 480, height = 800) 480x800 ILI9881 Display.init(Display.ILI9881, width = 1280, height = 800) \u9ed8\u8ba4\u503c 1280x800 Display.init(Display.ILI9881, width = 800, height = 1280) 800x1280 LT9611 Display.init(Display.LT9611, width = 1920, height = 1080, fps = 30) \u9ed8\u8ba4\u503c 1920x1080@30 Display.init(Display.LT9611, width = 1920, height = 1080, fps = 60) 1920x1080@60 Display.init(Display.LT9611, width = 1280, height = 720, fps = 60) 1280x720@60 Display.init(Display.LT9611, width = 1280, height = 720, fps = 50) 1280x720@50 Display.init(Display.LT9611, width = 1280, height = 720, fps = 30) 1280x720@30 Display.init(Display.LT9611, width = 640, height = 480, fps = 60) 640x480@60"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#layer","title":"layer","text":"<p>K230 \u63d0\u4f9b 2 \u5c42\u89c6\u9891\u56fe\u5c42\u652f\u6301\u548c 4 \u5c42 OSD \u56fe\u5c42\u652f\u6301\u3002\u5206\u5217\u5982\u4e0b\uff1a</p> \u663e\u793a\u5c42 \u8bf4\u660e \u5907\u6ce8 LAYER_VIDEO1 \u4ec5\u53ef\u5728<code>bind_layer</code> \u4e2d\u4f7f\u7528,\u652f\u6301\u786c\u4ef6\u65cb\u8f6c LAYER_VIDEO2 \u4ec5\u53ef\u5728<code>bind_layer</code> \u4e2d\u4f7f\u7528,\u4e0d\u652f\u6301\u786c\u4ef6\u65cb\u8f6c LAYER_OSD0 \u652f\u6301<code>show_image</code> \u548c<code>bind_layer</code> \u4f7f\u7528 LAYER_OSD1 \u652f\u6301<code>show_image</code> \u548c<code>bind_layer</code> \u4f7f\u7528 LAYER_OSD2 \u652f\u6301<code>show_image</code> \u548c<code>bind_layer</code> \u4f7f\u7528 LAYER_OSD3 \u652f\u6301<code>show_image</code> \u548c<code>bind_layer</code> \u4f7f\u7528"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#flag","title":"flag","text":"\u6807\u5fd7 \u8bf4\u660e \u5907\u6ce8 FLAG_ROTATION_0 \u65cb\u8f6c <code>0</code> \u5ea6 FLAG_ROTATION_90 \u65cb\u8f6c <code>90</code> \u5ea6 FLAG_ROTATION_180 \u65cb\u8f6c <code>180</code> \u5ea6 FLAG_ROTATION_270 \u65cb\u8f6c <code>270</code> \u5ea6 FLAG_MIRROR_NONE \u4e0d\u955c\u50cf FLAG_MIRROR_HOR \u6c34\u5e73\u955c\u50cf FLAG_MIRROR_VER \u5782\u76f4\u955c\u50cf FLAG_MIRROR_BOTH \u6c34\u5e73\u4e0e\u5782\u76f4\u955c\u50cf"},{"location":"EmbeddedSoft/CV/K230/Media/Display/#_4","title":"\u793a\u4f8b\u7a0b\u5e8f","text":"<pre><code>from media.display import *  # \u5bfc\u5165 display \u6a21\u5757\uff0c\u4f7f\u7528 display \u76f8\u5173\u63a5\u53e3\nfrom media.media import *    # \u5bfc\u5165 media \u6a21\u5757\uff0c\u4f7f\u7528 media \u76f8\u5173\u63a5\u53e3\nimport os, time, image       # \u5bfc\u5165 image \u6a21\u5757\uff0c\u4f7f\u7528 image \u76f8\u5173\u63a5\u53e3\n\n# \u4f7f\u7528 LCD \u4f5c\u4e3a\u663e\u793a\u8f93\u51fa\nDisplay.init(Display.ST7701, width=800, height=480, to_ide=True)\n# \u521d\u59cb\u5316\u5a92\u4f53\u7ba1\u7406\u5668\nMediaManager.init()\n\n# \u521b\u5efa\u7528\u4e8e\u7ed8\u56fe\u7684\u56fe\u50cf\nimg = image.Image(800, 480, image.RGB565)\nimg.clear()\nimg.draw_string_advanced(0, 0, 32, \"Hello World!\uff0c\u4f60\u597d\u4e16\u754c\uff01\uff01\uff01\", color=(255, 0, 0))\n\nDisplay.show_image(img)\n\ntry:\n    while True:\n        time.sleep(1)\n        os.exitpoint()\nexcept KeyboardInterrupt as e:\n    print(\" \u7528\u6237\u505c\u6b62\uff1a\", e)\nexcept BaseException as e:\n    print(f\" \u5f02\u5e38\uff1a{e}\")\n\nDisplay.deinit()\nMediaManager.deinit()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Media/","title":"Media","text":"<ul> <li>Sensor</li> <li>Display</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/","title":"Sensor","text":"<p>\u53c2\u89c1\uff1aSensor \u6a21\u5757 API \u624b\u518c \u2014 CanMV K230</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensor-api","title":"<code>Sensor</code> \u6a21\u5757 API \u624b\u518c","text":"<p>\u6ce8\u610f</p> <p>\u8be5\u6a21\u5757\u81ea\u56fa\u4ef6\u7248\u672c V0.7 \u8d77\u53d1\u751f\u4e86\u8f83\u5927\u6539\u52a8\uff0c\u82e5\u4f7f\u7528 V0.7 \u4e4b\u524d\u7684\u56fa\u4ef6\uff0c\u8bf7\u53c2\u8003\u65e7\u7248\u672c\u6587\u6863\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#_1","title":"\u6982\u8ff0","text":"<p>CanMV K230 \u5e73\u53f0\u7684 <code>sensor</code> \u6a21\u5757\u8d1f\u8d23\u56fe\u50cf\u91c7\u96c6\u4e0e\u6570\u636e\u5904\u7406\u3002\u8be5\u6a21\u5757\u63d0\u4f9b\u4e86\u4e00\u5957\u9ad8\u7ea7 API\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u63a5\u53e3\u8f7b\u677e\u83b7\u53d6\u4e0d\u540c\u683c\u5f0f\u4e0e\u5c3a\u5bf8\u7684\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u4e86\u89e3\u5e95\u5c42\u786c\u4ef6\u7684\u5177\u4f53\u5b9e\u73b0\u3002\u5176\u67b6\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p></p> <p>\u56fe\u4e2d\uff0csensor 0\u3001sensor 1 \u548c sensor 2 \u5206\u522b\u4ee3\u8868\u4e09\u4e2a\u56fe\u50cf\u8f93\u5165\u4f20\u611f\u5668\u8bbe\u5907\uff1bCamera Device 0\u3001Camera Device 1 \u548c Camera Device 2 \u5bf9\u5e94\u76f8\u5e94\u7684\u56fe\u50cf\u5904\u7406\u5355\u5143\uff1boutput channel 0\u3001output channel 1 \u548c output channel 2 \u8868\u793a\u6bcf\u4e2a\u56fe\u50cf\u5904\u7406\u5355\u5143\u6700\u591a\u652f\u6301\u4e09\u4e2a\u8f93\u51fa\u901a\u9053\u3002\u901a\u8fc7\u8f6f\u4ef6\u914d\u7f6e\uff0c\u4e0d\u540c\u7684\u4f20\u611f\u5668\u8bbe\u5907\u53ef\u4ee5\u7075\u6d3b\u6620\u5c04\u5230\u76f8\u5e94\u7684\u56fe\u50cf\u5904\u7406\u5355\u5143\u3002</p> <p>CanMV K230 \u7684 <code>sensor</code> \u6a21\u5757\u6700\u591a\u652f\u6301\u4e09\u8def\u56fe\u50cf\u4f20\u611f\u5668\u7684\u540c\u65f6\u63a5\u5165\uff0c\u6bcf\u4e00\u8def\u5747\u53ef\u72ec\u7acb\u5b8c\u6210\u56fe\u50cf\u6570\u636e\u7684\u91c7\u96c6\u3001\u6355\u83b7\u548c\u5904\u7406\u3002\u6b64\u5916\uff0c\u6bcf\u4e2a\u89c6\u9891\u901a\u9053\u53ef\u5e76\u884c\u8f93\u51fa\u4e09\u8def\u56fe\u50cf\u6570\u636e\u4f9b\u540e\u7aef\u6a21\u5757\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5177\u4f53\u652f\u6301\u7684\u4f20\u611f\u5668\u6570\u91cf\u3001\u8f93\u5165\u5206\u8fa8\u7387\u548c\u8f93\u51fa\u901a\u9053\u6570\u5c06\u53d7\u9650\u4e8e\u5f00\u53d1\u677f\u7684\u786c\u4ef6\u914d\u7f6e\u548c\u5185\u5b58\u5927\u5c0f\uff0c\u56e0\u6b64\u9700\u6839\u636e\u9879\u76ee\u9700\u6c42\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#api","title":"API \u4ecb\u7ecd","text":""},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#_2","title":"\u6784\u9020\u51fd\u6570","text":"<p>\u63cf\u8ff0</p> <p>\u901a\u8fc7 <code>csi id</code> \u548c\u56fe\u50cf\u4f20\u611f\u5668\u7c7b\u578b\u6784\u5efa <code>Sensor</code> \u5bf9\u8c61\u3002</p> <p>\u5728\u56fe\u50cf\u5904\u7406\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u901a\u5e38\u9700\u8981\u9996\u5148\u521b\u5efa\u4e00\u4e2a <code>Sensor</code> \u5bf9\u8c61\u3002CanMV K230 \u8f6f\u4ef6\u53ef\u4ee5\u81ea\u52a8\u68c0\u6d4b\u5185\u7f6e\u7684\u56fe\u50cf\u4f20\u611f\u5668\uff0c\u65e0\u9700\u7528\u6237\u624b\u52a8\u6307\u5b9a\u5177\u4f53\u578b\u53f7\uff0c\u53ea\u9700\u8bbe\u7f6e\u4f20\u611f\u5668\u7684\u6700\u5927\u8f93\u51fa\u5206\u8fa8\u7387\u548c\u5e27\u7387\u3002\u6709\u5173\u652f\u6301\u7684\u56fe\u50cf\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u56fe\u50cf\u4f20\u611f\u5668\u652f\u6301\u5217\u8868\u5982\u679c\u8bbe\u5b9a\u7684\u5206\u8fa8\u7387\u6216\u5e27\u7387\u4e0e\u5f53\u524d\u4f20\u611f\u5668\u7684\u9ed8\u8ba4\u914d\u7f6e\u4e0d\u7b26\uff0c\u7cfb\u7edf\u4f1a\u81ea\u52a8\u8c03\u6574\u4e3a\u6700\u4f18\u914d\u7f6e\uff0c\u6700\u7ec8\u7684\u914d\u7f6e\u53ef\u5728\u65e5\u5fd7\u4e2d\u67e5\u770b\uff0c\u4f8b\u5982 <code>use sensor 23, output 640x480@90</code>\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor = Sensor(id, [width, height, fps])\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa \u8bf4\u660e id <code>csi</code> \u7aef\u53e3\uff0c\u652f\u6301 <code>0-2</code>\uff0c\u5177\u4f53\u7aef\u53e3\u8bf7\u53c2\u8003\u786c\u4ef6\u539f\u7406\u56fe \u8f93\u5165 \u53ef\u9009\uff0c\u4e0d\u540c\u578b\u53f7\u5f00\u53d1\u677f\u7684\u9ed8\u8ba4\u503c\u4e0d\u540c width <code>sensor</code> \u6700\u5927\u8f93\u51fa\u56fe\u50cf\u5bbd\u5ea6 \u8f93\u5165 \u53ef\u9009\uff0c\u9ed8\u8ba4 <code>1920</code> height <code>sensor</code> \u6700\u5927\u8f93\u51fa\u56fe\u50cf\u9ad8\u5ea6 \u8f93\u5165 \u53ef\u9009\uff0c\u9ed8\u8ba4 <code>1080</code> fps <code>sensor</code> \u6700\u5927\u8f93\u51fa\u56fe\u50cf\u5e27\u7387 \u8f93\u5165 \u53ef\u9009\uff0c\u9ed8\u8ba4 <code>30</code> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 Sensor \u5bf9\u8c61 \u4f20\u611f\u5668\u5bf9\u8c61 <p>\u4e3e\u4f8b</p> <pre><code>sensor = Sensor(id=0)\nsensor = Sensor(id=0, width=1280, height=720, fps=60)\nsensor = Sensor(id=0, width=640, height=480)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorreset","title":"sensor.reset","text":"<p>\u63cf\u8ff0</p> <p>\u590d\u4f4d <code>sensor</code> \u5bf9\u8c61\u3002\u5728\u6784\u9020 <code>Sensor</code> \u5bf9\u8c61\u540e\uff0c\u5fc5\u987b\u8c03\u7528\u6b64\u51fd\u6570\u4ee5\u7ee7\u7eed\u6267\u884c\u5176\u4ed6\u64cd\u4f5c\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.reset()\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa \u65e0 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u4e3e\u4f8b</p> <pre><code># \u521d\u59cb\u5316 sensor \u8bbe\u5907 0 \u4ee5\u53ca\u4f20\u611f\u5668 OV5647\nsensor.reset()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorset_framesize","title":"sensor.set_framesize","text":"<p>\u63cf\u8ff0</p> <p>\u8bbe\u7f6e\u6307\u5b9a\u901a\u9053\u7684\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8\u3002\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 <code>framesize</code> \u53c2\u6570\u6216\u76f4\u63a5\u6307\u5b9a <code>width</code> \u548c <code>height</code> \u6765\u914d\u7f6e\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8\u3002\u5bbd\u5ea6\u4f1a\u81ea\u52a8\u5bf9\u9f50\u5230 16 \u50cf\u7d20\u5bbd\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.set_framesize(framesize=FRAME_SIZE_INVALID, chn=CAM_CHN_ID_0, alignment=0, crop = None,  **kwargs)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa framesize sensor \u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8 \u8f93\u5165 chn sensor \u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 width \u8f93\u51fa\u56fe\u50cf\u5bbd\u5ea6\uff0ckw_arg \u8f93\u5165 height \u8f93\u51fa\u56fe\u50cf\u9ad8\u5ea6\uff0ckw_arg \u8f93\u5165 crop \u8f93\u51fa\u56fe\u50cf\u88c1\u526a\u533a\u57df; \u5f53\u8f93\u5165\u4e3a <code>crop</code>=<code>True</code> \u65f6\uff0c\u4ece\u753b\u9762\u4e2d\u5fc3\u81ea\u52a8\u88c1\u5207\u51fa\u5408\u9002\u7684\u533a\u57df; \u5f53\u8f93\u5165 <code>crop</code> \u4e3a <code>(crop_x, crop_y, crop_w, crop_h)</code> \u65f6, <code>crop_x</code> \u548c <code>crop_y</code> \u4e3a\u88c1\u526a\u533a\u57df\u7684\u5de6\u4e0a\u89d2\u5750\u6807\uff0c<code>crop_w</code> \u548c <code>crop_h</code> \u4e3a\u88c1\u526a\u533a\u57df\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6; \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8\u4e0d\u5f97\u8d85\u8fc7\u56fe\u50cf\u4f20\u611f\u5668\u7684\u5b9e\u9645\u8f93\u51fa\u80fd\u529b\u3002</li> <li>\u5404\u901a\u9053\u7684\u6700\u5927\u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8\u53d7\u786c\u4ef6\u9650\u5236\u3002</li> </ul> <p>\u4e3e\u4f8b</p> <pre><code># \u914d\u7f6e sensor \u8bbe\u5907 0\uff0c\u8f93\u51fa\u901a\u9053 0\uff0c\u8f93\u51fa\u56fe\u5c3a\u5bf8\u4e3a 640x480\nsensor.set_framesize(chn=CAM_CHN_ID_0, width=640, height=480)\n\n# \u914d\u7f6e sensor \u8bbe\u5907 0\uff0c\u8f93\u51fa\u901a\u9053 1\uff0c\u8f93\u51fa\u56fe\u5c3a\u5bf8\u4e3a 320x240\nsensor.set_framesize(chn=CAM_CHN_ID_1, width=320, height=240)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorset_pixformat","title":"sensor.set_pixformat","text":"<p>\u63cf\u8ff0</p> <p>\u914d\u7f6e\u6307\u5b9a\u901a\u9053\u7684\u56fe\u50cf\u4f20\u611f\u5668\u8f93\u51fa\u56fe\u50cf\u683c\u5f0f\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.set_pixformat(pix_format, chn=CAM_CHN_ID_0)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa pix_format \u8f93\u51fa\u56fe\u50cf\u683c\u5f0f\uff0c\u53ef\u9009\u503c\u89c1\u6570\u636e\u7ed3\u6784\u63cf\u8ff0 \u8f93\u5165 chn sensor \u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u4e3e\u4f8b</p> <pre><code># \u914d\u7f6e sensor \u8bbe\u5907 0\uff0c\u8f93\u51fa\u901a\u9053 0\uff0c\u8f93\u51fa NV12 \u683c\u5f0f\nsensor.set_pixformat(sensor.YUV420SP, chn=CAM_CHN_ID_0)\n\n# \u914d\u7f6e sensor \u8bbe\u5907 0\uff0c\u8f93\u51fa\u901a\u9053 1\uff0c\u8f93\u51fa RGB888 \u683c\u5f0f\nsensor.set_pixformat(sensor.RGB888, chn=CAM_CHN_ID_1)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorset_hmirror","title":"sensor.set_hmirror","text":"<p>\u63cf\u8ff0</p> <p>\u914d\u7f6e\u56fe\u50cf\u4f20\u611f\u5668\u662f\u5426\u8fdb\u884c\u6c34\u5e73\u955c\u50cf\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.set_hmirror(enable)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa enable <code>True</code> \u5f00\u542f\u6c34\u5e73\u955c\u50cf\u529f\u80fd <code>False</code> \u5173\u95ed\u6c34\u5e73\u955c\u50cf\u529f\u80fd \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u4e3e\u4f8b</p> <pre><code>sensor.set_hmirror(True)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorset_vflip","title":"sensor.set_vflip","text":"<p>\u63cf\u8ff0</p> <p>\u914d\u7f6e\u56fe\u50cf\u4f20\u611f\u5668\u662f\u5426\u8fdb\u884c\u5782\u76f4\u7ffb\u8f6c\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.set_vflip(enable)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa enable <code>True</code> \u5f00\u542f\u5782\u76f4\u7ffb\u8f6c\u529f\u80fd <code>False</code> \u5173\u95ed\u5782\u76f4\u7ffb\u8f6c\u529f\u80fd \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u4e3e\u4f8b</p> <pre><code>sensor.set_vflip(True)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorrun","title":"sensor.run","text":"<p>\u63cf\u8ff0</p> <p>\u542f\u52a8\u56fe\u50cf\u4f20\u611f\u5668\u7684\u8f93\u51fa\u3002\u5fc5\u987b\u5728\u8c03\u7528 <code>MediaManager.init()</code> \u4e4b\u540e\u6267\u884c\u6b64\u64cd\u4f5c\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.run()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u5f53\u540c\u65f6\u4f7f\u7528\u591a\u4e2a\u4f20\u611f\u5668\uff08\u6700\u591a 3 \u4e2a\uff09\u65f6\uff0c\u4ec5\u9700\u5176\u4e2d\u4e00\u4e2a\u6267\u884c <code>run</code> \u5373\u53ef\u3002</li> </ul> <p>\u4e3e\u4f8b</p> <pre><code># \u542f\u52a8 sensor \u8bbe\u5907\u8f93\u51fa\u6570\u636e\u6d41\nsensor.run()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorstop","title":"sensor.stop","text":"<p>\u63cf\u8ff0</p> <p>\u505c\u6b62\u56fe\u50cf\u4f20\u611f\u5668\u8f93\u51fa\u3002\u5fc5\u987b\u5728 <code>MediaManager.deinit()</code> \u4e4b\u524d\u8c03\u7528\u6b64\u65b9\u6cd5\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.stop()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 \u65e0 <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u5982\u679c\u540c\u65f6\u4f7f\u7528\u591a\u4e2a\u56fe\u50cf\u4f20\u611f\u5668\uff08\u6700\u591a 3 \u4e2a\uff09\uff0c\u6bcf\u4e2a\u4f20\u611f\u5668\u90fd\u9700\u5355\u72ec\u8c03\u7528 <code>stop</code>\u3002</li> </ul> <p>\u4e3e\u4f8b</p> <pre><code># \u505c\u6b62 sensor \u8bbe\u5907 0 \u7684\u6570\u636e\u6d41\u8f93\u51fa\nsensor.stop()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorsnapshot","title":"sensor.snapshot","text":"<p>\u63cf\u8ff0</p> <p>\u4ece\u6307\u5b9a\u8f93\u51fa\u901a\u9053\u4e2d\u6355\u83b7\u4e00\u5e27\u56fe\u50cf\u6570\u636e\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.snapshot(chn=CAM_CHN_ID_0, timeout = 1000, dump_frame = False)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa chn sensor \u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 timeout sensor \u83b7\u53d6\u4e00\u5e27\u8d85\u65f6\u65f6\u95f4\uff0c \u9ed8\u8ba4 1000 ms \u8f93\u5165 dump_frame \u5982\u679c\u4e3a True \u8fd4\u56de py_video_frame_info, \u5426\u5219\u8fd4\u56de Image \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 image \u5bf9\u8c61 \u6216 py_video_frame_info \u6355\u83b7\u7684\u56fe\u50cf\u6570\u636e \u5176\u4ed6 \u6355\u83b7\u5931\u8d25 <p>\u4e3e\u4f8b</p> <pre><code># \u4ece sensor \u8bbe\u5907 0 \u7684\u901a\u9053 0 \u6355\u83b7\u4e00\u5e27\u56fe\u50cf\u6570\u636e\nsensor.snapshot()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorbind_info","title":"sensor.bind_info","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u4f20\u611f\u5668\u901a\u9053\u7684\u7ed1\u5b9a\u4fe1\u606f\uff0c\u7528\u4e8e\u4e0e\u5176\u4ed6\u6a21\u5757\uff08\u5982\u663e\u793a\u6a21\u5757\uff09\u8fdb\u884c\u7ed1\u5b9a\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.bind_info(x=0, y=0, chn=CAM_CHN_ID_0)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa x \u7ed1\u5b9a\u533a\u57df\u7684\u6c34\u5e73\u8d77\u59cb\u5750\u6807 \u8f93\u5165 y \u7ed1\u5b9a\u533a\u57df\u7684\u5782\u76f4\u8d77\u59cb\u5750\u6807 \u8f93\u5165 chn \u4f20\u611f\u5668\u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 dict \u5bf9\u8c61 \u5305\u542b\u901a\u9053\u7684\u6e90\u4fe1\u606f\u3001\u533a\u57df\u5c3a\u5bf8\u548c\u50cf\u7d20\u683c\u5f0f <p>\u4e3e\u4f8b</p> <pre><code># \u83b7\u53d6\u4f20\u611f\u5668\u901a\u90530\u7684\u7ed1\u5b9a\u4fe1\u606f\ninfo = sensor.bind_info(chn=CAM_CHN_ID_0)\nprint(info)  # \u8f93\u51fa\u5982 {'src': (0, 0, 0), 'rect': (0, 0, 640, 480), 'pix_format': 2}\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorget_hmirror","title":"sensor.get_hmirror","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u5f53\u524d\u6c34\u5e73\u955c\u50cf\u529f\u80fd\u7684\u542f\u7528\u72b6\u6001\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.get_hmirror()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 bool <code>True</code> \u8868\u793a\u542f\u7528\uff0c<code>False</code> \u8868\u793a\u5173\u95ed <p>\u4e3e\u4f8b</p> <pre><code>hmirror_enabled = sensor.get_hmirror()\nprint(\"\u6c34\u5e73\u955c\u50cf\u5df2\u542f\u7528:\", hmirror_enabled)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorget_vflip","title":"sensor.get_vflip","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u5f53\u524d\u5782\u76f4\u7ffb\u8f6c\u529f\u80fd\u7684\u542f\u7528\u72b6\u6001\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.get_vflip()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 bool <code>True</code> \u8868\u793a\u542f\u7528\uff0c<code>False</code> \u8868\u793a\u5173\u95ed <p>\u4e3e\u4f8b</p> <pre><code>vflip_enabled = sensor.get_vflip()\nprint(\"\u5782\u76f4\u7ffb\u8f6c\u5df2\u542f\u7528:\", vflip_enabled)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorwidth","title":"sensor.width","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6307\u5b9a\u901a\u9053\u7684\u5f53\u524d\u8f93\u51fa\u56fe\u50cf\u5bbd\u5ea6\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.width(chn=CAM_CHN_ID_0)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa chn \u4f20\u611f\u5668\u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 int \u56fe\u50cf\u5bbd\u5ea6\uff08\u50cf\u7d20\uff09 <p>\u4e3e\u4f8b</p> <pre><code>current_width = sensor.width(chn=CAM_CHN_ID_0)\nprint(\"\u5f53\u524d\u5bbd\u5ea6:\", current_width)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorheight","title":"sensor.height","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6307\u5b9a\u901a\u9053\u7684\u5f53\u524d\u8f93\u51fa\u56fe\u50cf\u9ad8\u5ea6\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.height(chn=CAM_CHN_ID_0)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa chn \u4f20\u611f\u5668\u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 int \u56fe\u50cf\u9ad8\u5ea6\uff08\u50cf\u7d20\uff09 <p>\u4e3e\u4f8b</p> <pre><code>current_height = sensor.height(chn=CAM_CHN_ID_0)\nprint(\"\u5f53\u524d\u9ad8\u5ea6:\", current_height)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorget_pixformat","title":"sensor.get_pixformat","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6307\u5b9a\u901a\u9053\u7684\u5f53\u524d\u50cf\u7d20\u683c\u5f0f\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.get_pixformat(chn=CAM_CHN_ID_0)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa chn \u4f20\u611f\u5668\u8f93\u51fa\u901a\u9053\u53f7 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 int \u50cf\u7d20\u683c\u5f0f\u679a\u4e3e\u503c\uff08\u5982 <code>sensor.RGB888</code>\uff09 <p>\u4e3e\u4f8b</p> <pre><code>current_format = sensor.get_pixformat(chn=CAM_CHN_ID_0)\nprint(\"\u5f53\u524d\u50cf\u7d20\u683c\u5f0f:\", current_format)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorget_type","title":"sensor.get_type","text":"<p>\u63cf\u8ff0 \u83b7\u53d6\u5f53\u524d\u4f20\u611f\u5668\u7684\u7c7b\u578b\u6807\u8bc6\u7b26\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>sensor.get_type()\n</code></pre> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 int \u4f20\u611f\u5668\u7c7b\u578b\u679a\u4e3e\u503c <p>\u4e3e\u4f8b</p> <pre><code>sensor_type = sensor.get_type()\nprint(\"\u4f20\u611f\u5668\u7c7b\u578b:\", sensor_type)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensoragain","title":"sensor.again","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u4f20\u611f\u5668\u7684\u6a21\u62df\u589e\u76ca\u503c\uff08\u5355\u4f4d\uff1adB\uff09\u3002</p> <p>\u8bed\u6cd5</p> <pre><code># \u83b7\u53d6\u589e\u76ca\ngain = sensor.again()\n\n# \u8bbe\u7f6e\u589e\u76ca\nsensor.again(desired_gain)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa desired_gain \u76ee\u6807\u589e\u76ca\u503c\uff08\u8bbe\u7f6e\u65f6\u4f7f\u7528\uff09 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 k_sensor_gain \u5f53\u524d\u589e\u76ca\u5bf9\u8c61\uff08\u83b7\u53d6\u65f6\u8fd4\u56de\uff09 int \u64cd\u4f5c\u7ed3\u679c\uff08\u8bbe\u7f6e\u65f6\u8fd4\u56de\uff09 <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u4ec5\u90e8\u5206 sensor \u652f\u6301\uff0c\u5982 <code>sc132gs</code></li> <li>\u8bbe\u7f6e\u589e\u76ca\u65f6\u9700\u786e\u4fdd\u4f20\u611f\u5668\u5df2\u521d\u59cb\u5316\u4e14\u5904\u4e8e\u8fd0\u884c\u72b6\u6001\u3002</li> </ul> <p>\u4e3e\u4f8b</p> <pre><code># \u83b7\u53d6\u5f53\u524d\u589e\u76ca\ncurrent_gain = sensor.again()\nprint(\"\u5f53\u524d\u589e\u76ca:\", current_gain)\n\n# \u8bbe\u7f6e\u589e\u76ca\u4e3a10 dB\nresult = sensor.again(10)\nif result == 0:\n    print(\"\u589e\u76ca\u8bbe\u7f6e\u6210\u529f\")\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#analog-gain","title":"\u6a21\u62df\u589e\u76ca (Analog Gain) \u7684\u542b\u4e49","text":"<ol> <li>\u4f5c\u7528</li> </ol> <p>\u6a21\u62df\u589e\u76ca\u53d1\u751f\u5728\u4fe1\u53f7\u88ab\u8f6c\u6362\u4e3a\u6570\u5b57\u6570\u636e\u4e4b\u524d\u3002\u5b83\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u63d0\u9ad8\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u3002</p> <ul> <li> <p>\u589e\u5f3a\u4fe1\u53f7\uff1a \u5f53\u5149\u7ebf\u8f83\u5f31\u65f6\uff0c\u5149\u7535\u4e8c\u6781\u7ba1\u4ea7\u751f\u7684\u7535\u4fe1\u53f7\u5fae\u5f31\u3002\u6a21\u62df\u589e\u76ca\u7535\u8def\u4f1a\u5c06\u8fd9\u4e2a\u5fae\u5f31\u7684\u4fe1\u53f7\u7ebf\u6027\u653e\u5927\uff0c\u4f7f\u5176\u66f4\u5bb9\u6613\u88ab\u540e\u7eed\u7684 ADC\uff08\u6a21\u6570\u8f6c\u6362\u5668\uff09\u8bc6\u522b\u548c\u5904\u7406\u3002</p> </li> <li> <p>\u5355\u4f4d</p> </li> </ul> <p>\u589e\u76ca\u503c\u901a\u5e38\u4ee5 \u5206\u8d1d (dB) \u4e3a\u5355\u4f4d\u8868\u793a\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u653e\u5927\u6bd4\u7387\u7684\u5bf9\u6570\u503c\u3002</p> <ul> <li> <p>\u4f8b\u5982\uff0c\u589e\u76ca\u4e3a $10\\text{ dB}$ \u610f\u5473\u7740\u4fe1\u53f7\u7684\u529f\u7387\u5927\u7ea6\u88ab\u653e\u5927\u4e86 $10$ \u500d\u3002</p> </li> <li> <p>\u4e0e\u6570\u5b57\u589e\u76ca (Digital Gain) \u7684\u533a\u522b</p> </li> </ul> \u7279\u6027 \u6a21\u62df\u589e\u76ca (Analog Gain - again()) \u6570\u5b57\u589e\u76ca (Digital Gain - \u901a\u5e38\u662f dgain()) \u53d1\u751f\u4f4d\u7f6e ADC \u4e4b\u524d\uff08\u5728\u6a21\u62df\u57df\uff09 ADC \u4e4b\u540e\uff08\u5728\u6570\u5b57\u57df\uff09 \u5bf9\u4fe1\u566a\u6bd4 (SNR) \u7684\u5f71\u54cd \u80fd\u63d0\u9ad8 SNR\u3002\u56e0\u4e3a\u5b83\u653e\u5927\u4e86\u6709\u6548\u4fe1\u53f7\u548c\u566a\u58f0\uff0c\u4f46\u5728\u653e\u5927\u566a\u58f0\u524d\uff0c\u4fe1\u53f7\u8d28\u91cf\u66f4\u597d\u3002 \u4e0d\u80fd\u63d0\u9ad8 SNR\u3002\u5b83\u53ea\u662f\u5c06\u5df2\u7ecf\u6570\u5b57\u5316\u7684\u4fe1\u53f7\u548c\u566a\u58f0\u4e00\u8d77\u653e\u5927\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u56fe\u50cf\u566a\u70b9\u66f4\u660e\u663e\u3002 \u56fe\u50cf\u8d28\u91cf \u66f4\u4f18\u3002\u662f\u4f4e\u5149\u7167\u4e0b\u9996\u9009\u7684\u4eae\u5ea6\u63d0\u5347\u65b9\u5f0f\u3002 \u8f83\u5dee\u3002\u4ec5\u5728\u6a21\u62df\u589e\u76ca\u8fbe\u5230\u4e0a\u9650\u540e\u4f5c\u4e3a\u8865\u5145\u624b\u6bb5\u3002 <ol> <li>\u8c03\u6574\u7b56\u7565\uff08\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\uff09</li> </ol> <p>\u5728\u56fe\u50cf\u5904\u7406\u4e2d\uff0c\u901a\u5e38\u4f1a\u9075\u5faa\u4ee5\u4e0b\u7b56\u7565\u6765\u589e\u52a0\u56fe\u50cf\u4eae\u5ea6\uff1a</p> <ol> <li>\u5ef6\u957f\u66dd\u5149\u65f6\u95f4 (Exposure Time)\uff1a \u8fd9\u662f\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u7684\u9996\u9009\u65b9\u6cd5\uff0c\u4f46\u4f1a\u53d7\u5230\u7269\u4f53\u79fb\u52a8\uff08\u52a8\u6001\u6a21\u7cca\uff09\u7684\u9650\u5236\u3002</li> <li>\u589e\u52a0\u6a21\u62df\u589e\u76ca (<code>again</code>)\uff1a \u5f53\u66dd\u5149\u65f6\u95f4\u4e0d\u80fd\u518d\u5ef6\u957f\u65f6\uff0c\u589e\u52a0\u6a21\u62df\u589e\u76ca\u662f\u4e0b\u4e00\u4e2a\u9009\u62e9\u3002</li> <li>\u589e\u52a0\u6570\u5b57\u589e\u76ca (<code>dgain</code>)\uff1a \u5f53\u6a21\u62df\u589e\u76ca\u8fbe\u5230\u4e0a\u9650\u65f6\uff0c\u624d\u8003\u8651\u4f7f\u7528\u6570\u5b57\u589e\u76ca\uff0c\u4ee5\u907f\u514d\u5f15\u5165\u8fc7\u591a\u566a\u70b9\u3002</li> </ol>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorauto_focus","title":"sensor.auto_focus","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u7684\u542f\u7528\u72b6\u6001\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>Sensor.auto_focus(enable = None)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa enable \u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u7684\u542f\u7528\u72b6\u6001\uff08\u8bbe\u7f6e\u65f6\u4f7f\u7528\uff09 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 bool \u83b7\u53d6\u65f6\u8fd4\u56de\u5f53\u524d\u72b6\u6001\uff1a<code>True</code> \u8868\u793a\u542f\u7528\uff0c<code>False</code> \u8868\u793a\u5173\u95ed\uff1b \u8bbe\u7f6e\u65f6\u8fd4\u56de\u64cd\u4f5c\u7ed3\u679c\uff1a<code>True</code> \u8868\u793a\u6210\u529f\uff0c<code>False</code> \u8868\u793a\u5931\u8d25\u3002 <p>\u4e3e\u4f8b</p> <pre><code># \u83b7\u53d6\u81ea\u52a8\u5bf9\u7126\u72b6\u6001\nauto_focus_get = sensor.auto_focus()\nprint(\"sensor.auto_focus():\", auto_focus_get)\n\n# \u8bbe\u7f6e\u81ea\u52a8\u5bf9\u7126\nsensor.auto_focus(True)\n</code></pre> <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u4ec5\u90e8\u5206 sensor \u652f\u6301\u3002</li> <li>\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u65f6\u9700\u5728\u8fd0\u884c\u4e4b\u524d\u8bbe\u7f6e\u3002</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorfocus_caps","title":"sensor.focus_caps","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u4f20\u611f\u5668\u7684\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u53ca\u5176\u8303\u56f4\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>focus_caps_tuple = Sensor.focus_caps()\n</code></pre> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 focus_caps_tuple \u5305\u542b\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u53ca\u5176\u8303\u56f4\u7684\u5143\u7ec4\uff1a<code>(isSupport, minPos, maxPos)</code>; <code>isSupport</code> \u8868\u793a\u662f\u5426\u652f\u6301\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\uff0c<code>minPos</code> \u8868\u793a\u6700\u5c0f\u5bf9\u7126\u4f4d\u7f6e\uff0c<code>maxPos</code> \u8868\u793a\u6700\u5927\u5bf9\u7126\u4f4d\u7f6e\u3002 <p>\u4e3e\u4f8b</p> <pre><code># \u83b7\u53d6\u81ea\u52a8\u5bf9\u7126\u529f\u80fd\u53ca\u5176\u8303\u56f4\nfocus_caps_tuple = sensor.focus_caps()\nprint(\"focus_caps_tuple:\", focus_caps_tuple)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#sensorfocus_pos","title":"sensor.focus_pos","text":"<p>\u63cf\u8ff0</p> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u4f20\u611f\u5668\u7684\u5f53\u524d\u5bf9\u7126\u4f4d\u7f6e\u3002</p> <p>\u8bed\u6cd5</p> <pre><code>focus_pos = sensor.focus_pos(pos = None)\n</code></pre> <p>\u53c2\u6570</p> \u53c2\u6570\u540d\u79f0 \u63cf\u8ff0 \u8f93\u5165/\u8f93\u51fa pos \u5bf9\u7126\u4f4d\u7f6e\uff08\u8bbe\u7f6e\u65f6\u4f7f\u7528\uff09 \u8f93\u5165 <p>\u8fd4\u56de\u503c</p> \u8fd4\u56de\u503c \u63cf\u8ff0 focus_pos \u83b7\u53d6\u65f6\u8fd4\u56de\u5f53\u524d\u5bf9\u7126\u4f4d\u7f6e\uff1b \u8bbe\u7f6e\u65f6\u8fd4\u56de\u64cd\u4f5c\u7ed3\u679c\uff1a<code>True</code> \u8868\u793a\u6210\u529f\uff0c<code>False</code> \u8868\u793a\u5931\u8d25\u3002 <p>\u4e3e\u4f8b</p> <pre><code># \u83b7\u53d6\u5f53\u524d\u5bf9\u7126\u4f4d\u7f6e\ncurrent_focus_pos = sensor.focus_pos()\nprint(\"\u5f53\u524d\u5bf9\u7126\u4f4d\u7f6e:\", current_focus_pos)\n\n# \u8bbe\u7f6e\u5bf9\u7126\u4f4d\u7f6e\nsensor.focus_pos(300)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#pos","title":"pos\u63cf\u8ff0","text":"\u5c5e\u6027 \u63cf\u8ff0 \u6570\u636e\u7c7b\u578b <code>int</code> (\u6574\u6570) \u7269\u7406\u542b\u4e49 \u955c\u5934\u9a6c\u8fbe\u7684\u6b65\u8fdb\u8ba1\u6570\u6216\u7269\u7406\u4f4d\u79fb\u91cf\u3002 \u8303\u56f4 \u7531\u955c\u5934\u6a21\u7ec4\u7684\u673a\u68b0\u7ed3\u6784\u51b3\u5b9a\u3002\u901a\u5e38 $0$ \u5230\u51e0\u5343\u3002 \u6700\u5927\u503c/\u6700\u5c0f\u503c \u901a\u5e38\u4ee3\u8868\u955c\u5934\u7684\u6700\u8fd1\u5bf9\u7126\u8ddd\u79bb\u548c\u65e0\u7a77\u8fdc\u5bf9\u7126\u3002 \u5355\u4f4d \u65e0\u6807\u51c6\u5355\u4f4d\uff08\u662f\u7535\u673a\u6b65\u8fdb\u5355\u4f4d\uff09\uff0c\u4f46\u4ee3\u8868\u7126\u5e73\u9762\u4e0e\u4f20\u611f\u5668\u5e73\u9762\u7684\u8ddd\u79bb\u3002"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#_3","title":"\u6570\u636e\u7ed3\u6784\u63cf\u8ff0","text":""},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#frame_size","title":"frame_size","text":"\u56fe\u50cf\u5e27\u5c3a\u5bf8 \u5206\u8fa8\u7387 QQCIF 88x72 QCIF 176x144 CIF 352x288 QSIF 176x120 SIF 352x240 QQVGA 160x120 QVGA 320x240 VGA 640x480 HQQVGA 120x80 HQVGA 240x160 HVGA 480x320 B64X64 64x64 B128X64 128x64 B128X128 128x128 B160X160 160x160 B320X320 320x320 QQVGA2 128x160 WVGA 720x480 WVGA2 752x480 SVGA 800x600 XGA 1024x768 WXGA 1280x768 SXGA 1280x1024 SXGAM 1280x960 UXGA 1600x1200 HD 1280x720 FHD 1920x1080 QHD 2560x1440 QXGA 2048x1536 WQXGA 2560x1600 WQXGA2 2592x1944"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#pixel_format","title":"pixel_format","text":"\u50cf\u7d20\u683c\u5f0f \u8bf4\u660e RGB565 16 \u4f4d RGB \u683c\u5f0f RGB888 24 \u4f4d RGB \u683c\u5f0f RGBP888 \u5206\u79bb\u7684 24 \u4f4d RGB YUV420SP \u534a\u5e73\u9762 YUV GRAYSCALE \u7070\u5ea6\u56fe"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#channel","title":"channel","text":"\u901a\u9053\u53f7 \u8bf4\u660e CAM_CHN_ID_0 \u901a\u9053 0 CAM_CHN_ID_1 \u901a\u9053 1 CAM_CHN_ID_2 \u901a\u9053 2 CAM_CHN_ID_MAX \u975e\u6cd5\u901a\u9053"},{"location":"EmbeddedSoft/CV/K230/Media/Sensor/#_4","title":"\u56fe\u50cf\u4f20\u611f\u5668\u652f\u6301\u5217\u8868","text":"\u56fe\u50cf\u4f20\u611f\u5668\u578b\u53f7 \u5206\u8fa8\u7387 Width x Height \u5e27\u7387 OV5647 2592x1944 10 FPS 1920x1080 30 FPS 1280x960 45 FPS 1280x720 60 FPS 640x480 90 FPS GC2093 1920x1080 30 FPS 1920x1080 60 FPS 1280x960 60 FPS 1280x720 90 FPS IMX335 1920x1080 30 FPS 2592x1944 30 FPS"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/","title":"FPIOA","text":"<p>\u53c2\u8003\uff1aFPIOA \u4f7f\u7528\u6559\u7a0b \u2014 CanMV K230</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#_1","title":"\u6982\u8ff0","text":"<p>\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0cSoC\uff08System on Chip\uff09\u901a\u5e38\u96c6\u6210\u4e86\u591a\u79cd\u5916\u8bbe\u6a21\u5757\uff0c\u5982 UART\u3001SPI\u3001I2C\u3001PWM \u548c GPIO \u7b49\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7269\u7406\u5f15\u811a\u6570\u91cf\u6709\u9650\uff0c\u8fd9\u4e9b\u6a21\u5757\u5f80\u5f80\u9700\u8981\u5171\u4eab\u5f15\u811a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u51b2\u7a81\uff0c\u5c31\u9700\u8981\u4f7f\u7528\u00a0IOMUX\uff08\u5f15\u811a\u590d\u7528\uff09\u673a\u5236\u3002\u5728 K230 \u82af\u7247\u4e2d\uff0c\u8fd9\u4e00\u673a\u5236\u88ab\u79f0\u4e3a\u00a0FPIOA\uff08Field Programmable IO Array\uff09\u3002</p> <p>\u5728STM32\u4e2d\uff0c\u8fd9\u79f0\u4e3aAFIO</p> <p>FPIOA \u5141\u8bb8\u6211\u4eec\u4e3a\u4efb\u610f\u5f15\u811a\u5206\u914d\u6240\u9700\u7684\u529f\u80fd\u3002\u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u5c06\u5f15\u811a 10 \u8bbe\u7f6e\u4e3a UART0 \u7684\u53d1\u9001\u811a\uff0c\u4e5f\u53ef\u4ee5\u8bbe\u7f6e\u4e3a GPIO \u7528\u4e8e\u901a\u7528\u8f93\u5165\u8f93\u51fa\u3002</p> <p>\u5b83\u5145\u5f53\u4e86\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u4ea4\u53c9\u5f00\u5173\u77e9\u9635 (Crossbar Switch)\uff0c\u5141\u8bb8\u5c06\u4efb\u610f\u4e00\u4e2a\u7269\u7406\u5f15\u811a\u6620\u5c04\u5230\u4efb\u610f\u4e00\u4e2a\u5185\u90e8\u529f\u80fd\u3002</p> <p>FPIOA \u6709\u4ec0\u4e48\u4f5c\u7528\uff1f</p> <ul> <li> <p>\u63d0\u5347\u7075\u6d3b\u6027\uff1a\u5f00\u53d1\u8005\u53ef\u6839\u636e\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u81ea\u7531\u5206\u914d\u5f15\u811a\u529f\u80fd\u3002</p> </li> <li> <p>\u51cf\u5c11\u9650\u5236\uff1a\u4e00\u5957\u786c\u4ef6\u53ef\u9002\u914d\u591a\u79cd\u5f15\u811a\u914d\u7f6e\uff0c\u4fbf\u4e8e\u6a21\u5757\u5316\u8bbe\u8ba1\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#fpioa-api","title":"<code>FPIOA</code> \u6a21\u5757 API \u624b\u518c","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#_2","title":"\u6982\u8ff0","text":"<p>FPIOA\uff08Pin Multiplexer\uff09\u6a21\u5757\u4e3b\u8981\u8d1f\u8d23\u914d\u7f6e\u7269\u7406\u5f15\u811a\uff08PAD\uff09\u7684\u529f\u80fd\u3002\u5728 SoC \u4e2d\uff0c\u867d\u7136\u6709\u591a\u79cd\u529f\u80fd\u53ef\u7528\uff0c\u4f46\u7531\u4e8e\u5f15\u811a\u6570\u91cf\u6709\u9650\uff0c\u591a\u4e2a\u529f\u80fd\u53ef\u80fd\u4f1a\u5171\u4eab\u540c\u4e00\u4e2a I/O \u5f15\u811a\u3002\u6b64\u65f6\uff0c\u6bcf\u4e2a\u5f15\u811a\u5728\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6fc0\u6d3b\u4e00\u79cd\u529f\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7 IOMUX\uff08\u5373 FPIOA\uff09\u6765\u9009\u62e9\u5408\u9002\u7684\u529f\u80fd\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#api","title":"API \u4ecb\u7ecd","text":"<p>FPIOA \u7c7b\u4f4d\u4e8e <code>machine</code> \u6a21\u5757\u4e2d\u3002</p> <p>\u793a\u4f8b</p> <pre><code>from machine import FPIOA\n\n# \u5b9e\u4f8b\u5316 FPIOA \u5bf9\u8c61\nfpioa = FPIOA()\n\n# \u6253\u5370\u6240\u6709\u5f15\u811a\u7684\u914d\u7f6e\nfpioa.help()\n\n# \u6253\u5370\u6307\u5b9a\u5f15\u811a\u7684\u8be6\u7ec6\u914d\u7f6e\nfpioa.help(0)\n\n# \u6253\u5370\u6307\u5b9a\u529f\u80fd\u7684\u6240\u6709\u53ef\u7528\u914d\u7f6e\u5f15\u811a\nfpioa.help(FPIOA.IIC0_SDA, func=True)\n\n# \u8bbe\u7f6e Pin0 \u4e3a GPIO0\nfpioa.set_function(0, FPIOA.GPIO0)\n\n# \u8bbe\u7f6e Pin2 \u4e3a GPIO2\uff0c\u540c\u65f6\u914d\u7f6e\u5176\u5b83\u53c2\u6570\nfpioa.set_function(2, FPIOA.GPIO2, ie=1, oe=1, pu=0, pd=0, st=1, ds=7)\n\n# \u83b7\u53d6\u6307\u5b9a\u529f\u80fd\u5f53\u524d\u6240\u7528\u7684\u5f15\u811a\nfpioa.get_pin_num(FPIOA.UART0_TXD)\n\n# \u83b7\u53d6\u6307\u5b9a\u5f15\u811a\u5f53\u524d\u7684\u529f\u80fd\nfpioa.get_pin_func(0)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#_3","title":"\u5b9e\u4f8b\u5316","text":"<pre><code>fpioa = FPIOA()\n</code></pre> <p>\u53c2\u6570</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#set_function","title":"<code>set_function</code> \u65b9\u6cd5","text":"<pre><code>FPIOA.set_function(pin, func, ie=-1, oe=-1, pu=-1, pd=-1, st=-1, ds=-1)\n</code></pre> <p>\u8bbe\u7f6e\u5f15\u811a\u7684\u529f\u80fd\u3002</p> <p>\u53c2\u6570(\u503c\u4e3a-1\u8868\u793a\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u53c2\u6570)</p> <ul> <li><code>pin</code>: \u5f15\u811a\u53f7\uff0c\u8303\u56f4\uff1a[0, 63]</li> <li><code>func</code>: \u529f\u80fd\u53f7</li> <li><code>ie</code>: \u8f93\u5165\u4f7f\u80fd\uff0c\u53ef\u9009\u53c2\u6570</li> <li><code>oe</code>: \u8f93\u51fa\u4f7f\u80fd\uff0c\u53ef\u9009\u53c2\u6570</li> <li><code>pu</code>: \u4e0a\u62c9\u4f7f\u80fd\uff0c\u53ef\u9009\u53c2\u6570</li> <li><code>pd</code>: \u4e0b\u62c9\u4f7f\u80fd\uff0c\u53ef\u9009\u53c2\u6570</li> <li><code>st</code>: \u65bd\u5bc6\u7279\u89e6\u53d1\u5668\u4f7f\u80fd\uff0c\u53ef\u9009\u53c2\u6570</li> <li><code>ds</code>: \u9a71\u52a8\u80fd\u529b\uff0c\u53ef\u9009\u53c2\u6570</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p> \u53c2\u6570 \u82f1\u6587\u5168\u79f0 \u542b\u4e49 \u89e3\u91ca <code>ie</code> Input Enable \u8f93\u5165\u4f7f\u80fd <code>1</code>\uff1a\u4f7f\u80fd\u8f93\u5165\u3002\u5f15\u811a\u53ef\u4ee5\u63a5\u6536\u5916\u90e8\u4fe1\u53f7\u3002\u7528\u4e8e\u5c06\u5f15\u811a\u914d\u7f6e\u4e3a\u8f93\u5165\u6a21\u5f0f\uff0c\u65e0\u8bba\u662f\u4f5c\u4e3a\u901a\u7528\u8f93\u5165\u8fd8\u662f\u5916\u8bbe\u529f\u80fd\uff08\u5982 UART RX, SPI MISO\uff09\u3002 <code>oe</code> Output Enable \u8f93\u51fa\u4f7f\u80fd <code>1</code>\uff1a\u4f7f\u80fd\u8f93\u51fa\u3002\u5f15\u811a\u53ef\u4ee5\u9a71\u52a8\u5916\u90e8\u8d1f\u8f7d\u3002\u7528\u4e8e\u5c06\u5f15\u811a\u914d\u7f6e\u4e3a\u8f93\u51fa\u6a21\u5f0f\uff0c\u65e0\u8bba\u662f\u4f5c\u4e3a\u901a\u7528\u8f93\u51fa\u8fd8\u662f\u5916\u8bbe\u529f\u80fd\uff08\u5982 UART TX, SPI MOSI\uff09\u3002 <code>pu</code> Pull-Up \u4e0a\u62c9\u7535\u963b\u4f7f\u80fd <code>1</code>\uff1a\u4f7f\u80fd\u5185\u90e8\u4e0a\u62c9\u7535\u963b\u3002\u5c06\u5f15\u811a\u7535\u538b\u62c9\u9ad8\u5230 VCC\uff0c\u5e38\u7528\u4e8e\u8f93\u5165\u5f15\u811a\uff0c\u4ee5\u786e\u4fdd\u5728\u60ac\u7a7a\u65f6\u6709\u786e\u5b9a\u7684\u9ad8\u7535\u5e73\u3002 <code>pd</code> Pull-Down \u4e0b\u62c9\u7535\u963b\u4f7f\u80fd <code>1</code>\uff1a\u4f7f\u80fd\u5185\u90e8\u4e0b\u62c9\u7535\u963b\u3002\u5c06\u5f15\u811a\u7535\u538b\u62c9\u4f4e\u5230\u5730 (GND)\uff0c\u5e38\u7528\u4e8e\u8f93\u5165\u5f15\u811a\uff0c\u4ee5\u786e\u4fdd\u5728\u60ac\u7a7a\u65f6\u6709\u786e\u5b9a\u7684\u4f4e\u7535\u5e73\u3002 <code>st</code> Schmitt Trigger \u65bd\u5bc6\u7279\u89e6\u53d1\u5668\u4f7f\u80fd <code>1</code>\uff1a\u4f7f\u80fd\u65bd\u5bc6\u7279\u89e6\u53d1\u5668\u3002\u7528\u4e8e\u8f93\u5165\u5f15\u811a\u3002\u5b83\u5f15\u5165\u56de\u6ede\uff08Hysteresis\uff09\uff0c\u80fd\u6709\u6548\u6d88\u9664\u8f93\u5165\u4fe1\u53f7\u7684\u566a\u58f0\u548c\u6296\u52a8\uff0c\u4f7f\u4e0d\u7a33\u5b9a\u4fe1\u53f7\u8f6c\u5316\u4e3a\u6e05\u6670\u7684\u6570\u5b57\u4fe1\u53f7\u3002 <code>ds</code> Drive Strength \u9a71\u52a8\u80fd\u529b/\u5f3a\u5ea6 \u53d6\u503c\u8303\u56f4\u901a\u5e38\u662f 1 \u5230 8 \u6216\u66f4\u9ad8\u3002\u6570\u5b57\u8d8a\u5927\uff0c\u5f15\u811a\u80fd\u63d0\u4f9b\u7684\u7535\u6d41\u8d8a\u5927\uff0c\u4fe1\u53f7\u7684\u4e0a\u5347/\u4e0b\u964d\u901f\u5ea6\u8d8a\u5feb\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u901a\u4fe1\u3002"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#get_pin_num","title":"<code>get_pin_num</code> \u65b9\u6cd5","text":"<pre><code>fpioa.get_pin_num(func)\n</code></pre> <p>\u83b7\u53d6\u6307\u5b9a\u529f\u80fd\u5f53\u524d\u6240\u5728\u7684\u5f15\u811a\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>func</code>: \u529f\u80fd\u53f7</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u5f15\u811a\u53f7\uff0c\u6216 <code>None</code> \u5982\u679c\u672a\u627e\u5230\u76f8\u5e94\u529f\u80fd\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#get_pin_func","title":"<code>get_pin_func</code> \u65b9\u6cd5","text":"<pre><code>fpioa.get_pin_func(pin)\n</code></pre> <p>\u83b7\u53d6\u6307\u5b9a\u5f15\u811a\u5f53\u524d\u7684\u529f\u80fd\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>pin</code>: \u5f15\u811a\u53f7</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u5f15\u811a\u5f53\u524d\u7684\u529f\u80fd\u53f7\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/FPIOA/#help","title":"<code>help</code> \u65b9\u6cd5","text":"<pre><code>fpioa.help([number, func=False])\n</code></pre> <p>\u6253\u5370\u5f15\u811a\u914d\u7f6e\u63d0\u793a\u4fe1\u606f\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>number</code>: \u5f15\u811a\u53f7\u6216\u529f\u80fd\u53f7\uff0c \u53ef\u9009\u53c2\u6570</li> <li><code>func</code>: \u662f\u5426\u542f\u7528\u529f\u80fd\u53f7\u67e5\u8be2\uff0c\u9ed8\u8ba4\u4e3a <code>False</code></li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u53ef\u80fd\u4e3a\u4ee5\u4e0b\u4e09\u79cd\uff1a</p> <ol> <li>\u6240\u6709\u5f15\u811a\u7684\u914d\u7f6e\u4fe1\u606f\uff08\u672a\u8bbe\u7f6e <code>number</code>\uff09</li> <li>\u6307\u5b9a\u5f15\u811a\u7684\u8be6\u7ec6\u914d\u7f6e\u4fe1\u606f\uff08\u8bbe\u7f6e\u4e86 <code>number</code>\uff0c\u672a\u8bbe\u7f6e <code>func</code> \u6216\u8bbe\u7f6e\u4e3a <code>False</code>\uff09</li> <li>\u6307\u5b9a\u529f\u80fd\u7684\u6240\u6709\u53ef\u914d\u7f6e\u5f15\u811a\u53f7\uff08\u8bbe\u7f6e\u4e86 <code>number</code>\uff0c\u5e76\u5c06 <code>func</code> \u8bbe\u7f6e\u4e3a <code>True</code>\uff09</li> </ol>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/","title":"GPIO","text":"<p>\u53c2\u8003\uff1aGPIO \u4f7f\u7528\u6559\u7a0b \u2014 CanMV K230</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#gpio","title":"\u4ec0\u4e48\u662f GPIO\uff1f","text":"<p>GPIO\uff08\u901a\u7528\u8f93\u5165\u8f93\u51fa\uff09\u5f15\u811a\u662f\u5fae\u63a7\u5236\u5668\u6700\u57fa\u7840\u7684\u5916\u8bbe\u4e4b\u4e00\u3002\u5b83\u4eec\u53ef\u4ee5\u88ab\u914d\u7f6e\u4e3a\uff1a</p> <ul> <li> <p>\u8f93\u5165\u6a21\u5f0f\uff1a\u7528\u4e8e\u8bfb\u53d6\u5916\u90e8\u4fe1\u53f7\uff08\u5982\u6309\u952e\uff09</p> </li> <li> <p>\u8f93\u51fa\u6a21\u5f0f\uff1a\u7528\u4e8e\u63a7\u5236\u5916\u90e8\u8bbe\u5907\uff08\u5982 LED\uff09</p> </li> <li> <p>\u5e26\u4e0a\u4e0b\u62c9\u7535\u963b\u4e0e\u9a71\u52a8\u80fd\u529b\uff1a\u9002\u5e94\u4e0d\u540c\u7535\u6c14\u8fde\u63a5\u573a\u666f</p> </li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#k230-gpio","title":"K230 GPIO \u7279\u6027\u6982\u89c8","text":"\u7279\u6027 \u63cf\u8ff0 \u5f15\u811a\u6570\u91cf \u5171 64 \u4e2a GPIO \u5f15\u811a \u6a21\u5f0f\u652f\u6301 \u8f93\u5165\u3001\u8f93\u51fa \u9a71\u52a8\u80fd\u529b 0\uff5e7 \u53ef\u8c03\uff08\u6570\u503c\u8d8a\u5927\uff0c\u9a71\u52a8\u80fd\u529b\u8d8a\u5f3a\uff09 \u4e0a\u4e0b\u62c9\u914d\u7f6e \u652f\u6301\u4e0a\u62c9\uff08PULL_UP\uff09\u3001\u4e0b\u62c9\uff08PULL_DOWN\uff09\u3001\u65e0\u4e0a\u4e0b\u62c9\uff08PULL_NONE\uff09"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#pin-api","title":"<code>Pin</code> \u6a21\u5757 API \u624b\u518c","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_1","title":"\u6982\u8ff0","text":"<p>K230 \u82af\u7247\u5185\u90e8\u5305\u542b 64 \u4e2a GPIO\uff08\u901a\u7528\u8f93\u5165\u8f93\u51fa\uff09\u5f15\u811a\uff0c\u6bcf\u4e2a\u5f15\u811a\u5747\u53ef\u914d\u7f6e\u4e3a\u8f93\u5165\u6216\u8f93\u51fa\u6a21\u5f0f\uff0c\u5e76\u652f\u6301\u4e0a\u4e0b\u62c9\u7535\u963b\u914d\u7f6e\u548c\u9a71\u52a8\u80fd\u529b\u8bbe\u7f6e\u3002\u8fd9\u4e9b\u5f15\u811a\u80fd\u591f\u7075\u6d3b\u7528\u4e8e\u5404\u79cd\u6570\u5b57\u8f93\u5165\u8f93\u51fa\u573a\u666f\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#api","title":"API \u4ecb\u7ecd","text":"<p><code>Pin</code> \u7c7b\u4f4d\u4e8e <code>machine</code> \u6a21\u5757\u4e2d\uff0c\u7528\u4e8e\u63a7\u5236 K230 \u82af\u7247\u7684 GPIO \u5f15\u811a\u3002</p> <p>\u793a\u4f8b</p> <pre><code>from machine import Pin\n\n# \u5c06\u5f15\u811a 2 \u914d\u7f6e\u4e3a\u8f93\u51fa\u6a21\u5f0f\uff0c\u65e0\u4e0a\u4e0b\u62c9\uff0c\u9a71\u52a8\u80fd\u529b\u4e3a 7\npin = Pin(2, Pin.OUT, pull=Pin.PULL_NONE, drive=7)\n\n# \u8bbe\u7f6e\u5f15\u811a 2 \u8f93\u51fa\u9ad8\u7535\u5e73\npin.value(1)\n\n# \u8bbe\u7f6e\u5f15\u811a 2 \u8f93\u51fa\u4f4e\u7535\u5e73\npin.value(0)\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_2","title":"\u5b9e\u4f8b\u5316","text":"<pre><code>pin = Pin(index, mode, pull=Pin.PULL_NONE, value = -1, drive=7, alt = -1)\n</code></pre> <p>\u53c2\u6570</p> <ul> <li><code>index</code>: \u5f15\u811a\u7f16\u53f7\uff0c\u8303\u56f4\u4e3a [0, 63]\u3002</li> <li><code>mode</code>: \u5f15\u811a\u7684\u6a21\u5f0f\uff0c\u652f\u6301\u8f93\u5165\u6a21\u5f0f\u6216\u8f93\u51fa\u6a21\u5f0f\u3002</li> <li><code>pull</code>: \u4e0a\u4e0b\u62c9\u914d\u7f6e\uff08\u53ef\u9009\uff09\uff0c\u9ed8\u8ba4\u4e3a <code>Pin.PULL_NONE</code>\u3002</li> <li><code>drive</code>: \u9a71\u52a8\u80fd\u529b\u914d\u7f6e\uff08\u53ef\u9009\uff09\uff0c\u9ed8\u8ba4\u503c\u4e3a 7\u3002</li> <li><code>value</code>: \u8bbe\u7f6e\u5f15\u811a\u9ed8\u8ba4\u8f93\u51fa\u503c</li> <li><code>alt</code>: \u76ee\u524d\u672a\u4f7f\u7528</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#init","title":"<code>init</code> \u65b9\u6cd5","text":"<pre><code>Pin.init(mode, pull=Pin.PULL_NONE, drive=7)\n</code></pre> <p>\u7528\u4e8e\u521d\u59cb\u5316\u5f15\u811a\u7684\u6a21\u5f0f\u3001\u4e0a\u4e0b\u62c9\u914d\u7f6e\u53ca\u9a71\u52a8\u80fd\u529b\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>mode</code>: \u5f15\u811a\u7684\u6a21\u5f0f\uff08\u8f93\u5165\u6216\u8f93\u51fa\uff09\u3002</li> <li><code>pull</code>: \u4e0a\u4e0b\u62c9\u914d\u7f6e\uff08\u53ef\u9009\uff09\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>Pin.PULL_NONE</code>\u3002</li> <li><code>drive</code>: \u9a71\u52a8\u80fd\u529b\uff08\u53ef\u9009\uff09\uff0c\u9ed8\u8ba4\u503c\u4e3a 7\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#value","title":"<code>value</code> \u65b9\u6cd5","text":"<pre><code>Pin.value([value])\n</code></pre> <p>\u83b7\u53d6\u5f15\u811a\u7684\u8f93\u5165\u7535\u5e73\u503c\u6216\u8bbe\u7f6e\u5f15\u811a\u7684\u8f93\u51fa\u7535\u5e73\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>value</code>: \u8f93\u51fa\u503c\uff08\u53ef\u9009\uff09\uff0c\u5982\u679c\u4f20\u9012\u8be5\u53c2\u6570\u5219\u8bbe\u7f6e\u5f15\u811a\u8f93\u51fa\u4e3a\u6307\u5b9a\u503c\u3002\u5982\u679c\u4e0d\u4f20\u53c2\u5219\u8fd4\u56de\u5f15\u811a\u7684\u5f53\u524d\u8f93\u5165\u7535\u5e73\u503c\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u7a7a\u6216\u5f53\u524d\u5f15\u811a\u7684\u8f93\u5165\u7535\u5e73\u503c\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#mode","title":"<code>mode</code> \u65b9\u6cd5","text":"<pre><code>Pin.mode([mode])\n</code></pre> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u5f15\u811a\u7684\u6a21\u5f0f\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>mode</code>: \u5f15\u811a\u6a21\u5f0f\uff08\u8f93\u5165\u6216\u8f93\u51fa\uff09\uff0c\u5982\u679c\u4e0d\u4f20\u53c2\u5219\u8fd4\u56de\u5f53\u524d\u5f15\u811a\u7684\u6a21\u5f0f\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u7a7a\u6216\u5f53\u524d\u5f15\u811a\u6a21\u5f0f\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#pull","title":"<code>pull</code> \u65b9\u6cd5","text":"<pre><code>Pin.pull([pull])\n</code></pre> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u5f15\u811a\u7684\u4e0a\u4e0b\u62c9\u914d\u7f6e\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>pull</code>: \u4e0a\u4e0b\u62c9\u914d\u7f6e\uff08\u53ef\u9009\uff09\uff0c\u5982\u679c\u4e0d\u4f20\u53c2\u5219\u8fd4\u56de\u5f53\u524d\u4e0a\u4e0b\u62c9\u914d\u7f6e\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u7a7a\u6216\u5f53\u524d\u5f15\u811a\u7684\u4e0a\u4e0b\u62c9\u914d\u7f6e\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#drive","title":"<code>drive</code> \u65b9\u6cd5","text":"<pre><code>Pin.drive([drive])\n</code></pre> <p>\u83b7\u53d6\u6216\u8bbe\u7f6e\u5f15\u811a\u7684\u9a71\u52a8\u80fd\u529b\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>drive</code>: \u9a71\u52a8\u80fd\u529b\uff08\u53ef\u9009\uff09\uff0c\u5982\u679c\u4e0d\u4f20\u53c2\u5219\u8fd4\u56de\u5f53\u524d\u9a71\u52a8\u80fd\u529b\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u7a7a\u6216\u5f53\u524d\u5f15\u811a\u7684\u9a71\u52a8\u80fd\u529b\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#on","title":"<code>on</code> \u65b9\u6cd5","text":"<pre><code>Pin.on()\n</code></pre> <p>\u5c06\u5f15\u811a\u8f93\u51fa\u8bbe\u7f6e\u4e3a\u9ad8\u7535\u5e73\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#off","title":"<code>off</code> \u65b9\u6cd5","text":"<pre><code>Pin.off()\n</code></pre> <p>\u5c06\u5f15\u811a\u8f93\u51fa\u8bbe\u7f6e\u4e3a\u4f4e\u7535\u5e73\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#high","title":"<code>high</code> \u65b9\u6cd5","text":"<pre><code>Pin.high()\n</code></pre> <p>\u5c06\u5f15\u811a\u8f93\u51fa\u8bbe\u7f6e\u4e3a\u9ad8\u7535\u5e73\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#low","title":"<code>low</code> \u65b9\u6cd5","text":"<pre><code>Pin.low()\n</code></pre> <p>\u5c06\u5f15\u811a\u8f93\u51fa\u8bbe\u7f6e\u4e3a\u4f4e\u7535\u5e73\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#irq","title":"<code>irq</code> \u65b9\u6cd5","text":"<pre><code>Pin.irq(handler=None, trigger=Pin.IRQ_FALLING | Pin.IRQ_RISING, *, priority=1, wake=None, hard=False, debounce = 10)\n</code></pre> <p>\u4f7f\u80fd IO \u4e2d\u65ad\u529f\u80fd</p> <ul> <li><code>handler</code>: \u56de\u8c03\u51fd\u6570\uff0c\u5fc5\u987b\u8bbe\u7f6e</li> <li><code>trigger</code>: \u89e6\u53d1\u6a21\u5f0f</li> <li><code>priority</code>: \u4e0d\u652f\u6301</li> <li><code>wake</code>: \u4e0d\u652f\u6301</li> <li><code>hard</code>: \u4e0d\u652f\u6301</li> <li><code>debounce</code>: \u9ad8\u7535\u5e73\u548c\u4f4e\u7535\u5e73\u89e6\u53d1\u65f6\uff0c\u6700\u5c0f\u89e6\u53d1\u95f4\u9694\uff0c\u5355\u4f4d\u4e3a <code>ms</code>\uff0c\u6700\u5c0f\u503c\u4e3a <code>5</code></li> </ul> <p>\u8fd4\u56de\u503c</p> <p>mq_irq \u5bf9\u8c61</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_3","title":"\u5e38\u91cf\u5b9a\u4e49","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_4","title":"\u6a21\u5f0f","text":"<ul> <li>Pin.IN: \u8f93\u5165\u6a21\u5f0f</li> <li>Pin.OUT: \u8f93\u51fa\u6a21\u5f0f</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_5","title":"\u4e0a\u4e0b\u62c9\u6a21\u5f0f","text":"<ul> <li>PULL_NONE: \u5173\u6389\u4e0a\u4e0b\u62c9</li> <li>PULL_UP: \u4f7f\u80fd\u4e0a\u62c9</li> <li>PULL_DOWN: \u4f7f\u80fd\u4e0b\u62c9</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_6","title":"\u4e2d\u65ad\u89e6\u53d1\u6a21\u5f0f","text":"<ul> <li>IRQ_FALLING: \u4e0b\u964d\u6cbf\u89e6\u53d1</li> <li>IRQ_RISING: \u4e0a\u5347\u6cbf\u89e6\u53d1</li> <li>IRQ_LOW_LEVEL: \u4f4e\u7535\u5e73\u89e6\u53d1</li> <li>IRQ_HIGH_LEVEL: \u9ad8\u7535\u5e73\u89e6\u53d1</li> <li>IRQ_BOTH: \u8fb9\u6cbf\u89e6\u53d1</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/GPIO/#_7","title":"\u9a71\u52a8\u80fd\u529b","text":"<p>\u5177\u4f53\u914d\u7f6e\u5bf9\u5e94\u7684\u7535\u6d41\u8f93\u51fa\u80fd\u529b\u53c2\u89c1fpioa</p> <ul> <li>DRIVE_0</li> <li>DRIVE_1</li> <li>DRIVE_2</li> <li>DRIVE_3</li> <li>DRIVE_4</li> <li>DRIVE_5</li> <li>DRIVE_6</li> <li>DRIVE_7</li> <li>DRIVE_8</li> <li>DRIVE_9</li> <li>DRIVE_10</li> <li>DRIVE_11</li> <li>DRIVE_12</li> <li>DRIVE_13</li> <li>DRIVE_14</li> <li>DRIVE_15</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Peripherals/","title":"Peripherals","text":"<ul> <li>FPIOA</li> <li>GPIO</li> <li>Timer</li> <li>PWM</li> <li>UART</li> <li>I2C</li> <li>SPI</li> <li>RTC</li> <li>ADC</li> <li>WGD</li> <li>FFT</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/","title":"Timer","text":"<p>\u53c2\u8003\uff1aTimer \u4f7f\u7528\u6559\u7a0b \u2014 CanMV K230</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#timer","title":"\u4ec0\u4e48\u662f Timer\uff1f","text":"<p>Timer\uff08\u5b9a\u65f6\u5668\uff09 \u662f\u4e00\u79cd\u7528\u4e8e \u6309\u56fa\u5b9a\u65f6\u95f4\u95f4\u9694\u6267\u884c\u4efb\u52a1\u7684\u786c\u4ef6\u6a21\u5757\u3002\u5e38\u89c1\u7528\u9014\u5305\u62ec\uff1a</p> <ul> <li> <p>\u5468\u671f\u6027\u6267\u884c\u56de\u8c03\u51fd\u6570\uff08\u5982\u5b9a\u65f6\u6253\u5370\u3001\u5b9a\u65f6\u91c7\u96c6\uff09</p> </li> <li> <p>\u5355\u6b21\u5ef6\u8fdf\u89e6\u53d1\u4e8b\u4ef6\uff08\u5982\u542f\u52a8\u5ef6\u8fdf\uff09</p> </li> <li> <p>\u5b9e\u73b0\u8f6f\u4ef6\u8ba1\u65f6\u5668\u3001\u65f6\u949f\u3001\u4efb\u52a1\u8c03\u5ea6\u7b49\u529f\u80fd</p> </li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#k230-timer","title":"K230 Timer \u7279\u6027","text":"\u7279\u6027 \u63cf\u8ff0 \u5b9a\u65f6\u5668\u6570\u91cf \u5185\u7f6e 6 \u4e2a\u786c\u4ef6\u5b9a\u65f6\u5668 \u65f6\u95f4\u7cbe\u5ea6 \u6700\u5c0f\u5468\u671f\u4e3a\u00a01 \u5fae\u79d2 \u652f\u6301\u6a21\u5f0f \u5355\u6b21\u6a21\u5f0f\uff08<code>ONE_SHOT</code>\uff09\u3001\u5468\u671f\u6a21\u5f0f\uff08<code>PERIODIC</code>\uff09 \u652f\u6301\u7c7b\u578b \u786c\u4ef6\u5b9a\u65f6\u5668 / \u8f6f\u4ef6\u5b9a\u65f6\u5668\uff08\u7f16\u53f7\u4e3a -1\uff09 \u56de\u8c03\u673a\u5236 \u8d85\u65f6\u65f6\u81ea\u52a8\u8c03\u7528\u7528\u6237\u6307\u5b9a\u7684\u51fd\u6570"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#timer-api","title":"<code>Timer</code> \u6a21\u5757 API \u624b\u518c","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#_1","title":"\u6982\u8ff0","text":"<p>K230 \u5185\u90e8\u96c6\u6210\u4e86 6 \u4e2a Timer \u786c\u4ef6\u6a21\u5757\uff0c\u6700\u5c0f\u5b9a\u65f6\u5468\u671f\u4e3a 1 \u6beb\u79d2\uff08ms\uff09\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#api","title":"API \u4ecb\u7ecd","text":"<p>Timer \u7c7b\u4f4d\u4e8e <code>machine</code> \u6a21\u5757\u4e2d\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#_2","title":"\u793a\u4f8b\u4ee3\u7801","text":"<pre><code>from machine import Timer\nimport time\n\n# \u5b9e\u4f8b\u5316\u4e00\u4e2a\u8f6f\u5b9a\u65f6\u5668\ntim = Timer(-1)\n\n# \u914d\u7f6e\u5b9a\u65f6\u5668\uff0c\u5355\u6b21\u6a21\u5f0f\uff0c\u5468\u671f 100 \u6beb\u79d2\uff0c\u56de\u8c03\u51fd\u6570\u6253\u5370 1\ntim.init(period=100, mode=Timer.ONE_SHOT, callback=lambda t: print(1))\ntime.sleep(0.2)\n\n# \u914d\u7f6e\u5b9a\u65f6\u5668\uff0c\u5468\u671f\u6a21\u5f0f\uff0c\u5468\u671f 1000 \u6beb\u79d2\uff0c\u56de\u8c03\u51fd\u6570\u6253\u5370 2\ntim.init(freq=1, mode=Timer.PERIODIC, callback=lambda t: print(2))\ntime.sleep(2)\n\n# \u91ca\u653e\u5b9a\u65f6\u5668\u8d44\u6e90\ntim.deinit()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#_3","title":"\u6784\u9020\u51fd\u6570","text":"<pre><code>timer = Timer(index, mode=Timer.PERIODIC, freq=-1, period=-1, callback=None)\n</code></pre> <p>\u53c2\u6570</p> <ul> <li><code>index</code>: Timer \u6a21\u5757\u7f16\u53f7\uff0c\u53d6\u503c\u8303\u56f4\u4e3a [-1, 5]\uff0c\u5176\u4e2d -1 \u8868\u793a\u8f6f\u4ef6\u5b9a\u65f6\u5668\u3002</li> <li><code>mode</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u6a21\u5f0f\uff0c\u53ef\u4ee5\u662f\u5355\u6b21\u6216\u5468\u671f\u6a21\u5f0f\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>freq</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u9891\u7387\uff0c\u652f\u6301\u6d6e\u70b9\u6570\uff0c\u5355\u4f4d\u4e3a\u8d6b\u5179\uff08Hz\uff09\uff0c\u6b64\u53c2\u6570\u4f18\u5148\u7ea7\u9ad8\u4e8e <code>period</code>\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>period</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u5468\u671f\uff0c\u5355\u4f4d\u4e3a\u6beb\u79d2\uff08ms\uff09\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>callback</code>: \u8d85\u65f6\u56de\u8c03\u51fd\u6570\uff0c\u5fc5\u987b\u8bbe\u7f6e\u5e76\u5e94\u5e26\u6709\u4e00\u4e2a\u53c2\u6570\u3002</li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#init","title":"<code>init</code> \u65b9\u6cd5","text":"<pre><code>Timer.init(mode=Timer.PERIODIC, freq=-1, period=-1, callback=None)\n</code></pre> <p>\u521d\u59cb\u5316\u5b9a\u65f6\u5668\u53c2\u6570\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>mode</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u6a21\u5f0f\uff0c\u53ef\u4ee5\u662f\u5355\u6b21\u6216\u5468\u671f\u6a21\u5f0f\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>freq</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u9891\u7387\uff0c\u652f\u6301\u6d6e\u70b9\u6570\uff0c\u5355\u4f4d\u4e3a\u8d6b\u5179\uff08Hz\uff09\uff0c\u6b64\u53c2\u6570\u4f18\u5148\u7ea7\u9ad8\u4e8e <code>period</code>\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>period</code>: \u5b9a\u65f6\u5668\u8fd0\u884c\u5468\u671f\uff0c\u5355\u4f4d\u4e3a\u6beb\u79d2\uff08ms\uff09\uff08\u53ef\u9009\u53c2\u6570\uff09\u3002</li> <li><code>callback</code>: \u8d85\u65f6\u56de\u8c03\u51fd\u6570\uff0c\u5fc5\u987b\u8bbe\u7f6e\u5e76\u5e94\u5e26\u6709\u4e00\u4e2a\u53c2\u6570\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/Timer/#deinit","title":"<code>deinit</code> \u65b9\u6cd5","text":"<pre><code>Timer.deinit()\n</code></pre> <p>\u91ca\u653e\u5b9a\u65f6\u5668\u8d44\u6e90\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/","title":"UART","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#uart","title":"\u4ec0\u4e48\u662f UART\uff1f","text":"<p>UART\uff08\u901a\u7528\u5f02\u6b65\u6536\u53d1\u4f20\u8f93\u5668\uff09\u00a0\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u4e32\u53e3\u901a\u4fe1\u534f\u8bae\uff0c\u7528\u4e8e\u5728\u8bbe\u5907\u95f4\u901a\u8fc7\u4e32\u884c\u65b9\u5f0f\u53d1\u9001\u4e0e\u63a5\u6536\u6570\u636e\u3002\u5e7f\u6cdb\u7528\u4e8e\uff1a</p> <ul> <li> <p>\u4e32\u53e3\u7ec8\u7aef\u8c03\u8bd5</p> </li> <li> <p>\u4e0e\u4f20\u611f\u5668\u6216\u5916\u90e8\u6a21\u5757\u901a\u4fe1\uff08\u5982 GPS\u3001GSM \u6a21\u5757\uff09</p> </li> <li> <p>\u4e3b\u4ece\u8bbe\u5907\u4e4b\u95f4\u7684\u6570\u636e\u4ea4\u6362</p> </li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#k230-uart","title":"K230 UART \u7279\u6027","text":"\u7279\u6027 \u63cf\u8ff0 \u652f\u6301\u901a\u9053\u6570\u91cf 5 \u8def UART\uff1aUART0 ~ UART4 \u53ef\u7528\u901a\u9053\u8bf4\u660e UART0\u88ab\u7cfb\u7edf\u8c03\u8bd5\u5360\u7528\uff0cUART1\u30012\u30013\u30014 \u53ef\u7528 \u53ef\u914d\u7f6e\u53c2\u6570 \u6ce2\u7279\u7387\u3001\u6570\u636e\u4f4d\u3001\u505c\u6b62\u4f4d\u3001\u6821\u9a8c\u4f4d\u7b49 \u5f15\u811a\u590d\u7528 \u652f\u6301\u901a\u8fc7\u00a0IOMUX\u00a0\u8bbe\u7f6e\u4efb\u610f IO \u4e3a UART \u529f\u80fd \u6570\u636e\u64cd\u4f5c\u65b9\u5f0f \u652f\u6301\u00a0<code>write()</code>\u3001<code>read()</code>\u3001<code>readline()</code>\u3001<code>readinto()</code>\u00a0\u7b49\u65b9\u6cd5"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#uart-api","title":"<code>UART</code> \u6a21\u5757 API \u624b\u518c","text":""},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#_1","title":"\u6982\u8ff0","text":"<p>K230 \u5185\u90e8\u96c6\u6210\u4e86\u4e94\u4e2a UART\uff08\u901a\u7528\u5f02\u6b65\u6536\u53d1\u4f20\u8f93\u5668\uff09\u786c\u4ef6\u6a21\u5757\uff0c\u5176\u4e2d UART0 \u88ab\u5c0f\u6838 SH \u5360\u7528\uff0cUART3 \u88ab\u5927\u6838 SH \u5360\u7528\uff0c\u5269\u4f59\u7684 UART1\u3001UART2 \u548c UART4 \u53ef\u4f9b\u7528\u6237\u4f7f\u7528\u3002UART \u7684 I/O \u914d\u7f6e\u53ef\u53c2\u8003 IOMUX \u6a21\u5757\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#api","title":"API \u4ecb\u7ecd","text":"<p>UART \u7c7b\u4f4d\u4e8e <code>machine</code> \u6a21\u5757\u4e2d\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#_2","title":"\u793a\u4f8b\u4ee3\u7801","text":"<pre><code>from machine import UART\n\n# \u914d\u7f6e UART1: \u6ce2\u7279\u7387 115200, 8 \u4f4d\u6570\u636e\u4f4d, \u65e0\u5947\u5076\u6821\u9a8c, 1 \u4e2a\u505c\u6b62\u4f4d\nu1 = UART(UART.UART1, baudrate=115200, bits=UART.EIGHTBITS, parity=UART.PARITY_NONE, stop=UART.STOPBITS_ONE)\n\n# \u5199\u5165\u6570\u636e\u5230 UART\nu1.write(\"UART1 test\")\n\n# \u4ece UART \u8bfb\u53d6\u6570\u636e\nr = u1.read()\n\n# \u8bfb\u53d6\u4e00\u884c\u6570\u636e\nr = u1.readline()\n\n# \u5c06\u6570\u636e\u8bfb\u5165\u5b57\u8282\u7f13\u51b2\u533a\nb = bytearray(8)\nr = u1.readinto(b)\n\n# \u91ca\u653e UART \u8d44\u6e90\nu1.deinit()\n</code></pre>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#_3","title":"\u5b9e\u4f8b\u5316","text":"<pre><code>uart = UART(id, baudrate=115200, bits=UART.EIGHTBITS, parity=UART.PARITY_NONE, stop=UART.STOPBITS_ONE, timeout = 0)\n</code></pre> <p>\u53c2\u6570</p> <ul> <li><code>id</code>: UART \u6a21\u5757\u7f16\u53f7\uff0c\u6709\u6548\u503c\u4e3a <code>UART1</code>\u3001<code>UART2</code>\u3001<code>UART4</code>\u3002</li> <li><code>baudrate</code>: UART \u6ce2\u7279\u7387\uff0c\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u503c\u4e3a 115200\u3002</li> <li><code>bits</code>: \u6bcf\u4e2a\u5e27\u7684\u6570\u636e\u4f4d\u6570\uff0c\u6709\u6548\u503c\u4e3a <code>FIVEBITS</code>\u3001<code>SIXBITS</code>\u3001<code>SEVENBITS</code>\u3001<code>EIGHTBITS</code>\uff0c\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>EIGHTBITS</code>\u3002</li> <li><code>parity</code>: \u5947\u5076\u6821\u9a8c\uff0c\u6709\u6548\u503c\u4e3a <code>PARITY_NONE</code>\u3001<code>PARITY_ODD</code>\u3001<code>PARITY_EVEN</code>\uff0c\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>PARITY_NONE</code>\u3002</li> <li><code>stop</code>: \u505c\u6b62\u4f4d\u6570\uff0c\u6709\u6548\u503c\u4e3a <code>STOPBITS_ONE</code>\u3001<code>STOPBITS_TWO</code>\uff0c\u53ef\u9009\u53c2\u6570\uff0c\u9ed8\u8ba4\u503c\u4e3a <code>STOPBITS_ONE</code>\u3002</li> <li><code>timeout</code>: \u8bfb\u6570\u636e\u8d85\u65f6\uff0c\u5355\u4f4d\u4e3a <code>ms</code></li> </ul>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#init","title":"<code>init</code> \u65b9\u6cd5","text":"<pre><code>UART.init(baudrate=115200, bits=UART.EIGHTBITS, parity=UART.PARITY_NONE, stop=UART.STOPBITS_ONE)\n</code></pre> <p>\u914d\u7f6e UART \u53c2\u6570\u3002</p> <p>\u53c2\u6570</p> <p>\u53c2\u8003\u6784\u9020\u51fd\u6570\u3002</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#read","title":"<code>read</code> \u65b9\u6cd5","text":"<pre><code>UART.read([nbytes])\n</code></pre> <p>\u8bfb\u53d6\u5b57\u7b26\u3002\u5982\u679c\u6307\u5b9a\u4e86 <code>nbytes</code>\uff0c\u5219\u6700\u591a\u8bfb\u53d6\u8be5\u6570\u91cf\u7684\u5b57\u8282\uff1b\u5426\u5219\uff0c\u5c06\u5c3d\u53ef\u80fd\u591a\u5730\u8bfb\u53d6\u6570\u636e\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>nbytes</code>: \u6700\u591a\u8bfb\u53d6\u7684\u5b57\u8282\u6570\uff0c\u53ef\u9009\u53c2\u6570\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u8bfb\u53d6\u5b57\u8282\u7684\u5b57\u8282\u5bf9\u8c61\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#readline","title":"<code>readline</code> \u65b9\u6cd5","text":"<pre><code>UART.readline()\n</code></pre> <p>\u8bfb\u53d6\u4e00\u884c\u6570\u636e\uff0c\u5e76\u4ee5\u6362\u884c\u7b26\u7ed3\u675f\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u8bfb\u53d6\u5b57\u8282\u7684\u5b57\u8282\u5bf9\u8c61\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#readinto","title":"<code>readinto</code> \u65b9\u6cd5","text":"<pre><code>UART.readinto(buf[, nbytes])\n</code></pre> <p>\u5c06\u5b57\u8282\u8bfb\u53d6\u5230 <code>buf</code> \u4e2d\u3002\u5982\u679c\u6307\u5b9a\u4e86 <code>nbytes</code>\uff0c\u5219\u6700\u591a\u8bfb\u53d6\u8be5\u6570\u91cf\u7684\u5b57\u8282\uff1b\u5426\u5219\uff0c\u6700\u591a\u8bfb\u53d6 <code>len(buf)</code> \u6570\u91cf\u7684\u5b57\u8282\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>buf</code>: \u4e00\u4e2a\u7f13\u51b2\u533a\u5bf9\u8c61\u3002</li> <li><code>nbytes</code>: \u6700\u591a\u8bfb\u53d6\u7684\u5b57\u8282\u6570\uff0c\u53ef\u9009\u53c2\u6570\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u8bfb\u53d6\u5e76\u5b58\u5165 <code>buf</code> \u7684\u5b57\u8282\u6570\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#write","title":"<code>write</code> \u65b9\u6cd5","text":"<pre><code>UART.write(buf)\n</code></pre> <p>\u5c06\u5b57\u8282\u7f13\u51b2\u533a\u5199\u5165 UART\u3002</p> <p>\u53c2\u6570</p> <ul> <li><code>buf</code>: \u4e00\u4e2a\u7f13\u51b2\u533a\u5bf9\u8c61\u3002</li> </ul> <p>\u8fd4\u56de\u503c</p> <p>\u8fd4\u56de\u5199\u5165\u7684\u5b57\u8282\u6570\u3002</p>"},{"location":"EmbeddedSoft/CV/K230/Peripherals/UART/#deinit","title":"<code>deinit</code> \u65b9\u6cd5","text":"<pre><code>UART.deinit()\n</code></pre> <p>\u91ca\u653e UART \u8d44\u6e90\u3002</p> <p>\u53c2\u6570</p> <p>\u65e0</p> <p>\u8fd4\u56de\u503c</p> <p>\u65e0</p>"},{"location":"EmbeddedSoft/Connective/Connective/","title":"\u76ee\u5f55","text":"<ul> <li>I2C</li> <li>USART</li> <li>SPI</li> <li>CAN</li> </ul>"},{"location":"EmbeddedSoft/Connective/I2C/","title":"I2C","text":""},{"location":"EmbeddedSoft/Connective/I2C/#i2c","title":"\u4e00\u3001 I2C \u534f\u8bae\u7b80\u4ecb","text":"<p>I2C\uff08Inter-Integrated Circuit\uff09\u662f\u7531\u98de\u5229\u6d66\u516c\u53f8\u5f00\u53d1\u7684\u4e00\u79cd\u7b80\u5355\u7684\u3001\u53cc\u5411\u3001\u4e8c\u7ebf\u5236\u3001\u540c\u6b65\u4e32\u884c\u603b\u7ebf\u3002</p> <ul> <li>\u53cc\u5411\uff1a\u6570\u636e\u53ef\u4ee5\u5728\u4e3b\u8bbe\u5907\u548c\u4ece\u8bbe\u5907\u4e4b\u95f4\u53cc\u5411\u4f20\u8f93\u3002</li> <li>\u4e8c\u7ebf\u5236\uff1a\u4ec5\u9700\u8981\u4e24\u6839\u4fe1\u53f7\u7ebf\u2014\u2014\u4e32\u884c\u6570\u636e\u7ebf\uff08SDA\uff09 \u548c\u4e32\u884c\u65f6\u949f\u7ebf\uff08SCL\uff09\u3002\u8fd9\u6781\u5927\u5730\u51cf\u5c11\u4e86\u82af\u7247\u7ba1\u811a\u6570\u91cf\u548cPCB\u8d70\u7ebf\u3002</li> <li>\u540c\u6b65\uff1a\u901a\u4fe1\u53cc\u65b9\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u949f\u4fe1\u53f7\uff08SCL\uff09\u6765\u540c\u6b65\u6570\u636e\u4f4d\uff08SDA\uff09\u7684\u4f20\u8f93\u8282\u594f\u3002</li> <li>\u591a\u4e3b\u4ece\uff1a\u603b\u7ebf\u652f\u6301\u591a\u4e2a\u4e3b\u8bbe\u5907\uff08Master\uff09\u548c\u591a\u4e2a\u4ece\u8bbe\u5907\uff08Slave\uff09\uff0c\u901a\u8fc7\u5730\u5740\u5bfb\u5740\u3002</li> <li>\u4f4e\u901f\uff1a\u901a\u5e38\u7528\u4e8e\u677f\u5185IC\u4e4b\u95f4\u7684\u77ed\u8ddd\u79bb\u901a\u4fe1\uff0c\u901f\u5ea6\u4ece\u6807\u51c6\u6a21\u5f0f\uff08100kbps\uff09\u5230\u8d85\u5feb\u901f\u6a21\u5f0f\uff085Mbps\uff09\u4e0d\u7b49\u3002</li> </ul>"},{"location":"EmbeddedSoft/Connective/I2C/#_1","title":"\u4e8c\u3001 \u786c\u4ef6\u8fde\u63a5","text":"<ul> <li>SDA\uff08Serial Data Line\uff09\uff1a\u4e32\u884c\u6570\u636e\u7ebf\uff0c\u7528\u4e8e\u4f20\u8f93\u6570\u636e\u3002</li> <li>SCL\uff08Serial Clock Line\uff09\uff1a\u4e32\u884c\u65f6\u949f\u7ebf\uff0c\u7528\u4e8e\u4ea7\u751f\u540c\u6b65\u65f6\u949f\u3002</li> </ul> <p>\u6240\u6709\u8bbe\u5907\u90fd\u5e76\u8054\u5728\u603b\u7ebf\u4e0a\uff0c\u5e76\u4e14\u603b\u7ebf\u63a5\u53e3\u662f\u5f00\u6f0f drain\uff08OD:Output Drain) \u6216 \u96c6\u7535\u6781\u5f00\u8def\uff08OC\uff09 \u7ed3\u6784\u3002\u89c4\u5b9a\u5728SDA\u548cSCL\u7ebf\u4e0a\u5404\u63a5\u4e00\u4e2a\u4e0a\u62c9\u7535\u963b\uff08Rp\uff09 \u5230\u7535\u6e90VCC\uff0c\u5728\u91ca\u653e\u603b\u7ebf\u65f6\u4fdd\u6301\u9ad8\u7535\u5e73\u3002</p> <ul> <li>\u4f5c\u7528\uff1a<ol> <li>\u5f53\u6ca1\u6709\u8bbe\u5907\u4e3b\u52a8\u62c9\u4f4e\u7535\u5e73\u65f6\uff0c\u7531\u4e0a\u62c9\u7535\u963b\u5c06\u603b\u7ebf\u7f6e\u4e8e\u9ad8\u7535\u5e73\uff08\u7a7a\u95f2\u72b6\u6001\uff09\u3002</li> <li>\u5141\u8bb8\u4e0d\u540c\u7535\u538b\u7b49\u7ea7\u7684\u5668\u4ef6\u5728\u603b\u7ebf\u4e0a\u5171\u5b58\uff08\u53ea\u8981\u5b83\u4eec\u7684VCC\u4e0d\u540c\uff09\u3002</li> </ol> </li> </ul>"},{"location":"EmbeddedSoft/Connective/I2C/#_2","title":"\u4e09\u3001 \u6838\u5fc3\u6982\u5ff5","text":"<ol> <li> <p>\u4e3b\u8bbe\u5907\uff08Master\uff09</p> <ul> <li>\u53d1\u8d77\u548c\u7ec8\u6b62\u4f20\u8f93\u7684\u8bbe\u5907\u3002</li> <li>\u4ea7\u751f\u65f6\u949f\u4fe1\u53f7SCL\u3002</li> <li>\u8d1f\u8d23\u5bfb\u5740\u4ece\u8bbe\u5907\u3002</li> </ul> </li> <li> <p>\u4ece\u8bbe\u5907\uff08Slave\uff09</p> <ul> <li>\u88ab\u4e3b\u8bbe\u5907\u5bfb\u5740\u7684\u8bbe\u5907\u3002</li> <li>\u54cd\u5e94\u4e3b\u8bbe\u5907\u7684\u547d\u4ee4\u3002</li> </ul> </li> <li> <p>\u5730\u5740\uff08Address\uff09</p> <ul> <li>\u6bcf\u4e2a\u4ece\u8bbe\u5907\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u76847\u4f4d\u621610\u4f4d\u5730\u5740\uff0c\u7528\u4e8e\u4e3b\u8bbe\u5907\u7684\u5bfb\u5740\u3002</li> <li>7\u4f4d\u5730\u5740\u662f\u6700\u5e38\u89c1\u7684\u683c\u5f0f\uff0c\u7406\u8bba\u4e0a\u53ef\u8fde\u63a5127\u4e2a\u8bbe\u5907\uff08\u5730\u57400x00\u4fdd\u7559\uff0c\u6709\u4e9b\u5730\u5740\u6709\u7279\u6b8a\u7528\u9014\uff09\u3002</li> <li>10\u4f4d\u5730\u5740\u7528\u4e8e\u6269\u5c55\u66f4\u591a\u8bbe\u5907\u3002</li> </ul> </li> <li> <p>\u901f\u5ea6\u6a21\u5f0f</p> <ul> <li>\u6807\u51c6\u6a21\u5f0f\uff08Standard-mode\uff09\uff1a 100 kbit/s</li> <li>\u5feb\u901f\u6a21\u5f0f\uff08Fast-mode\uff09\uff1a 400 kbit/s</li> <li>\u5feb\u901f\u6a21\u5f0f+\uff08Fast-mode Plus\uff09\uff1a 1 Mbit/s</li> <li>\u9ad8\u901f\u6a21\u5f0f\uff08High-speed mode\uff09\uff1a 3.4 Mbit/s</li> <li>\u8d85\u5feb\u901f\u6a21\u5f0f\uff08Ultra Fast-mode\uff09\uff1a 5 Mbit/s</li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/Connective/I2C/#_3","title":"\u56db\u3001 \u901a\u4fe1\u6d41\u7a0b\u4e0e\u5e27\u683c\u5f0f","text":"<p>\u4e00\u6b21\u5b8c\u6574\u7684I2C\u901a\u4fe1\u5305\u542b\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a</p> <p>\u8d77\u59cb\u6761\u4ef6\uff08START\uff09 -&gt; \u4ece\u673a\u5730\u5740\u5e27\uff08Address Frame\uff09+ \u8bfb\u5199\u4f4d\uff08R/W#\uff09+ \u5e94\u7b54\u4f4d\uff08ACK/NACK\uff09 -&gt; \u6570\u636e\u5e27\uff08Data Frames\uff09+ \u5e94\u7b54\u4f4d\uff08ACK/NACK\uff09 -&gt; \u505c\u6b62\u6761\u4ef6\uff08STOP\uff09</p> <p>![[../photosource/I2C-Bits-Protocol.jpg]]</p>"},{"location":"EmbeddedSoft/Connective/I2C/#1-sp","title":"1. \u8d77\u59cb\uff08S\uff09\u548c\u505c\u6b62\uff08P\uff09\u6761\u4ef6","text":"<ul> <li>\u8d77\u59cb\u6761\u4ef6\uff08S\uff09\uff1a \u5f53SCL\u4e3a\u9ad8\u7535\u5e73\u65f6\uff0cSDA\u7ebf\u51fa\u73b0\u4e00\u4e2a\u4e0b\u964d\u6cbf\u3002<ul> <li>\u8868\u793a\u4e00\u6b21\u4f20\u8f93\u7684\u5f00\u59cb\u3002</li> </ul> </li> </ul> <ul> <li>\u505c\u6b62\u6761\u4ef6\uff08P\uff09\uff1a \u5f53SCL\u4e3a\u9ad8\u7535\u5e73\u65f6\uff0cSDA\u7ebf\u51fa\u73b0\u4e00\u4e2a\u4e0a\u5347\u6cbf\u3002<ul> <li>\u8868\u793a\u4e00\u6b21\u4f20\u8f93\u7684\u7ed3\u675f\u3002 </li> </ul> </li> </ul> <p>\u6ce8\u610f\uff1a\u5728SCL\u4e3a\u9ad8\u671f\u95f4\uff0cSDA\u7684\u7a33\u5b9a\u53d8\u5316\u88ab\u7528\u4e8e\u8868\u793a\u8d77\u59cb\u548c\u505c\u6b62\u6761\u4ef6\uff0c\u5728\u6570\u636e\u4f20\u8f93\u671f\u95f4\uff0cSDA\u7684\u53d8\u5316\u5fc5\u987b\u53d1\u751f\u5728SCL\u4e3a\u4f4e\u65f6\u3002</p>"},{"location":"EmbeddedSoft/Connective/I2C/#_4","title":"*\u57fa\u672c\u5355\u5143","text":""},{"location":"EmbeddedSoft/Connective/I2C/#2","title":"2.\u5730\u5740\u5e27 + \u8bfb\u5199\u4f4d","text":"<p>\u8d77\u59cb\u6761\u4ef6\u540e\uff0c\u4e3b\u8bbe\u5907\u53d1\u9001\u7684\u7b2c\u4e00\u4e2a\u5b57\u8282\u662f\u5730\u5740\u5e27\u3002</p> <ul> <li>7\u4f4d\u5730\u5740\u683c\u5f0f\uff1a<ul> <li>\u8fd9\u4e2a\u5b57\u8282\u7531 7\u4f4d\u4ece\u673a\u5730\u5740 \u548c 1\u4f4d\u8bfb\u5199\u63a7\u5236\u4f4d\uff08R/W#\uff09 \u7ec4\u6210\u3002</li> <li>\u4f4d\u5e8f\uff1a MSB\uff08\u6700\u9ad8\u6709\u6548\u4f4d\uff09\u5148\u884c\u3002</li> <li>R/W#\u4f4d\uff1a<ul> <li>0\uff1a \u8868\u793a\u4e3b\u8bbe\u5907\u5199\uff08Write\uff09 \u6570\u636e\u5230\u4ece\u8bbe\u5907\u3002</li> <li>1\uff1a \u8868\u793a\u4e3b\u8bbe\u5907\u8bfb\uff08Read\uff09 \u4ece\u8bbe\u5907\u7684\u6570\u636e\u3002</li> </ul> </li> </ul> </li> </ul> BIT 7 BIT 6 BIT 5 BIT 4 BIT 3 BIT 2 BIT 1 BIT 0 \u63cf\u8ff0 A6 A5 A4 A3 A2 A1 A0 R/W# 7\u4f4d\u5730\u5740 + \u8bfb\u5199\u4f4d"},{"location":"EmbeddedSoft/Connective/I2C/#3-acknack","title":"3. \u5e94\u7b54\uff08ACK\uff09\u4e0e\u975e\u5e94\u7b54\uff08NACK\uff09\u4f4d","text":"<p>\u6bcf\u4e2a\u5b57\u8282\uff08\u5305\u62ec\u5730\u5740\u5e27\u548c\u6240\u6709\u6570\u636e\u5e27\uff09\u4f20\u8f93\u5b8c\u6bd5\u540e\uff0c\u540e\u9762\u90fd\u7d27\u8ddf\u7740\u4e00\u4e2a\u5e94\u7b54\u4f4d\u3002</p> <ul> <li>\u65f6\u949f\uff1a \u5e94\u7b54\u4f4d\u7531\u63a5\u6536\u65b9\u5728\u7b2c9\u4e2a\u65f6\u949f\u8109\u51b2\u671f\u95f4\u63a7\u5236SDA\u7ebf\u3002</li> <li>\u5e94\u7b54\uff08ACK\uff09\uff1a<ul> <li>\u63a5\u6536\u65b9\u5c06SDA\u7ebf\u62c9\u4f4e\u3002</li> <li>\u8868\u793a\u63a5\u6536\u65b9\u5df2\u6210\u529f\u63a5\u6536\u5230\u8be5\u5b57\u8282\uff0c\u5e76\u51c6\u5907\u597d\u63a5\u6536\u4e0b\u4e00\u4e2a\u5b57\u8282\u3002</li> </ul> </li> <li>\u975e\u5e94\u7b54\uff08NACK\uff09\uff1a<ul> <li>\u63a5\u6536\u65b9\u4fdd\u6301SDA\u7ebf\u4e3a\u9ad8\u3002</li> <li>\u8868\u793a\uff1a<ol> <li>\u63a5\u6536\u65b9\u672a\u6210\u529f\u63a5\u6536\u6570\u636e\uff08\u5982CRC\u9519\u8bef\uff09\u3002</li> <li>\u63a5\u6536\u65b9\u65e0\u6cd5\u63a5\u6536\u66f4\u591a\u6570\u636e\u3002</li> <li>\u4e3b\u8bbe\u5907\u5728\u8bfb\u64cd\u4f5c\u65f6\uff0c\u53d1\u9001NACK\u6765\u544a\u77e5\u4ece\u8bbe\u5907\u8fd9\u662f\u6700\u540e\u4e00\u4e2a\u8981\u8bfb\u53d6\u7684\u5b57\u8282\uff0c\u968f\u540e\u4e3b\u8bbe\u5907\u5e94\u4ea7\u751f\u505c\u6b62\u6761\u4ef6\u3002</li> </ol> </li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/Connective/I2C/#4","title":"4. \u6570\u636e\u5e27","text":"<p>\u5730\u5740\u5e27\u88ab\u5e94\u7b54\u540e\uff0c\u5f00\u59cb\u4f20\u8f93\u6570\u636e\u5e27\u3002</p> <ul> <li>\u6bcf\u4e2a\u6570\u636e\u5e27\u4e3a8\u4f4d\uff081\u4e2a\u5b57\u8282\uff09\u3002</li> <li>\u540c\u6837\u662fMSB\u5148\u884c\u3002</li> <li>\u6bcf\u4e2a\u6570\u636e\u5e27\u540e\u90fd\u7d27\u8ddf\u7740\u4e00\u4e2a\u5e94\u7b54\u4f4d\uff08ACK/NACK\uff09\uff0c\u7531\u6570\u636e\u63a5\u6536\u65b9\u53d1\u51fa\u3002<ul> <li>\u5199\u64cd\u4f5c\uff1a \u4ece\u8bbe\u5907\u662f\u63a5\u6536\u65b9\uff0c\u6545\u4ece\u8bbe\u5907\u53d1ACK\u3002</li> <li>\u8bfb\u64cd\u4f5c\uff1a \u4e3b\u8bbe\u5907\u662f\u63a5\u6536\u65b9\uff0c\u6545\u4e3b\u8bbe\u5907\u53d1ACK\u3002</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/Connective/I2C/#_5","title":"\u4e94\u3001 \u5b8c\u6574\u7684\u901a\u4fe1\u5e8f\u5217","text":""},{"location":"EmbeddedSoft/Connective/I2C/#_6","title":"\u4e3b\u8bbe\u5907\u5411\u4ece\u8bbe\u5907\u5199\u5165\u5b57\u8282","text":"<ol> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u8d77\u59cb\u6761\u4ef6\uff08S\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90017\u4f4d\u4ece\u673a\u5730\u5740 + \u5199\u4f4d\uff080\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90018\u4f4d\u6570\u636e\uff08\u901a\u5e38\u662f\u5bc4\u5b58\u5668\u5730\u5740\u6216\u8005\u547d\u4ee4\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90018\u4f4d\u6570\u636e\uff08\u6307\u9488\u81ea\u52a8\u540e\u79fb\u4e00\u4e2a\u5b57\u8282\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>...(\u91cd\u590d5~6\uff0c\u76f4\u5230\u6570\u636e\u53d1\u9001\u5b8c\u6bd5)</li> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u505c\u6b62\u6761\u4ef6\uff08P\uff09\u3002</li> </ol>"},{"location":"EmbeddedSoft/Connective/I2C/#_7","title":"\u5355\u5b57\u8282\u5b9e\u4f8b","text":""},{"location":"EmbeddedSoft/Connective/I2C/#_8","title":"I2C","text":""},{"location":"EmbeddedSoft/Connective/I2C/#_9","title":"\u4e3b\u8bbe\u5907\u4ece\u4ece\u8bbe\u5907\u8bfb\u53d6\u5b57\u8282","text":"<ol> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u8d77\u59cb\u6761\u4ef6\uff08S\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90017\u4f4d\u4ece\u673a\u5730\u5740 + \u8bfb\u4f4d\uff081\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u53d1\u90018\u4f4d\u6570\u636e\uff08\u6570\u636e\u57fa\u4e8e\u4e0a\u6b21\u901a\u4fe1\u65f6\u7684\u5185\u5b58\u5730\u5740\u6307\u9488\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u53d1\u90018\u4f4d\u6570\u636e\uff08\u6307\u9488\u81ea\u52a8\u540e\u79fb\u4e00\u4e2a\u5b57\u8282\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u8fd4\u56de\u5e94\u7b54\u4f4d ACK\uff080\uff09\u3002</li> <li>...(\u91cd\u590d6~7)</li> <li>\u4ece\u8bbe\u5907\u53d1\u90018\u4f4d\u6570\u636e</li> <li>\u4e3b\u8bbe\u5907\u8fd4\u56de\u975e\u5e94\u7b54\u4f4d NACK\uff081\uff09\uff08\u7ed3\u675f\u8bfb\u53d6\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u505c\u6b62\u6761\u4ef6\uff08P\uff09\u3002</li> </ol>"},{"location":"EmbeddedSoft/Connective/I2C/#_10","title":"\u590d\u5408\u683c\u5f0f\uff08\u6700\u5e38\u7528\uff09","text":"<p>\u4e3b\u8bbe\u5907\u5148\u5199\u5165\u4ece\u8bbe\u5907\u7684\u5185\u90e8\u5bc4\u5b58\u5668\u5730\u5740\uff0c\u7136\u540e\u91cd\u65b0\u542f\u52a8\u5e76\u8bfb\u53d6\u6570\u636e\u3002\u8fd9\u5728\u64cd\u4f5c\u4f20\u611f\u5668\u3001EEPROM\u7b49\u8bbe\u5907\u65f6\u975e\u5e38\u5e38\u89c1\u3002</p> <ol> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u8d77\u59cb\u6761\u4ef6\uff08S\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90017\u4f4d\u4ece\u673a\u5730\u5740 + \u5199\u4f4d\uff080\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56deACK\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u9001\u8981\u5199\u5165\u7684\u5bc4\u5b58\u5668\u5730\u5740\uff08\u4f8b\u5982 0x00\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56deACK\u3002</li> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u91cd\u590d\u8d77\u59cb\u6761\u4ef6\uff08Sr\uff09\uff08\u529f\u80fd\u548c\u8d77\u59cb\u6761\u4ef6S\u4e00\u6837\uff0c\u4f46\u4e0d\u4f1a\u5148\u4ea7\u751f\u505c\u6b62\u6761\u4ef6\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u53d1\u90017\u4f4d\u4ece\u673a\u5730\u5740 + \u8bfb\u4f4d\uff081\uff09\u3002</li> <li>\u4ece\u8bbe\u5907\u8fd4\u56deACK\u3002</li> <li>\u4ece\u8bbe\u5907\u53d1\u9001\u5bc4\u5b58\u56680x00\u4e2d\u7684\u6570\u636e\u3002</li> <li>\u4e3b\u8bbe\u5907\u8fd4\u56deNACK\uff08\u8868\u793a\u8bfb\u53d6\u7ed3\u675f\uff0c\u4e5f\u53ef\u5411\u4e0a\u9762\u4f8b\u5b50\u4e00\u6837\uff0c\u8fd4\u56de0\uff0c\u76f4\u5230\u8bfb\u5b8c\u6240\u6709\u6570\u636e\u518d\u8fd4\u56de1\uff09\u3002</li> <li>\u4e3b\u8bbe\u5907\u4ea7\u751f\u505c\u6b62\u6761\u4ef6\uff08P\uff09\u3002 ![[../photosource/I2C\u6307\u5b9a\u5730\u5740\u8bfb.png]]</li> </ol>"},{"location":"EmbeddedSoft/Connective/I2C/#clock-stretching","title":"\u516d\u3001 \u65f6\u949f\u62c9\u4f38\uff08Clock Stretching\uff09","text":"<ul> <li>\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u8bbe\u5907\u5982\u679c\u6765\u4e0d\u53ca\u5904\u7406\uff08\u4f8b\u5982\u6b63\u5728\u5904\u7406\u4e2d\u65ad\uff09\uff0c\u5b83\u53ef\u4ee5\u5728SCL\u7ebf\u4e0a\u6267\u884c\u65f6\u949f\u62c9\u4f38\u3002</li> <li>\u5177\u4f53\u505a\u6cd5\u662f\uff1a\u4ece\u8bbe\u5907\u5728\u63a5\u6536\u5230\u4e00\u4e2a\u4f4d\u540e\uff0c\u4e3b\u52a8\u62c9\u4f4eSCL\u7ebf\uff0c\u5f3a\u5236\u5c06\u65f6\u949f\u4fdd\u6301\u4e3a\u4f4e\u7535\u5e73\u3002\u8fd9\u4f1a\u6682\u505c\u6574\u4e2a\u901a\u4fe1\uff0c\u76f4\u5230\u4ece\u8bbe\u5907\u51c6\u5907\u597d\u5e76\u91ca\u653eSCL\u7ebf\uff0c\u65f6\u949f\u624d\u6062\u590d\u4e3a\u9ad8\u3002</li> <li>\u8fd9\u662f\u4e00\u79cd\u4ece\u8bbe\u5907\u63a7\u5236\u901a\u4fe1\u8282\u594f\u7684\u6d41\u63a7\u673a\u5236\u3002</li> </ul> <p>\u6ce2\u5f62\u793a\u610f\u56fe\uff1a</p> <pre><code>SDA   ___/X_X_X_X_..._X_X_________________...\nSCL   ~~~|_|_|_|_|...|_|\\\\\\\\\\\\\\\\\\\\________... (\u4ece\u673a\u62c9\u4f4eSCL)\n        |             | |        |\n     \u6b63\u5e38\u65f6\u949f        \u4ece\u673a\u5f00\u59cb   \u4ece\u673a\u91ca\u653eSCL\uff0c\n                      \u62c9\u4f38\u65f6\u949f   \u901a\u4fe1\u7ee7\u7eed\n</code></pre>"},{"location":"EmbeddedSoft/Connective/I2C/#_11","title":"\u4e03\u3001 \u603b\u7ed3\u4e0e\u8981\u70b9","text":"\u9879\u76ee \u63cf\u8ff0 \u4fe1\u53f7\u7ebf SDA\uff08\u6570\u636e\uff09\uff0c SCL\uff08\u65f6\u949f\uff09 \u62d3\u6251 \u6240\u6709\u8bbe\u5907\u5e76\u8054\uff0c\u9700\u4e0a\u62c9\u7535\u963b \u5bfb\u5740 7\u4f4d\u621610\u4f4d\u5730\u5740\uff0c\u652f\u6301\u591a\u4e3b\u4ece \u8d77\u59cb\u6761\u4ef6 SCL\u9ad8\u65f6\uff0cSDA\u4e0b\u964d\u6cbf \u505c\u6b62\u6761\u4ef6 SCL\u9ad8\u65f6\uff0cSDA\u4e0a\u5347\u6cbf \u6570\u636e\u6709\u6548\u6027 SCL\u9ad8\u671f\u95f4\uff0cSDA\u5fc5\u987b\u4fdd\u6301\u7a33\u5b9a \u6570\u636e\u53d8\u5316 SCL\u4f4e\u671f\u95f4\uff0cSDA\u53ef\u4ee5\u53d8\u5316 \u5b57\u8282\u683c\u5f0f 8\u4f4d\u6570\u636e\uff0cMSB\u5148\u884c\uff0c\u540e\u8ddf1\u4f4dACK/NACK ACK \u63a5\u6536\u65b9\u62c9\u4f4eSDA\uff0c\u8868\u793a\u6210\u529f NACK \u63a5\u6536\u65b9\u4e0d\u62c9\u4f4eSDA\uff0c\u8868\u793a\u5931\u8d25\u6216\u7ed3\u675f"},{"location":"EmbeddedSoft/Connective/SPI/","title":"\u4e00\u3001 SPI \u534f\u8bae\u7b80\u4ecb","text":"<p>SPI\uff08Serial Peripheral Interface\uff09\u662f\u7531\u6469\u6258\u7f57\u62c9\u516c\u53f8\u5f00\u53d1\u7684\u4e00\u79cd\u5168\u53cc\u5de5\u3001\u540c\u6b65\u3001\u56db\u7ebf\u5236\u7684\u4e32\u884c\u901a\u4fe1\u603b\u7ebf\u3002\u5b83\u4ee5\u5176\u7b80\u5355\u6027\u548c\u9ad8\u901f\u5ea6\u800c\u95fb\u540d\u3002</p> <ul> <li> <p>\u5168\u53cc\u5de5\uff1a\u6570\u636e\u53ef\u4ee5\u540c\u65f6\u5728\u4e3b\u8bbe\u5907\u548c\u4ece\u8bbe\u5907\u4e4b\u95f4\u53cc\u5411\u4f20\u8f93\u3002</p> </li> <li> <p>\u540c\u6b65\uff1a\u901a\u4fe1\u53cc\u65b9\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u949f\u4fe1\u53f7\uff08SCLK\uff09\u6765\u540c\u6b65\u6570\u636e\u4f4d\uff08MOSI, MISO\uff09\u7684\u4f20\u8f93\u8282\u594f\u3002</p> </li> <li> <p>\u56db\u7ebf\u5236\uff1a\u901a\u5e38\u9700\u8981\u56db\u6839\u4fe1\u53f7\u7ebf\uff08\u6709\u65f6\u53ef\u7cbe\u7b80\u4e3a\u4e09\u7ebf\uff09\u3002</p> </li> <li> <p>\u9ad8\u901f\uff1a\u76f8\u6bd4 I2C\uff0cSPI \u53ef\u4ee5\u8fbe\u5230\u5f88\u9ad8\u7684\u901f\u5ea6\uff08\u51e0\u5341 Mbps \u751a\u81f3\u4e0a\u767e Mbps\uff09\uff0c\u6ca1\u6709\u6807\u51c6\u4e0a\u9650\u3002\uff08\u63a8\u633d\u8f93\u51fa\u80fd\u529b\u5f3a\uff09</p> </li> <li> <p>\u65e0\u5bfb\u5740\uff1a\u901a\u8fc7\u786c\u4ef6\u7247\u9009\uff08CS\uff09\u7ebf\u9009\u62e9\u4ece\u8bbe\u5907\uff0c\u4e0d\u652f\u6301\u8f6f\u4ef6\u5bfb\u5740\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/Connective/SPI/#_1","title":"\u4e8c\u3001 \u786c\u4ef6\u8fde\u63a5\u4e0e\u4fe1\u53f7\u7ebf","text":"<p> SPI \u901a\u4fe1\u81f3\u5c11\u9700\u8981\u56db\u6839\u7ebf\uff1a</p> \u4fe1\u53f7\u7ebf\u540d\u79f0 \u5168\u79f0 \u65b9\u5411 \u63cf\u8ff0 SCLK / SCK Serial Clock \u4e3b \u2192 \u4ece \u4e3b\u8bbe\u5907\u4ea7\u751f\u7684\u540c\u6b65\u65f6\u949f\u4fe1\u53f7\u3002 MOSI / SDO Master Out Slave In \u4e3b \u2192 \u4ece \u6570\u636e\u4ece\u4e3b\u8bbe\u5907\u8f93\u51fa\uff0c\u8f93\u5165\u5230\u4ece\u8bbe\u5907\u3002 MISO / SDI Master In Slave Out \u4ece \u2192 \u4e3b \u6570\u636e\u4ece\u4ece\u8bbe\u5907\u8f93\u51fa\uff0c\u8f93\u5165\u5230\u4e3b\u8bbe\u5907\u3002 CS / SS Chip Select / Slave Select \u4e3b \u2192 \u4ece \u4e3b\u8bbe\u5907\u7528\u4e8e\u9009\u62e9\u76ee\u6807\u4ece\u8bbe\u5907\u7684\u4fe1\u53f7\uff08\u4f4e\u7535\u5e73\u6709\u6548\uff09\u3002 <p>[!NOTE]</p> <p>\u8f93\u51fa\u5f15\u811a\u914d\u7f6e\u4e3a\u63a8\u633d\u8f93\u51fa\uff0c\u8f93\u5165\u5f15\u811a\u914d\u7f6e\u4e3a\u6d6e\u7a7a\u6216\u4e0a\u62c9\u8f93\u5165\u3002</p> <p>\u5f53\u4ece\u673a\u672a\u88ab\u9009\u4e2d\u65f6\uff0c\u5176\u8f93\u51fa\u5f15\u811aMISO\u5fc5\u987b\u4e3a\u9ad8\u963b\u6001\uff08\u65ad\u5f00\uff09</p>"},{"location":"EmbeddedSoft/Connective/SPI/#_2","title":"\u4e09\u3001 \u6838\u5fc3\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/Connective/SPI/#_3","title":"\u79fb\u4f4d\u793a\u610f","text":""},{"location":"EmbeddedSoft/Connective/SPI/#1-cpolcpha","title":"1. \u65f6\u949f\u6781\u6027\uff08CPOL\uff09\u4e0e\u65f6\u949f\u76f8\u4f4d\uff08CPHA\uff09","text":"<p>\u8fd9\u662f SPI \u6700\u6838\u5fc3\u4e5f\u6700\u5bb9\u6613\u6df7\u6dc6\u7684\u6982\u5ff5\uff0c\u5b83\u4eec\u5171\u540c\u5b9a\u4e49\u4e86\u65f6\u949f\u7684\u7a33\u5b9a\u7535\u5e73\u548c\u6570\u636e\u91c7\u6837\u7684\u8fb9\u6cbf\u3002\u6709\u56db\u79cd\u53ef\u80fd\u7684\u6a21\u5f0f\u3002</p> <ul> <li> <p>\u65f6\u949f\u6781\u6027 CPOL\uff1a</p> <ul> <li> <p>CPOL = 0\uff1a \u65f6\u949f\u7a7a\u95f2\u72b6\u6001\uff08\u5f53 CS \u6709\u6548\u4f46\u65e0\u6570\u636e\u4f20\u8f93\u65f6\uff09\u4e3a\u4f4e\u7535\u5e73\u3002</p> </li> <li> <p>CPOL = 1\uff1a \u65f6\u949f\u7a7a\u95f2\u72b6\u6001\u4e3a\u9ad8\u7535\u5e73\u3002</p> </li> </ul> </li> <li> <p>\u65f6\u949f\u76f8\u4f4d CPHA\uff1a</p> <ul> <li> <p>CPHA = 0\uff1a \u6570\u636e\u5728\u65f6\u949f\u7684\u7b2c\u4e00\u4e2a\u8fb9\u6cbf\uff08\u5373 SCK \u4ece\u7a7a\u95f2\u72b6\u6001\u53d8\u5316\u7684\u7b2c\u4e00\u4e2a\u8fb9\u6cbf\uff09\u88ab\u91c7\u6837\u3002</p> </li> <li> <p>CPHA = 1\uff1a \u6570\u636e\u5728\u65f6\u949f\u7684\u7b2c\u4e8c\u4e2a\u8fb9\u6cbf\uff08\u5373 SCK \u4ece\u7a7a\u95f2\u72b6\u6001\u53d8\u5316\u7684\u7b2c\u4e8c\u4e2a\u8fb9\u6cbf\uff09\u88ab\u91c7\u6837\u3002</p> </li> </ul> </li> </ul> <p>\u56db\u79cd SPI \u6a21\u5f0f\u603b\u7ed3\uff1a</p> \u6a21\u5f0f CPOL CPHA \u65f6\u949f\u7a7a\u95f2\u72b6\u6001 \u6570\u636e\u91c7\u6837\u65f6\u523b \u6570\u636e\u79fb\u4f4d/\u53d8\u5316\u65f6\u523b Mode 0 0 0 Low \u7b2c\u4e00\u4e2a\u65f6\u949f\u8fb9\u6cbf\uff08\u4e0a\u5347\u6cbf\uff09 \u5728\u4e0b\u964d\u6cbf\uff08\u7b2c\u4e8c\u4e2a\u8fb9\u6cbf\uff09 Mode 1 0 1 Low \u7b2c\u4e8c\u4e2a\u65f6\u949f\u8fb9\u6cbf\uff08\u4e0b\u964d\u6cbf\uff09 \u5728\u4e0a\u5347\u6cbf\uff08\u7b2c\u4e00\u4e2a\u8fb9\u6cbf\uff09 Mode 2 1 0 High \u7b2c\u4e00\u4e2a\u65f6\u949f\u8fb9\u6cbf\uff08\u4e0b\u964d\u6cbf\uff09 \u5728\u4e0a\u5347\u6cbf\uff08\u7b2c\u4e8c\u4e2a\u8fb9\u6cbf\uff09 Mode 3 1 1 High \u7b2c\u4e8c\u4e2a\u65f6\u949f\u8fb9\u6cbf\uff08\u4e0a\u5347\u6cbf\uff09 \u5728\u4e0b\u964d\u6cbf\uff08\u7b2c\u4e00\u4e2a\u8fb9\u6cbf\uff09 <p>\u5173\u952e\u8bb0\u5fc6\u70b9\uff1aMode 0\u00a0\u548c\u00a0Mode 3\u00a0\u662f\u6700\u5e38\u7528\u7684\u4e24\u79cd\u6a21\u5f0f\u3002\u4e3b\u8bbe\u5907\u548c\u4ece\u8bbe\u5907\u7684 CPOL \u4e0e CPHA \u8bbe\u7f6e\u5fc5\u987b\u5b8c\u5168\u4e00\u81f4\uff0c\u5426\u5219\u65e0\u6cd5\u6b63\u5e38\u901a\u4fe1\u3002</p>"},{"location":"EmbeddedSoft/Connective/SPI/#2-bit-order","title":"2. \u6570\u636e\u987a\u5e8f\uff08Bit Order\uff09","text":"<ul> <li>\u53ef\u4ee5\u914d\u7f6e\u4e3a\u00a0MSB First\uff08\u6700\u9ad8\u4f4d\u5148\u884c\uff09\u6216\u00a0LSB First\uff08\u6700\u4f4e\u4f4d\u5148\u884c\uff09\u3002MSB First \u662f\u9ed8\u8ba4\u548c\u6700\u5e38\u89c1\u7684\u65b9\u5f0f\u3002</li> </ul>"},{"location":"EmbeddedSoft/Connective/SPI/#3-frame-size","title":"3. \u6570\u636e\u5e27\u5927\u5c0f\uff08Frame Size\uff09","text":"<ul> <li>\u901a\u5e38\u4e3a\u00a08\u4f4d\uff081\u4e2a\u5b57\u8282\uff09\uff0c\u4f46\u8bb8\u591a\u73b0\u4ee3\u63a7\u5236\u5668\u652f\u6301\u4efb\u610f\u957f\u5ea6\u7684\u6570\u636e\u5e27\uff08\u5982 4\u4f4d\uff0c 12\u4f4d\uff0c 16\u4f4d\u7b49\uff09\u3002</li> </ul>"},{"location":"EmbeddedSoft/Connective/SPI/#_4","title":"\u56db\u3001 \u901a\u4fe1\u6d41\u7a0b\u4e0e\u5e27\u683c\u5f0f","text":"<p>SPI \u7684\u901a\u4fe1\u6d41\u7a0b\u975e\u5e38\u7b80\u5355\uff0c\u7531\u4e3b\u8bbe\u5907\u5b8c\u5168\u63a7\u5236\u3002</p>"},{"location":"EmbeddedSoft/Connective/SPI/#_5","title":"\u57fa\u672c\u901a\u4fe1\u6d41\u7a0b","text":"<ol> <li>\u521d\u59cb\u5316\uff1a\u4e3b\u8bbe\u5907\u914d\u7f6e\u65f6\u949f\u6781\u6027\uff08CPOL\uff09\u548c\u76f8\u4f4d\uff08CPHA\uff09\u3002</li> <li>\u7247\u9009(\u5f00\u59cb)\uff1a\u4e3b\u8bbe\u5907\u5c06\u76ee\u6807\u4ece\u8bbe\u5907\u7684\u00a0CS \u7ebf\u62c9\u4f4e\uff08\u6709\u6548\uff09\uff0c\u6fc0\u6d3b\u8be5\u4ece\u8bbe\u5907\u3002</li> </ol> <ol> <li> <p>\u6570\u636e\u4f20\u8f93\uff1a\u4e3b\u8bbe\u5907\u5728 SCLK \u7ebf\u4e0a\u4ea7\u751f\u65f6\u949f\u8109\u51b2\uff0c\u540c\u65f6\uff1a</p> <ul> <li> <p>\u901a\u8fc7\u00a0MOSI\u00a0\u7ebf\u5c06\u6570\u636e\u4e00\u4f4d\u4e00\u4f4d\u5730\u53d1\u9001\u7ed9\u4ece\u8bbe\u5907\u3002</p> </li> <li> <p>\u901a\u8fc7\u00a0MISO\u00a0\u7ebf\u4ece\u4ece\u8bbe\u5907\u4e00\u4f4d\u4e00\u4f4d\u5730\u63a5\u6536\u6570\u636e\u3002</p> </li> <li> <p>\u8fd9\u662f\u4e00\u4e2a\u540c\u65f6\u8fdb\u884c\uff08\u5168\u53cc\u5de5\uff09\u00a0\u7684\u8fc7\u7a0b\u3002</p> </li> <li>\u4f20\u8f93\u7ed3\u675f\uff1a\u4e3b\u8bbe\u5907\u5c06\u00a0CS \u7ebf\u62c9\u9ad8\uff08\u65e0\u6548\uff09\uff0c\u7ed3\u675f\u901a\u4fe1\u3002</li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/Connective/SPI/#8","title":"\u5e27\u683c\u5f0f\u6ce2\u5f62\u56fe\uff08\u4ee5 8 \u4f4d\u6570\u636e\u4f20\u8f93\u4e3a\u4f8b\uff09","text":"<p>\u7531\u4e8e SPI \u6709\u56db\u79cd\u6a21\u5f0f\uff0c\u5176\u6ce2\u5f62\u4e5f\u4e0d\u540c\u3002\u4e0b\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86\u56db\u79cd\u6a21\u5f0f\u4e0b\u7684\u6570\u636e\u4f20\u8f93\u65f6\u5e8f\u3002</p> <p>\u901a\u7528\u65f6\u5e8f\u6982\u5ff5\uff1a</p> <ul> <li>\u6570\u636e\u91c7\u6837\u65f6\u523b\uff1a\u63a5\u6536\u7aef\u8bfb\u53d6\u6570\u636e\u4f4d\u7684\u65f6\u673a\u3002</li> <li>\u6570\u636e\u53d8\u5316\u65f6\u523b\uff1a\u53d1\u9001\u7aef\u6539\u53d8\u6570\u636e\u4f4d\u7684\u65f6\u673a\u3002</li> </ul>"},{"location":"EmbeddedSoft/Connective/SPI/#0","title":"\u6a21\u5f0f0","text":""},{"location":"EmbeddedSoft/Connective/SPI/#1","title":"\u6a21\u5f0f1","text":""},{"location":"EmbeddedSoft/Connective/SPI/#2","title":"\u6a21\u5f0f2","text":""},{"location":"EmbeddedSoft/Connective/SPI/#3","title":"\u6a21\u5f0f3","text":"<p>[!NOTE]</p> <p>\u53cc\u7ebf\u4ea4\u53c9\u8868\u793a\u4f20\u8f93\u4e00\u4e2aBit\u3002</p> <p>MISO\u4e3a\u7f6e\u4e8e\u4e2d\u95f4\u7684\u4e00\u6761\u7ebf\uff0c\u8868\u793a\u9ad8\u963b\u6001\uff0c\u5373\u65ad\u5f00\u3002\u4ece\u6b64\u72b6\u6001\u53d8\u4e3a\u62c9\u5f00\u7684\u72b6\u6001\uff0c\u8868\u793a\u4ece\u673a\u88ab\u9009\u4e2d\uff0c\u5141\u8bb8\u5f00\u59cb\u901a\u4fe1\u3002</p> <p>\u6240\u6709\u6570\u636e\u4f20\u5b8c\u540e\uff0cMOSI\u53d8\u6362\u7535\u5e73\u662f\u4e3a\u4e86\u56de\u5230\u9ed8\u8ba4\u7535\u5e73\uff0c\u5982\u679c\u6ca1\u6709\u9ed8\u8ba4\u7535\u5e73\u4e5f\u53ef\u4ee5\u4e0d\u53d8\u6362\u3002</p>"},{"location":"EmbeddedSoft/Connective/SPI/#_6","title":"\u591a\u5b57\u8282\u8fde\u7eed\u4f20\u8f93","text":"<p>SPI \u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u5730\u8fdb\u884c\u591a\u5b57\u8282\u8fde\u7eed\u4f20\u8f93\u3002\u4e3b\u8bbe\u5907\u53ea\u9700\u5728 CS \u6709\u6548\u671f\u95f4\uff0c\u6301\u7eed\u4ea7\u751f SCLK \u65f6\u949f\u5468\u671f\u5373\u53ef\u3002\u6bcf\u4e2a SCLK \u5468\u671f\u4f20\u8f93\u4e00\u4f4d\u6570\u636e\u3002</p> <p></p>"},{"location":"EmbeddedSoft/Connective/SPI/#spi_1","title":"\u4e94\u3001 SPI \u4f18\u7f3a\u70b9\u603b\u7ed3","text":"\u4f18\u70b9 \u7f3a\u70b9 \u5168\u53cc\u5de5\u901a\u4fe1\uff0c\u6548\u7387\u9ad8 \u9700\u89814\u6839\u7ebf\uff0c\u6bd4 I2C \u5360\u7528\u66f4\u591a GPIO \u534f\u8bae\u975e\u5e38\u7b80\u5355\uff0c\u786c\u4ef6\u5b9e\u73b0\u5bb9\u6613 \u65e0\u786c\u4ef6\u5e94\u7b54\u673a\u5236\uff0c\u65e0\u6cd5\u786e\u8ba4\u6570\u636e\u662f\u5426\u88ab\u6210\u529f\u63a5\u6536 \u901a\u4fe1\u901f\u5ea6\u975e\u5e38\u9ad8\uff0c\u6ca1\u6709\u6807\u51c6\u4e0a\u9650 \u65e0\u6807\u51c6\u7684\u5bfb\u5740\u673a\u5236\uff0c\u9700\u8981\u5927\u91cf\u72ec\u7acb\u7684 CS \u7ebf\u8fde\u63a5\u591a\u4ece\u673a \u901a\u4fe1\u8fc7\u7a0b\u7531\u4e3b\u8bbe\u5907\u5b8c\u5168\u63a7\u5236\uff0c\u65e0\u9700\u590d\u6742\u7684\u4ef2\u88c1\u548c\u540c\u6b65 \u6ca1\u6709\u5b98\u65b9\u7684\u6807\u51c6\u534f\u8bae\uff0c\u4e0d\u540c\u5382\u5546\u8bbe\u5907\u53ef\u80fd\u6709\u4e0d\u540c\u8981\u6c42 \u6570\u636e\u4f20\u8f93\u7ed3\u6784\u7075\u6d3b\uff0c\u6ca1\u6709\u56fa\u5b9a\u7684\u6570\u636e\u5e27\u683c\u5f0f"},{"location":"EmbeddedSoft/Connective/SPI/#spi-i2c","title":"\u516d\u3001 SPI \u4e0e I2C \u7684\u5feb\u901f\u5bf9\u6bd4","text":"\u7279\u6027 SPI I2C \u7ebf\u6570 4 \u7ebf\uff08CS, SCLK, MOSI, MISO\uff09 2 \u7ebf\uff08SDA, SCL\uff09 \u901f\u5ea6 \u9ad8\uff08\u901a\u5e38 &gt;10 Mbps\uff09 \u4e2d\u4f4e\u901f\uff08\u6807\u51c6\u6a21\u5f0f 100kbps\uff0c\u5feb\u901f\u6a21\u5f0f 400kbps\uff09 \u901a\u4fe1\u65b9\u5f0f \u5168\u53cc\u5de5 \u534a\u53cc\u5de5 \u62d3\u6251 \u70b9\u5bf9\u70b9\u3001\u72ec\u7acbCS\u3001\u83ca\u82b1\u94fe \u591a\u4e3b\u4ece\u3001\u603b\u7ebf\u578b\u3001\u8f6f\u4ef6\u5bfb\u5740 \u534f\u8bae\u590d\u6742\u5ea6 \u7b80\u5355 \u76f8\u5bf9\u590d\u6742\uff08\u6709\u8d77\u59cb\u3001\u505c\u6b62\u3001\u5e94\u7b54\u4f4d\uff09 \u6d41\u63a7/\u5e94\u7b54 \u65e0 \u6709\uff08ACK/NACK \u4f4d\uff09 \u786c\u4ef6\u5f00\u9500 \u4e3b\u8bbe\u5907 CS \u7ebf\u968f\u4ece\u8bbe\u5907\u6570\u91cf\u589e\u52a0 \u786c\u4ef6\u8d44\u6e90\u56fa\u5b9a"},{"location":"EmbeddedSoft/Connective/USART/","title":"\u57fa\u672c\u6982\u5ff5","text":"<p>\u901a\u7528\u540c\u6b65\u5f02\u6b65\u6536\u53d1\u4f20\u8f93\u5668\uff08USART\uff09\u662f\u4e00\u79cd\u5168\u53cc\u5de5\u4e32\u884c\u901a\u4fe1\u63a5\u53e3\uff0c\u8bbe\u5907\u901a\u8fc7\u5b9a\u4e49\u660e\u786e\u7684\u5e27\u7ed3\u6784\u8fdb\u884c\u6570\u636e\u4ea4\u6362\uff0c\u652f\u6301\u540c\u6b65\u548c\u5f02\u6b65\u4e24\u79cd\u5de5\u4f5c\u6a21\u5f0f\u3002 \u5bf9\u6bd4</p> \u540d\u79f0 \u5f15\u811a \u53cc\u5de5 \u65f6\u949f \u7535\u5e73 \u8bbe\u5907 USART TX\u3001RX \u5168\u53cc\u5de5 \u5f02\u6b65 \u5355\u7aef \u70b9\u5bf9\u70b9 I2C SCL\u3001SDA \u534a\u53cc\u5de5 \u540c\u6b65 \u5355\u7aef \u591a\u8bbe\u5907 SPI SCLK\u3001MOSI\u3001MISO\u3001CS \u5168\u53cc\u5de5 \u540c\u6b65 \u5355\u7aef \u591a\u8bbe\u5907 CAN CAN_H\u3001CAN_L \u534a\u53cc\u5de5 \u5f02\u6b65 \u5dee\u5206 \u591a\u8bbe\u5907 USB DP\u3001DM \u534a\u53cc\u5de5 \u5f02\u6b65 \u5dee\u5206 \u70b9\u5bf9\u70b9 # \u786c\u4ef6\u8fde\u63a5\u65b9\u5f0f <p></p>"},{"location":"EmbeddedSoft/Connective/USART/#_2","title":"\u4fe1\u53f7\u7535\u5e73\u6807\u51c6","text":"\u7535\u5e73\u6807\u51c6 \u903b\u8f910 \u903b\u8f911 \u5e94\u7528\u573a\u666f TTL 0V 3.3V/5V \u677f\u5185\u901a\u4fe1 RS232 +3V to +15V -3V to -15V \u957f\u8ddd\u79bb\u901a\u4fe1 RS485 -1.5V to -6V +1.5V to +6V \u5de5\u4e1a\u73b0\u573a"},{"location":"EmbeddedSoft/Connective/USART/#_3","title":"\u5173\u952e\u53c2\u6570","text":"<ul> <li> <p>\u6ce2\u7279\u7387\uff1a\u4e32\u53e3\u901a\u4fe1\u7684\u901f\u7387</p> </li> <li> <p>\u8d77\u59cb\u4f4d\uff1a\u6807\u5fd7\u4e00\u4e2a\u6570\u636e\u5e27\u7684\u5f00\u59cb\uff0c\u56fa\u5b9a\u4e3a\u4f4e\u7535\u5e73</p> </li> <li> <p>\u6570\u636e\u4f4d\uff1a\u6570\u636e\u5e27\u7684\u6709\u6548\u8f7d\u8377\uff0c1\u4e3a\u9ad8\u7535\u5e73\uff0c0\u4e3a\u4f4e\u7535\u5e73\uff0c\u4f4e\u4f4d\u5148\u884c</p> </li> <li> <p>\u6821\u9a8c\u4f4d\uff1a\u7528\u4e8e\u6570\u636e\u9a8c\u8bc1\uff0c\u6839\u636e\u6570\u636e\u4f4d\u8ba1\u7b97\u5f97\u6765(\u53ef\u8bbe\u7f6e)</p> </li> <li> <p>\u505c\u6b62\u4f4d\uff1a\u7528\u4e8e\u6570\u636e\u5e27\u95f4\u9694\uff0c\u56fa\u5b9a\u4e3a\u9ad8\u7535\u5e73\uff08\u4e00\u822c\u4e3a1\u4f4d\uff09</p> </li> </ul>"},{"location":"EmbeddedSoft/Connective/USART/#_4","title":"\u5e27\u7ed3\u6784\u7ec4\u6210","text":"<p>\u5178\u578b\u6570\u636e\u5e27\u683c\u5f0f\uff1a</p> <p></p> <p></p> <p>\u5e27\u7ed3\u6784\u53c2\u6570\u914d\u7f6e\u8868\uff1a</p> \u7ec4\u6210\u90e8\u5206 \u4f4d\u6570 \u529f\u80fd\u8bf4\u660e \u8d77\u59cb\u4f4d 1\u4f4d \u6807\u5fd7\u4f20\u8f93\u5f00\u59cb\uff0c\u5c06\u7ebf\u8def\u4ece\u7a7a\u95f2\u9ad8\u7535\u5e73\u62c9\u4f4e \u6570\u636e\u4f4d 5-9\u4f4d \u6709\u6548\u6570\u636e\u8f7d\u8377\uff0c\u901a\u5e38\u4e3a8\u4f4d \u6821\u9a8c\u4f4d 0/1\u4f4d \u5947\u5076\u6821\u9a8c\uff0c\u7528\u4e8e\u9519\u8bef\u68c0\u6d4b \u505c\u6b62\u4f4d 1/1.5/2\u4f4d \u6807\u5fd7\u5e27\u7ed3\u675f\uff0c\u8fd4\u56de\u9ad8\u7535\u5e73\u72b6\u6001"},{"location":"EmbeddedSoft/Connective/USART/#_5","title":"\u901a\u4fe1\u6a21\u5f0f\u5bf9\u6bd4","text":"\u6a21\u5f0f \u6570\u636e\u6d41\u5411 \u786c\u4ef6\u9700\u6c42 \u5e94\u7528\u573a\u666f \u5168\u53cc\u5de5 \u540c\u65f6\u53cc\u5411\u4f20\u8f93 TX\u3001RX\u72ec\u7acb \u5b9e\u65f6\u6570\u636e\u4ea4\u4e92 \u534a\u53cc\u5de5 \u53cc\u5411\u4ea4\u66ff\u4f20\u8f93 \u5355\u6570\u636e\u7ebf \u591a\u8bbe\u5907\u5171\u4eab\u603b\u7ebf \u5355\u5de5 \u5355\u5411\u4f20\u8f93 \u5355\u5411\u7ebf\u8def \u4f20\u611f\u5668\u6570\u636e\u8bfb\u53d6"},{"location":"EmbeddedSoft/Connective/USART/#_6","title":"\u6ce2\u7279\u7387","text":"<p>\u6ce2\u7279\u7387\u4e0e\u7cbe\u5ea6\u5173\u7cfb\uff1a</p> <pre><code>\u5b9e\u9645\u6ce2\u7279\u7387\u8bef\u5dee(%) = |(\u5b9e\u9645\u503c - \u7406\u8bba\u503c)| / \u7406\u8bba\u503c \u00d7 100%\n\u8981\u6c42\uff1a\u8bef\u5dee &lt; 2.5% (\u901a\u5e38\u6807\u51c6)\n</code></pre> <p>\u5e38\u7528\u6ce2\u7279\u7387\u914d\u7f6e\u8868\uff1a</p> \u6ce2\u7279\u7387 \u65f6\u949f\u6e90\u8981\u6c42 \u9002\u7528\u573a\u666f 9600 \u4f4e\u9891\u65f6\u949f \u957f\u8ddd\u79bb\u901a\u4fe1 115200 \u6807\u51c6\u65f6\u949f \u5e38\u7528\u901f\u7387 460800 \u9ad8\u9891\u65f6\u949f \u9ad8\u901f\u4f20\u8f93 1M\u4ee5\u4e0a \u4e13\u7528\u65f6\u949f \u7279\u6b8a\u5e94\u7528"},{"location":"EmbeddedSoft/Connective/USART/#_7","title":"\u65f6\u5e8f\u4f8b\u5b50","text":""},{"location":"EmbeddedSoft/Control_Algos/Control_Algos/","title":"Control Algos","text":"<ul> <li>PID</li> <li>LQR</li> <li>SMC</li> </ul>"},{"location":"EmbeddedSoft/RTOS/RTOS/","title":"RTOS\u7b80\u4ecb","text":"<p>\u4e0e\u4e4b\u5bf9\u5e94\u7684\u6982\u5ff5\u662f\u88f8\u673a\u3002RTOS\u5168\u79f0Real Time OS \u5373\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u3002\u901a\u5e38\u7528\u4e8e\u9700\u8981\u5728\u4e25\u683c\u65f6\u95f4\u9650\u5236\u5185\u5bf9\u5916\u90e8\u4e8b\u4ef6\u505a\u51fa\u53cd\u5e94\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff0c\u6709\u591a\u4efb\u52a1\u5904\u7406\u3001\u8c03\u5ea6\u3001\u5b9e\u65f6\u8c03\u5ea6\u7b49\u529f\u80fd\u3002\u7279\u70b9\uff1a\u5206\u800c\u6cbb\u4e4b\u3001\u5ef6\u65f6\u4e0b\u653e\u3001\u62a2\u5360\u5f0f\u3001\u4efb\u52a1\u5806\u6808\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7</p>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_1","title":"\u524d\u7f6e\u77e5\u8bc6","text":"<p>\u89c1ARM_Arch</p>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_2","title":"\u57fa\u7840\u77e5\u8bc6","text":""},{"location":"EmbeddedSoft/RTOS/RTOS/#_3","title":"\u4efb\u52a1\u8c03\u5ea6","text":""},{"location":"EmbeddedSoft/RTOS/RTOS/#_4","title":"\u65b9\u5f0f","text":"<ol> <li>\u62a2\u5360\u5f0f\uff1a\u9ad8\u7684\u62a2\u5360\u4f4e\u7684\uff0c\u9ad8\u4f18\u5148\u7ea7\u963b\u585e\u65f6\uff08\u6bd4\u5982delay\u51fd\u6570\uff09\u4f1a\u628aCPU\u63a7\u5236\u6743\u4e0b\u653e\u3002</li> <li>\u65f6\u95f4\u7247\uff1a\u540c\u7b49\u4f18\u5148\u7ea7\u6709\u65f6\u949f\u8282\u62cd\u8d1f\u8d23\u5207\u6362\u4efb\u52a1\uff08\u4e00\u4e2a\u65f6\u95f4\u7247\u4e3aSysTick\u4e2d\u65ad\u5468\u671f\uff09\u3002\u6ca1\u6709\u7528\u5b8c\u7684\u65f6\u95f4\u7247\uff08\u4e2d\u95f4\u6709\u963b\u585e\u4f1a\u76f4\u63a5\u8df3\u8fc7\uff0c\u6267\u884c\u4e0d\u5230\u4e00\u4e2a\u65f6\u95f4\u7247\uff09\u76f4\u63a5\u4e22\u5f03\u3002</li> <li>\u534f\u7a0b\u5f0f\u8c03\u5ea6\uff1a\u4e0d\u88ab\u62a2\u5360\uff08\u5df2\u7ecf\u8fc7\u65f6\uff09</li> </ol>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_5","title":"\u4efb\u52a1\u72b6\u6001","text":"<ol> <li>\u8fd0\u884c\u6001</li> <li>\u5c31\u7eea\u6001</li> <li>\u963b\u585e\u6001\uff1a\u63a7\u5236\u6743\u4e0b\u653e\u7ed9\u4e0b\u7ea7\u5c31\u7eea\u6001</li> <li>\u6302\u8d77\u6001\uff1a\u8c03\u7528<code>vTaskSuspend()</code>\u6682\u505c\uff0c\u89e3\u9664(<code>vTaskResume()</code>)\u540e\u56de\u5230\u5c31\u7eea\u6001\u3002</li> </ol>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_6","title":"* \u4efb\u52a1\u72b6\u6001\u5217\u8868","text":"<ol> <li>\u5c31\u7eea\u5217\u8868\uff1a<code>pxReadyTasksLists[x]</code>\uff0cx\u4ee3\u8868\u4f18\u5148\u7ea7\u6570\u76ee\uff0832\u4e2d\u4e3a31\uff0c\u6700\u4f4e\u4f18\u5148\u7ea7\u4fdd\u7559\u7ed9\u7a7a\u95f2\u4efb\u52a1\uff09\u3002\u540c\u65f6\u670932\u4f4d\u6570\u503c\u5b58\u50a8\u4efb\u52a1\u5b58\u5728\u6807\u5fd7\u4f4d\u3002</li> <li>\u963b\u585e\u5217\u8868\uff1a<code>pxDelayedTaskList</code></li> <li>\u6302\u8d77\u5217\u8868\uff1a<code>xSuspendedTaskList</code></li> </ol>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_7","title":"\u4efb\u52a1\u5806\u6808","text":"<ul> <li>Tasks_Stack</li> </ul>"},{"location":"EmbeddedSoft/RTOS/RTOS/#_8","title":"\u76ee\u5f55","text":"<ul> <li>\u4efb\u52a1\u5207\u6362</li> <li>FreeRTOS</li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/","title":"Tasks Stack","text":""},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_1","title":"\u4efb\u52a1\u5806\u6808","text":"<p>\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u975e\u5e38\u5f62\u8c61\u7684\u6bd4\u55bb\u6765\u7406\u89e3\uff1a</p> <p>\u60f3\u8c61\u4e00\u4e0b\uff0c\u6bcf\u4e2a\u4efb\u52a1\uff08\u6216\u7ebf\u7a0b\uff09\u5c31\u50cf\u4e00\u4e2a\u5c0f\u578b\u529e\u516c\u5ba4\u91cc\u7684\u4e00\u4e2a\u5de5\u4f5c\u4eba\u5458\u3002</p> <p>\u8fd9\u4e2a\u5de5\u4f5c\u4eba\u5458\u7684\u4efb\u52a1\u5806\u6808\uff0c\u5c31\u662f\u4ed6/\u5979\u4e13\u5c5e\u7684\u79c1\u4eba\u529e\u516c\u684c\u684c\u9762\u3002</p> <ol> <li> <p>\u4e34\u65f6\u5de5\u4f5c\u533a\uff1a\u8fd9\u4e2a\u684c\u9762\u4e0a\u7528\u6765\u4e34\u65f6\u653e\u7f6e\u4ed6\u6b63\u5728\u5904\u7406\u7684\u6587\u4ef6\u3001\u8bb0\u4e0b\u7684\u4fbf\u7b7e\u3001\u8ba1\u7b97\u4e2d\u7684\u8349\u7a3f\u7eb8\u3002\u5b83\u4e0d\u9002\u5408\u957f\u671f\u5b58\u653e\u6863\u6848\uff0c\u53ea\u670d\u52a1\u4e8e\u5f53\u524d\u6b63\u5728\u505a\u7684\u5de5\u4f5c\u3002</p> </li> <li> <p>\u540e\u8fdb\u5148\u51fa\uff08LIFO\uff09\uff1a\u5c31\u50cf\u684c\u9762\u4e0a\u4f60\u53ea\u80fd\u628a\u6587\u4ef6\u4e00\u5806\u4e00\u5806\u5730\u53e0\u653e\uff0c\u4f60\u603b\u662f\u6700\u5148\u5904\u7406\u6700\u4e0a\u9762\uff08\u6700\u540e\u653e\u4e0a\u53bb\uff09\u7684\u90a3\u4efd\u6587\u4ef6\u3002\u5728\u5806\u6808\u91cc\uff0c\u6570\u636e\u4e5f\u662f\u4ee5\u201c\u538b\u5165\u201d\u548c\u201c\u5f39\u51fa\u201d\u7684\u65b9\u5f0f\u64cd\u4f5c\u3002</p> </li> <li> <p>\u4e13\u5c5e\u72ec\u7acb\uff1a\u6bcf\u4e2a\u5de5\u4f5c\u4eba\u5458\uff08\u4efb\u52a1\uff09\u90fd\u6709\u81ea\u5df1\u7684\u684c\u9762\uff08\u5806\u6808\uff09\uff0c\u4e92\u4e0d\u5e72\u6270\u3002A \u4efb\u52a1\u4e0d\u80fd\u4e5f\u4e0d\u4f1a\u53bb B \u4efb\u52a1\u7684\u684c\u9762\u4e0a\u62ff\u4e1c\u897f\uff0c\u8fd9\u4fdd\u8bc1\u4e86\u4efb\u52a1\u7684\u72ec\u7acb\u6027\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_2","title":"\u5728\u6280\u672f\u4e0a\u7684\u5b9a\u4e49\uff1a","text":"<p>\u4efb\u52a1\u5806\u6808\u662f\u4e00\u5757\u5728\u5185\u5b58\u4e2d\u9884\u5148\u5206\u914d\u7684\u3001\u8fde\u7eed\u7684\u5185\u5b58\u533a\u57df\uff0c\u5b83\u4e13\u95e8\u7528\u4e8e\u4e00\u4e2a\u72ec\u7acb\u7684\u4efb\u52a1\uff08\u6216\u7ebf\u7a0b\uff09\u3002\u5b83\u7684\u4e3b\u8981\u4f5c\u7528\u662f\uff1a</p> <ul> <li> <p>\u5b58\u653e\u5c40\u90e8\u53d8\u91cf\uff1a\u51fd\u6570\u5185\u90e8\u58f0\u660e\u7684\u975e\u9759\u6001\u5c40\u90e8\u53d8\u91cf\u3002</p> </li> <li> <p>\u4fdd\u5b58\u51fd\u6570\u8c03\u7528\u73b0\u573a\uff1a\u5f53\u53d1\u751f\u51fd\u6570\u8c03\u7528\u65f6\uff0c\u4fdd\u5b58\u5f53\u524d\u51fd\u6570\u7684\u8fd4\u56de\u5730\u5740\u3001\u5bc4\u5b58\u5668\u503c\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u4fbf\u88ab\u8c03\u51fd\u6570\u8fd4\u56de\u540e\u80fd\u7ee7\u7eed\u6267\u884c\u3002</p> </li> <li> <p>\u5728\u4efb\u52a1\u5207\u6362\u65f6\u4fdd\u5b58\u4e0a\u4e0b\u6587\uff1a\u5f53\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u884c\u4efb\u52a1\u8c03\u5ea6\uff0c\u4ece\u4e00\u4e2a\u4efb\u52a1\u5207\u6362\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5c06\u5f53\u524d\u4efb\u52a1\u7684CPU\u5bc4\u5b58\u5668\u72b6\u6001\u5168\u90e8\u4fdd\u5b58\u5230\u5b83\u81ea\u5df1\u7684\u5806\u6808\u91cc\uff0c\u4ee5\u4fbf\u4e0b\u6b21\u6062\u590d\u6267\u884c\u3002</p> </li> </ul> <p>\u6ca1\u6709\u5806\u6808\u4f1a\u600e\u6837\uff1f \u5982\u679c\u6ca1\u6709\u8fd9\u4e2a\u79c1\u4eba\u7684\u201c\u529e\u516c\u684c\u684c\u9762\u201d\uff0c\u4efb\u52a1\u5c31\u65e0\u6cd5\u8fdb\u884c\u51fd\u6570\u8c03\u7528\uff0c\u5c40\u90e8\u53d8\u91cf\u4e5f\u65e0\u5904\u5b58\u653e\uff0c\u591a\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u6570\u636e\u4f1a\u5b8c\u5168\u6df7\u4e71\u3002\u56e0\u6b64\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5fc5\u987b\u62e5\u6709\u81ea\u5df1\u72ec\u7acb\u7684\u5806\u6808\u7a7a\u95f4\u3002</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_3","title":"\u4e8c\u3001\u4efb\u52a1\u5806\u6808\u9700\u8981\u7684\u5927\u5c0f\u600e\u4e48\u786e\u5b9a\uff1f","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5806\u6808\u5927\u5c0f\u5206\u914d\u662f\u5728\u7f16\u8bd1\u65f6\u9759\u6001\u786e\u5b9a\u7684\uff08\u5bf9\u4e8e\u5927\u591a\u6570RTOS\u800c\u8a00\uff09\u3002\u5206\u914d\u592a\u5c0f\u4f1a\u5bfc\u81f4\u4e25\u91cd\u95ee\u9898\uff0c\u5206\u914d\u592a\u5927\u53c8\u4f1a\u6d6a\u8d39\u5b9d\u8d35\u7684\u5185\u5b58\u3002</p> <p>\u9996\u5148\u8981\u4fdd\u8bc1\u5927\u4e8eCPU\u5bc4\u5b58\u5668\u603b\u5927\u5c0f\uff0c\u4e0d\u7136\u8fde\u73b0\u573a\u90fd\u4fdd\u62a4\u4e0d\u4e86</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#1","title":"1. \u5806\u6808\u5927\u5c0f\u4e0d\u8db3\u7684\u540e\u679c","text":"<ul> <li> <p>\u5806\u6808\u6ea2\u51fa\uff1a\u8fd9\u662f\u6700\u76f4\u63a5\u7684\u540e\u679c\u3002\u5f53\u4efb\u52a1\u4f7f\u7528\u7684\u5806\u6808\u7a7a\u95f4\u8d85\u8fc7\u4e86\u9884\u5206\u914d\u7684\u5927\u5c0f\uff0c\u5c31\u4f1a\u7834\u574f\u5806\u6808\u4e4b\u5916\u7684\u5185\u5b58\u533a\u57df\u3002</p> </li> <li> <p>\u6570\u636e\u635f\u574f\uff1a\u6ea2\u51fa\u7684\u5806\u6808\u53ef\u80fd\u4f1a\u8986\u76d6\u5176\u4ed6\u4efb\u52a1\u7684\u6570\u636e\u533a\u3001\u5806\u533a\u751a\u81f3\u4ee3\u7801\u533a\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u884c\u4e3a\u5f02\u5e38\u3001\u6570\u636e\u88ab\u83ab\u540d\u4fee\u6539\u3002</p> </li> <li> <p>\u7cfb\u7edf\u5d29\u6e83\uff1a\u6700\u5e38\u8868\u73b0\u4e3a\u7cfb\u7edf\u201c\u6b7b\u673a\u201d\u3001\u8dd1\u98de\u6216\u89e6\u53d1\u786c\u4ef6\u9519\u8bef\u5f02\u5e38\uff08\u5982 HardFault in ARM Cortex-M\uff09\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#2","title":"2. \u786e\u5b9a\u5806\u6808\u5927\u5c0f\u7684\u65b9\u6cd5\uff08\u7ec4\u5408\u4f7f\u7528\uff09","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u5de5\u7a0b\u5b9e\u8df5\u95ee\u9898\uff0c\u6ca1\u6709\u5355\u4e00\u7684\u6807\u51c6\u7b54\u6848\uff0c\u901a\u5e38\u9700\u8981\u591a\u79cd\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_4","title":"\u65b9\u6cd5\u4e00\uff1a\u7406\u8bba\u4f30\u7b97\uff08\u9759\u6001\u5206\u6790\uff09","text":"<p>\u8fd9\u662f\u7b2c\u4e00\u6b65\uff0c\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u6765\u7c97\u7565\u4f30\u7b97\u3002</p> <ul> <li> <p>\u8ba1\u7b97\u51fd\u6570\u8c03\u7528\u94fe\u7684\u6df1\u5ea6\uff1a\u627e\u5230\u4efb\u52a1\u5165\u53e3\u51fd\u6570\u5f00\u59cb\uff0c\u6700\u6df1\u7684\u51fd\u6570\u8c03\u7528\u8def\u5f84\u3002\u4f8b\u5982\uff1a<code>TaskMain -&gt; FunctionA -&gt; FunctionB -&gt; FunctionC</code>\uff0c\u6df1\u5ea6\u4e3a4\u5c42\u3002</p> </li> <li> <p>\u8ba1\u7b97\u6bcf\u4e00\u5c42\u7684\u6808\u5e27\u5927\u5c0f\uff1a</p> <ul> <li> <p>\u6bcf\u4e00\u5c42\u51fd\u6570\u8c03\u7528\u90fd\u9700\u8981\u5728\u5806\u6808\u4e0a\u4fdd\u5b58\u8fd4\u56de\u5730\u5740\u3001\u5e27\u6307\u9488\u3001\u4ee5\u53ca\u51fd\u6570\u5185\u90e8\u7684\u5c40\u90e8\u53d8\u91cf\u3001\u7f16\u8bd1\u5668\u4e34\u65f6\u53d8\u91cf\u7b49\u3002</p> </li> <li> <p>\u5c06\u6240\u6709\u5c42\u7ea7\u7684\u6808\u5e27\u5927\u5c0f\u76f8\u52a0\uff0c\u5f97\u5230\u4e00\u4e2a\u57fa\u7840\u503c\u3002</p> </li> </ul> </li> <li> <p>\u8003\u8651\u4efb\u52a1\u5207\u6362\u7684\u5f00\u9500\uff1a\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u884c\u4efb\u52a1\u5207\u6362\u65f6\uff0c\u9700\u8981\u5c06CPU\u7684\u6240\u6709\u901a\u7528\u5bc4\u5b58\u5668\u3001\u72b6\u6001\u5bc4\u5b58\u5668\u7b49\u538b\u5165\u5806\u6808\uff0c\u8fd9\u90e8\u5206\u7a7a\u95f4\u4e5f\u8981\u7b97\u8fdb\u53bb\u3002</p> </li> <li> <p>\u52a0\u4e0a\u4e2d\u65ad\u670d\u52a1\u4f8b\u7a0b\u7684\u989d\u5916\u5f00\u9500\uff1a\u5f53\u4e2d\u65ad\u53d1\u751f\u65f6\uff0c\u4f1a\u4f7f\u7528\u5f53\u524d\u4efb\u52a1\u7684\u5806\u6808\u6765\u4fdd\u5b58\u4e0a\u4e0b\u6587\u3002\u901a\u5e38\u8981\u6309\u6700\u5927\u4e2d\u65ad\u5d4c\u5957\u5c42\u6570\u6765\u9884\u7559\u7a7a\u95f4\u3002</p> </li> </ul> <p>\u7f3a\u70b9\uff1a\u8fd9\u79cd\u65b9\u6cd5\u975e\u5e38\u7e41\u7410\uff0c\u4e14\u5bb9\u6613\u4f4e\u4f30\uff0c\u56e0\u4e3a\u7f16\u8bd1\u5668\u4f18\u5316\u3001\u9012\u5f52\u51fd\u6570\u3001\u5927\u578b\u6570\u7ec4\u3001\u51fd\u6570\u6307\u9488\u8c03\u7528\u7b49\u90fd\u4f1a\u4f7f\u5f97\u51c6\u786e\u8ba1\u7b97\u53d8\u5f97\u56f0\u96be\u3002</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_5","title":"\u65b9\u6cd5\u4e8c\uff1a\u7ecf\u9a8c\u503c\u6cd5","text":"<p>\u5bf9\u4e8e\u7b80\u5355\u7684\u9879\u76ee\u6216\u6709\u7ecf\u9a8c\u7684\u5de5\u7a0b\u5e08\uff0c\u53ef\u4ee5\u6839\u636e\u82af\u7247\u67b6\u6784\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u7ed9\u51fa\u4e00\u4e2a\u521d\u59cb\u7684\u201c\u731c\u503c\u201d\u3002</p> <ul> <li> <p>\u5bf9\u4e8e\u7b80\u5355\u7684\u88f8\u673a\u7a0b\u5e8f\u6216\u7a7a\u95f2\u4efb\u52a1\uff1a\u53ef\u80fd\u53ea\u9700\u8981 128 - 512 \u5b57\u8282\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u4e2d\u7b49\u590d\u6742\u5ea6\u7684\u4efb\u52a1\uff08\u5982\u5904\u7406\u534f\u8bae\u3001\u63a7\u5236\u5916\u8bbe\uff09\uff1a\u53ef\u80fd\u9700\u8981 1KB - 4KB\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u975e\u5e38\u590d\u6742\u7684\u4efb\u52a1\uff08\u5982\u8fd0\u884c\u6587\u4ef6\u7cfb\u7edf\u3001LVGL GUI\u3001TCP/IP\u534f\u8bae\u6808\uff09\uff1a\u53ef\u80fd\u9700\u8981 8KB \u751a\u81f3\u51e0\u5341KB\u3002</p> </li> <li> <p>\u53c2\u8003\u82af\u7247\u5382\u5546\u7684\u4f8b\u7a0b\uff1a\u82af\u7247\u4f9b\u5e94\u5546\u63d0\u4f9b\u7684SDK\u548cDemo\u4ee3\u7801\u4e2d\u7684\u4efb\u52a1\u5806\u6808\u914d\u7f6e\u662f\u5f88\u597d\u7684\u53c2\u8003\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_6","title":"\u65b9\u6cd5\u4e09\uff1a\u5b9e\u9a8c\u6d4b\u91cf\u6cd5\uff08\u6700\u53ef\u9760\u3001\u6700\u5e38\u7528\uff09","text":"<p>\u8fd9\u662f\u5728\u5b9e\u9645\u8fd0\u884c\u65f6\u8fdb\u884c\u6d4b\u91cf\uff0c\u662f\u786e\u5b9a\u5806\u6808\u5927\u5c0f\u7684\u9ec4\u91d1\u6cd5\u5219\u3002</p> <p>\u6838\u5fc3\u601d\u60f3\uff1a\u00a0\u5728\u4efb\u52a1\u8fd0\u884c\u524d\uff0c\u7528\u7279\u5b9a\u7684\u6a21\u5f0f\uff08\u5982\u00a0<code>0xDEADBEEF</code>,\u00a0<code>0xAAAAAAAA</code>\u00a0\u7b49\uff09\u586b\u5145\u6574\u4e2a\u5806\u6808\u7a7a\u95f4\u3002\u8ba9\u7cfb\u7edf\u5728\u6700\u6076\u52a3\u7684\u8fd0\u884c\u6761\u4ef6\u4e0b\uff08\u6700\u5927\u8d1f\u8f7d\u3001\u6700\u6df1\u8c03\u7528\u3001\u6700\u957f\u8fd0\u884c\u65f6\u95f4\uff09\u8fd0\u884c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u518d\u53bb\u68c0\u67e5\u5806\u6808\u7a7a\u95f4\u91cc\u8fd8\u6709\u591a\u5c11\u586b\u5145\u6a21\u5f0f\u6ca1\u6709\u88ab\u8986\u76d6\u3002\u88ab\u8986\u76d6\u7684\u90e8\u5206\u5c31\u662f\u4f7f\u7528\u8fc7\u7684\uff0c\u5269\u4f59\u7684\u90e8\u5206\u5c31\u662f\u201c\u6c34\u4f4d\u7ebf\u201d\u4ee5\u4e0b\u7684\u7a7a\u95f2\u7a7a\u95f4\u3002</p> <p>\u5177\u4f53\u64cd\u4f5c\uff1a</p> <ol> <li> <p>RTOS \u63d0\u4f9b\u7684\u5de5\u5177\uff1a\u50cf FreeRTOS, uC/OS \u7b49\u4e3b\u6d41RTOS\u90fd\u63d0\u4f9b\u4e86\u5806\u6808\u4f7f\u7528\u91cf\u7edf\u8ba1\u7684\u529f\u80fd\uff08\u5982\u00a0<code>uxTaskGetStackHighWaterMark()</code>\uff09\u3002</p> <ul> <li>\u9ad8\u6c34\u4f4d\u7ebf\uff1a\u6307\u4ece\u4efb\u52a1\u5f00\u59cb\u8fd0\u884c\u4ee5\u6765\uff0c\u5806\u6808\u7a7a\u95f4\u8fbe\u5230\u8fc7\u7684\u6700\u5927\u4f7f\u7528\u6df1\u5ea6\u3002<code>\u5806\u6808\u603b\u5927\u5c0f - \u9ad8\u6c34\u4f4d\u7ebf = \u5269\u4f59\u7684\u5b89\u5168\u7a7a\u95f4</code>\u3002</li> </ul> </li> <li> <p>\u624b\u52a8\u586b\u5145\u6cd5\uff1a\u5982\u679c\u6ca1\u6709\u73b0\u6210\u5de5\u5177\uff0c\u53ef\u4ee5\u624b\u52a8\u5728\u4efb\u52a1\u521b\u5efa\u540e\uff0c\u7528\u00a0<code>memset</code>\u00a0\u5c06\u5806\u6808\u533a\u57df\u586b\u5145\u4e3a\u4e00\u4e2a\u9b54\u6570\uff0c\u7136\u540e\u5728\u8c03\u8bd5\u5668\u4e2d\u67e5\u770b\uff0c\u6216\u8005\u7f16\u5199\u4e00\u4e2a\u51fd\u6570\u6765\u68c0\u67e5\u4ece\u5806\u6808\u672b\u7aef\u5f00\u59cb\uff0c\u8fde\u7eed\u9b54\u6570\u7684\u957f\u5ea6\u3002</p> </li> </ol> <p>\u5982\u4f55\u6839\u636e\u6d4b\u91cf\u7ed3\u679c\u8c03\u6574\uff1f \u5047\u8bbe\u4f60\u7ed9\u4e00\u4e2a\u4efb\u52a1\u5206\u914d\u4e86 4KB (4096 bytes) \u7684\u5806\u6808\uff0c\u6d4b\u91cf\u5230\u7684\u9ad8\u6c34\u4f4d\u7ebf\u662f 2048 bytes\u3002</p> <ul> <li> <p>\u5df2\u4f7f\u7528\uff1a2048 bytes</p> </li> <li> <p>\u7a7a\u95f2\uff1a4096 - 2048 = 2048 bytes</p> </li> <li> <p>\u5b89\u5168\u88d5\u91cf\uff1a\u4f60\u9700\u8981\u9884\u7559\u4e00\u4e2a\u5b89\u5168\u88d5\u91cf\uff08\u6bd4\u5982 25%-33%\uff09\uff0c\u4ee5\u9632\u672a\u6d4b\u8bd5\u5230\u7684\u8fb9\u754c\u60c5\u51b5\u3002</p> <ul> <li>\u5b89\u5168\u5927\u5c0f = 2048 * 1.33 \u2248 2724 bytes</li> </ul> </li> <li> <p>\u6700\u7ec8\u63a8\u8350\u5927\u5c0f\uff1a\u4f60\u53ef\u4ee5\u5c06\u8be5\u4efb\u52a1\u7684\u5806\u6808\u5927\u5c0f\u8c03\u6574\u4e3a\u00a03KB (3072 bytes)\uff0c\u8fd9\u6837\u65e2\u5b89\u5168\u53c8\u8282\u7701\u4e86 1KB \u7684\u5185\u5b58\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Stack/#_7","title":"\u4e09\u3001\u603b\u7ed3","text":"\u65b9\u9762 \u8bf4\u660e \u662f\u4ec0\u4e48 \u4efb\u52a1\u7684\u201c\u79c1\u4eba\u529e\u516c\u684c\u201d\uff0c\u7528\u4e8e\u5b58\u653e\u5c40\u90e8\u53d8\u91cf\u3001\u51fd\u6570\u8c03\u7528\u8bb0\u5f55\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u3002\u6bcf\u4e2a\u4efb\u52a1\u72ec\u7acb\u62e5\u6709\u3002 \u4e3a\u4ec0\u4e48\u91cd\u8981 \u5806\u6808\u6ea2\u51fa\u4f1a\u5bfc\u81f4\u6570\u636e\u635f\u574f\u548c\u7cfb\u7edf\u5d29\u6e83\uff0c\u662f\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u6700\u5e38\u89c1\u7684\u9519\u8bef\u4e4b\u4e00\u3002 \u5927\u5c0f\u786e\u5b9a\u65b9\u6cd5 1.\u00a0\u7406\u8bba\u4f30\u7b97\uff1a\u5206\u6790\u4ee3\u7801\uff0c\u8ba1\u7b97\u8c03\u7528\u6df1\u5ea6\u548c\u5c40\u90e8\u53d8\u91cf\u5927\u5c0f\u3002\uff08\u4e0d\u7cbe\u786e\uff0c\u4f5c\u4e3a\u8d77\u70b9\uff09  2.\u00a0\u7ecf\u9a8c\u8d4b\u503c\uff1a\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u548c\u82af\u7247\u67b6\u6784\u7ed9\u51fa\u521d\u59cb\u503c\u3002\uff08\u5feb\u901f\uff0c\u4f46\u4e0d\u4fdd\u8bc1\uff09  3.\u00a0\u5b9e\u9a8c\u6d4b\u91cf\uff1a\u6700\u53ef\u9760\u7684\u65b9\u6cd5\u3002\u5728\u6700\u5927\u8d1f\u8f7d\u4e0b\u8fd0\u884c\uff0c\u901a\u8fc7\u201c\u9ad8\u6c34\u4f4d\u7ebf\u201d\u6d4b\u91cf\u5b9e\u9645\u4f7f\u7528\u91cf\uff0c\u5e76\u52a0\u4e0a\u5b89\u5168\u88d5\u91cf\u3002"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/","title":"\u4efb\u52a1\u6808\u7684\u57fa\u672c\u7ed3\u6784","text":"<p>RTOS\u4e2d\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u72ec\u7acb\u7684\u6808\u7a7a\u95f4\uff0c\u6808\u5e95\u4f4d\u4e8e\u9ad8\u5730\u5740\uff0c\u6808\u5411\u4f4e\u5730\u5740\u589e\u957f\uff1a</p> <pre><code>\u5178\u578b\u4efb\u52a1\u6808\u5185\u5b58\u5e03\u5c40\uff1a\n+------------------+ \u2190 \u6808\u8d77\u59cb\u5730\u5740 (\u9ad8\u5730\u5740)\n|   \u6808\u4fdd\u62a4\u533a\u57df     |\n|   (\u9b54\u6570\u586b\u5145)     |\n+------------------+\n|   \u672a\u4f7f\u7528\u6808\u7a7a\u95f4   |\n|                 |\n+------------------+\n|   \u51fd\u6570\u8c03\u7528\u5e27     |\n|   \u5c40\u90e8\u53d8\u91cf       |\n+------------------+\n|   \u4efb\u52a1\u4e0a\u4e0b\u6587     |\n|   (\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668) |\n+------------------+ \u2190 \u5f53\u524d\u6808\u6307\u9488SP\u4f4d\u7f6e (\u4f4e\u5730\u5740)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_2","title":"\u4efb\u52a1\u4e0a\u4e0b\u6587","text":"<p>\u4efb\u52a1\u4e0a\u4e0b\u6587\u5305\u542b\u5904\u7406\u5668\u5b8c\u6574\u72b6\u6001\uff0c\u5206\u4e3a\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u548c\u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58\u4e24\u90e8\u5206\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_3","title":"\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668","text":"<ul> <li>R0-R3\uff1a\u53c2\u6570\u5bc4\u5b58\u5668</li> <li>R12\uff1a\u4e34\u65f6\u5bc4\u5b58\u5668  </li> <li>LR\uff08R14\uff09\uff1a\u94fe\u63a5\u5bc4\u5b58\u5668</li> <li>PC\uff08R15\uff09\uff1a\u7a0b\u5e8f\u8ba1\u6570\u5668</li> <li>xPSR\uff1a\u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668</li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_4","title":"\u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668","text":"<ul> <li>R4-R11\uff1a\u901a\u7528\u5bc4\u5b58\u5668</li> <li>\u53ef\u9009\uff1a\u6d6e\u70b9\u5bc4\u5b58\u5668\u3001\u7279\u6b8a\u529f\u80fd\u5bc4\u5b58\u5668</li> </ul>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_5","title":"\u4efb\u52a1\u5207\u6362\u89e6\u53d1\u673a\u5236","text":"<p>\u4efb\u52a1\u5207\u6362\u7531\u591a\u79cd\u6761\u4ef6\u89e6\u53d1\uff0c\u5b8c\u6574\u7684\u5207\u6362\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <pre><code>\u4efb\u52a1\u5207\u6362\u5b8c\u6574\u6d41\u7a0b\uff1a\n+------------------+\n|  \u4efb\u52a1A\u6b63\u5e38\u8fd0\u884c   |\n|  SP_A\u6307\u5411\u4efb\u52a1A\u6808 |\n+------------------+\n         |\n         \u2193 (\u4e2d\u65ad/\u7cfb\u7edf\u8c03\u7528\u89e6\u53d1)\n+------------------+\n|  \u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58    |\n|  R0-R3, R12,     |\n|  LR, PC, xPSR    |\n+------------------+\n         |\n         \u2193 (\u8fdb\u5165PendSV\u5f02\u5e38)\n+------------------+\n|  \u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58    |\n|  R4-R11\u5230\u4efb\u52a1A\u6808 |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u66f4\u65b0TCB_A\u4e2d\u7684   |\n|  \u6808\u6307\u9488SP_A      |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u8c03\u5ea6\u5668\u9009\u62e9      |\n|  \u4e0b\u4e00\u4e2a\u4efb\u52a1B     |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u4eceTCB_B\u52a0\u8f7d     |\n|  \u6808\u6307\u9488SP_B      |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u8f6f\u4ef6\u624b\u52a8\u6062\u590d    |\n|  R4-R11\u4ece\u4efb\u52a1B\u6808 |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u66f4\u65b0PSP\u4e3aSP_B   |\n+------------------+\n         |\n         \u2193 (\u5f02\u5e38\u8fd4\u56de)\n+------------------+\n|  \u786c\u4ef6\u81ea\u52a8\u6062\u590d    |\n|  R0-R3, R12,     |\n|  LR, PC, xPSR    |\n+------------------+\n         |\n         \u2193\n+------------------+\n|  \u4efb\u52a1B\u7ee7\u7eed\u8fd0\u884c   |\n|  SP_B\u6307\u5411\u4efb\u52a1B\u6808 |\n+------------------+\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_6","title":"\u5b8c\u6574\u7684\u6808\u5e27\u7ed3\u6784","text":"<p>\u4efb\u52a1\u5207\u6362\u65f6\u7684\u5b8c\u6574\u6808\u5e27\u7ed3\u6784\uff1a</p> <pre><code>\u5b8c\u6574\u4efb\u52a1\u6808\u5e27\u5e03\u5c40\uff1a\n+------------------+ \u2190 \u6808\u5e95 (\u9ad8\u5730\u5740)\n|   \u6808\u4fdd\u62a4\u533a\u57df     |\n|   (0xDEADBEEF\u7b49) |\n+------------------+\n|   \u5386\u53f2\u6808\u6570\u636e     |\n|   \u51fd\u6570\u8c03\u7528\u5e27     |\n|   \u5c40\u90e8\u53d8\u91cf\u7b49      |\n+------------------+\n|  \u624b\u52a8\u4fdd\u5b58\u5bc4\u5b58\u5668   | \u2190 \u8f6f\u4ef6\u4fdd\u5b58\u533a\u57df\u5f00\u59cb\n|      R11        |\n+------------------+\n|      R10        |\n+------------------+\n|      R9         |\n+------------------+\n|      R8         |\n+------------------+\n|      R7         |\n+------------------+\n|      R6         |\n+------------------+\n|      R5         |\n+------------------+\n|      R4         |\n+------------------+ \u2190 \u624b\u52a8\u4fdd\u5b58\u7ed3\u675f\n|  \u81ea\u52a8\u4fdd\u5b58\u5bc4\u5b58\u5668   | \u2190 \u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u533a\u57df\u5f00\u59cb\n|      R0         |\n+------------------+\n|      R1         |\n+------------------+\n|      R2         |\n+------------------+\n|      R3         |\n+------------------+\n|      R12        |\n+------------------+\n|      LR         |\n+------------------+\n|      PC         |\n+------------------+\n|      xPSR       |\n+------------------+ \u2190 \u5f53\u524dPSP\u6307\u5411\u7684\u4f4d\u7f6e (\u6808\u9876)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_7","title":"\u4e0a\u4e0b\u6587\u4fdd\u5b58\u8fc7\u7a0b\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_8","title":"\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u9636\u6bb5","text":"<p>\u5f53\u4e2d\u65ad\u6216\u5f02\u5e38\u53d1\u751f\u65f6\uff0c\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u90e8\u5206\u5bc4\u5b58\u5668\uff1a</p> <pre><code>\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u8fc7\u7a0b\uff1a\n\u539f\u59cb\u6808\u72b6\u6001\uff1a\n+------------------+ \u2190 PSP\n|   \u4efb\u52a1\u6b63\u5e38\u6570\u636e   |\n|                 |\n+------------------+\n\n\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u540e\uff1a\n+------------------+ \u2190 \u539f\u59cbPSP\n|   \u4efb\u52a1\u6b63\u5e38\u6570\u636e   |\n|                 |\n+------------------+\n|      xPSR       | \u2190 \u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u5f00\u59cb\n+------------------+\n|       PC        |\n+------------------+\n|       LR        |\n+------------------+\n|      R12        |\n+------------------+\n|       R3        |\n+------------------+\n|       R2        |\n+------------------+\n|       R1        |\n+------------------+\n|       R0        |\n+------------------+ \u2190 \u65b0\u7684PSP (\u786c\u4ef6\u4fdd\u5b58\u540e\u6808\u9876)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_9","title":"\u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58\u9636\u6bb5","text":"<p>\u5728PendSV\u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u4e2d\u624b\u52a8\u4fdd\u5b58\u5269\u4f59\u5bc4\u5b58\u5668\uff1a</p> <pre><code>PendSV_Handler:\n    ; \u83b7\u53d6\u5f53\u524d\u4efb\u52a1\u6808\u6307\u9488\n    MRS R0, PSP                   ; R0 = \u5f53\u524dPSP\u503c\n\n    ; \u68c0\u67e5\u662f\u5426\u662f\u7b2c\u4e00\u6b21\u4efb\u52a1\u5207\u6362\n    CBZ R0, skip_save             ; \u5982\u679cPSP\u4e3a0\uff0c\u8df3\u8fc7\u4fdd\u5b58\n\n    ; \u624b\u52a8\u4fdd\u5b58R4-R11\u5230\u4efb\u52a1\u6808\n    STMDB R0!, {R4-R11}           ; \u9012\u51cf\u5b58\u50a8\uff0c\u5411\u4f4e\u5730\u5740\u4fdd\u5b58\n\n    ; \u66f4\u65b0TCB\u4e2d\u7684\u6808\u6307\u9488\n    LDR R1, =CurrentTCB\n    LDR R2, [R1]\n    STR R0, [R2]                  ; \u4fdd\u5b58\u65b0PSP\u5230TCB\n\nskip_save:\n    ; \u7ee7\u7eed\u6267\u884c\u4efb\u52a1\u5207\u6362...\n</code></pre> <p>\u4fdd\u5b58\u540e\u7684\u6808\u72b6\u6001\u53d8\u5316\uff1a</p> <pre><code>\u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58\u540e\u7684\u6808\uff1a\n+------------------+ \u2190 \u539f\u59cbPSP\n|   \u4efb\u52a1\u6b63\u5e38\u6570\u636e   |\n|                 |\n+------------------+\n|      xPSR       |\n+------------------+\n|       PC        |\n+------------------+\n|       LR        |\n+------------------+\n|      R12        |\n+------------------+\n|       R3        |\n+------------------+\n|       R2        |\n+------------------+\n|       R1        |\n+------------------+\n|       R0        |\n+------------------+ \u2190 \u786c\u4ef6\u4fdd\u5b58\u540e\u4f4d\u7f6e\n|       R4        | \u2190 \u8f6f\u4ef6\u624b\u52a8\u4fdd\u5b58\u5f00\u59cb\n+------------------+\n|       R5        |\n+------------------+\n|       R6        |\n+------------------+\n|       R7        |\n+------------------+\n|       R8        |\n+------------------+\n|       R9        |\n+------------------+\n|      R10        |\n+------------------+\n|      R11        |\n+------------------+ \u2190 \u65b0\u7684PSP (\u8f6f\u4ef6\u4fdd\u5b58\u540e\u6808\u9876)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_10","title":"\u4e0a\u4e0b\u6587\u6062\u590d\u8fc7\u7a0b\u8be6\u89e3","text":"<p>\u4e0a\u4e0b\u6587\u6062\u590d\u662f\u4fdd\u5b58\u8fc7\u7a0b\u7684\u7cbe\u786e\u9006\u5e8f\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_11","title":"\u8f6f\u4ef6\u624b\u52a8\u6062\u590d\u9636\u6bb5","text":"<pre><code>PendSV_Handler_Restore:\n    ; \u5207\u6362\u5230\u65b0\u4efb\u52a1\n    LDR R1, =NextTCB\n    LDR R2, [R1]\n    LDR R0, [R2]                 ; \u83b7\u53d6\u65b0\u4efb\u52a1\u7684\u6808\u6307\u9488\n\n    ; \u624b\u52a8\u6062\u590dR4-R11\n    LDMIA R0!, {R4-R11}          ; \u9012\u589e\u52a0\u8f7d\uff0c\u4ece\u6808\u4e2d\u6062\u590d\u5bc4\u5b58\u5668\n\n    ; \u66f4\u65b0PSP\u6307\u5411\u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u533a\u57df\n    MSR PSP, R0\n\n    ; \u51c6\u5907\u5f02\u5e38\u8fd4\u56de\n    ORR LR, LR, #0x04           ; \u786e\u4fdd\u8fd4\u56de\u65f6\u4f7f\u7528PSP\n    BX LR                        ; \u5f02\u5e38\u8fd4\u56de\uff0c\u89e6\u53d1\u786c\u4ef6\u81ea\u52a8\u6062\u590d\n</code></pre> <p>\u6062\u590d\u8fc7\u7a0b\u4e2d\u7684\u6808\u72b6\u6001\u53d8\u5316\uff1a</p> <pre><code>\u4e0a\u4e0b\u6587\u6062\u590d\u8fc7\u7a0b\uff1a\n\u6062\u590d\u524d\u7684\u6808\u72b6\u6001\uff1a\n+------------------+ \u2190 \u6808\u5e95\n|   \u4efb\u52a1B\u5386\u53f2\u6570\u636e   |\n|                 |\n+------------------+\n|      xPSR       |\n+------------------+\n|       PC        |\n+------------------+\n|       LR        |\n+------------------+\n|      R12        |\n+------------------+\n|       R3        |\n+------------------+\n|       R2        |\n+------------------+\n|       R1        |\n+------------------+\n|       R0        |\n+------------------+\n|       R4        |\n+------------------+\n|       R5        |\n+------------------+\n|       R6        |\n+------------------+\n|       R7        |\n+------------------+\n|       R8        |\n+------------------+\n|       R9        |\n+------------------+\n|      R10        |\n+------------------+\n|      R11        |\n+------------------+ \u2190 \u6062\u590d\u524dPSP\u6307\u5411\u7684\u4f4d\u7f6e\n\n\u6062\u590dR4-R11\u540e\uff1a\n+------------------+ \u2190 \u6808\u5e95\n|   \u4efb\u52a1B\u5386\u53f2\u6570\u636e   |\n|                 |\n+------------------+\n|      xPSR       |\n+------------------+\n|       PC        |\n+------------------+\n|       LR        |\n+------------------+\n|      R12        |\n+------------------+\n|       R3        |\n+------------------+\n|       R2        |\n+------------------+\n|       R1        |\n+------------------+\n|       R0        |\n+------------------+ \u2190 \u6062\u590d\u540ePSP\u6307\u5411\u7684\u4f4d\u7f6e\n|   \u5df2\u5f39\u51fa\u7684R4-R11  | (\u6570\u636e\u4ecd\u5728\u6808\u4e2d\uff0c\u4f46\u5bc4\u5b58\u5668\u5df2\u6062\u590d)\n+------------------+\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_12","title":"\u786c\u4ef6\u81ea\u52a8\u6062\u590d\u9636\u6bb5","text":"<p>\u5f02\u5e38\u8fd4\u56de\u65f6\uff0c\u786c\u4ef6\u81ea\u52a8\u6062\u590d\u5269\u4f59\u5bc4\u5b58\u5668\uff1a</p> <pre><code>\u786c\u4ef6\u81ea\u52a8\u6062\u590d\u8fc7\u7a0b\uff1a\n\u5f02\u5e38\u8fd4\u56de\u65f6\u786c\u4ef6\u81ea\u52a8\uff1a\n1. \u4ecePSP\u6307\u5411\u7684\u4f4d\u7f6e\u5f00\u59cb\u5f39\u51fa\u5bc4\u5b58\u5668\n2. \u4f9d\u6b21\u6062\u590dR0, R1, R2, R3, R12, LR, PC, xPSR\n3. PSP\u81ea\u52a8\u66f4\u65b0\u5230\u539f\u59cb\u4f4d\u7f6e\n\n\u6062\u590d\u540e\u7684\u6808\u72b6\u6001\uff1a\n+------------------+ \u2190 \u6808\u5e95\n|   \u4efb\u52a1B\u5386\u53f2\u6570\u636e   |\n|                 |\n+------------------+ \u2190 \u6062\u590d\u540ePSP\u6307\u5411\u7684\u4f4d\u7f6e\n|   \u5df2\u5f39\u51fa\u7684\u6240\u6709     |\n|   \u4e0a\u4e0b\u6587\u6570\u636e      |\n+------------------+\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_13","title":"\u5b8c\u6574\u7684\u6c47\u7f16\u5b9e\u73b0","text":"<p>\u57fa\u4e8eARM Cortex-M\u67b6\u6784\u7684\u5b8c\u6574\u4efb\u52a1\u5207\u6362\u4ee3\u7801\uff1a</p> <pre><code>; \u5e38\u91cf\u5b9a\u4e49\nCurrentTCB    DCD 0    ; \u5f53\u524d\u4efb\u52a1TCB\u6307\u9488\nNextTCB       DCD 0    ; \u4e0b\u4e00\u4e2a\u4efb\u52a1TCB\u6307\u9488\n\n; PendSV\u5f02\u5e38\u5904\u7406\u7a0b\u5e8f - \u4efb\u52a1\u5207\u6362\u6838\u5fc3\nPendSV_Handler:\n    ; \u7981\u7528\u4e2d\u65ad\uff0c\u786e\u4fdd\u539f\u5b50\u64cd\u4f5c\n    CPSID I\n\n    ; \u68c0\u67e5\u662f\u5426\u9700\u8981\u4fdd\u5b58\u5f53\u524d\u4efb\u52a1\u4e0a\u4e0b\u6587\n    MRS R0, PSP                   ; \u83b7\u53d6\u5f53\u524d\u4efb\u52a1\u6808\u6307\u9488\n    CBZ R0, PendSV_Restore        ; \u5982\u679cPSP\u4e3a0\uff0c\u662f\u7b2c\u4e00\u6b21\u5207\u6362\uff0c\u8df3\u8fc7\u4fdd\u5b58\n\n    ; \u4fdd\u5b58\u5f53\u524d\u4efb\u52a1\u4e0a\u4e0b\u6587\n    ; \u786c\u4ef6\u5df2\u81ea\u52a8\u4fdd\u5b58: xPSR, PC, LR, R12, R0-R3\n    ; \u9700\u8981\u624b\u52a8\u4fdd\u5b58: R4-R11\n    STMDB R0!, {R4-R11}           ; \u4fdd\u5b58R4-R11\u5230\u4efb\u52a1\u6808\n\n    ; \u66f4\u65b0\u5f53\u524d\u4efb\u52a1\u7684\u6808\u6307\u9488\u5230TCB\n    LDR R1, =CurrentTCB\n    LDR R2, [R1]\n    STR R0, [R2]                  ; \u4fdd\u5b58\u66f4\u65b0\u540e\u7684PSP\u5230TCB\n\nPendSV_Restore:\n    ; \u6062\u590d\u4e0b\u4e00\u4e2a\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\n    LDR R1, =NextTCB\n    LDR R2, [R1]\n    LDR R0, [R2]                  ; \u83b7\u53d6\u65b0\u4efb\u52a1\u7684\u6808\u6307\u9488\n\n    ; \u624b\u52a8\u6062\u590dR4-R11\n    LDMIA R0!, {R4-R11}           ; \u4ece\u6808\u4e2d\u6062\u590dR4-R11\n\n    ; \u66f4\u65b0PSP\u4e3a\u65b0\u4efb\u52a1\u7684\u6808\u6307\u9488\n    MSR PSP, R0\n\n    ; \u66f4\u65b0CurrentTCB\u4e3aNextTCB\n    LDR R1, =CurrentTCB\n    LDR R2, =NextTCB\n    LDR R3, [R2]\n    STR R3, [R1]\n\n    ; \u542f\u7528\u4e2d\u65ad\n    CPSIE I\n\n    ; \u5f02\u5e38\u8fd4\u56de\uff0c\u786c\u4ef6\u5c06\u81ea\u52a8\u6062\u590d: xPSR, PC, LR, R12, R0-R3\n    ORR LR, LR, #0x04             ; \u786e\u4fdd\u8fd4\u56de\u65f6\u4f7f\u7528PSP\n    BX LR                         ; \u8fd4\u56de\u5230\u65b0\u4efb\u52a1\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_14","title":"\u6808\u6307\u9488\u7ba1\u7406\u673a\u5236","text":"<p>\u4efb\u52a1\u63a7\u5236\u5757(TCB)\u4e0e\u4efb\u52a1\u6808\u7684\u5173\u7cfb\uff1a</p> <pre><code>\u4efb\u52a1\u7ba1\u7406\u6570\u636e\u7ed3\u6784\uff1a\nTCB\u7ed3\u6784\uff1a\n+------------------+\n|  \u4efb\u52a1ID         |\n|  \u4efb\u52a1\u72b6\u6001       |\n|  \u4f18\u5148\u7ea7         |\n|  \u6808\u8d77\u59cb\u5730\u5740     | ---\u2192 +------------------+ \u2190 \u6808\u5e95\n|  \u6808\u5927\u5c0f         |      |   \u4efb\u52a1\u6808\u7a7a\u95f4     |\n|  \u5f53\u524d\u6808\u6307\u9488SP   | ---\u2192 |                 |\n|  \u7b49\u5f85\u4e8b\u4ef6       |      |                 |\n|  \u65f6\u95f4\u7247\u8ba1\u6570\u5668   |      |                 |\n+------------------+      +------------------+ \u2190 SP\u6307\u5411\u7684\u4f4d\u7f6e\n</code></pre> <p>\u4efb\u52a1\u5207\u6362\u65f6\u7684\u6808\u6307\u9488\u66f4\u65b0\uff1a</p> <pre><code>\u6808\u6307\u9488\u5207\u6362\u8fc7\u7a0b\uff1a\n\u4efb\u52a1A \u2192 \u4efb\u52a1B\u5207\u6362\uff1a\n\n\u4efb\u52a1A\u8fd0\u884c\u4e2d:\nTCB_A.SP \u2192 +------------------+ \u2190 PSP\n           |   \u4efb\u52a1A\u6808\u6570\u636e     |\n           |                 |\n           +------------------+\n\n\u4fdd\u5b58\u4e0a\u4e0b\u6587\u540e:\nTCB_A.SP \u2192 +------------------+ \u2190 \u65b0PSP (\u4fdd\u5b58\u540e)\n           |   \u4efb\u52a1A\u4e0a\u4e0b\u6587     |\n           |                 |\n           +------------------+\n\n\u5207\u6362\u5230\u4efb\u52a1B:\nTCB_B.SP \u2192 +------------------+ \u2190 \u65b0PSP (\u6062\u590d\u524d)\n           |   \u4efb\u52a1B\u4e0a\u4e0b\u6587     |\n           |                 |\n           +------------------+\n\n\u6062\u590d\u4e0a\u4e0b\u6587\u540e:\nTCB_B.SP \u2192 +------------------+ \u2190 PSP (\u6062\u590d\u540e)\n           |   \u4efb\u52a1B\u6808\u6570\u636e     |\n           |                 |\n           +------------------+\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_15","title":"\u4e0d\u540c\u67b6\u6784\u7684\u5dee\u5f02\u5904\u7406","text":"<p>\u5404\u67b6\u6784\u4e0a\u4e0b\u6587\u4fdd\u5b58\u7684\u5dee\u5f02\uff1a</p> \u7279\u6027 ARM Cortex-M x86 RISC-V \u81ea\u52a8\u4fdd\u5b58\u5bc4\u5b58\u5668 xPSR, PC, LR, R12, R0-R3 SS, SP, FLAGS, CS, IP PC, \u72b6\u6001\u5bc4\u5b58\u5668 \u624b\u52a8\u4fdd\u5b58\u5bc4\u5b58\u5668 R4-R11 \u901a\u7528\u5bc4\u5b58\u5668 \u8c03\u7528\u8005\u4fdd\u5b58\u5bc4\u5b58\u5668 \u6808\u589e\u957f\u65b9\u5411 \u6ee1\u9012\u51cf \u6ee1\u9012\u51cf \u53ef\u914d\u7f6e \u5f02\u5e38\u8fd4\u56de\u6307\u4ee4 BX LR IRET MRET"},{"location":"EmbeddedSoft/RTOS/Tasks_Switch/#_16","title":"\u6808\u6ea2\u51fa\u68c0\u6d4b\u4e0e\u4f18\u5316","text":"<p>\u6808\u4f7f\u7528\u76d1\u63a7\u548c\u4f18\u5316\u7b56\u7565\uff1a</p> <pre><code>\u6808\u4f7f\u7528\u5206\u6790\uff1a\n+------------------+ \u2190 \u6808\u5e95\n|   \u6808\u4fdd\u62a4\u533a\u57df     |\n|   (\u9b54\u6570\u68c0\u6d4b)     |\n+------------------+ \u2190 \u6808\u8fb9\u754c\n|   \u672a\u4f7f\u7528\u7a7a\u95f4     |\n|                 |\n+------------------+ \u2190 \u6700\u5927\u5386\u53f2\u4f7f\u7528\u6c34\u4f4d\n|   \u5df2\u4f7f\u7528\u7a7a\u95f4     |\n|                 |\n+------------------+ \u2190 \u5f53\u524dSP\u4f4d\u7f6e\n|   \u5b89\u5168\u8fb9\u9645       |\n+------------------+ \u2190 \u6808\u9876\n\n\u4f18\u5316\u7b56\u7565\uff1a\n\u2022 \u6839\u636e\u6700\u5927\u4f7f\u7528\u6c34\u4f4d\u8c03\u6574\u6808\u5927\u5c0f\n\u2022 \u8bbe\u7f6e\u6808\u4fdd\u62a4\u533a\u57df\u68c0\u6d4b\u6ea2\u51fa\n\u2022 \u907f\u514d\u6df1\u5ea6\u9012\u5f52\u548c\u5927\u578b\u5c40\u90e8\u53d8\u91cf\n\u2022 \u4f7f\u7528\u9759\u6001\u5206\u6790\u5de5\u5177\u786e\u5b9a\u5408\u9002\u6808\u5927\u5c0f\n</code></pre> <p>\u8fd9\u6837\u7684\u8bbe\u8ba1\u786e\u4fdd\u4e86\u4efb\u52a1\u5207\u6362\u65f6\u4e0a\u4e0b\u6587\u7684\u5b8c\u6574\u4fdd\u5b58\u548c\u7cbe\u786e\u6062\u590d\uff0c\u662fRTOS\u53ef\u9760\u8fd0\u884c\u7684\u57fa\u7840\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/","title":"\u4e8b\u4ef6\u7ec4\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_2","title":"\u4ec0\u4e48\u662f\u4e8b\u4ef6\u7ec4","text":"<p>\u4e8b\u4ef6\u7ec4\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u4efb\u52a1\u540c\u6b65\u7684\u673a\u5236\uff0c\u5141\u8bb8\u4efb\u52a1\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u6216\u5168\u90e8\u53d1\u751f\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u591a\u6761\u4ef6\u7b49\u5f85\u7cfb\u7edf\uff1a</p> <ul> <li>\u6bcf\u4e2a\u4e8b\u4ef6\u7528\u4e00\u4e2a\u4f4d\u8868\u793a\uff1a32\u4f4d\u7cfb\u7edf\u652f\u630132\u4e2a\u72ec\u7acb\u4e8b\u4ef6</li> <li>\u4efb\u52a1\u53ef\u4ee5\u7b49\u5f85\u4e8b\u4ef6\u7ec4\u5408\uff1a\u53ef\u4ee5\u7b49\u5f85\u4efb\u610f\u4e8b\u4ef6\u3001\u6240\u6709\u4e8b\u4ef6\u6216\u7279\u5b9a\u4e8b\u4ef6\u7ec4\u5408</li> <li>\u4e8b\u4ef6\u72b6\u6001\u6301\u4e45\u5316\uff1a\u4e8b\u4ef6\u4e00\u65e6\u53d1\u751f\u5c31\u4f1a\u4fdd\u6301\uff0c\u76f4\u5230\u88ab\u663e\u5f0f\u6e05\u9664</li> <li>\u591a\u4efb\u52a1\u53ef\u4ee5\u7b49\u5f85\u76f8\u540c\u4e8b\u4ef6\uff1a\u652f\u6301\u4e00\u5bf9\u591a\u7684\u901a\u77e5\u673a\u5236</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_3","title":"\u4e8b\u4ef6\u7ec4\u7684\u5f62\u8c61\u6bd4\u55bb","text":"<p>\u673a\u573a\u822a\u73ed\u4fe1\u606f\u724c\uff1a - \u6bcf\u4e2a\u822a\u73ed\u72b6\u6001\u7528\u4e00\u4e2a\u6307\u793a\u706f\u8868\u793a\uff08\u4e8b\u4ef6\u4f4d\uff09 - \u65c5\u5ba2\u53ef\u4ee5\u7b49\u5f85\u7279\u5b9a\u822a\u73ed\uff08\u7b49\u5f85\u5355\u4e2a\u4e8b\u4ef6\uff09 - \u63a5\u673a\u4eba\u5458\u53ef\u4ee5\u7b49\u5f85\u591a\u4e2a\u822a\u73ed\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\uff08\u7b49\u5f85\u4efb\u610f\u4e8b\u4ef6\uff09 - \u65c5\u884c\u56e2\u9886\u961f\u9700\u8981\u7b49\u5f85\u6240\u6709\u56e2\u5458\u822a\u73ed\u5230\u8fbe\uff08\u7b49\u5f85\u6240\u6709\u4e8b\u4ef6\uff09 - \u822a\u73ed\u5230\u8fbe\u540e\u6307\u793a\u706f\u4fdd\u6301\u4eae\u8d77\uff0c\u76f4\u5230\u88ab\u91cd\u7f6e\uff08\u4e8b\u4ef6\u6301\u4e45\u5316\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_4","title":"\u4e8b\u4ef6\u7ec4\u7684\u6838\u5fc3\u7279\u70b9","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_5","title":"\u4f18\u52bf\u7279\u6027","text":"<p>\u591a\u6761\u4ef6\u7b49\u5f85\uff1a - \u53ef\u4ee5\u540c\u65f6\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6\u6761\u4ef6 - \u652f\u6301\"\u4efb\u610f\u4e8b\u4ef6\"\u548c\"\u6240\u6709\u4e8b\u4ef6\"\u4e24\u79cd\u7b49\u5f85\u6a21\u5f0f - \u7075\u6d3b\u7684\u4e8b\u4ef6\u7ec4\u5408\u6761\u4ef6</p> <p>\u5e7f\u64ad\u901a\u77e5\uff1a - \u4e00\u4e2a\u4e8b\u4ef6\u53ef\u4ee5\u5524\u9192\u591a\u4e2a\u7b49\u5f85\u7684\u4efb\u52a1 - \u652f\u6301\u4e00\u5bf9\u591a\u7684\u901a\u4fe1\u6a21\u5f0f - \u9ad8\u6548\u7684\u4e8b\u4ef6\u5206\u53d1\u673a\u5236</p> <p>\u65e0\u4e22\u5931\u4e8b\u4ef6\uff1a - \u4e8b\u4ef6\u72b6\u6001\u4f1a\u4e00\u76f4\u4fdd\u6301 - \u4e0d\u4f1a\u88ab\u65b0\u4e8b\u4ef6\u8986\u76d6 - \u786e\u4fdd\u4e0d\u4f1a\u9519\u8fc7\u91cd\u8981\u4e8b\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_6","title":"\u9002\u7528\u573a\u666f","text":"<p>\u591a\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\uff1a</p> <pre><code>// \u7b49\u5f85\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u538b\u529b\u4f20\u611f\u5668\u6570\u636e\u5168\u90e8\u5c31\u7eea\nxEventGroupWaitBits(events, TEMP_READY | HUMID_READY | PRESS_READY, \n                   pdTRUE, pdTRUE, portMAX_DELAY);\n</code></pre> <p>\u7cfb\u7edf\u72b6\u6001\u76d1\u63a7\uff1a</p> <pre><code>// \u7b49\u5f85\u4efb\u610f\u9519\u8bef\u4e8b\u4ef6\u53d1\u751f\nxEventGroupWaitBits(events, ERROR_MASK, pdTRUE, pdFALSE, portMAX_DELAY);\n</code></pre> <p>\u591a\u9636\u6bb5\u521d\u59cb\u5316\uff1a</p> <pre><code>// \u7b49\u5f85\u6240\u6709\u5b50\u7cfb\u7edf\u521d\u59cb\u5316\u5b8c\u6210\nxEventGroupWaitBits(events, INIT_MASK, pdTRUE, pdTRUE, portMAX_DELAY);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#freertos","title":"FreeRTOS\u4e8b\u4ef6\u7ec4\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_7","title":"\u521b\u5efa\u548c\u5220\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupcreate-","title":"xEventGroupCreate - \u521b\u5efa\u4e8b\u4ef6\u7ec4","text":"<pre><code>EventGroupHandle_t xEventGroupCreate(void);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u4e8b\u4ef6\u7ec4\u53e5\u67c4 - \u5931\u8d25\uff1aNULL\uff08\u5185\u5b58\u4e0d\u8db3\u65f6\uff09</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>EventGroupHandle_t xSystemEvents;\n\nvoid vInitializeEventGroups(void) {\n    xSystemEvents = xEventGroupCreate();\n    if(xSystemEvents == NULL) {\n        printf(\"ERROR: Failed to create system events group\\n\");\n    } else {\n        printf(\"System events group created successfully\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupcreatestatic-","title":"xEventGroupCreateStatic - \u9759\u6001\u521b\u5efa","text":"<pre><code>EventGroupHandle_t xEventGroupCreateStatic(StaticEventGroup_t *pxEventGroupBuffer);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#veventgroupdelete-","title":"vEventGroupDelete - \u5220\u9664\u4e8b\u4ef6\u7ec4","text":"<pre><code>void vEventGroupDelete(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_8","title":"\u4e8b\u4ef6\u8bbe\u7f6e\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupsetbits-","title":"xEventGroupSetBits - \u8bbe\u7f6e\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupSetBits(EventGroupHandle_t xEventGroup,\n                              const EventBits_t uxBitsToSet);\n</code></pre> <p>\u529f\u80fd\uff1a\u8bbe\u7f6e\u6307\u5b9a\u7684\u4e8b\u4ef6\u4f4d\uff0c\u5e76\u8fd4\u56de\u8bbe\u7f6e\u540e\u7684\u4e8b\u4ef6\u7ec4\u72b6\u6001</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>// \u8bbe\u7f6e\u6e29\u5ea6\u4f20\u611f\u5668\u5c31\u7eea\u4e8b\u4ef6\nEventBits_t current_bits = xEventGroupSetBits(xSystemEvents, TEMP_READY_BIT);\nprintf(\"Event bits after setting: 0x%08lX\\n\", current_bits);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupsetbitsfromisr-","title":"xEventGroupSetBitsFromISR - \u4e2d\u65ad\u4e2d\u8bbe\u7f6e\u4e8b\u4ef6\u4f4d","text":"<pre><code>BaseType_t xEventGroupSetBitsFromISR(EventGroupHandle_t xEventGroup,\n                                    const EventBits_t uxBitsToSet,\n                                    BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_9","title":"\u4e8b\u4ef6\u7b49\u5f85\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupwaitbits-","title":"xEventGroupWaitBits - \u7b49\u5f85\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupWaitBits(EventGroupHandle_t xEventGroup,\n                               const EventBits_t uxBitsToWaitFor,\n                               const BaseType_t xClearOnExit,\n                               const BaseType_t xWaitForAllBits,\n                               TickType_t xTicksToWait);\n</code></pre> <p>\u53c2\u6570\u8be6\u89e3\uff1a - <code>uxBitsToWaitFor</code>\uff1a\u8981\u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d\u63a9\u7801 - <code>xClearOnExit</code>\uff1a\u9000\u51fa\u65f6\u662f\u5426\u6e05\u9664\u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d   - <code>pdTRUE</code>\uff1a\u6210\u529f\u7b49\u5f85\u540e\u6e05\u9664\u8fd9\u4e9b\u4e8b\u4ef6\u4f4d   - <code>pdFALSE</code>\uff1a\u4fdd\u6301\u4e8b\u4ef6\u4f4d\u72b6\u6001\u4e0d\u53d8 - <code>xWaitForAllBits</code>\uff1a\u7b49\u5f85\u6a21\u5f0f   - <code>pdTRUE</code>\uff1a\u7b49\u5f85\u6240\u6709\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u90fd\u8bbe\u7f6e   - <code>pdFALSE</code>\uff1a\u7b49\u5f85\u4efb\u610f\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u8bbe\u7f6e - <code>xTicksToWait</code>\uff1a\u8d85\u65f6\u65f6\u95f4</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u6ee1\u8db3\u6761\u4ef6\u7684\u4e8b\u4ef6\u4f4d\u72b6\u6001 - \u8d85\u65f6\uff1a\u8d85\u65f6\u65f6\u523b\u7684\u4e8b\u4ef6\u4f4d\u72b6\u6001\uff08\u53ef\u80fd\u4e0d\u6ee1\u8db3\u7b49\u5f85\u6761\u4ef6\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_10","title":"\u4e8b\u4ef6\u6e05\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupclearbits-","title":"xEventGroupClearBits - \u6e05\u9664\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupClearBits(EventGroupHandle_t xEventGroup,\n                                const EventBits_t uxBitsToClear);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupclearbitsfromisr-","title":"xEventGroupClearBitsFromISR - \u4e2d\u65ad\u4e2d\u6e05\u9664\u4e8b\u4ef6\u4f4d","text":"<pre><code>BaseType_t xEventGroupClearBitsFromISR(EventGroupHandle_t xEventGroup,\n                                      const EventBits_t uxBitsToClear);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_11","title":"\u4e8b\u4ef6\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupgetbits-","title":"xEventGroupGetBits - \u83b7\u53d6\u5f53\u524d\u4e8b\u4ef6\u4f4d\u72b6\u6001","text":"<pre><code>EventBits_t xEventGroupGetBits(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupgetbitsfromisr-","title":"xEventGroupGetBitsFromISR - \u4e2d\u65ad\u4e2d\u83b7\u53d6\u4e8b\u4ef6\u4f4d\u72b6\u6001","text":"<pre><code>EventBits_t xEventGroupGetBitsFromISR(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_12","title":"\u540c\u6b65\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#xeventgroupsync-","title":"xEventGroupSync - \u4e8b\u4ef6\u7ec4\u540c\u6b65","text":"<pre><code>EventBits_t xEventGroupSync(EventGroupHandle_t xEventGroup,\n                           const EventBits_t uxBitsToSet,\n                           const EventBits_t uxBitsToWaitFor,\n                           TickType_t xTicksToWait);\n</code></pre> <p>\u7279\u6b8a\u529f\u80fd\uff1a\u539f\u5b50\u64cd\u4f5c\uff0c\u5148\u8bbe\u7f6e\u6307\u5b9a\u4e8b\u4ef6\u4f4d\uff0c\u7136\u540e\u7b49\u5f85\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u6761\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#freertos_1","title":"FreeRTOS\u4e8b\u4ef6\u7ec4\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#1and","title":"\u6a21\u5f0f1\uff1a\u591a\u6761\u4ef6\u7b49\u5f85\uff08AND\u6761\u4ef6\uff09","text":"<pre><code>// \u5b9a\u4e49\u4e8b\u4ef6\u4f4d\n#define SENSOR_TEMP_READY    (1UL &lt;&lt; 0)\n#define SENSOR_HUMID_READY   (1UL &lt;&lt; 1)\n#define SENSOR_PRESS_READY   (1UL &lt;&lt; 2)\n#define ALL_SENSORS_READY    (SENSOR_TEMP_READY | SENSOR_HUMID_READY | SENSOR_PRESS_READY)\n\nvoid vDataFusionTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u5c31\u7eea\n        uxBits = xEventGroupWaitBits(xSensorEvents, \n                                    ALL_SENSORS_READY,    // \u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d\n                                    pdTRUE,              // \u6210\u529f\u7b49\u5f85\u540e\u6e05\u9664\u8fd9\u4e9b\u4f4d\n                                    pdTRUE,              // \u9700\u8981\u7b49\u5f85\u6240\u6709\u4f4d\n                                    portMAX_DELAY);      // \u65e0\u9650\u671f\u7b49\u5f85\n\n        if((uxBits &amp; ALL_SENSORS_READY) == ALL_SENSORS_READY) {\n            // \u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u90fd\u5df2\u5c31\u7eea\uff0c\u8fdb\u884c\u6570\u636e\u878d\u5408\n            perform_data_fusion();\n            printf(\"Data fusion completed\\n\");\n        }\n    }\n}\n\nvoid vTemperatureSensorTask(void *pvParameters) {\n    while(1) {\n        // \u8bfb\u53d6\u6e29\u5ea6\u4f20\u611f\u5668\n        read_temperature_sensor();\n\n        // \u8bbe\u7f6e\u6e29\u5ea6\u5c31\u7eea\u4e8b\u4ef6\n        xEventGroupSetBits(xSensorEvents, SENSOR_TEMP_READY);\n\n        vTaskDelay(1000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#2or","title":"\u6a21\u5f0f2\uff1a\u4efb\u610f\u6761\u4ef6\u7b49\u5f85\uff08OR\u6761\u4ef6\uff09","text":"<pre><code>#define NETWORK_CONNECTED    (1UL &lt;&lt; 0)\n#define BLUETOOTH_CONNECTED  (1UL &lt;&lt; 1)\n#define SERIAL_CONNECTED     (1UL &lt;&lt; 2)\n#define ANY_CONNECTION       (NETWORK_CONNECTED | BLUETOOTH_CONNECTED | SERIAL_CONNECTED)\n\nvoid vCommunicationTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u4efb\u610f\u8fde\u63a5\u5efa\u7acb\n        uxBits = xEventGroupWaitBits(xCommEvents,\n                                    ANY_CONNECTION,      // \u7b49\u5f85\u4efb\u610f\u8fde\u63a5\u4e8b\u4ef6\n                                    pdFALSE,            // \u4e0d\u6e05\u9664\u4e8b\u4ef6\u4f4d\n                                    pdFALSE,            // \u4efb\u610f\u4e8b\u4ef6\u4f4d\u8bbe\u7f6e\u5373\u53ef\n                                    portMAX_DELAY);\n\n        // \u68c0\u67e5\u5177\u4f53\u54ea\u4e2a\u8fde\u63a5\u5c31\u7eea\n        if(uxBits &amp; NETWORK_CONNECTED) {\n            printf(\"Network connection established\\n\");\n            handle_network_communication();\n        }\n        if(uxBits &amp; BLUETOOTH_CONNECTED) {\n            printf(\"Bluetooth connection established\\n\");\n            handle_bluetooth_communication();\n        }\n        if(uxBits &amp; SERIAL_CONNECTED) {\n            printf(\"Serial connection established\\n\");\n            handle_serial_communication();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#3","title":"\u6a21\u5f0f3\uff1a\u4e8b\u4ef6\u7ec4\u540c\u6b65","text":"<pre><code>#define TASK1_COMPLETE       (1UL &lt;&lt; 0)\n#define TASK2_COMPLETE       (1UL &lt;&lt; 1)\n#define TASK3_COMPLETE       (1UL &lt;&lt; 2)\n#define ALL_TASKS_COMPLETE   (TASK1_COMPLETE | TASK2_COMPLETE | TASK3_COMPLETE)\n\nvoid vSynchronizationTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u4f7f\u7528\u540c\u6b65\u529f\u80fd\uff1a\u8bbe\u7f6e\u81ea\u5df1\u7684\u5b8c\u6210\u4f4d\uff0c\u5e76\u7b49\u5f85\u6240\u6709\u4efb\u52a1\u5b8c\u6210\n        uxBits = xEventGroupSync(xSyncEvents,\n                                TASK1_COMPLETE,         // \u8bbe\u7f6e\u81ea\u5df1\u7684\u5b8c\u6210\u4f4d\n                                ALL_TASKS_COMPLETE,     // \u7b49\u5f85\u6240\u6709\u4efb\u52a1\u5b8c\u6210\n                                portMAX_DELAY);\n\n        // \u6240\u6709\u4efb\u52a1\u90fd\u5df2\u5b8c\u6210\uff0c\u6267\u884c\u540c\u6b65\u540e\u64cd\u4f5c\n        printf(\"All tasks synchronized, performing collective operation\\n\");\n        perform_collective_operation();\n\n        // \u6e05\u9664\u6240\u6709\u5b8c\u6210\u4f4d\uff0c\u51c6\u5907\u4e0b\u4e00\u8f6e\u540c\u6b65\n        xEventGroupClearBits(xSyncEvents, ALL_TASKS_COMPLETE);\n\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#cmsis-rtos-v2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u7b49\u6548\u529f\u80fd","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#cmsis-rtos-v2_1","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7","text":"<p>CMSIS-RTOS v2 \u4f7f\u7528\u4e8b\u4ef6\u6807\u5fd7\uff08Event Flags\uff09 \u5bf9\u8c61\u6765\u63d0\u4f9b\u7c7b\u4f3cFreeRTOS\u4e8b\u4ef6\u7ec4\u7684\u529f\u80fd\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_13","title":"\u4e8b\u4ef6\u6807\u5fd7\u51fd\u6570\u5bf9\u7167\u8868","text":"FreeRTOS\u4e8b\u4ef6\u7ec4 CMSIS-RTOS v2\u4e8b\u4ef6\u6807\u5fd7 \u8bf4\u660e <code>xEventGroupCreate()</code> <code>osEventFlagsNew()</code> \u521b\u5efa\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 <code>vEventGroupDelete()</code> <code>osEventFlagsDelete()</code> \u5220\u9664\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 <code>xEventGroupSetBits()</code> <code>osEventFlagsSet()</code> \u8bbe\u7f6e\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupClearBits()</code> <code>osEventFlagsClear()</code> \u6e05\u9664\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupWaitBits()</code> <code>osEventFlagsWait()</code> \u7b49\u5f85\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupGetBits()</code> <code>osEventFlagsGet()</code> \u83b7\u53d6\u5f53\u524d\u4e8b\u4ef6\u6807\u5fd7\u72b6\u6001 <code>xEventGroupSync()</code> \u65e0\u76f4\u63a5\u7b49\u6548 \u9700\u8981\u624b\u52a8\u5b9e\u73b0"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#cmsis-rtos-v2_2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u8be6\u7ec6\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagsnew-","title":"osEventFlagsNew - \u521b\u5efa\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>osEventFlagsId_t osEventFlagsNew(const osEventFlagsAttr_t *attr);\n</code></pre> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>osEventFlagsId_t system_events;\n\nvoid initialize_event_flags(void) {\n    osEventFlagsAttr_t event_attr = {\n        .name = \"SystemEvents\"\n    };\n    system_events = osEventFlagsNew(&amp;event_attr);\n\n    if(system_events == NULL) {\n        printf(\"ERROR: Failed to create event flags\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagsset-","title":"osEventFlagsSet - \u8bbe\u7f6e\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsSet(osEventFlagsId_t ef_id, uint32_t flags);\n</code></pre> <ul> <li><code>flags</code> \u5bf9\u5e94\u4e8b\u4ef6\u6807\u5fd7\u4f4d\uff0c\u6bd4\u59820000 0010 \u5c31\u662f\u5bf9\u5e94\u7b2c\u4e8c\u4e2a\u4e8b\u4ef6\u3002\u82e5\u7b2c\u4e8c\u4e2a\u4e8b\u4ef6\u53d1\u751f\uff0c\u5219\u4f1a\u8fd4\u56de\u8be5\u4f4d\u4e3a1\u7684\u7ed3\u679c\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagsclear-","title":"osEventFlagsClear - \u6e05\u9664\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsClear(osEventFlagsId_t ef_id, uint32_t flags);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagswait-","title":"osEventFlagsWait - \u7b49\u5f85\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsWait(osEventFlagsId_t ef_id, uint32_t flags, uint32_t options, uint32_t timeout);\n</code></pre> <p>\u7b49\u5f85\u9009\u9879\uff1a</p> <pre><code>osFlagsWaitAny    // \u7b49\u5f85\u4efb\u610f\u6307\u5b9a\u6807\u5fd7\nosFlagsWaitAll    // \u7b49\u5f85\u6240\u6709\u6307\u5b9a\u6807\u5fd7\nosFlagsNoClear    // \u7b49\u5f85\u540e\u4e0d\u6e05\u9664\u6807\u5fd7\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagsget-","title":"osEventFlagsGet - \u83b7\u53d6\u4e8b\u4ef6\u6807\u5fd7\u72b6\u6001","text":"<pre><code>uint32_t osEventFlagsGet(osEventFlagsId_t ef_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#oseventflagsdelete-","title":"osEventFlagsDelete - \u5220\u9664\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>osStatus_t osEventFlagsDelete(osEventFlagsId_t ef_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_14","title":"\u4e8b\u4ef6\u7ec4/\u4e8b\u4ef6\u6807\u5fd7\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_15","title":"\u8bbe\u8ba1\u539f\u5219","text":"<p>\u5408\u7406\u89c4\u5212\u4e8b\u4ef6\u4f4d\uff1a</p> <pre><code>// \u6309\u529f\u80fd\u5206\u7ec4\u4e8b\u4ef6\u4f4d\n#define SENSOR_EVENTS   0x000000FF  // \u4f4e8\u4f4d\uff1a\u4f20\u611f\u5668\u4e8b\u4ef6\n#define NETWORK_EVENTS  0x0000FF00  // \u4e2d8\u4f4d\uff1a\u7f51\u7edc\u4e8b\u4ef6  \n#define SYSTEM_EVENTS   0x00FF0000  // \u9ad88\u4f4d\uff1a\u7cfb\u7edf\u4e8b\u4ef6\n#define USER_EVENTS     0xFF000000  // \u6700\u9ad88\u4f4d\uff1a\u7528\u6237\u4e8b\u4ef6\n</code></pre> <p>\u907f\u514d\u4e8b\u4ef6\u4f4d\u51b2\u7a81\uff1a</p> <pre><code>// \u4e0d\u597d\u7684\u505a\u6cd5\uff1a\u968f\u610f\u4f7f\u7528\u4e8b\u4ef6\u4f4d\n#define EVENT_A (1UL &lt;&lt; 0)\n#define EVENT_B (1UL &lt;&lt; 1) \n// \u5176\u4ed6\u6a21\u5757\u53ef\u80fd\u610f\u5916\u4f7f\u7528\u76f8\u540c\u4f4d\n\n// \u597d\u7684\u505a\u6cd5\uff1a\u6a21\u5757\u5316\u5206\u914d\n#define MODULE1_EVENT_A (1UL &lt;&lt; 0)\n#define MODULE1_EVENT_B (1UL &lt;&lt; 1)\n#define MODULE2_EVENT_A (1UL &lt;&lt; 8)  // \u4ece\u7b2c8\u4f4d\u5f00\u59cb\n#define MODULE2_EVENT_B (1UL &lt;&lt; 9)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/EventGroup/#_16","title":"\u6027\u80fd\u4f18\u5316","text":"<p>\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7b49\u5f85\uff1a</p> <pre><code>// \u4e0d\u597d\u7684\u505a\u6cd5\uff1a\u9891\u7e41\u7b49\u5f85\u77ed\u8d85\u65f6\nwhile(1) {\n    flags = xEventGroupWaitBits(events, MASK, pdTRUE, pdFALSE, 10);\n    if(flags &amp; MASK) {\n        // \u5904\u7406\u4e8b\u4ef6\n    }\n}\n\n// \u597d\u7684\u505a\u6cd5\uff1a\u5408\u7406\u8bbe\u7f6e\u8d85\u65f6\u6216\u4f7f\u7528\u65e0\u8d85\u65f6\u7b49\u5f85\nflags = xEventGroupWaitBits(events, MASK, pdTRUE, pdFALSE, portMAX_DELAY);\n// \u6216\u8005\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u8bbe\u7f6e\u5408\u7406\u8d85\u65f6\n</code></pre> <p>\u6279\u91cf\u5904\u7406\u4e8b\u4ef6\uff1a</p> <pre><code>void event_processor_task(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u591a\u4e2a\u76f8\u5173\u4e8b\u4ef6\n        uxBits = xEventGroupWaitBits(xEvents, \n                                    EVENT_MASK1 | EVENT_MASK2 | EVENT_MASK3,\n                                    pdFALSE, pdFALSE, portMAX_DELAY);\n\n        // \u6279\u91cf\u5904\u7406\u6240\u6709\u5f85\u5904\u7406\u4e8b\u4ef6\n        if(uxBits &amp; EVENT_MASK1) {\n            handle_event1();\n            xEventGroupClearBits(xEvents, EVENT_MASK1);\n        }\n        if(uxBits &amp; EVENT_MASK2) {\n            handle_event2(); \n            xEventGroupClearBits(xEvents, EVENT_MASK2);\n        }\n        if(uxBits &amp; EVENT_MASK3) {\n            handle_event3();\n            xEventGroupClearBits(xEvents, EVENT_MASK3);\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/","title":"\u4e8b\u4ef6\u7ec4\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_2","title":"\u4ec0\u4e48\u662f\u4e8b\u4ef6\u7ec4","text":"<p>\u4e8b\u4ef6\u7ec4\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u4efb\u52a1\u540c\u6b65\u7684\u673a\u5236\uff0c\u5141\u8bb8\u4efb\u52a1\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\u6216\u5168\u90e8\u53d1\u751f\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u591a\u6761\u4ef6\u7b49\u5f85\u7cfb\u7edf\uff1a</p> <ul> <li>\u6bcf\u4e2a\u4e8b\u4ef6\u7528\u4e00\u4e2a\u4f4d\u8868\u793a\uff1a32\u4f4d\u7cfb\u7edf\u652f\u630132\u4e2a\u72ec\u7acb\u4e8b\u4ef6</li> <li>\u4efb\u52a1\u53ef\u4ee5\u7b49\u5f85\u4e8b\u4ef6\u7ec4\u5408\uff1a\u53ef\u4ee5\u7b49\u5f85\u4efb\u610f\u4e8b\u4ef6\u3001\u6240\u6709\u4e8b\u4ef6\u6216\u7279\u5b9a\u4e8b\u4ef6\u7ec4\u5408</li> <li>\u4e8b\u4ef6\u72b6\u6001\u6301\u4e45\u5316\uff1a\u4e8b\u4ef6\u4e00\u65e6\u53d1\u751f\u5c31\u4f1a\u4fdd\u6301\uff0c\u76f4\u5230\u88ab\u663e\u5f0f\u6e05\u9664</li> <li>\u591a\u4efb\u52a1\u53ef\u4ee5\u7b49\u5f85\u76f8\u540c\u4e8b\u4ef6\uff1a\u652f\u6301\u4e00\u5bf9\u591a\u7684\u901a\u77e5\u673a\u5236</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_3","title":"\u4e8b\u4ef6\u7ec4\u7684\u5f62\u8c61\u6bd4\u55bb","text":"<p>\u673a\u573a\u822a\u73ed\u4fe1\u606f\u724c\uff1a - \u6bcf\u4e2a\u822a\u73ed\u72b6\u6001\u7528\u4e00\u4e2a\u6307\u793a\u706f\u8868\u793a\uff08\u4e8b\u4ef6\u4f4d\uff09 - \u65c5\u5ba2\u53ef\u4ee5\u7b49\u5f85\u7279\u5b9a\u822a\u73ed\uff08\u7b49\u5f85\u5355\u4e2a\u4e8b\u4ef6\uff09 - \u63a5\u673a\u4eba\u5458\u53ef\u4ee5\u7b49\u5f85\u591a\u4e2a\u822a\u73ed\u4e2d\u7684\u4efb\u610f\u4e00\u4e2a\uff08\u7b49\u5f85\u4efb\u610f\u4e8b\u4ef6\uff09 - \u65c5\u884c\u56e2\u9886\u961f\u9700\u8981\u7b49\u5f85\u6240\u6709\u56e2\u5458\u822a\u73ed\u5230\u8fbe\uff08\u7b49\u5f85\u6240\u6709\u4e8b\u4ef6\uff09 - \u822a\u73ed\u5230\u8fbe\u540e\u6307\u793a\u706f\u4fdd\u6301\u4eae\u8d77\uff0c\u76f4\u5230\u88ab\u91cd\u7f6e\uff08\u4e8b\u4ef6\u6301\u4e45\u5316\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_4","title":"\u4e8b\u4ef6\u7ec4\u7684\u6838\u5fc3\u7279\u70b9","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_5","title":"\u4f18\u52bf\u7279\u6027","text":"<p>\u591a\u6761\u4ef6\u7b49\u5f85\uff1a - \u53ef\u4ee5\u540c\u65f6\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6\u6761\u4ef6 - \u652f\u6301\"\u4efb\u610f\u4e8b\u4ef6\"\u548c\"\u6240\u6709\u4e8b\u4ef6\"\u4e24\u79cd\u7b49\u5f85\u6a21\u5f0f - \u7075\u6d3b\u7684\u4e8b\u4ef6\u7ec4\u5408\u6761\u4ef6</p> <p>\u5e7f\u64ad\u901a\u77e5\uff1a - \u4e00\u4e2a\u4e8b\u4ef6\u53ef\u4ee5\u5524\u9192\u591a\u4e2a\u7b49\u5f85\u7684\u4efb\u52a1 - \u652f\u6301\u4e00\u5bf9\u591a\u7684\u901a\u4fe1\u6a21\u5f0f - \u9ad8\u6548\u7684\u4e8b\u4ef6\u5206\u53d1\u673a\u5236</p> <p>\u65e0\u4e22\u5931\u4e8b\u4ef6\uff1a - \u4e8b\u4ef6\u72b6\u6001\u4f1a\u4e00\u76f4\u4fdd\u6301 - \u4e0d\u4f1a\u88ab\u65b0\u4e8b\u4ef6\u8986\u76d6 - \u786e\u4fdd\u4e0d\u4f1a\u9519\u8fc7\u91cd\u8981\u4e8b\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_6","title":"\u9002\u7528\u573a\u666f","text":"<p>\u591a\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\uff1a</p> <pre><code>// \u7b49\u5f85\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u538b\u529b\u4f20\u611f\u5668\u6570\u636e\u5168\u90e8\u5c31\u7eea\nxEventGroupWaitBits(events, TEMP_READY | HUMID_READY | PRESS_READY, \n                   pdTRUE, pdTRUE, portMAX_DELAY);\n</code></pre> <p>\u7cfb\u7edf\u72b6\u6001\u76d1\u63a7\uff1a</p> <pre><code>// \u7b49\u5f85\u4efb\u610f\u9519\u8bef\u4e8b\u4ef6\u53d1\u751f\nxEventGroupWaitBits(events, ERROR_MASK, pdTRUE, pdFALSE, portMAX_DELAY);\n</code></pre> <p>\u591a\u9636\u6bb5\u521d\u59cb\u5316\uff1a</p> <pre><code>// \u7b49\u5f85\u6240\u6709\u5b50\u7cfb\u7edf\u521d\u59cb\u5316\u5b8c\u6210\nxEventGroupWaitBits(events, INIT_MASK, pdTRUE, pdTRUE, portMAX_DELAY);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#freertos","title":"FreeRTOS\u4e8b\u4ef6\u7ec4\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_7","title":"\u521b\u5efa\u548c\u5220\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupcreate-","title":"xEventGroupCreate - \u521b\u5efa\u4e8b\u4ef6\u7ec4","text":"<pre><code>EventGroupHandle_t xEventGroupCreate(void);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u4e8b\u4ef6\u7ec4\u53e5\u67c4 - \u5931\u8d25\uff1aNULL\uff08\u5185\u5b58\u4e0d\u8db3\u65f6\uff09</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>EventGroupHandle_t xSystemEvents;\n\nvoid vInitializeEventGroups(void) {\n    xSystemEvents = xEventGroupCreate();\n    if(xSystemEvents == NULL) {\n        printf(\"ERROR: Failed to create system events group\\n\");\n    } else {\n        printf(\"System events group created successfully\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupcreatestatic-","title":"xEventGroupCreateStatic - \u9759\u6001\u521b\u5efa","text":"<pre><code>EventGroupHandle_t xEventGroupCreateStatic(StaticEventGroup_t *pxEventGroupBuffer);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#veventgroupdelete-","title":"vEventGroupDelete - \u5220\u9664\u4e8b\u4ef6\u7ec4","text":"<pre><code>void vEventGroupDelete(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_8","title":"\u4e8b\u4ef6\u8bbe\u7f6e\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupsetbits-","title":"xEventGroupSetBits - \u8bbe\u7f6e\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupSetBits(EventGroupHandle_t xEventGroup,\n                              const EventBits_t uxBitsToSet);\n</code></pre> <p>\u529f\u80fd\uff1a\u8bbe\u7f6e\u6307\u5b9a\u7684\u4e8b\u4ef6\u4f4d\uff0c\u5e76\u8fd4\u56de\u8bbe\u7f6e\u540e\u7684\u4e8b\u4ef6\u7ec4\u72b6\u6001</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>// \u8bbe\u7f6e\u6e29\u5ea6\u4f20\u611f\u5668\u5c31\u7eea\u4e8b\u4ef6\nEventBits_t current_bits = xEventGroupSetBits(xSystemEvents, TEMP_READY_BIT);\nprintf(\"Event bits after setting: 0x%08lX\\n\", current_bits);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupsetbitsfromisr-","title":"xEventGroupSetBitsFromISR - \u4e2d\u65ad\u4e2d\u8bbe\u7f6e\u4e8b\u4ef6\u4f4d","text":"<pre><code>BaseType_t xEventGroupSetBitsFromISR(EventGroupHandle_t xEventGroup,\n                                    const EventBits_t uxBitsToSet,\n                                    BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_9","title":"\u4e8b\u4ef6\u7b49\u5f85\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupwaitbits-","title":"xEventGroupWaitBits - \u7b49\u5f85\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupWaitBits(EventGroupHandle_t xEventGroup,\n                               const EventBits_t uxBitsToWaitFor,\n                               const BaseType_t xClearOnExit,\n                               const BaseType_t xWaitForAllBits,\n                               TickType_t xTicksToWait);\n</code></pre> <p>\u53c2\u6570\u8be6\u89e3\uff1a - <code>uxBitsToWaitFor</code>\uff1a\u8981\u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d\u63a9\u7801 - <code>xClearOnExit</code>\uff1a\u9000\u51fa\u65f6\u662f\u5426\u6e05\u9664\u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d   - <code>pdTRUE</code>\uff1a\u6210\u529f\u7b49\u5f85\u540e\u6e05\u9664\u8fd9\u4e9b\u4e8b\u4ef6\u4f4d   - <code>pdFALSE</code>\uff1a\u4fdd\u6301\u4e8b\u4ef6\u4f4d\u72b6\u6001\u4e0d\u53d8 - <code>xWaitForAllBits</code>\uff1a\u7b49\u5f85\u6a21\u5f0f   - <code>pdTRUE</code>\uff1a\u7b49\u5f85\u6240\u6709\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u90fd\u8bbe\u7f6e   - <code>pdFALSE</code>\uff1a\u7b49\u5f85\u4efb\u610f\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u8bbe\u7f6e - <code>xTicksToWait</code>\uff1a\u8d85\u65f6\u65f6\u95f4</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u6ee1\u8db3\u6761\u4ef6\u7684\u4e8b\u4ef6\u4f4d\u72b6\u6001 - \u8d85\u65f6\uff1a\u8d85\u65f6\u65f6\u523b\u7684\u4e8b\u4ef6\u4f4d\u72b6\u6001\uff08\u53ef\u80fd\u4e0d\u6ee1\u8db3\u7b49\u5f85\u6761\u4ef6\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_10","title":"\u4e8b\u4ef6\u6e05\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupclearbits-","title":"xEventGroupClearBits - \u6e05\u9664\u4e8b\u4ef6\u4f4d","text":"<pre><code>EventBits_t xEventGroupClearBits(EventGroupHandle_t xEventGroup,\n                                const EventBits_t uxBitsToClear);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupclearbitsfromisr-","title":"xEventGroupClearBitsFromISR - \u4e2d\u65ad\u4e2d\u6e05\u9664\u4e8b\u4ef6\u4f4d","text":"<pre><code>BaseType_t xEventGroupClearBitsFromISR(EventGroupHandle_t xEventGroup,\n                                      const EventBits_t uxBitsToClear);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_11","title":"\u4e8b\u4ef6\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupgetbits-","title":"xEventGroupGetBits - \u83b7\u53d6\u5f53\u524d\u4e8b\u4ef6\u4f4d\u72b6\u6001","text":"<pre><code>EventBits_t xEventGroupGetBits(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupgetbitsfromisr-","title":"xEventGroupGetBitsFromISR - \u4e2d\u65ad\u4e2d\u83b7\u53d6\u4e8b\u4ef6\u4f4d\u72b6\u6001","text":"<pre><code>EventBits_t xEventGroupGetBitsFromISR(EventGroupHandle_t xEventGroup);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_12","title":"\u540c\u6b65\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#xeventgroupsync-","title":"xEventGroupSync - \u4e8b\u4ef6\u7ec4\u540c\u6b65","text":"<pre><code>EventBits_t xEventGroupSync(EventGroupHandle_t xEventGroup,\n                           const EventBits_t uxBitsToSet,\n                           const EventBits_t uxBitsToWaitFor,\n                           TickType_t xTicksToWait);\n</code></pre> <p>\u7279\u6b8a\u529f\u80fd\uff1a\u539f\u5b50\u64cd\u4f5c\uff0c\u5148\u8bbe\u7f6e\u6307\u5b9a\u4e8b\u4ef6\u4f4d\uff0c\u7136\u540e\u7b49\u5f85\u6307\u5b9a\u4e8b\u4ef6\u4f4d\u6761\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#freertos_1","title":"FreeRTOS\u4e8b\u4ef6\u7ec4\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#1and","title":"\u6a21\u5f0f1\uff1a\u591a\u6761\u4ef6\u7b49\u5f85\uff08AND\u6761\u4ef6\uff09","text":"<pre><code>// \u5b9a\u4e49\u4e8b\u4ef6\u4f4d\n#define SENSOR_TEMP_READY    (1UL &lt;&lt; 0)\n#define SENSOR_HUMID_READY   (1UL &lt;&lt; 1)\n#define SENSOR_PRESS_READY   (1UL &lt;&lt; 2)\n#define ALL_SENSORS_READY    (SENSOR_TEMP_READY | SENSOR_HUMID_READY | SENSOR_PRESS_READY)\n\nvoid vDataFusionTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u5c31\u7eea\n        uxBits = xEventGroupWaitBits(xSensorEvents, \n                                    ALL_SENSORS_READY,    // \u7b49\u5f85\u7684\u4e8b\u4ef6\u4f4d\n                                    pdTRUE,              // \u6210\u529f\u7b49\u5f85\u540e\u6e05\u9664\u8fd9\u4e9b\u4f4d\n                                    pdTRUE,              // \u9700\u8981\u7b49\u5f85\u6240\u6709\u4f4d\n                                    portMAX_DELAY);      // \u65e0\u9650\u671f\u7b49\u5f85\n\n        if((uxBits &amp; ALL_SENSORS_READY) == ALL_SENSORS_READY) {\n            // \u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u90fd\u5df2\u5c31\u7eea\uff0c\u8fdb\u884c\u6570\u636e\u878d\u5408\n            perform_data_fusion();\n            printf(\"Data fusion completed\\n\");\n        }\n    }\n}\n\nvoid vTemperatureSensorTask(void *pvParameters) {\n    while(1) {\n        // \u8bfb\u53d6\u6e29\u5ea6\u4f20\u611f\u5668\n        read_temperature_sensor();\n\n        // \u8bbe\u7f6e\u6e29\u5ea6\u5c31\u7eea\u4e8b\u4ef6\n        xEventGroupSetBits(xSensorEvents, SENSOR_TEMP_READY);\n\n        vTaskDelay(1000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#2or","title":"\u6a21\u5f0f2\uff1a\u4efb\u610f\u6761\u4ef6\u7b49\u5f85\uff08OR\u6761\u4ef6\uff09","text":"<pre><code>#define NETWORK_CONNECTED    (1UL &lt;&lt; 0)\n#define BLUETOOTH_CONNECTED  (1UL &lt;&lt; 1)\n#define SERIAL_CONNECTED     (1UL &lt;&lt; 2)\n#define ANY_CONNECTION       (NETWORK_CONNECTED | BLUETOOTH_CONNECTED | SERIAL_CONNECTED)\n\nvoid vCommunicationTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u4efb\u610f\u8fde\u63a5\u5efa\u7acb\n        uxBits = xEventGroupWaitBits(xCommEvents,\n                                    ANY_CONNECTION,      // \u7b49\u5f85\u4efb\u610f\u8fde\u63a5\u4e8b\u4ef6\n                                    pdFALSE,            // \u4e0d\u6e05\u9664\u4e8b\u4ef6\u4f4d\n                                    pdFALSE,            // \u4efb\u610f\u4e8b\u4ef6\u4f4d\u8bbe\u7f6e\u5373\u53ef\n                                    portMAX_DELAY);\n\n        // \u68c0\u67e5\u5177\u4f53\u54ea\u4e2a\u8fde\u63a5\u5c31\u7eea\n        if(uxBits &amp; NETWORK_CONNECTED) {\n            printf(\"Network connection established\\n\");\n            handle_network_communication();\n        }\n        if(uxBits &amp; BLUETOOTH_CONNECTED) {\n            printf(\"Bluetooth connection established\\n\");\n            handle_bluetooth_communication();\n        }\n        if(uxBits &amp; SERIAL_CONNECTED) {\n            printf(\"Serial connection established\\n\");\n            handle_serial_communication();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#3","title":"\u6a21\u5f0f3\uff1a\u4e8b\u4ef6\u7ec4\u540c\u6b65","text":"<pre><code>#define TASK1_COMPLETE       (1UL &lt;&lt; 0)\n#define TASK2_COMPLETE       (1UL &lt;&lt; 1)\n#define TASK3_COMPLETE       (1UL &lt;&lt; 2)\n#define ALL_TASKS_COMPLETE   (TASK1_COMPLETE | TASK2_COMPLETE | TASK3_COMPLETE)\n\nvoid vSynchronizationTask(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u4f7f\u7528\u540c\u6b65\u529f\u80fd\uff1a\u8bbe\u7f6e\u81ea\u5df1\u7684\u5b8c\u6210\u4f4d\uff0c\u5e76\u7b49\u5f85\u6240\u6709\u4efb\u52a1\u5b8c\u6210\n        uxBits = xEventGroupSync(xSyncEvents,\n                                TASK1_COMPLETE,         // \u8bbe\u7f6e\u81ea\u5df1\u7684\u5b8c\u6210\u4f4d\n                                ALL_TASKS_COMPLETE,     // \u7b49\u5f85\u6240\u6709\u4efb\u52a1\u5b8c\u6210\n                                portMAX_DELAY);\n\n        // \u6240\u6709\u4efb\u52a1\u90fd\u5df2\u5b8c\u6210\uff0c\u6267\u884c\u540c\u6b65\u540e\u64cd\u4f5c\n        printf(\"All tasks synchronized, performing collective operation\\n\");\n        perform_collective_operation();\n\n        // \u6e05\u9664\u6240\u6709\u5b8c\u6210\u4f4d\uff0c\u51c6\u5907\u4e0b\u4e00\u8f6e\u540c\u6b65\n        xEventGroupClearBits(xSyncEvents, ALL_TASKS_COMPLETE);\n\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#cmsis-rtos-v2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u7b49\u6548\u529f\u80fd","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#cmsis-rtos-v2_1","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7","text":"<p>CMSIS-RTOS v2 \u4f7f\u7528\u4e8b\u4ef6\u6807\u5fd7\uff08Event Flags\uff09 \u5bf9\u8c61\u6765\u63d0\u4f9b\u7c7b\u4f3cFreeRTOS\u4e8b\u4ef6\u7ec4\u7684\u529f\u80fd\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_13","title":"\u4e8b\u4ef6\u6807\u5fd7\u51fd\u6570\u5bf9\u7167\u8868","text":"FreeRTOS\u4e8b\u4ef6\u7ec4 CMSIS-RTOS v2\u4e8b\u4ef6\u6807\u5fd7 \u8bf4\u660e <code>xEventGroupCreate()</code> <code>osEventFlagsNew()</code> \u521b\u5efa\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 <code>vEventGroupDelete()</code> <code>osEventFlagsDelete()</code> \u5220\u9664\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 <code>xEventGroupSetBits()</code> <code>osEventFlagsSet()</code> \u8bbe\u7f6e\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupClearBits()</code> <code>osEventFlagsClear()</code> \u6e05\u9664\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupWaitBits()</code> <code>osEventFlagsWait()</code> \u7b49\u5f85\u4e8b\u4ef6\u6807\u5fd7 <code>xEventGroupGetBits()</code> <code>osEventFlagsGet()</code> \u83b7\u53d6\u5f53\u524d\u4e8b\u4ef6\u6807\u5fd7\u72b6\u6001 <code>xEventGroupSync()</code> \u65e0\u76f4\u63a5\u7b49\u6548 \u9700\u8981\u624b\u52a8\u5b9e\u73b0"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#cmsis-rtos-v2_2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u8be6\u7ec6\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagsnew-","title":"osEventFlagsNew - \u521b\u5efa\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>osEventFlagsId_t osEventFlagsNew(const osEventFlagsAttr_t *attr);\n</code></pre> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>osEventFlagsId_t system_events;\n\nvoid initialize_event_flags(void) {\n    osEventFlagsAttr_t event_attr = {\n        .name = \"SystemEvents\"\n    };\n    system_events = osEventFlagsNew(&amp;event_attr);\n\n    if(system_events == NULL) {\n        printf(\"ERROR: Failed to create event flags\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagsset-","title":"osEventFlagsSet - \u8bbe\u7f6e\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsSet(osEventFlagsId_t ef_id, uint32_t flags);\n</code></pre> <ul> <li><code>flags</code> \u5bf9\u5e94\u4e8b\u4ef6\u6807\u5fd7\u4f4d\uff0c\u6bd4\u59820000 0010 \u5c31\u662f\u5bf9\u5e94\u7b2c\u4e8c\u4e2a\u4e8b\u4ef6\u3002\u82e5\u7b2c\u4e8c\u4e2a\u4e8b\u4ef6\u53d1\u751f\uff0c\u5219\u4f1a\u8fd4\u56de\u8be5\u4f4d\u4e3a1\u7684\u7ed3\u679c\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagsclear-","title":"osEventFlagsClear - \u6e05\u9664\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsClear(osEventFlagsId_t ef_id, uint32_t flags);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagswait-","title":"osEventFlagsWait - \u7b49\u5f85\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>uint32_t osEventFlagsWait(osEventFlagsId_t ef_id, uint32_t flags, uint32_t options, uint32_t timeout);\n</code></pre> <p>\u7b49\u5f85\u9009\u9879\uff1a</p> <pre><code>osFlagsWaitAny    // \u7b49\u5f85\u4efb\u610f\u6307\u5b9a\u6807\u5fd7\nosFlagsWaitAll    // \u7b49\u5f85\u6240\u6709\u6307\u5b9a\u6807\u5fd7\nosFlagsNoClear    // \u7b49\u5f85\u540e\u4e0d\u6e05\u9664\u6807\u5fd7\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagsget-","title":"osEventFlagsGet - \u83b7\u53d6\u4e8b\u4ef6\u6807\u5fd7\u72b6\u6001","text":"<pre><code>uint32_t osEventFlagsGet(osEventFlagsId_t ef_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#oseventflagsdelete-","title":"osEventFlagsDelete - \u5220\u9664\u4e8b\u4ef6\u6807\u5fd7","text":"<pre><code>osStatus_t osEventFlagsDelete(osEventFlagsId_t ef_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_14","title":"\u4e8b\u4ef6\u7ec4/\u4e8b\u4ef6\u6807\u5fd7\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_15","title":"\u8bbe\u8ba1\u539f\u5219","text":"<p>\u5408\u7406\u89c4\u5212\u4e8b\u4ef6\u4f4d\uff1a</p> <pre><code>// \u6309\u529f\u80fd\u5206\u7ec4\u4e8b\u4ef6\u4f4d\n#define SENSOR_EVENTS   0x000000FF  // \u4f4e8\u4f4d\uff1a\u4f20\u611f\u5668\u4e8b\u4ef6\n#define NETWORK_EVENTS  0x0000FF00  // \u4e2d8\u4f4d\uff1a\u7f51\u7edc\u4e8b\u4ef6  \n#define SYSTEM_EVENTS   0x00FF0000  // \u9ad88\u4f4d\uff1a\u7cfb\u7edf\u4e8b\u4ef6\n#define USER_EVENTS     0xFF000000  // \u6700\u9ad88\u4f4d\uff1a\u7528\u6237\u4e8b\u4ef6\n</code></pre> <p>\u907f\u514d\u4e8b\u4ef6\u4f4d\u51b2\u7a81\uff1a</p> <pre><code>// \u4e0d\u597d\u7684\u505a\u6cd5\uff1a\u968f\u610f\u4f7f\u7528\u4e8b\u4ef6\u4f4d\n#define EVENT_A (1UL &lt;&lt; 0)\n#define EVENT_B (1UL &lt;&lt; 1) \n// \u5176\u4ed6\u6a21\u5757\u53ef\u80fd\u610f\u5916\u4f7f\u7528\u76f8\u540c\u4f4d\n\n// \u597d\u7684\u505a\u6cd5\uff1a\u6a21\u5757\u5316\u5206\u914d\n#define MODULE1_EVENT_A (1UL &lt;&lt; 0)\n#define MODULE1_EVENT_B (1UL &lt;&lt; 1)\n#define MODULE2_EVENT_A (1UL &lt;&lt; 8)  // \u4ece\u7b2c8\u4f4d\u5f00\u59cb\n#define MODULE2_EVENT_B (1UL &lt;&lt; 9)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Event_Group/#_16","title":"\u6027\u80fd\u4f18\u5316","text":"<p>\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7b49\u5f85\uff1a</p> <pre><code>// \u4e0d\u597d\u7684\u505a\u6cd5\uff1a\u9891\u7e41\u7b49\u5f85\u77ed\u8d85\u65f6\nwhile(1) {\n    flags = xEventGroupWaitBits(events, MASK, pdTRUE, pdFALSE, 10);\n    if(flags &amp; MASK) {\n        // \u5904\u7406\u4e8b\u4ef6\n    }\n}\n\n// \u597d\u7684\u505a\u6cd5\uff1a\u5408\u7406\u8bbe\u7f6e\u8d85\u65f6\u6216\u4f7f\u7528\u65e0\u8d85\u65f6\u7b49\u5f85\nflags = xEventGroupWaitBits(events, MASK, pdTRUE, pdFALSE, portMAX_DELAY);\n// \u6216\u8005\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u8bbe\u7f6e\u5408\u7406\u8d85\u65f6\n</code></pre> <p>\u6279\u91cf\u5904\u7406\u4e8b\u4ef6\uff1a</p> <pre><code>void event_processor_task(void *pvParameters) {\n    EventBits_t uxBits;\n\n    while(1) {\n        // \u7b49\u5f85\u591a\u4e2a\u76f8\u5173\u4e8b\u4ef6\n        uxBits = xEventGroupWaitBits(xEvents, \n                                    EVENT_MASK1 | EVENT_MASK2 | EVENT_MASK3,\n                                    pdFALSE, pdFALSE, portMAX_DELAY);\n\n        // \u6279\u91cf\u5904\u7406\u6240\u6709\u5f85\u5904\u7406\u4e8b\u4ef6\n        if(uxBits &amp; EVENT_MASK1) {\n            handle_event1();\n            xEventGroupClearBits(xEvents, EVENT_MASK1);\n        }\n        if(uxBits &amp; EVENT_MASK2) {\n            handle_event2(); \n            xEventGroupClearBits(xEvents, EVENT_MASK2);\n        }\n        if(uxBits &amp; EVENT_MASK3) {\n            handle_event3();\n            xEventGroupClearBits(xEvents, EVENT_MASK3);\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/FreeRTOS/","title":"FreeRTOS","text":"<ul> <li>\u7b80\u4ecb</li> <li>\u76ee\u5f55</li> <li>\u4f8b\u7a0b\u5e93</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/FreeRTOS/#_1","title":"\u7b80\u4ecb","text":"<p>FreeRTOS\u610f\u4e3a\u514d\u8d39\u7684RTOS\uff0c\u5168\u5f00\u6e90\u514d\u8d39\uff0c\u4f53\u79ef\u5c0f\u3001\u53ef\u88c1\u526a\u3001\u5bb9\u6613\u79fb\u690d\u3001\u652f\u6301\u62a2\u5360/\u534f\u7a0b/\u65f6\u95f4\u7247\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/FreeRTOS/#_2","title":"\u76ee\u5f55","text":"<ul> <li>\u79fb\u690d</li> <li>\u5185\u5b58\u7ba1\u7406</li> <li>\u5217\u8868\u548c\u5217\u8868\u9879</li> <li>\u4efb\u52a1\u521b\u5efa\u548c\u5220\u9664</li> <li>\u4efb\u52a1\u6302\u8d77\u548c\u6062\u590d</li> <li>\u7a7a\u95f2\u4efb\u52a1\u4e0e\u4efb\u52a1\u8c03\u5ea6</li> <li>\u4e2d\u65ad\u7ba1\u7406</li> <li>\u4efb\u52a1\u8c03\u5ea6\u5668\u7684\u6302\u8d77</li> <li>\u4efb\u52a1\u8c03\u5ea6</li> <li>\u540c\u6b65\u3001\u4e92\u65a5\u4e0e\u901a\u4fe1</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/FreeRTOS/#_3","title":"\u4f8b\u7a0b\u5e93","text":"<ul> <li>\u5b66\u4e601 \u4efb\u52a1\u521b\u5efa\u548c\u5220\u9664</li> <li>\u5b66\u4e602 \u4efb\u52a1\u6302\u8d77\u548c\u6062\u590d</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/","title":"\u4ecb\u7ecd","text":"<p>\u7a7a\u95f2\u4efb\u52a1(Idle\u4efb\u52a1)\u7684\u4f5c\u7528\u4e4b\u4e00\uff1a\u91ca\u653e\u88ab\u5220\u9664\u7684\u4efb\u52a1\u7684\u5185\u5b58\u3002</p> <p>\u9664\u4e86\u4e0a\u8ff0\u76ee\u7684\u4e4b\u5916\uff0c\u4e3a\u4ec0\u4e48\u5fc5\u987b\u8981\u6709\u7a7a\u95f2\u4efb\u52a1\uff1f\u4e00\u4e2a\u826f\u597d\u7684\u7a0b\u5e8f\uff0c\u5b83\u7684\u4efb\u52a1\u90fd\u662f\u4e8b\u4ef6\u9a71\u52a8\u7684\uff1a\u5e73\u65f6\u5927\u90e8\u5206\u65f6\u95f4\u5904\u4e8e\u963b\u585e\u72b6\u6001\u3002\u6709\u53ef\u80fd\u6211\u4eec\u81ea\u5df1\u521b\u5efa\u7684\u6240\u6709\u4efb\u52a1\u90fd\u65e0\u6cd5\u6267\u884c\uff0c\u4f46\u662f\u8c03\u5ea6\u5668\u5fc5\u987b\u80fd\u627e\u5230\u4e00\u4e2a\u53ef\u4ee5\u8fd0\u884c\u7684\u4efb\u52a1\uff1a\u6240\u4ee5\uff0c\u6211\u4eec\u8981\u63d0\u4f9b\u7a7a\u95f2\u4efb\u52a1\u3002\u5728\u4f7f\u7528vTaskStartScheduler()\u51fd\u6570\u6765\u521b\u5efa\u3001\u542f\u52a8\u8c03\u5ea6\u5668\u65f6\uff0c\u8fd9\u4e2a\u51fd\u6570\u5185\u90e8\u4f1a\u521b\u5efa\u7a7a\u95f2\u4efb\u52a1\uff1a</p> <ul> <li>\u7a7a\u95f2\u4efb\u52a1\u4f18\u5148\u7ea7\u4e3a0\uff1a\u5b83\u4e0d\u80fd\u963b\u788d\u7528\u6237\u4efb\u52a1\u8fd0\u884c</li> <li>\u7a7a\u95f2\u4efb\u52a1\u8981\u4e48\u5904\u4e8e\u5c31\u7eea\u6001\uff0c\u8981\u4e48\u5904\u4e8e\u8fd0\u884c\u6001\uff0c\u6c38\u8fdc\u4e0d\u4f1a\u963b\u585e</li> </ul> <p>\u7a7a\u95f2\u4efb\u52a1\u7684\u4f18\u5148\u7ea7\u4e3a0\uff0c\u8fd9\u610f\u5473\u7740\u4e00\u65e6\u67d0\u4e2a\u7528\u6237\u7684\u4efb\u52a1\u53d8\u4e3a\u5c31\u7eea\u6001\uff0c\u90a3\u4e48\u7a7a\u95f2\u4efb\u52a1\u9a6c\u4e0a\u88ab\u5207\u6362\u51fa\u53bb\uff0c\u8ba9\u8fd9\u4e2a\u7528\u6237\u4efb\u52a1\u8fd0\u884c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8bf4\u7528\u6237\u4efb\u52a1\"\u62a2\u5360\"(pre-empt)\u4e86\u7a7a\u95f2\u4efb\u52a1\uff0c\u8fd9\u662f\u7531\u8c03\u5ea6\u5668\u5b9e\u73b0\u7684\u3002</p> <p>\u8981\u6ce8\u610f\u7684\u662f\uff1a\u5982\u679c\u4f7f\u7528vTaskDelete()\u6765\u5220\u9664\u4efb\u52a1\uff0c\u90a3\u4e48\u4f60\u5c31\u8981\u786e\u4fdd\u7a7a\u95f2\u4efb\u52a1\u6709\u673a\u4f1a\u6267\u884c\uff0c\u5426\u5219\u5c31\u65e0\u6cd5\u91ca\u653e\u88ab\u5220\u9664\u4efb\u52a1\u7684\u5185\u5b58\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u4e00\u4e2a\u7a7a\u95f2\u4efb\u52a1\u7684\u94a9\u5b50\u51fd\u6570(Idle Task Hook Functions)\uff0c\u7a7a\u95f2\u4efb\u52a1\u7684\u5faa\u73af\u6bcf\u6267\u884c\u4e00\u6b21\uff0c\u5c31\u4f1a\u8c03\u7528\u4e00\u6b21\u94a9\u5b50\u51fd\u6570\u3002\u94a9\u5b50\u51fd\u6570\u7684\u4f5c\u7528\u6709\u8fd9\u4e9b\uff1a</p> <ul> <li>\u6267\u884c\u4e00\u4e9b\u4f4e\u4f18\u5148\u7ea7\u7684\u3001\u540e\u53f0\u7684\u3001\u9700\u8981\u8fde\u7eed\u6267\u884c\u7684\u51fd\u6570</li> <li>\u6d4b\u91cf\u7cfb\u7edf\u7684\u7a7a\u95f2\u65f6\u95f4\uff1a\u7a7a\u95f2\u4efb\u52a1\u80fd\u88ab\u6267\u884c\u5c31\u610f\u5473\u7740\u6240\u6709\u7684\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u90fd\u505c\u6b62\u4e86\uff0c\u6240\u4ee5\u6d4b\u91cf\u7a7a\u95f2\u4efb\u52a1\u5360\u636e\u7684\u65f6\u95f4\uff0c\u5c31\u53ef\u4ee5\u7b97\u51fa\u5904\u7406\u5668\u5360\u7528\u7387\u3002</li> <li>\u8ba9\u7cfb\u7edf\u8fdb\u5165\u7701\u7535\u6a21\u5f0f\uff1a\u7a7a\u95f2\u4efb\u52a1\u80fd\u88ab\u6267\u884c\u5c31\u610f\u5473\u7740\u6ca1\u6709\u91cd\u8981\u7684\u4e8b\u60c5\u8981\u505a\uff0c\u5f53\u7136\u53ef\u4ee5\u8fdb\u5165\u7701\u7535\u6a21\u5f0f\u4e86\u3002</li> <li>\u7a7a\u95f2\u4efb\u52a1\u7684\u94a9\u5b50\u51fd\u6570\u7684\u9650\u5236\uff1a</li> <li>\u4e0d\u80fd\u5bfc\u81f4\u7a7a\u95f2\u4efb\u52a1\u8fdb\u5165\u963b\u585e\u72b6\u6001\u3001\u6682\u505c\u72b6\u6001</li> <li>\u5982\u679c\u4f60\u4f1a\u4f7f\u7528vTaskDelete()\u6765\u5220\u9664\u4efb\u52a1\uff0c\u90a3\u4e48\u94a9\u5b50\u51fd\u6570\u8981\u975e\u5e38\u9ad8\u6548\u5730\u6267\u884c\u3002\u5982\u679c\u7a7a\u95f2\u4efb\u52a1\u79fb\u690d\u5361\u5728\u94a9\u5b50\u51fd\u6570\u91cc\u7684\u8bdd\uff0c\u5b83\u5c31\u65e0\u6cd5\u91ca\u653e\u5185\u5b58\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_2","title":"\u4f7f\u7528\u94a9\u5b50\u51fd\u6570","text":"<p>\u5728<code>FreeRTOS\\Source\\tasks.c</code>\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u5982\u4e0b\u4ee3\u7801\uff0c\u6240\u4ee5\u524d\u63d0\u5c31\u662f\uff1a</p> <ul> <li>\u628a\u8fd9\u4e2a\u5b8f\u5b9a\u4e49\u4e3a1\uff1aconfigUSE_IDLE_HOOK</li> <li>\u5b9e\u73b0vApplicationIdleHook\u51fd\u6570</li> </ul> <pre><code>#if ( configUSE_IDLE_HOOK == 1 )\n{\n    extern void vApplicationIdleHook( void );\n\n    /* Call the user defined function from within the idle task.  This\n    allows the application designer to add background functionality\n    without the overhead of a separate task.\n    NOTE: vApplicationIdleHook() MUST NOT, UNDER ANY CIRCUMSTANCES,\n    CALL A FUNCTION THAT MIGHT BLOCK. */\n    vApplicationIdleHook();\n}\n#endif /* configUSE_IDLE_HOOK */\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_3","title":"\u8c03\u5ea6","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_4","title":"\u6982\u5ff5","text":"<p>\u8fd9\u4e9b\u77e5\u8bc6\u5728\u524d\u9762\u90fd\u63d0\u5230\u8fc7\u4e86\uff0c\u8fd9\u91cc\u603b\u7ed3\u4e00\u4e0b\u3002</p> <p>\u6b63\u5728\u8fd0\u884c\u7684\u4efb\u52a1\uff0c\u88ab\u79f0\u4e3a\"\u6b63\u5728\u4f7f\u7528\u5904\u7406\u5668\"\uff0c\u5b83\u5904\u4e8e\u8fd0\u884c\u72b6\u6001\u3002\u5728\u5355\u5904\u7406\u7cfb\u7edf\u4e2d\uff0c\u4efb\u4f55\u65f6\u95f4\u91cc\u53ea\u80fd\u6709\u4e00\u4e2a\u4efb\u52a1\u5904\u4e8e\u8fd0\u884c\u72b6\u6001\u3002</p> <p>\u975e\u8fd0\u884c\u72b6\u6001\u7684\u4efb\u52a1\uff0c\u5b83\u5904\u4e8e\u8fd93\u4e2d\u72b6\u6001\u4e4b\u4e00\uff1a\u963b\u585e(Blocked)\u3001\u6682\u505c(Suspended)\u3001\u5c31\u7eea(Ready)\u3002\u5c31\u7eea\u6001\u7684\u4efb\u52a1\uff0c\u53ef\u4ee5\u88ab\u8c03\u5ea6\u5668\u6311\u9009\u51fa\u6765\u5207\u6362\u4e3a\u8fd0\u884c\u72b6\u6001\uff0c\u8c03\u5ea6\u5668\u6c38\u8fdc\u90fd\u662f\u6311\u9009\u6700\u9ad8\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u6001\u4efb\u52a1\u5e76\u8ba9\u5b83\u8fdb\u5165\u8fd0\u884c\u72b6\u6001\u3002</p> <p>\u963b\u585e\u72b6\u6001\u7684\u4efb\u52a1\uff0c\u5b83\u5728\u7b49\u5f85\"\u4e8b\u4ef6\"\uff0c\u5f53\u4e8b\u4ef6\u53d1\u751f\u65f6\u4efb\u52a1\u5c31\u4f1a\u8fdb\u5165\u5c31\u7eea\u72b6\u6001\u3002\u4e8b\u4ef6\u5206\u4e3a\u4e24\u7c7b\uff1a\u65f6\u95f4\u76f8\u5173\u7684\u4e8b\u4ef6\u3001\u540c\u6b65\u4e8b\u4ef6 - \u65f6\u95f4\u76f8\u5173\u7684\u4e8b\u4ef6\uff0c\u5c31\u662f\u8bbe\u7f6e\u8d85\u65f6\u65f6\u95f4\uff1a\u5728\u6307\u5b9a\u65f6\u95f4\u5185\u963b\u585e\uff0c\u65f6\u95f4\u5230\u4e86\u5c31\u8fdb\u5165\u5c31\u7eea\u72b6\u6001\u3002\u4f7f\u7528\u65f6\u95f4\u76f8\u5173\u7684\u4e8b\u4ef6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5468\u671f\u6027\u7684\u529f\u80fd\u3001\u53ef\u4ee5\u5b9e\u73b0\u8d85\u65f6\u529f\u80fd\u3002 - \u540c\u6b65\u4e8b\u4ef6\u5c31\u662f\uff1a\u67d0\u4e2a\u4efb\u52a1\u5728\u7b49\u5f85\u67d0\u4e9b\u4fe1\u606f\uff0c\u522b\u7684\u4efb\u52a1\u6216\u8005\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4f1a\u7ed9\u5b83\u53d1\u9001\u4fe1\u606f\u3002\u600e\u4e48\"\u53d1\u9001\u4fe1\u606f\"\uff1f\u65b9\u6cd5\u5f88\u591a\uff0c\u6709\uff1a\u4efb\u52a1\u901a\u77e5(task notification)\u3001\u961f\u5217(queue)\u3001\u4e8b\u4ef6\u7ec4(event group)\u3001\u4fe1\u53f7\u91cf(semaphoe)\u3001\u4e92\u65a5\u91cf(mutex)\u7b49\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7528\u6765\u53d1\u9001\u540c\u6b65\u4fe1\u606f\uff0c\u6bd4\u5982\u8868\u793a\u67d0\u4e2a\u5916\u8bbe\u5f97\u5230\u4e86\u6570\u636e\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_5","title":"\u914d\u7f6e\u8c03\u5ea6\u7b97\u6cd5","text":"<p>\u6240\u8c13\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5c31\u662f\u600e\u4e48\u786e\u5b9a\u54ea\u4e2a\u5c31\u7eea\u6001\u7684\u4efb\u52a1\u53ef\u4ee5\u5207\u6362\u4e3a\u8fd0\u884c\u72b6\u6001\u3002</p> <p>\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6FreeRTOSConfig.h\u7684\u4e24\u4e2a\u914d\u7f6e\u9879\u6765\u914d\u7f6e\u8c03\u5ea6\u7b97\u6cd5\uff1aconfigUSE_PREEMPTION\u3001configUSE_TIME_SLICING\u3002</p> <p>\u8fd8\u6709\u7b2c\u4e09\u4e2a\u914d\u7f6e\u9879\uff1aconfigUSE_TICKLESS_IDLE\uff0c\u5b83\u662f\u4e00\u4e2a\u9ad8\u7ea7\u9009\u9879\uff0c\u7528\u4e8e\u5173\u95edTick\u4e2d\u65ad\u6765\u5b9e\u73b0\u7701\u7535\uff0c\u540e\u7eed\u5355\u72ec\u8bb2\u89e3\u3002\u73b0\u5728\u6211\u4eec\u5047\u8bbeconfigUSE_TICKLESS_IDLE\u88ab\u8bbe\u4e3a0\uff0c\u5148\u4e0d\u4f7f\u7528\u8fd9\u4e2a\u529f\u80fd\u3002 \u8c03\u5ea6\u7b97\u6cd5\u7684\u884c\u4e3a\u4e3b\u8981\u4f53\u73b0\u5728\u4e24\u65b9\u9762\uff1a\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u5148\u8fd0\u884c\u3001\u540c\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u6001\u4efb\u52a1\u5982\u4f55\u88ab\u9009\u4e2d\u3002\u8c03\u5ea6\u7b97\u6cd5\u8981\u786e\u4fdd\u540c\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u6001\u4efb\u52a1\uff0c\u80fd\"\u8f6e\u6d41\"\u8fd0\u884c\uff0c\u7b56\u7565\u662f\"\u8f6e\u8f6c\u8c03\u5ea6\"(Round Robin Scheduling)\u3002\u8f6e\u8f6c\u8c03\u5ea6\u5e76\u4e0d\u4fdd\u8bc1\u4efb\u52a1\u7684\u8fd0\u884c\u65f6\u95f4\u662f\u516c\u5e73\u5206\u914d\u7684\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u7ec6\u5316\u65f6\u95f4\u7684\u5206\u914d\u65b9\u6cd5\u3002 \u4ece3\u4e2a\u89d2\u5ea6\u7edf\u4e00\u7406\u89e3\u591a\u79cd\u8c03\u5ea6\u7b97\u6cd5\uff1a</p> <ul> <li>\u53ef\u5426\u62a2\u5360\uff1f\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u80fd\u5426\u4f18\u5148\u6267\u884c(\u914d\u7f6e\u9879: configUSE_PREEMPTION)<ul> <li>\u53ef\u4ee5\uff1a\u88ab\u79f0\u4f5c\"\u53ef\u62a2\u5360\u8c03\u5ea6\"(Pre-emptive)\uff0c\u9ad8\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u4efb\u52a1\u9a6c\u4e0a\u6267\u884c\uff0c\u4e0b\u9762\u518d\u7ec6\u5316\u3002</li> <li>\u4e0d\u53ef\u4ee5\uff1a\u4e0d\u80fd\u62a2\u5c31\u53ea\u80fd\u534f\u5546\u4e86\uff0c\u88ab\u79f0\u4f5c\"\u5408\u4f5c\u8c03\u5ea6\u6a21\u5f0f\"(Co-operative Scheduling)<ul> <li>\u5f53\u524d\u4efb\u52a1\u6267\u884c\u65f6\uff0c\u66f4\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u5c31\u7eea\u4e86\u4e5f\u4e0d\u80fd\u9a6c\u4e0a\u8fd0\u884c\uff0c\u53ea\u80fd\u7b49\u5f85\u5f53\u524d\u4efb\u52a1\u4e3b\u52a8\u8ba9\u51faCPU\u8d44\u6e90\u3002</li> <li>\u5176\u4ed6\u540c\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u4e5f\u53ea\u80fd\u7b49\u5f85\uff1a\u66f4\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u90fd\u4e0d\u80fd\u62a2\u5360\uff0c\u5e73\u7ea7\u7684\u66f4\u5e94\u8be5\u8001\u5b9e\u70b9</li> </ul> </li> </ul> </li> <li>\u53ef\u62a2\u5360\u7684\u524d\u63d0\u4e0b\uff0c\u540c\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u662f\u5426\u8f6e\u6d41\u6267\u884c(\u914d\u7f6e\u9879\uff1aconfigUSE_TIME_SLICING)<ul> <li>\u8f6e\u6d41\u6267\u884c\uff1a\u88ab\u79f0\u4e3a\"\u65f6\u95f4\u7247\u8f6e\u8f6c\"(Time Slicing)\uff0c\u540c\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u8f6e\u6d41\u6267\u884c\uff0c\u4f60\u6267\u884c\u4e00\u4e2a\u65f6\u95f4\u7247\u3001\u6211\u518d\u6267\u884c\u4e00\u4e2a\u65f6\u95f4\u7247</li> <li>\u4e0d\u8f6e\u6d41\u6267\u884c\uff1a\u82f1\u6587\u4e3a\"without Time Slicing\"\uff0c\u5f53\u524d\u4efb\u52a1\u4f1a\u4e00\u76f4\u6267\u884c\uff0c\u76f4\u5230\u4e3b\u52a8\u653e\u5f03\u3001\u6216\u8005\u88ab\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u62a2\u5360</li> </ul> </li> <li>\u5728\"\u53ef\u62a2\u5360\"+\"\u65f6\u95f4\u7247\u8f6e\u8f6c\"\u7684\u524d\u63d0\u4e0b\uff0c\u8fdb\u4e00\u6b65\u7ec6\u5316\uff1a\u7a7a\u95f2\u4efb\u52a1\u662f\u5426\u8ba9\u6b65\u4e8e\u7528\u6237\u4efb\u52a1(\u914d\u7f6e\u9879\uff1aconfigIDLE_SHOULD_YIELD)<ul> <li>\u7a7a\u95f2\u4efb\u52a1\u4f4e\u4eba\u4e00\u7b49\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u5faa\u73af\uff0c\u5c31\u770b\u770b\u662f\u5426\u4e3b\u52a8\u8ba9\u4f4d\u7ed9\u7528\u6237\u4efb\u52a1</li> <li>\u7a7a\u95f2\u4efb\u52a1\u8ddf\u7528\u6237\u4efb\u52a1\u4e00\u6837\uff0c\u5927\u5bb6\u8f6e\u6d41\u6267\u884c\uff0c\u6ca1\u6709\u8c01\u66f4\u7279\u6b8a \u5217\u8868\u5982\u4e0b\uff1a</li> </ul> </li> </ul> \u914d\u7f6e\u9879 A B C D E configUSE_PREEMPTION 1 1 1 1 0 configUSE_TIME_SLICING 1 1 0 0 x configIDLE_SHOULD_YIELD 1 0 1 0 x \u8bf4\u660e \u5e38\u7528 \u5f88\u5c11\u7528 \u5f88\u5c11\u7528 \u5f88\u5c11\u7528 \u51e0\u4e4e\u4e0d\u7528 <p>\u6ce8\uff1a</p> <ul> <li>A\uff1a\u53ef\u62a2\u5360+\u65f6\u95f4\u7247\u8f6e\u8f6c+\u7a7a\u95f2\u4efb\u52a1\u8ba9\u6b65</li> <li>B\uff1a\u53ef\u62a2\u5360+\u65f6\u95f4\u7247\u8f6e\u8f6c+\u7a7a\u95f2\u4efb\u52a1\u4e0d\u8ba9\u6b65</li> <li>C\uff1a\u53ef\u62a2\u5360+\u975e\u65f6\u95f4\u7247\u8f6e\u8f6c+\u7a7a\u95f2\u4efb\u52a1\u8ba9\u6b65</li> <li>D\uff1a\u53ef\u62a2\u5360+\u975e\u65f6\u95f4\u7247\u8f6e\u8f6c+\u7a7a\u95f2\u4efb\u52a1\u4e0d\u8ba9\u6b65</li> <li>E\uff1a\u5408\u4f5c\u8c03\u5ea6</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_6","title":"\u8c03\u5ea6\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_7","title":"\u5bf9\u6bd4\u6548\u679c: \u62a2\u5360\u4e0e\u5426","text":"<p>\u5728\u00a0FreeRTOSConfig.h\u00a0\u4e2d\uff0c\u5b9a\u4e49\u8fd9\u6837\u7684\u5b8f\uff0c\u5bf9\u6bd4\u903b\u8f91\u5206\u6790\u4eea\u7684\u6548\u679c\uff1a</p> <pre><code>// \u5b9e\u9a8c1\uff1a\u62a2\u5360\n##define configUSE_PREEMPTION       1\n##define configUSE_TIME_SLICING      1\n##define configIDLE_SHOULD_YIELD        1\n\n// \u5b9e\u9a8c2\uff1a\u4e0d\u62a2\u5360\n##define configUSE_PREEMPTION       0\n##define configUSE_TIME_SLICING      1\n##define configIDLE_SHOULD_YIELD        1\n</code></pre> <p>\u5bf9\u6bd4\u7ed3\u679c\u4e3a\uff1a</p> <ul> <li>\u62a2\u5360\u65f6\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u5c31\u7eea\u65f6\uff0c\u5c31\u53ef\u4ee5\u9a6c\u4e0a\u6267\u884c</li> <li>\u4e0d\u62a2\u5360\u65f6\uff1a\u4f18\u5148\u7ea7\u5931\u53bb\u610f\u4e49\u4e86\uff0c\u65e2\u7136\u4e0d\u80fd\u62a2\u5360\u5c31\u53ea\u80fd\u534f\u5546\u4e86\uff0c\u56fe\u4e2d\u4efb\u52a11\u4e00\u76f4\u5728\u8fd0\u884c(\u4e00\u70b9\u90fd\u6ca1\u6709\u534f\u5546\u7cbe\u795e)\uff0c\u5176\u4ed6\u4efb\u52a1\u90fd\u65e0\u6cd5\u6267\u884c\u3002\u5373\u4f7f\u4efb\u52a13\u7684vTaskDelay\u5df2\u7ecf\u8d85\u65f6\u3001\u5373\u4f7f\u5b83\u7684\u4f18\u5148\u7ea7\u66f4\u9ad8\uff0c\u90fd\u6ca1\u529e\u6cd5\u6267\u884c\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_8","title":"\u5bf9\u6bd4\u6548\u679c: \u65f6\u95f4\u7247\u8f6e\u8f6c\u4e0e\u5426","text":"<p>\u5728\u00a0FreeRTOSConfig.h\u00a0\u4e2d\uff0c\u5b9a\u4e49\u8fd9\u6837\u7684\u5b8f\uff0c\u5bf9\u6bd4\u903b\u8f91\u5206\u6790\u4eea\u7684\u6548\u679c\uff1a</p> <pre><code>// \u5b9e\u9a8c1\uff1a\u65f6\u95f4\u7247\u8f6e\u8f6c\n##define configUSE_PREEMPTION       1\n##define configUSE_TIME_SLICING      1\n##define configIDLE_SHOULD_YIELD        1\n\n// \u5b9e\u9a8c2\uff1a\u65f6\u95f4\u7247\u4e0d\u8f6e\u8f6c\n##define configUSE_PREEMPTION       1\n##define configUSE_TIME_SLICING      0\n##define configIDLE_SHOULD_YIELD        1\n</code></pre> <p>\u4ece\u4e0b\u9762\u7684\u5bf9\u6bd4\u56fe\u53ef\u4ee5\u77e5\u9053\uff1a</p> <ul> <li>\u65f6\u95f4\u7247\u8f6e\u8f6c\uff1a\u5728Tick\u4e2d\u65ad\u4e2d\u4f1a\u5f15\u8d77\u4efb\u52a1\u5207\u6362</li> <li>\u65f6\u95f4\u7247\u4e0d\u8f6e\u8f6c\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u5c31\u7eea\u65f6\u4f1a\u5f15\u8d77\u4efb\u52a1\u5207\u6362\uff0c\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u4e0d\u518d\u8fd0\u884c\u65f6\u4e5f\u4f1a\u5f15\u8d77\u4efb\u52a1\u5207\u6362\u3002\u53ef\u4ee5\u770b\u5230\u4efb\u52a13\u5c31\u7eea\u540e\u53ef\u4ee5\u9a6c\u4e0a\u6267\u884c\uff0c\u5b83\u8fd0\u884c\u5b8c\u6bd5\u540e\u5bfc\u81f4\u4efb\u52a1\u5207\u6362\u3002\u5176\u4ed6\u65f6\u95f4\u6ca1\u6709\u4efb\u52a1\u5207\u6362\uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a11\u3001\u4efb\u52a12\u90fd\u8fd0\u884c\u4e86\u5f88\u957f\u65f6\u95f4\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Idle%26Scheduling/#_9","title":"\u5bf9\u6bd4\u6548\u679c: \u7a7a\u95f2\u4efb\u52a1\u8ba9\u6b65","text":"<p>\u5728\u00a0FreeRTOSConfig.h\u00a0\u4e2d\uff0c\u5b9a\u4e49\u8fd9\u6837\u7684\u5b8f\uff0c\u5bf9\u6bd4\u903b\u8f91\u5206\u6790\u4eea\u7684\u6548\u679c\uff1a</p> <pre><code>// \u5b9e\u9a8c1\uff1a\u7a7a\u95f2\u4efb\u52a1\u8ba9\u6b65\n##define configUSE_PREEMPTION       1\n##define configUSE_TIME_SLICING      1\n##define configIDLE_SHOULD_YIELD        1\n\n// \u5b9e\u9a8c2\uff1a\u7a7a\u95f2\u4efb\u52a1\u4e0d\u8ba9\u6b65\n##define configUSE_PREEMPTION       1\n##define configUSE_TIME_SLICING      1\n##define configIDLE_SHOULD_YIELD        0\n</code></pre> <p>\u4ece\u4e0b\u9762\u7684\u5bf9\u6bd4\u56fe\u53ef\u4ee5\u77e5\u9053\uff1a</p> <ul> <li>\u8ba9\u6b65\u65f6\uff1a\u5728\u7a7a\u95f2\u4efb\u52a1\u7684\u6bcf\u4e2a\u5faa\u73af\u4e2d\uff0c\u4f1a\u4e3b\u52a8\u8ba9\u51fa\u5904\u7406\u5668\uff0c\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230flagIdelTaskrun\u7684\u6ce2\u5f62\u5f88\u5c0f</li> <li>\u4e0d\u8ba9\u6b65\u65f6\uff1a\u7a7a\u95f2\u4efb\u52a1\u8ddf\u4efb\u52a11\u3001\u4efb\u52a12\u540c\u7b49\u5f85\u9047\uff0c\u5b83\u4eec\u7684\u6ce2\u5f62\u5bbd\u5ea6\u662f\u5dee\u4e0d\u591a\u7684</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/","title":"Interrupt Mgmt","text":"<ul> <li>\u4e2d\u65ad\u57fa\u7840\u6982\u5ff5<ul> <li>\u4ec0\u4e48\u662f\u4e2d\u65ad</li> <li>\u4e2d\u65ad\u7c7b\u578b</li> <li>\u4e2d\u65ad\u5904\u7406\u8fc7\u7a0b</li> </ul> </li> <li>\u4e2d\u65ad\u548c\u5f02\u5e38\u7684\u533a\u522b<ul> <li>\u6bd4\u55bb\uff1a\u516c\u53f8\u8fd0\u8425</li> <li>\u6280\u672f\u5c42\u9762\u7684\u533a\u522b</li> <li>\u4e3a\u4ec0\u4e48\u53eb\u201c\u7cfb\u7edf\u5f02\u5e38\u201d\uff1f</li> <li>\u201c\u5f02\u5e38\u5728\u54ea\uff1f\u201d- \u5f02\u5e38\u7684\u5b9e\u9645\u4f4d\u7f6e</li> <li>\u4e3a\u4ec0\u4e48RTOS\u8981\u7528SVC\u5f02\u5e38\uff1f</li> <li>\u603b\u7ed3</li> </ul> </li> <li>FreeRTOS\u4e2d\u65ad\u4f18\u5148\u7ea7\u5206\u7ec4<ul> <li>\u4f18\u5148\u7ea7\u6570\u503c\u89c4\u5219</li> <li>Cortex-M \u4f18\u5148\u7ea7\u5206\u7ec4\u673a\u5236<ul> <li>\u4f18\u5148\u7ea7\u5206\u7ec4\u539f\u7406</li> </ul> </li> <li>FreeRTOS \u4e2d\u65ad\u4f18\u5148\u7ea7\u914d\u7f6e<ul> <li>FreeRTOSConfig.h \u5173\u952e\u914d\u7f6e</li> <li>FreeRTOS \u4e2d\u65ad\u4f18\u5148\u7ea7\u5c42\u6b21\u7ed3\u6784<ul> <li>\u9ad8\u4e8econfigMAX_SYSCALL_INTERRUPT_PRIORITY</li> <li>\u4f4e\u4e8econfigMAX_SYSCALL_INTERRUPT_PRIORITY</li> <li>\u4f18\u5148\u7ea715: \u6700\u4f4e\u4f18\u5148\u7ea7</li> </ul> </li> </ul> </li> </ul> </li> <li>\u4e34\u754c\u6bb5\uff08Critical Section\uff09<ul> <li>\u4e34\u754c\u6bb5\u57fa\u672c\u6982\u5ff5</li> <li>\u4ee3\u7801\u4fdd\u62a4</li> <li>\u4e2d\u65ad\u5c4f\u853d\u57fa\u7840\u6982\u5ff5<ul> <li>\u4ec0\u4e48\u662f\u4e2d\u65ad\u5c4f\u853d</li> <li>\u5c4f\u853d\u7528\u9014</li> <li>\u5bc4\u5b58\u5668\u8bf4\u660e</li> </ul> </li> <li>\u4e2d\u65ad\u5c4f\u853d\u63a7\u5236 API<ul> <li>FreeRTOS<ul> <li>\u5e95\u5c42</li> <li>\u5e94\u7528\u5c42</li> </ul> </li> <li>CMSIS v2<ul> <li>\u5e95\u5c42</li> <li>NVIC \u4e2d\u65ad\u63a7\u5236\u5668 API</li> <li>\u5e94\u7528\u5c42<ul> <li>\u5c01\u88c5\u53c2\u8003</li> </ul> </li> </ul> </li> <li>API\u5bf9\u6bd4</li> </ul> </li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_1","title":"\u4e2d\u65ad\u57fa\u7840\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_2","title":"\u4ec0\u4e48\u662f\u4e2d\u65ad","text":"<p>\u4e2d\u65ad\u662f\u5904\u7406\u5668\u54cd\u5e94\u5916\u90e8\u4e8b\u4ef6\u7684\u4e00\u79cd\u673a\u5236\uff0c\u5f53\u5916\u8bbe\u9700\u8981CPU\u5904\u7406\u65f6\uff0c\u4f1a\u901a\u8fc7\u4e2d\u65ad\u4fe1\u53f7\u901a\u77e5CPU\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_3","title":"\u4e2d\u65ad\u7c7b\u578b","text":"<ul> <li> <p>\u786c\u4ef6\u4e2d\u65ad\uff1a\u7531\u5916\u8bbe\u4ea7\u751f\uff08\u5b9a\u65f6\u5668\u3001\u4e32\u53e3\u3001GPIO\u7b49\uff09</p> </li> <li> <p>\u8f6f\u4ef6\u4e2d\u65ad\uff1a\u7531\u8f6f\u4ef6\u6307\u4ee4\u89e6\u53d1</p> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_4","title":"\u4e2d\u65ad\u5904\u7406\u8fc7\u7a0b","text":"<ol> <li>\u4fdd\u5b58\u5f53\u524d\u6267\u884c\u4e0a\u4e0b\u6587</li> <li>\u8df3\u8f6c\u5230\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\uff08ISR\uff09</li> <li>\u6267\u884cISR\u4e2d\u7684\u5904\u7406\u903b\u8f91</li> <li>\u6062\u590d\u4e0a\u4e0b\u6587\u5e76\u8fd4\u56de</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_5","title":"\u4e2d\u65ad\u548c\u5f02\u5e38\u7684\u533a\u522b","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_6","title":"\u6bd4\u55bb\uff1a\u516c\u53f8\u8fd0\u8425","text":"<p>\u628aCPU\u60f3\u8c61\u6210\u4e00\u5bb6\u516c\u53f8\u7684CEO\u3002</p> <ul> <li>\u4e2d\u65ad (Interrupt) = \u5916\u90e8\u6765\u7535</li> <li>\u6765\u81ea\u516c\u53f8\u5916\u90e8\uff1a\u5ba2\u6237\u3001\u4f9b\u5e94\u5546\u3001\u653f\u5e9c\u673a\u6784</li> <li>\u4f18\u5148\u7ea7\u76f8\u5bf9\u8f83\u4f4e\uff0cCEO\u53ef\u4ee5\u8bf4\u201c\u7a0d\u7b49\uff0c\u6211\u5904\u7406\u5b8c\u5185\u90e8\u4e8b\u52a1\u518d\u63a5\u201d</li> <li> <p>\u4f8b\u5b50\uff1aUSB\u8bbe\u5907\u63d2\u5165\u3001\u7f51\u7edc\u6570\u636e\u5230\u8fbe\u3001\u6309\u952e\u6309\u4e0b</p> </li> <li> <p>\u5f02\u5e38 (Exception) = \u5185\u90e8\u7d27\u6025\u4f1a\u8bae</p> </li> <li>\u6765\u81ea\u516c\u53f8\u5185\u90e8\uff1a\u8d22\u52a1\u5371\u673a\u3001\u6cd5\u5f8b\u8bc9\u8bbc\u3001\u7cfb\u7edf\u5d29\u6e83</li> <li>\u4f18\u5148\u7ea7\u5f88\u9ad8\uff0cCEO\u5fc5\u987b\u7acb\u5373\u5904\u7406</li> <li>\u4f8b\u5b50\uff1a\u9664\u96f6\u9519\u8bef\u3001\u5185\u5b58\u8bbf\u95ee\u8fdd\u89c4\u3001\u7cfb\u7edf\u8c03\u7528</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_7","title":"\u6280\u672f\u5c42\u9762\u7684\u533a\u522b","text":"\u7279\u6027 \u4e2d\u65ad (Interrupt) \u5f02\u5e38 (Exception) \u6765\u6e90 \u5916\u90e8\u786c\u4ef6\u8bbe\u5907 CPU\u5185\u90e8\u6267\u884c\u6307\u4ee4\u65f6\u4ea7\u751f \u540c\u6b65\u6027 \u5f02\u6b65\uff08\u968f\u65f6\u53ef\u80fd\u53d1\u751f\uff09 \u540c\u6b65\uff08\u7279\u5b9a\u6307\u4ee4\u5fc5\u7136\u89e6\u53d1\uff09 \u53ef\u9884\u6d4b\u6027 \u4e0d\u53ef\u9884\u6d4b \u76f8\u5bf9\u53ef\u9884\u6d4b \u5904\u7406\u4f18\u5148\u7ea7 \u76f8\u5bf9\u8f83\u4f4e \u76f8\u5bf9\u8f83\u9ad8 \u4f8b\u5b50 \u5b9a\u65f6\u5668\u3001UART\u3001GPIO \u9664\u96f6\u3001\u5185\u5b58\u9519\u8bef\u3001SVC\u6307\u4ee4"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_8","title":"\u4e3a\u4ec0\u4e48\u53eb\u201c\u7cfb\u7edf\u5f02\u5e38\u201d\uff1f","text":"<p>\u7cfb\u7edf\u5f02\u5e38\u662f\u5f02\u5e38\u7684\u4e00\u4e2a\u5b50\u7c7b\uff0c\u7279\u6307\u90a3\u4e9b\u7528\u4e8e\u64cd\u4f5c\u7cfb\u7edf\u6838\u5fc3\u529f\u80fd\u7684\u5f02\u5e38\uff1a</p> <pre><code>// \u8fd9\u4e9b\u5c31\u662fARM Cortex-M\u7684\u4e3b\u8981\u7cfb\u7edf\u5f02\u5e38\uff1a\nReset_Handler        // \u590d\u4f4d\nNMI_Handler          // \u4e0d\u53ef\u5c4f\u853d\u4e2d\u65ad  \nHardFault_Handler    // \u786c\u4ef6\u9519\u8bef\nMemManage_Handler    // \u5185\u5b58\u7ba1\u7406\u9519\u8bef\nBusFault_Handler     // \u603b\u7ebf\u9519\u8bef\nUsageFault_Handler   // \u6307\u4ee4\u4f7f\u7528\u9519\u8bef\nSVC_Handler          // \u7cfb\u7edf\u8c03\u7528 \u2190 \u5c31\u662f\u8fd9\u4e2a\uff01\nDebugMon_Handler     // \u8c03\u8bd5\u76d1\u63a7\nPendSV_Handler       // \u53ef\u6302\u8d77\u7684\u7cfb\u7edf\u8c03\u7528\nSysTick_Handler      // \u7cfb\u7edf\u5b9a\u65f6\u5668\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#-","title":"\u201c\u5f02\u5e38\u5728\u54ea\uff1f\u201d- \u5f02\u5e38\u7684\u5b9e\u9645\u4f4d\u7f6e","text":"<p>\u5728ARM Cortex-M\u4e2d\uff0c\u5f02\u5e38\u548c\u4e2d\u65ad\u90fd\u5728\u540c\u4e00\u4e2a\u5730\u65b9\u2014\u2014\u5f02\u5e38\u5411\u91cf\u8868\uff1a</p> <pre><code>// \u5728\u5185\u5b58\u5f00\u59cb\u7684\u4f4d\u7f6e\uff08\u901a\u5e38\u662f0x00000000\uff09\nVector_Table:\n    .long   __StackTop         /* \u6808\u9876 */\n    .long   Reset_Handler      /* \u590d\u4f4d\u5f02\u5e38 */\n    .long   NMI_Handler        /* NMI\u5f02\u5e38 */\n    .long   HardFault_Handler  /* \u786c\u4ef6\u9519\u8bef\u5f02\u5e38 */\n    // ... \u66f4\u591a\u7cfb\u7edf\u5f02\u5e38\n    .long   SVC_Handler        /* \u7cfb\u7edf\u8c03\u7528\u5f02\u5e38 \u2190 \u5c31\u5728\u8fd9\u91cc\uff01*/\n    // ... \u7136\u540e\u662f\u5916\u8bbe\u4e2d\u65ad\n    .long   TIM2_IRQHandler    /* \u5b9a\u65f6\u56682\u4e2d\u65ad */\n    .long   USART1_IRQHandler  /* \u4e32\u53e31\u4e2d\u65ad */\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#rtossvc","title":"\u4e3a\u4ec0\u4e48RTOS\u8981\u7528SVC\u5f02\u5e38\uff1f","text":"<p>\u770b\u8fd9\u4e2a\u4f8b\u5b50\u5c31\u660e\u767d\u4e86\uff1a</p> <pre><code>// \u7528\u6237\u4ee3\u7801\u8c03\u7528OS API\nosThreadNew(my_function, NULL, NULL);\n\n// \u7f16\u8bd1\u540e\u5b9e\u9645\u4e0a\uff1a\nBL osThreadNew        // \u8df3\u8f6c\u5230OS\u51fd\u6570\n  \u2193 (\u5728OS\u51fd\u6570\u5185\u90e8)\nSVC 0x03             // \u89e6\u53d1SVC\u5f02\u5e38\uff0c\u8fdb\u5165\u7279\u6743\u6a21\u5f0f\n  \u2193\nSVC_Handler()        // \u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u6267\u884c\u771f\u6b63\u7684\u7cfb\u7edf\u529f\u80fd\n</code></pre> <p>\u5173\u952e\u597d\u5904\uff1a - \u7528\u6237\u4ee3\u7801\u5728\u975e\u7279\u6743\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u5173\u952e\u786c\u4ef6 - \u901a\u8fc7SVC\u5f02\u5e38\u201c\u6572\u95e8\u201d\uff0c\u8ba9\u64cd\u4f5c\u7cfb\u7edf\u5728\u7279\u6743\u6a21\u5f0f\u4e0b\u4ee3\u4e3a\u6267\u884c - \u63d0\u4f9b\u4e86\u5b89\u5168\u8fb9\u754c\uff0c\u9632\u6b62\u7528\u6237\u7a0b\u5e8f\u7834\u574f\u7cfb\u7edf</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_9","title":"\u603b\u7ed3","text":"<ul> <li>\u4e2d\u65ad\uff1a\u5916\u90e8\u786c\u4ef6\u201c\u6253\u6270\u201dCPU</li> <li>\u5f02\u5e38\uff1aCPU\u81ea\u5df1\u6267\u884c\u6307\u4ee4\u65f6\u201c\u51fa\u72b6\u51b5\u201d\u6216\u201c\u9700\u8981\u7279\u6743\u201d</li> <li>\u7cfb\u7edf\u5f02\u5e38\uff1a\u64cd\u4f5c\u7cfb\u7edf\u4e13\u7528\u7684\u7279\u6b8a\u5f02\u5e38\u673a\u5236</li> <li>SVC\uff1a\u662f\u5e94\u7528\u7a0b\u5e8f\u5411\u64cd\u4f5c\u7cfb\u7edf\u8bf7\u6c42\u670d\u52a1\u7684\u201c\u5b89\u5168\u95e8\u201d</li> </ul> <p>\u5f02\u5e38\u5c31\u50cf\u662fCPU\u7684\u201c\u5185\u90e8\u7d27\u6025\u5904\u7406\u673a\u5236\u201d\uff0c\u800c\u4e2d\u65ad\u662f\u201c\u5916\u90e8\u6253\u6270\u673a\u5236\u201d\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#freertos","title":"FreeRTOS\u4e2d\u65ad\u4f18\u5148\u7ea7\u5206\u7ec4","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_10","title":"\u4f18\u5148\u7ea7\u6570\u503c\u89c4\u5219","text":"<ul> <li>\u6570\u503c\u8d8a\u5c0f\uff0c\u4f18\u5148\u7ea7\u8d8a\u9ad8</li> <li>\u4e0d\u540cCortex-M\u82af\u7247\u652f\u6301\u7684\u4f18\u5148\u7ea7\u4f4d\u6570\u4e0d\u540c\uff08STM32\u4e3a4\u4f4d\uff0c0~15\u4f18\u5148\u7ea7\uff09</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#cortex-m","title":"Cortex-M \u4f18\u5148\u7ea7\u5206\u7ec4\u673a\u5236","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_11","title":"\u4f18\u5148\u7ea7\u5206\u7ec4\u539f\u7406","text":"<p>Cortex-M\u5904\u7406\u5668\u4f7f\u7528\u4f18\u5148\u7ea7\u5206\u7ec4\u5bc4\u5b58\u5668\u6765\u5212\u5206\u62a2\u5360\u4f18\u5148\u7ea7\u548c\u5b50\u4f18\u5148\u7ea7\uff1a</p> <pre><code>// \u4f18\u5148\u7ea7\u5206\u7ec4\u914d\u7f6e\uff088\u4f4d\u4f18\u5148\u7ea7\u5bc4\u5b58\u5668\u793a\u4f8b\uff09\n// \u5206\u7ec40: 7\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c1\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:1]\u62a2\u5360, [0:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec41: 6\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c2\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:2]\u62a2\u5360, [1:0]\u5b50\u4f18\u5148\u7ea7  \n// \u5206\u7ec42: 5\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c3\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:3]\u62a2\u5360, [2:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec43: 4\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c4\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:4]\u62a2\u5360, [3:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec44: 3\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c5\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:5]\u62a2\u5360, [4:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec45: 2\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c6\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:6]\u62a2\u5360, [5:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec46: 1\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c7\u4f4d\u5b50\u4f18\u5148\u7ea7  [7:7]\u62a2\u5360, [6:0]\u5b50\u4f18\u5148\u7ea7\n// \u5206\u7ec47: 0\u4f4d\u62a2\u5360\u4f18\u5148\u7ea7\uff0c8\u4f4d\u5b50\u4f18\u5148\u7ea7  \u65e0\u62a2\u5360, [7:0]\u5b50\u4f18\u5148\u7ea7\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#freertos_1","title":"FreeRTOS \u4e2d\u65ad\u4f18\u5148\u7ea7\u914d\u7f6e","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#freertosconfigh","title":"FreeRTOSConfig.h \u5173\u952e\u914d\u7f6e","text":"<pre><code>// FreeRTOSConfig.h \u4e2d\u7684\u4e2d\u65ad\u914d\u7f6e\n\n/*  Cortex-M \u7279\u5b9a\u914d\u7f6e  */\n\n// \u6700\u4f4e\u4e2d\u65ad\u4f18\u5148\u7ea7\uff08\u6570\u503c\u8d8a\u5927\u4f18\u5148\u7ea7\u8d8a\u4f4e\uff09\n#define configLIBRARY_LOWEST_INTERRUPT_PRIORITY    15\n\n// \u5141\u8bb8\u8c03\u7528FreeRTOS API\u7684\u6700\u9ad8\u4e2d\u65ad\u4f18\u5148\u7ea7\n#define configLIBRARY_MAX_SYSCALL_INTERRUPT_PRIORITY 5\n\n// \u5185\u6838\u53ef\u7ba1\u7406\u7684\u4e2d\u65ad\u4f18\u5148\u7ea7\uff08\u81ea\u52a8\u8ba1\u7b97\uff09\uff0c__NVIC_PRIO_BITS\u5e94\u8be5\u8bbe\u4e3a4\uff0c\u5373group4\n#define configMAX_SYSCALL_INTERRUPT_PRIORITY \\\n    (configLIBRARY_MAX_SYSCALL_INTERRUPT_PRIORITY &lt;&lt; (8 - __NVIC_PRIO_BITS))\n\n// \u7cfb\u7edf\u8282\u62cd\u5668\u4e2d\u65ad\u4f18\u5148\u7ea7\n#define configKERNEL_INTERRUPT_PRIORITY \\\n    (configLIBRARY_LOWEST_INTERRUPT_PRIORITY &lt;&lt; (8 - __NVIC_PRIO_BITS))\n\n// PendSV \u4e2d\u65ad\u4f18\u5148\u7ea7  \n#define configPEND_SV_INTERRUPT_PRIORITY \\\n    (configLIBRARY_LOWEST_INTERRUPT_PRIORITY &lt;&lt; (8 - __NVIC_PRIO_BITS))\n\n// \u7cfb\u7edf\u8282\u62cd\u9891\u7387\uff08Hz\uff09\n#define configTICK_RATE_HZ 1000\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#freertos_2","title":"FreeRTOS \u4e2d\u65ad\u4f18\u5148\u7ea7\u5c42\u6b21\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#configmax_syscall_interrupt_priority","title":"\u9ad8\u4e8econfigMAX_SYSCALL_INTERRUPT_PRIORITY","text":"<ul> <li>\u4e0d\u80fd\u8c03\u7528FreeRTOS API\uff0c\u7528\u4e8e\u7d27\u6025\u4e2d\u65ad\uff0c\u9ad8\u4e8e\u4e00\u5207\u4efb\u52a1</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#configmax_syscall_interrupt_priority_1","title":"\u4f4e\u4e8econfigMAX_SYSCALL_INTERRUPT_PRIORITY","text":"<ul> <li>\u53ef\u4ee5\u8c03\u7528FreeRTOS API FromISR\u51fd\u6570</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#15","title":"\u4f18\u5148\u7ea715:    \u6700\u4f4e\u4f18\u5148\u7ea7","text":"<ul> <li>\u7528\u4e8eSysTick\uff08\u5fc3\u810f\uff09\u548cPendSV\uff08\u8c03\u5ea6\uff09</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#critical-section","title":"\u4e34\u754c\u6bb5\uff08Critical Section\uff09","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_12","title":"\u4e34\u754c\u6bb5\u57fa\u672c\u6982\u5ff5","text":"<p>\u4e34\u754c\u6bb5\uff08Critical Section\uff09\u662f\u6307\u5fc5\u987b\u539f\u5b50\u6027\u6267\u884c\u7684\u4ee3\u7801\u6bb5\uff0c\u5728\u8fd9\u6bb5\u4ee3\u7801\u6267\u884c\u671f\u95f4\u4e0d\u80fd\u88ab\u6253\u65ad\uff0c\u5fc5\u987b\u4fdd\u8bc1\u5176\u6267\u884c\u7684\u5b8c\u6574\u6027\u548c\u4e00\u81f4\u6027\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_13","title":"\u4ee3\u7801\u4fdd\u62a4","text":"<p>\u4ee3\u7801\u4f1a\u88ab\u4e2d\u65ad\u3001\u8c03\u5ea6\u6253\u65ad\uff0c\u4f46\u4e34\u754c\u6bb5\u4e0d\u80fd\u88ab\u6253\u65ad\uff0c\u56e0\u6b64\u6211\u4eec\u6709\u76f8\u5e94\u7684\u4ee3\u7801\u4fdd\u62a4\u63aa\u65bd\u3002\u5b83\u4eec\u901a\u8fc7\u4e2d\u65ad\u5c4f\u853d\u5b9e\u73b0\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_14","title":"\u4e2d\u65ad\u5c4f\u853d\u57fa\u7840\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_15","title":"\u4ec0\u4e48\u662f\u4e2d\u65ad\u5c4f\u853d","text":"<p>\u4e2d\u65ad\u5c4f\u853d\u662f\u901a\u8fc7\u8bbe\u7f6e\u7279\u5b9a\u5bc4\u5b58\u5668\u6765\u6682\u65f6\u7981\u6b62\u5904\u7406\u5668\u54cd\u5e94\u4e2d\u65ad\u7684\u673a\u5236\uff1a</p> <p>\u4e2d\u65ad\u5c4f\u853d\u7684\u4e3b\u8981\u76ee\u7684\uff1a</p> <ol> <li>\u4fdd\u62a4\u4e34\u754c\u533a\u4ee3\u7801</li> <li>\u9632\u6b62\u6570\u636e\u7ade\u4e89</li> <li>\u786e\u4fdd\u64cd\u4f5c\u7684\u539f\u5b50\u6027</li> <li>\u907f\u514d\u4efb\u52a1\u5207\u6362\u5728\u5173\u952e\u65f6\u671f\u53d1\u751f</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_16","title":"\u5c4f\u853d\u7528\u9014","text":"\u5c4f\u853d\u7ea7\u522b \u63a7\u5236\u5bc4\u5b58\u5668 \u4f5c\u7528\u8303\u56f4 \u4f7f\u7528\u573a\u666f \u5168\u5c40\u5c4f\u853d PRIMASK \u6240\u6709\u53ef\u5c4f\u853d\u5f02\u5e38 \u6700\u4e25\u683c\u7684\u4fdd\u62a4 \u4f18\u5148\u7ea7\u5c4f\u853d BASEPRI \u4f4e\u4e8e\u6307\u5b9a\u4f18\u5148\u7ea7\u7684\u4e2d\u65ad FreeRTOS\u5e38\u7528 \u6545\u969c\u5c4f\u853d FAULTMASK \u6240\u6709\u5f02\u5e38\uff08\u5305\u62ec\u6545\u969c\uff09 \u6545\u969c\u5904\u7406"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_17","title":"\u5bc4\u5b58\u5668\u8bf4\u660e","text":"\u5bc4\u5b58\u5668 \u4f4d\u5bbd \u590d\u4f4d\u503c \u5c4f\u853d\u8303\u56f4 \u5178\u578b\u5e94\u7528\u573a\u666f PRIMASK 1\u4f4d 0 \u6240\u6709\u53ef\u5c4f\u853d\u5f02\u5e38 \u77ed\u65f6\u95f4\u5168\u5c40\u4fdd\u62a4 BASEPRI 8\u4f4d 0 \u4f18\u5148\u7ea7\u4f4e\u4e8e\u8bbe\u5b9a\u503c\u7684\u5f02\u5e38 FreeRTOS\u4e34\u754c\u533a FAULTMASK 1\u4f4d 0 \u6240\u6709\u5f02\u5e38(\u9664NMI) \u6545\u969c\u5904\u7406 <p>[!Note]</p> <p>\u4e0a\u9762\u8bf4\u7684\u53ea\u662f\u4e3a\u4e86\u5e2e\u52a9\u7406\u89e3\u539f\u7406\uff0c\u5b9e\u9645\u4e0aFreeRTOS\u5df2\u7ecf\u5e2e\u6211\u4eec\u914d\u597d\u4e86\uff0c\u4e0d\u7528\u6211\u4eec\u81ea\u5df1\u64cd\u4f5c\u5bc4\u5b58\u5668\u3002\u6211\u4eec\u9700\u8981\u5c4f\u853d\u65f6\uff0cFreeRTOS\u6709\u63d0\u4f9b\u4e2d\u65ad\u7ba1\u7406API\u51fd\u6570\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#api","title":"\u4e2d\u65ad\u5c4f\u853d\u63a7\u5236 API","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#freertos_3","title":"FreeRTOS","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_18","title":"\u5e95\u5c42","text":"<pre><code>/*=== \u5e95\u5c42\u4e2d\u65ad\u63a7\u5236 ===*/\n\n// \u5168\u5c40\u4e2d\u65ad\u7981\u7528/\u542f\u7528\nvoid portDISABLE_INTERRUPTS(void);\nvoid portENABLE_INTERRUPTS(void);\n\n// \u7279\u5b9a\u4f18\u5148\u7ea7\u4e2d\u65ad\u5c4f\u853d\nUBaseType_t ulPortSetInterruptMask(void); // \u8fdb\u5165\u5c4f\u853d\uff0c\u8fd4\u56de\u539f\u72b6\u6001\uff0c\u4e4b\u540e\u53ef\u4ee5\u8bbe\u7f6e\u65b0\u7684\u4f18\u5148\u7ea7\u5c4f\u853d\nvoid vPortClearInterruptMask(UBaseType_t ulNewMask);  // \u9000\u51fa\u5c4f\u853d\uff0c\u6062\u590d\u72b6\u6001\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_19","title":"\u5e94\u7528\u5c42","text":"<pre><code>/*=== \u9876\u5c42API\u5b9e\u73b0 ===*/\n//\u4f18\u5148\u7ea7\u4e2d\u65ad\u5c4f\u853d\uff0c\u5373\u5c4f\u853d\u53ef\u7ba1\u63a7\u7684\u4f18\u5148\u7ea7\u533a\u95f4\uff08\u6bd4\u598215~5\uff09,\u7528\u4e8e\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4\nvoid taskENTER_CRITICAL();//\u5185\u90e8\u8c03\u7528portDISABLE_INTERRUPTS\nvoid taskEXIT_CRITICAL();//\u5185\u90e8\u8c03\u7528portENABLE_INTERRUPTS\n\n//\u4f18\u5148\u7ea7\u4e2d\u65ad\u5c4f\u853d\uff0c\u5373\u5c4f\u853d\u53ef\u7ba1\u63a7\u7684\u4f18\u5148\u7ea7\u533a\u95f4\uff08\u6bd4\u598215~5\uff09,\u7528\u4e8e\u4e2d\u65ad\u5185\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4\nUBaseType_t taskENTER_CRITICAL_FROM_ISR();//ulPortSetInterruptMask\nvoid taskEXIT_CRITICAL_FROM_IS(UBaseType_t save_status);//vPortClearInterruptMask\n\n// \u4e0a\u4e0b\u6587\u5207\u6362\u89e6\u53d1\nvoid portYIELD_FROM_ISR(BaseType_t xHigherPriorityTaskWoken);//\u4e2d\u65ad\u4e2d\u4f7f\u7528\nvoid portYIELD(void);//\u6b63\u5e38\u51fd\u6570\u4f7f\u7528\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#cmsis-v2","title":"CMSIS v2","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_20","title":"\u5e95\u5c42","text":"<pre><code>#include \"cmsis_compiler.h\"\n#include \"core_cm4.h\"\n/*=== \u5168\u5c40\u4e2d\u65ad\u63a7\u5236 ===*/\n\n// \u542f\u7528\u5168\u5c40\u4e2d\u65ad\n__STATIC_FORCEINLINE void __enable_irq(void)\n{\n    __asm volatile (\"cpsie i\" : : : \"memory\");\n}\n\n// \u7981\u7528\u5168\u5c40\u4e2d\u65ad  \n__STATIC_FORCEINLINE void __disable_irq(void)\n{\n    __asm volatile (\"cpsid i\" : : : \"memory\");\n}\n\n// \u542f\u7528\u6545\u969c\u4e2d\u65ad\n__STATIC_FORCEINLINE void __enable_fault_irq(void)\n{\n    __asm volatile (\"cpsie f\" : : : \"memory\");\n}\n\n// \u7981\u7528\u6545\u969c\u4e2d\u65ad\n__STATIC_FORCEINLINE void __disable_fault_irq(void)\n{\n    __asm volatile (\"cpsid f\" : : : \"memory\");\n}\n\n/*=== \u4f18\u5148\u7ea7\u5c4f\u853d\u63a7\u5236 ===*/\n\n// \u8bbe\u7f6e BASEPRI \u5bc4\u5b58\u5668\n__STATIC_FORCEINLINE void __set_BASEPRI(uint32_t basePri)\n{\n    __asm volatile (\"MSR basepri, %0\" : : \"r\" (basePri) : \"memory\");\n}\n\n// \u83b7\u53d6 BASEPRI \u5bc4\u5b58\u5668\u503c\n__STATIC_FORCEINLINE uint32_t __get_BASEPRI(void)\n{\n    uint32_t result;\n    __asm volatile (\"MRS %0, basepri\" : \"=r\" (result));\n    return result;\n}\n\n// \u8bbe\u7f6e FAULTMASK \u5bc4\u5b58\u5668\n__STATIC_FORCEINLINE void __set_FAULTMASK(uint32_t faultMask)\n{\n    __asm volatile (\"MSR faultmask, %0\" : : \"r\" (faultMask) : \"memory\");\n}\n\n// \u83b7\u53d6 FAULTMASK \u5bc4\u5b58\u5668\u503c\n__STATIC_FORCEINLINE uint32_t __get_FAULTMASK(void)\n{\n    uint32_t result;\n    __asm volatile (\"MRS %0, faultmask\" : \"=r\" (result));\n    return result;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#nvic-api","title":"NVIC \u4e2d\u65ad\u63a7\u5236\u5668 API","text":"<pre><code>#include \"core_cm4.h\"\n\n/*=== \u4e2d\u65ad\u4f7f\u80fd\u63a7\u5236 ===*/\n\n// \u4f7f\u80fd\u7279\u5b9a\u4e2d\u65ad\n__STATIC_FORCEINLINE void NVIC_EnableIRQ(IRQn_Type IRQn)\n{\n    NVIC-&gt;ISER[(((uint32_t)IRQn) &gt;&gt; 5UL)] = (uint32_t)(1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL));\n}\n\n// \u7981\u7528\u7279\u5b9a\u4e2d\u65ad\n__STATIC_FORCEINLINE void NVIC_DisableIRQ(IRQn_Type IRQn)\n{\n    NVIC-&gt;ICER[(((uint32_t)IRQn) &gt;&gt; 5UL)] = (uint32_t)(1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL));\n}\n\n// \u83b7\u53d6\u4e2d\u65ad\u4f7f\u80fd\u72b6\u6001\n__STATIC_FORCEINLINE uint32_t NVIC_GetEnableIRQ(IRQn_Type IRQn)\n{\n    return ((uint32_t)(((NVIC-&gt;ISER[(((uint32_t)IRQn) &gt;&gt; 5UL)] &amp; (1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL))) != 0UL) ? 1UL : 0UL));\n}\n\n/*=== \u4e2d\u65ad\u4f18\u5148\u7ea7\u63a7\u5236 ===*/\n\n// \u8bbe\u7f6e\u4e2d\u65ad\u4f18\u5148\u7ea7\n__STATIC_FORCEINLINE void NVIC_SetPriority(IRQn_Type IRQn, uint32_t priority)\n{\n    NVIC-&gt;IP[((uint32_t)IRQn)] = (uint8_t)((priority &lt;&lt; (8U - __NVIC_PRIO_BITS)) &amp; (uint32_t)0xFFUL);\n}\n\n// \u83b7\u53d6\u4e2d\u65ad\u4f18\u5148\u7ea7\n__STATIC_FORCEINLINE uint32_t NVIC_GetPriority(IRQn_Type IRQn)\n{\n    return ((uint32_t)(((NVIC-&gt;IP[((uint32_t)IRQn)] &gt;&gt; (8U - __NVIC_PRIO_BITS)) &amp; (uint32_t)0x03UL)));\n}\n\n/*=== \u4e2d\u65ad\u6302\u8d77\u63a7\u5236 ===*/\n\n// \u8bbe\u7f6e\u4e2d\u65ad\u6302\u8d77\n__STATIC_FORCEINLINE void NVIC_SetPendingIRQ(IRQn_Type IRQn)\n{\n    NVIC-&gt;ISPR[(((uint32_t)IRQn) &gt;&gt; 5UL)] = (uint32_t)(1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL));\n}\n\n// \u6e05\u9664\u4e2d\u65ad\u6302\u8d77\n__STATIC_FORCEINLINE void NVIC_ClearPendingIRQ(IRQn_Type IRQn)\n{\n    NVIC-&gt;ICPR[(((uint32_t)IRQn) &gt;&gt; 5UL)] = (uint32_t)(1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL));\n}\n\n// \u83b7\u53d6\u4e2d\u65ad\u6302\u8d77\u72b6\u6001\n__STATIC_FORCEINLINE uint32_t NVIC_GetPendingIRQ(IRQn_Type IRQn)\n{\n    return ((uint32_t)(((NVIC-&gt;ISPR[(((uint32_t)IRQn) &gt;&gt; 5UL)] &amp; (1UL &lt;&lt; (((uint32_t)IRQn) &amp; 0x1FUL))) != 0UL) ? 1UL : 0UL));\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_21","title":"\u5e94\u7528\u5c42","text":"<p>CMSIS\u6ca1\u6709\u63d0\u4f9b\u76f4\u63a5\u7c7b\u4f3cFreeRTOS\u4e2d\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4\u7684API\uff0c\u7528\u6237\u53ef\u4ee5\u81ea\u5df1\u6839\u636eCMSISI Core(\u5373\u4e0a\u9762\u7ed9\u51fa\u7684\u5e95\u5c42API)\u5c01\u88c5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528FreeRTOS\u7684API\u3002</p> <p>\u53e6\u5916\uff0cCMSIS\u7ed9\u51fa\u7684\u662f\u8c03\u5ea6\u5668\u9501\uff08\u76f8\u5f53\u4e8e\u9501\u4f4fPendsv\uff09\u7684API\uff0c\u9632\u6b62\u5176\u4ed6\u4efb\u52a1\u5207\u6362\uff0c\u4e5f\u53ef\u4ee5\u7528\u4e8e\u4e0d\u592a\u4e25\u683c\u7684\u4ee3\u7801\u4fdd\u62a4\u3002\uff08\u5728\u4e0b\u4e00\u8282\u4e2d\u8c03\u5ea6\u5668\u7684\u6302\u8d77\u548c\u6062\u590d\u8be6\u89e3\uff09</p> <pre><code>//\u9501\u4f4f\u8c03\u5ea6\u5668\nint32_t osKernelLock (void);\n//\u89e3\u9501\u8c03\u5ea6\u5668\nuint32_t osKernelUnlock (void);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#_22","title":"\u5c01\u88c5\u53c2\u8003","text":"<pre><code>#ifndef CRITICAL_H\n#define CRITICAL_H\n\n#include \"cmsis_gcc.h\"   \n\n/* \u914d\u7f6e\u9879\uff1a\u53c2\u8003 FreeRTOSConfig.h */\n#ifndef configMAX_SYSCALL_INTERRUPT_PRIORITY\n#define configMAX_SYSCALL_INTERRUPT_PRIORITY 5\n#endif\n\n/* \u83b7\u53d6 NVIC \u5b9e\u9645\u4f18\u5148\u7ea7\u4f4d\u6570 */\n#ifndef __NVIC_PRIO_BITS\n#define __NVIC_PRIO_BITS 4\n#endif\n\n/* ------------------------------\n   \u4efb\u52a1\u7ea7\u4e34\u754c\u533a (\u7b49\u4ef7 taskENTER_CRITICAL)\n   \u652f\u6301\u5d4c\u5957\u8ba1\u6570\n--------------------------------*/\nstatic inline void vTaskEnterCritical(void)\n{\n    static uint32_t ulCriticalNesting = 0;\n\n    if(ulCriticalNesting == 0)\n    {\n        /* \u7b2c\u4e00\u6b21\u8fdb\u5165\u4e34\u754c\u533a\uff0c\u63d0\u5347 BASEPRI */\n        __set_BASEPRI(configMAX_SYSCALL_INTERRUPT_PRIORITY &lt;&lt; (8 - __NVIC_PRIO_BITS));\n        __DSB();\n        __ISB();\n    }\n\n    ulCriticalNesting++;\n}\n\nstatic inline void vTaskExitCritical(void)\n{\n    extern uint32_t ulCriticalNesting;\n    if(ulCriticalNesting &gt; 0)\n    {\n        ulCriticalNesting--;\n\n        /* \u6700\u5916\u5c42\u4e34\u754c\u533a\u9000\u51fa\uff0c\u6062\u590d BASEPRI */\n        if(ulCriticalNesting == 0)\n        {\n            __set_BASEPRI(0);\n            __DSB();\n            __ISB();\n        }\n    }\n}\n\n/* ------------------------------\n   ISR \u4e0a\u4e0b\u6587\u4e34\u754c\u533a (\u7b49\u4ef7 taskENTER_CRITICAL_FROM_ISR)\n   \u8fd4\u56de\u539f BASEPRI \u4ee5\u4fbf\u6062\u590d\n--------------------------------*/\nstatic inline uint32_t ulEnterCriticalFromISR(void)\n{\n    uint32_t ulOriginalBASEPRI;\n\n    /* \u4fdd\u5b58\u539f BASEPRI */\n    ulOriginalBASEPRI = __get_BASEPRI();\n\n    /* \u63d0\u5347 BASEPRI \u5c4f\u853d\u4f4e\u4f18\u5148\u7ea7\u4e2d\u65ad */\n    __set_BASEPRI(configMAX_SYSCALL_INTERRUPT_PRIORITY &lt;&lt; (8 - __NVIC_PRIO_BITS));\n    __DSB();\n    __ISB();\n\n    return ulOriginalBASEPRI;\n}\n\nstatic inline void vExitCriticalFromISR(uint32_t ulOriginalBASEPRI)\n{\n    /* \u6062\u590d BASEPRI */\n    __set_BASEPRI(ulOriginalBASEPRI);\n    __DSB();\n    __ISB();\n}\n\n#endif // CRITICAL_H\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Interrupt_Mgmt/#api_1","title":"API\u5bf9\u6bd4","text":"\u529f\u80fd FreeRTOS API CMSIS v2 API \u63a8\u8350\u4f7f\u7528\u573a\u666f \u5168\u5c40\u4e2d\u65ad\u5f00\u5173\uff08\u5e95\u5c42\uff09 <code>portDISABLE_INTERRUPTS()</code> <code>portENABLE_INTERRUPTS()</code> <code>__disable_irq()</code> <code>__enable_irq()</code> FreeRTOS\uff1a\u5728\u7aef\u53e3\u5c42\u4f7f\u7528 CMSIS\uff1a\u88f8\u673a\u6216\u5e95\u5c42\u9a71\u52a8 \u4f18\u5148\u7ea7\u5c4f\u853d <code>taskENTER_CRITICAL()</code> <code>taskEXIT_CRITICAL()</code> <code>__set_BASEPRI()</code> <code>__get_BASEPRI()</code> FreeRTOS\u63a8\u8350\uff1a\u4efb\u52a1\u73af\u5883\u4fdd\u62a4 ISR\u4e2d\u65ad\u5c4f\u853d <code>taskENTER_CRITICAL_FROM_ISR()</code> <code>taskEXIT_CRITICAL_FROM_ISR()</code> <code>__set_BASEPRI()</code> <code>__get_BASEPRI()</code> FreeRTOS\u63a8\u8350\uff1aISR\u73af\u5883\u4fdd\u62a4 \u6545\u969c\u5c4f\u853d \u65e0\u76f4\u63a5API <code>__set_FAULTMASK()</code> <code>__get_FAULTMASK()</code> CMSIS\uff1a\u6545\u969c\u5904\u7406\u7a0b\u5e8f \u4e2d\u65ad\u4f7f\u80fd\u63a7\u5236 \u65e0\u76f4\u63a5API <code>NVIC_EnableIRQ()</code> <code>NVIC_DisableIRQ()</code> CMSIS\uff1a\u5916\u8bbe\u4e2d\u65ad\u7ba1\u7406 \u4f18\u5148\u7ea7\u914d\u7f6e \u901a\u8fc7<code>FreeRTOSConfig.h</code> <code>NVIC_SetPriority()</code> <code>NVIC_SetPriorityGrouping()</code> CMSIS\uff1a\u4e2d\u65ad\u4f18\u5148\u7ea7\u8bbe\u7f6e"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/","title":"KernelPend","text":"<ul> <li>\u57fa\u672c\u6982\u5ff5<ul> <li>\u4ec0\u4e48\u662f\u8c03\u5ea6\u5668\u6302\u8d77</li> <li>\u6302\u8d77\u8c03\u5ea6\u5668\u7684\u6548\u679c\uff1a</li> <li>\u5178\u578b\u4f7f\u7528\u573a\u666f\uff1a</li> <li>\u4e0e\u4e34\u754c\u533a\u7684\u533a\u522b</li> </ul> </li> <li>FreeRTOS API \u8be6\u89e3<ul> <li>\u51fd\u6570\u539f\u578b</li> <li>\u529f\u80fd\u7279\u6027<ul> <li>\u5d4c\u5957\u652f\u6301</li> <li>\u8fd4\u56de\u503c\u542b\u4e49</li> </ul> </li> <li>\u4f7f\u7528\u793a\u4f8b<ul> <li>\u57fa\u672c\u7528\u6cd5</li> </ul> </li> <li>\u51fd\u6570\u5185\u90e8\u5206\u6790<ul> <li>\u8c03\u5ea6\u5668\u6302\u8d77<ul> <li>Systick\u4e2d\u65ad\u670d\u52a1\u51fd\u6570</li> <li>FreeRTOS\u54cd\u5e94\u51fd\u6570</li> <li>\u4efb\u52a1\u8c03\u5ea6\u5668\u8282\u62cd\u51fd\u6570</li> </ul> </li> </ul> </li> </ul> </li> <li>CMSIS v2 API \u8be6\u89e3<ul> <li>\u51fd\u6570\u539f\u578b<ul> <li>\u72b6\u6001\u8fd4\u56de\u503c<ul> <li>osKernelLock() \u8fd4\u56de\u503c\uff1a</li> <li>osKernelUnlock() \u8fd4\u56de\u503c\uff1a</li> </ul> </li> </ul> </li> <li>\u529f\u80fd\u7279\u6027<ul> <li>\u5d4c\u5957\u652f\u6301</li> </ul> </li> <li>\u4f7f\u7528\u793a\u4f8b<ul> <li>\u57fa\u672c\u7528\u6cd5</li> <li>\u5d4c\u5957\u4f7f\u7528</li> </ul> </li> </ul> </li> <li>\u5bf9\u6bd4\u5206\u6790<ul> <li>API \u5bf9\u6bd4\u8868</li> <li>\u9002\u7528\u573a\u666f\u5bf9\u6bd4<ul> <li>FreeRTOS \u4f18\u52bf\u573a\u666f\uff1a</li> <li>CMSIS v2 \u4f18\u52bf\u573a\u666f\uff1a</li> </ul> </li> <li>\u6027\u80fd\u8003\u8651</li> </ul> </li> <li>\u5b9e\u8df5\u5e94\u7528<ul> <li>\u9009\u62e9\u6307\u5357<ul> <li>\u4f7f\u7528\u8c03\u5ea6\u5668\u6302\u8d77\u7684\u573a\u666f\uff1a</li> <li>\u4e0d\u9002\u5408\u4f7f\u7528\u7684\u573a\u666f\uff1a</li> </ul> </li> <li>\u6700\u4f73\u5b9e\u8df5<ul> <li>1. \u4fdd\u6301\u6302\u8d77\u65f6\u95f4\u6700\u77ed</li> <li>2. \u907f\u514d\u5728\u6302\u8d77\u671f\u95f4\u8c03\u7528\u963b\u585e\u51fd\u6570</li> <li>3. \u4e0e\u4e2d\u65ad\u7684\u6b63\u786e\u534f\u4f5c</li> </ul> </li> <li>\u8c03\u8bd5\u548c\u76d1\u63a7<ul> <li>FreeRTOS \u8c03\u8bd5\u6280\u5de7</li> <li>CMSIS v2 \u8c03\u8bd5\u6280\u5de7</li> </ul> </li> </ul> </li> <li>\u603b\u7ed3</li> <li>\u6ce8\u610f<ul> <li>\u5206\u6790</li> <li>\u5173\u952e\u5dee\u5f02\uff1a\u8fd4\u56de\u503c\u542b\u4e49<ul> <li>xTaskResumeAll() \u7684\u8fd4\u56de\u503c\uff1a</li> <li>osKernelUnlock() \u7684\u8fd4\u56de\u503c\uff1a</li> </ul> </li> <li>\u4e3a\u4ec0\u4e48\u8bbe\u8ba1\u4e0d\u540c\uff1f<ul> <li>1. \u62bd\u8c61\u5c42\u6b21\u4e0d\u540c</li> <li>2. \u9519\u8bef\u5904\u7406\u673a\u5236</li> <li>3. \u9501\u8ba1\u6570\u7ba1\u7406</li> </ul> </li> <li>\u8c03\u5ea6\u5207\u6362\u7684\u5904\u7406</li> <li>\u8fd9\u79cd\u8bbe\u8ba1\u7684\u5408\u7406\u6027<ul> <li>1. API \u7b80\u6d01\u6027</li> <li>2. \u5ef6\u8fdf\u8c03\u5ea6\u53ef\u63a5\u53d7</li> <li>3. \u4f7f\u7528\u573a\u666f\u4e0d\u540c</li> </ul> </li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_1","title":"\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_2","title":"\u4ec0\u4e48\u662f\u8c03\u5ea6\u5668\u6302\u8d77","text":"<p>\u8c03\u5ea6\u5668\u6302\u8d77\u662f\u6307\u6682\u65f6\u505c\u6b62\u64cd\u4f5c\u7cfb\u7edf\u7684\u4efb\u52a1\u8c03\u5ea6\u529f\u80fd\uff0c\u4f46\u4e0d\u5f71\u54cd\u4e2d\u65ad\u7684\u6b63\u5e38\u6267\u884c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_3","title":"\u6302\u8d77\u8c03\u5ea6\u5668\u7684\u6548\u679c\uff1a","text":"<ul> <li>\u7981\u6b62\u4efb\u52a1\u4e0a\u4e0b\u6587\u5207\u6362</li> <li>\u505c\u6b62\u65f6\u95f4\u7247\u8f6e\u8f6c</li> <li>\u6682\u505c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5</li> <li>\u4e0d\u5f71\u54cd\u4e2d\u65ad\u6267\u884c</li> <li>\u4e0d\u5f71\u54cd\u4e2d\u65ad\u4f18\u5148\u7ea7</li> <li>\u4e0d\u5c4f\u853d\u786c\u4ef6\u4e2d\u65ad</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_4","title":"\u5178\u578b\u4f7f\u7528\u573a\u666f\uff1a","text":"<ol> <li>\u4fdd\u62a4\u590d\u6742\u7684\u5171\u4eab\u6570\u636e\u7ed3\u6784\u64cd\u4f5c</li> <li>\u6267\u884c\u9700\u8981\u539f\u5b50\u6027\u7684\u591a\u6b65\u64cd\u4f5c</li> <li>\u9632\u6b62\u4efb\u52a1\u5207\u6362\u5e72\u6270\u5173\u952e\u4ee3\u7801\u6bb5</li> <li>\u4e0e\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u534f\u540c\u5de5\u4f5c</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_5","title":"\u4e0e\u4e34\u754c\u533a\u7684\u533a\u522b","text":"\u7279\u6027 \u8c03\u5ea6\u5668\u6302\u8d77 \u4e34\u754c\u533a \u4e2d\u65ad\u5f71\u54cd \u4e2d\u65ad\u6b63\u5e38\u6267\u884c \u5c4f\u853d\u90e8\u5206\u4e2d\u65ad \u4fdd\u62a4\u8303\u56f4 \u4ec5\u4efb\u52a1\u5207\u6362 \u4efb\u52a1\u5207\u6362+\u90e8\u5206\u4e2d\u65ad \u6027\u80fd\u5f71\u54cd \u8f83\u5c0f \u8f83\u5927 \u4f7f\u7528\u573a\u666f \u957f\u65f6\u95f4\u64cd\u4f5c \u77ed\u65f6\u95f4\u64cd\u4f5c"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#freertos-api","title":"FreeRTOS API \u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_6","title":"\u51fd\u6570\u539f\u578b","text":"<pre><code>/*=== \u8c03\u5ea6\u5668\u63a7\u5236 API ===*/\n\n// \u6302\u8d77\u6240\u6709\u4efb\u52a1\u8c03\u5ea6\uff08\u65e0\u8fd4\u56de\u503c\uff09\nvoid vTaskSuspendAll(void);\n\n// \u6062\u590d\u4efb\u52a1\u8c03\u5ea6\uff08\u8fd4\u56de\u662f\u5426\u9700\u8981\u8fdb\u884c\u4e0a\u4e0b\u6587\u5207\u6362\uff09\nBaseType_t xTaskResumeAll(void);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_7","title":"\u529f\u80fd\u7279\u6027","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_8","title":"\u5d4c\u5957\u652f\u6301","text":"<p>FreeRTOS \u7684\u8c03\u5ea6\u5668\u6302\u8d77\u652f\u6301\u5b8c\u5168\u5d4c\u5957\uff1a</p> <pre><code>// \u5d4c\u5957\u793a\u4f8b\nvTaskSuspendAll();    // \u7b2c1\u6b21\u6302\u8d77 - \u8c03\u5ea6\u5668\u505c\u6b62\nvTaskSuspendAll();    // \u7b2c2\u6b21\u6302\u8d77 - \u5d4c\u5957\u8ba1\u6570=2\nvTaskSuspendAll();    // \u7b2c3\u6b21\u6302\u8d77 - \u5d4c\u5957\u8ba1\u6570=3\n\nxTaskResumeAll();     // \u5d4c\u5957\u8ba1\u6570=2\nxTaskResumeAll();     // \u5d4c\u5957\u8ba1\u6570=1  \nxTaskResumeAll();     // \u5d4c\u5957\u8ba1\u6570=0\uff0c\u8c03\u5ea6\u5668\u771f\u6b63\u6062\u590d\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_9","title":"\u8fd4\u56de\u503c\u542b\u4e49","text":"<p><code>xTaskResumeAll()</code> \u8fd4\u56de\u503c\uff1a - <code>pdTRUE</code>\uff1a\u5728\u6302\u8d77\u671f\u95f4\u6709\u66f4\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u5c31\u7eea\uff0c\u9700\u8981\u7acb\u5373\u4e0a\u4e0b\u6587\u5207\u6362 - <code>pdFALSE</code>\uff1a\u6ca1\u6709\u9700\u8981\u7acb\u5373\u5207\u6362\u7684\u4efb\u52a1</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_10","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_11","title":"\u57fa\u672c\u7528\u6cd5","text":"<pre><code>void ProtectExtendedOperation(void)\n{\n    // \u6302\u8d77\u8c03\u5ea6\u5668\n    vTaskSuspendAll();\n\n    // \u6267\u884c\u4e0d\u53d7\u4efb\u52a1\u5207\u6362\u5f71\u54cd\u7684\u6269\u5c55\u64cd\u4f5c\n    UpdateComplexDataStructures();\n    ProcessMultipleSharedResources();\n    GenerateConsistentSystemState();\n\n    // \u6062\u590d\u8c03\u5ea6\u5668\n    BaseType_t xYieldRequired = xTaskResumeAll();\n\n    // \u5982\u679c\u9700\u8981\u7acb\u5373\u5207\u6362\u4efb\u52a1\n    if(xYieldRequired != pdFALSE) {\n        taskYIELD();\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_12","title":"\u51fd\u6570\u5185\u90e8\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_13","title":"\u8c03\u5ea6\u5668\u6302\u8d77","text":"<pre><code>void vTaskSuspendAll( void )\n{\n    /* A critical section is not required as the variable is of type\n    BaseType_t.  Please read Richard Barry's reply in the following link to a\n    post in the FreeRTOS support forum before reporting this as a bug! -\n    http://goo.gl/wu4acr */\n\n    /* portSOFRWARE_BARRIER() is only implemented for emulated/simulated ports that\n    do not otherwise exhibit real time behaviour. */\n    portSOFTWARE_BARRIER();\n\n    /* The scheduler is suspended if uxSchedulerSuspended is non-zero.  An increment\n    is used to allow calls to vTaskSuspendAll() to nest. */\n    ++uxSchedulerSuspended;\n\n    /* Enforces ordering for ports and optimised compilers that may otherwise place\n    the above increment elsewhere. */\n    portMEMORY_BARRIER();\n}\n</code></pre> <p>\u53ef\u89c1\uff0c\u771f\u6b63\u5b9e\u73b0\u4f5c\u7528\u7684\u53ea\u6709 <code>++uxSchedulerSuspended;</code>\u8fd9\u53e5\uff0c\u90a3\u4e48\u4e3a\u4ec0\u4e48\uff1f</p> <p>\u6211\u4eec\u8981\u5148\u770b\u770bPendSv\u8c03\u5ea6\u7684\u5e95\u5c42</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#systick","title":"Systick\u4e2d\u65ad\u670d\u52a1\u51fd\u6570","text":"<p>\u63d0\u4f9b\u5fc3\u8df3\u8282\u62cd\uff0c\u8fdb\u884c\u8c03\u5ea6\u3002</p> <pre><code>void SysTick_Handler (void) {\n  /* Clear overflow flag */\n  SysTick-&gt;CTRL;\n\n  if (xTaskGetSchedulerState() != taskSCHEDULER_NOT_STARTED) {\n    /* Call tick handler */\n    xPortSysTickHandler();\n  }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#freertos","title":"FreeRTOS\u54cd\u5e94\u51fd\u6570","text":"<p>\u76f8\u5e94\u7cfb\u7edf\u6ef4\u7b54\u8ba1\u65f6\u5668\u7684\u4e2d\u65ad\u670d\u52a1\u51fd\u6570</p> <pre><code>void xPortSysTickHandler( void )\n{\n    /* The SysTick runs at the lowest interrupt priority, so when this interrupt\n    executes all interrupts must be unmasked.  There is therefore no need to\n    save and then restore the interrupt mask value as its value is already\n    known. */\n    portDISABLE_INTERRUPTS();\n    {\n        /* Increment the RTOS tick. */\n        if( xTaskIncrementTick() != pdFALSE )\n        {\n            /* A context switch is required.  Context switching is performed in\n            the PendSV interrupt.  Pend the PendSV interrupt. */\n            //\u53ef\u4ee5\u53c2\u89c1Cortex-M3\u624b\u518c\uff0c\u8fd9\u4e2a\u53e5\u5b50\u4f7f\u82af\u7247\u4e2d\u65ad\u63a7\u5236\u5373\u72b6\u6001\u5bc4\u5b58\u5668ICSR(0xE000 ED04)\u4e2dPENDSVSET\u4f4d(28\u4f4d)\u7f6e1\uff0c\u5f00\u542f\u8c03\u5ea6\u3002\n            portNVIC_INT_CTRL_REG = portNVIC_PENDSVSET_BIT;\n        }\n    }\n    portENABLE_INTERRUPTS();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_14","title":"\u4efb\u52a1\u8c03\u5ea6\u5668\u8282\u62cd\u51fd\u6570","text":"<p>FreeRTOS \u4efb\u52a1\u8c03\u5ea6\u5668\u5185\u90e8\u7528\u4e8e\u589e\u52a0\u65f6\u949f\u8282\u62cd\u8ba1\u6570\u5e76\u6267\u884c\u76f8\u5173\u7ba1\u7406\u7684\u51fd\u6570</p> <pre><code>BaseType_t xTaskIncrementTick( void )\n{\n    BaseType_t xSwitchRequired = pdFALSE;\n        ...\n        if( uxSchedulerSuspended == ( UBaseType_t ) pdFALSE )\n    {   \n            ...\n        }\n    else\n    {\n        ++xPendedTicks;\n\n        /* The tick hook gets called at regular intervals, even if the\n        scheduler is locked. */\n        #if ( configUSE_TICK_HOOK == 1 )\n        {\n            vApplicationTickHook();\n        }\n        #endif\n    }\n        return xSwitchRequired;\n}\n</code></pre> <p>\u5e76\u4e14\u6709</p> <pre><code>PRIVILEGED_DATA static volatile UBaseType_t uxSchedulerSuspended    = ( UBaseType_t ) pdFALSE;\n</code></pre> <p>\u8bf4\u660e<code>uxSchedulerSuspended</code>\u672c\u6765\u4e3a<code>pdFALSE</code>\uff0c\u6211\u4eec++\u540e\u5b83\u4e0d\u662f\u4e86\uff0c\u56e0\u6b64\u4e4b\u540e\u7684\u8c03\u5ea6\u65e0\u6cd5\u8fdb\u884c\u3002</p> \u7279\u6027 \u63cf\u8ff0 \u53d8\u91cf\u7c7b\u578b \u8ba1\u6570\u5668\uff08\u53ef\u5d4c\u5957\uff09 \u6709\u6548\u503c <code>0</code>\uff1a\u8c03\u5ea6\u5668\u6d3b\u8dc3\uff1b <code>&gt;0</code>\uff1a\u8c03\u5ea6\u5668\u6302\u8d77 \u76f8\u5173\u51fd\u6570 <code>vTaskSuspendAll()</code>, <code>xTaskResumeAll()</code> \u4e3b\u8981\u76ee\u7684 \u5b9e\u73b0\u5185\u6838\u6570\u636e\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u4fdd\u62a4\u5173\u952e\u6bb5 \u5bf9\u8282\u62cd\u4e2d\u65ad\u7684\u5f71\u54cd \u6302\u8d77\u65f6\uff0c\u8282\u62cd\u6b63\u5e38\u8ba1\u6570\uff0c\u4efb\u52a1\u8d85\u65f6\u88ab\u8bb0\u5f55\u4f46\u5207\u6362\u88ab\u5ef6\u8fdf\uff0c<code>xTaskIncrementTick</code> \u8fd4\u56de <code>pdFALSE</code>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#cmsis-v2-api","title":"CMSIS v2 API \u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_15","title":"\u51fd\u6570\u539f\u578b","text":"<pre><code>/*=== CMSIS-RTOS v2 \u8c03\u5ea6\u5668\u63a7\u5236 API ===*/\n\n// \u6302\u8d77\u4efb\u52a1\u8c03\u5ea6\u5668\uff08\u8fd4\u56de\u6267\u884c\u72b6\u6001\uff09\nosStatus_t osKernelLock(void);\n\n// \u6062\u590d\u4efb\u52a1\u8c03\u5ea6\u5668\uff08\u8fd4\u56de\u6267\u884c\u72b6\u6001\uff09  \nosStatus_t osKernelUnlock(void);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_16","title":"\u72b6\u6001\u8fd4\u56de\u503c","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#oskernellock","title":"osKernelLock() \u8fd4\u56de\u503c\uff1a","text":"<ul> <li><code>osOK</code>\uff1a\u6210\u529f\u6302\u8d77\u8c03\u5ea6\u5668</li> <li><code>osError</code>\uff1a\u8c03\u5ea6\u5668\u672a\u542f\u52a8\u6216\u5176\u4ed6\u9519\u8bef</li> <li><code>osErrorISR</code>\uff1a\u5728\u4e2d\u65ad\u4e0a\u4e0b\u6587\u4e2d\u8c03\u7528\uff08\u9519\u8bef\u7528\u6cd5\uff09</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#oskernelunlock","title":"osKernelUnlock() \u8fd4\u56de\u503c\uff1a","text":"<ul> <li><code>osOK</code>\uff1a\u6210\u529f\u6062\u590d\u8c03\u5ea6\u5668</li> <li><code>osError</code>\uff1a\u8c03\u5ea6\u5668\u672a\u6302\u8d77\u6216\u5176\u4ed6\u9519\u8bef</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_17","title":"\u529f\u80fd\u7279\u6027","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_18","title":"\u5d4c\u5957\u652f\u6301","text":"<p>CMSIS v2 \u540c\u6837\u652f\u6301\u5b8c\u6574\u7684\u5d4c\u5957\u673a\u5236\uff1a</p> <pre><code>osKernelLock();     // \u9501\u5b9a\u8ba1\u6570 = 1\nosKernelLock();     // \u9501\u5b9a\u8ba1\u6570 = 2\nosKernelLock();     // \u9501\u5b9a\u8ba1\u6570 = 3\n\nosKernelUnlock();   // \u9501\u5b9a\u8ba1\u6570 = 2\nosKernelUnlock();   // \u9501\u5b9a\u8ba1\u6570 = 1\nosKernelUnlock();   // \u9501\u5b9a\u8ba1\u6570 = 0\uff0c\u8c03\u5ea6\u5668\u6062\u590d\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_19","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_20","title":"\u57fa\u672c\u7528\u6cd5","text":"<pre><code>void CMSIS_SchedulerControl(void)\n{\n    osStatus_t status;\n\n    // \u6302\u8d77\u8c03\u5ea6\u5668\n    status = osKernelLock();\n    if (status == osOK) {\n\n        // \u6267\u884c\u53d7\u4fdd\u62a4\u7684\u64cd\u4f5c\n        AccessSharedResources();\n        UpdateGlobalData();\n        PerformAtomicOperations();\n\n        // \u6062\u590d\u8c03\u5ea6\u5668\n        osStatus_t unlock_status = osKernelUnlock();\n        if (unlock_status != osOK) {\n            // \u5904\u7406\u6062\u590d\u5931\u8d25\n            HandleKernelError(unlock_status);\n        }\n    } else {\n        // \u5904\u7406\u6302\u8d77\u5931\u8d25\n        HandleKernelError(status);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_21","title":"\u5d4c\u5957\u4f7f\u7528","text":"<pre><code>void NestedKernelLockExample(void)\n{\n    osStatus_t status;\n\n    // \u7b2c\u4e00\u5c42\u9501\u5b9a\n    status = osKernelLock();\n    if (status != osOK) return;\n\n    OperationA();\n\n    // \u7b2c\u4e8c\u5c42\u9501\u5b9a\n    status = osKernelLock();\n    if (status != osOK) {\n        osKernelUnlock();  // \u6062\u590d\u7b2c\u4e00\u5c42\n        return;\n    }\n\n    OperationB();\n    osKernelUnlock();  // \u6062\u590d\u7b2c\u4e8c\u5c42\n\n    OperationC();\n    osKernelUnlock();  // \u6062\u590d\u7b2c\u4e00\u5c42\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_22","title":"\u5bf9\u6bd4\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#api","title":"API \u5bf9\u6bd4\u8868","text":"\u7279\u6027 FreeRTOS CMSIS v2 \u6302\u8d77\u51fd\u6570 <code>vTaskSuspendAll()</code> <code>osKernelLock()</code> \u6062\u590d\u51fd\u6570 <code>xTaskResumeAll()</code> <code>osKernelUnlock()</code> \u8fd4\u56de\u503c <code>BaseType_t</code>\uff08\u662f\u5426\u9700\u8981\u5207\u6362\uff09 <code>osStatus_t</code>\uff08\u6267\u884c\u72b6\u6001\uff09 \u5d4c\u5957\u652f\u6301 \u2705 \u5b8c\u5168\u652f\u6301 \u2705 \u5b8c\u5168\u652f\u6301 \u4e2d\u65ad\u5b89\u5168 \u274c \u4e0d\u80fd\u5728ISR\u4e2d\u4f7f\u7528 \u274c \u4e0d\u80fd\u5728ISR\u4e2d\u4f7f\u7528 \u9519\u8bef\u5904\u7406 \u7b80\u5355 \u8be6\u7ec6\u7684\u9519\u8bef\u7801"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_23","title":"\u9002\u7528\u573a\u666f\u5bf9\u6bd4","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#freertos_1","title":"FreeRTOS \u4f18\u52bf\u573a\u666f\uff1a","text":"<ul> <li>\u9700\u8981\u77e5\u9053\u6062\u590d\u540e\u662f\u5426\u9700\u8981\u4efb\u52a1\u5207\u6362</li> <li>\u5bf9\u6027\u80fd\u8981\u6c42\u6781\u9ad8\u7684\u7cfb\u7edf</li> <li>\u5df2\u7ecf\u6df1\u5ea6\u4f7f\u7528FreeRTOS\u7684\u9879\u76ee</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#cmsis-v2","title":"CMSIS v2 \u4f18\u52bf\u573a\u666f\uff1a","text":"<ul> <li>\u9700\u8981\u8de8RTOS\u79fb\u690d\u6027\u7684\u9879\u76ee</li> <li>\u9700\u8981\u8be6\u7ec6\u9519\u8bef\u5904\u7406\u7684\u7cfb\u7edf</li> <li>\u65b0\u9879\u76ee\u6216\u8981\u6c42\u6807\u51c6\u5316\u63a5\u53e3\u7684\u9879\u76ee</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_24","title":"\u6027\u80fd\u8003\u8651","text":"\u64cd\u4f5c FreeRTOS CMSIS v2 \u6302\u8d77\u5f00\u9500 \u5f88\u4f4e\uff08\u4e3b\u8981\u662f\u8ba1\u6570\u64cd\u4f5c\uff09 \u8f83\u4f4e\uff08\u5305\u542b\u72b6\u6001\u68c0\u67e5\uff09 \u6062\u590d\u5f00\u9500 \u4f4e\uff08\u5305\u542b\u5c31\u7eea\u4efb\u52a1\u68c0\u67e5\uff09 \u4f4e\uff08\u72b6\u6001\u9a8c\u8bc1\uff09 \u5185\u5b58\u5360\u7528 \u5f88\u5c0f\uff08\u4e00\u4e2a\u8ba1\u6570\u5668\uff09 \u8f83\u5c0f\uff08\u72b6\u6001\u7ba1\u7406\uff09"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_25","title":"\u5b9e\u8df5\u5e94\u7528","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_26","title":"\u9009\u62e9\u6307\u5357","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_27","title":"\u4f7f\u7528\u8c03\u5ea6\u5668\u6302\u8d77\u7684\u573a\u666f\uff1a","text":"<pre><code>// \u2705 \u9002\u5408\u4f7f\u7528\u8c03\u5ea6\u5668\u6302\u8d77\u7684\u60c5\u51b5\uff1a\n\n// 1. \u591a\u4e2a\u76f8\u5173\u7684\u5171\u4eab\u8d44\u6e90\u64cd\u4f5c\nvTaskSuspendAll();  // \u6216 osKernelLock()\nUpdateUserAccount();\nUpdateTransactionLog();\nUpdateSystemStatistics();\nxTaskResumeAll();   // \u6216 osKernelUnlock()\n\n// 2. \u590d\u6742\u6570\u636e\u7ed3\u6784\u7684\u7ef4\u62a4\nvTaskSuspendAll();\nRebalanceTreeStructure();\nUpdateAllRelatedNodes();\nVerifyDataConsistency();\nxTaskResumeAll();\n\n// 3. \u4e0eISR\u534f\u4f5c\u7684\u6269\u5c55\u64cd\u4f5c\nvTaskSuspendAll();\nSetupHardwareForBatchProcessing();\n// ISR\u53ef\u4ee5\u5728\u6b64\u671f\u95f4\u6536\u96c6\u6570\u636e\nWaitForDataCollectionComplete();\nProcessCollectedData();\nxTaskResumeAll();\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_28","title":"\u4e0d\u9002\u5408\u4f7f\u7528\u7684\u573a\u666f\uff1a","text":"<pre><code>// \u274c \u907f\u514d\u4f7f\u7528\u8c03\u5ea6\u5668\u6302\u8d77\u7684\u60c5\u51b5\uff1a\n\n// 1. \u975e\u5e38\u77ed\u7684\u64cd\u4f5c\uff08\u4f7f\u7528\u4e34\u754c\u533a\u66f4\u597d\uff09\ntaskENTER_CRITICAL();\nsingle_variable_update++;\ntaskEXIT_CRITICAL();\n\n// 2. \u9700\u8981\u4e2d\u65ad\u4fdd\u62a4\u7684\u64cd\u4f5c\uff08\u4f7f\u7528\u4e34\u754c\u533a\uff09\ntaskENTER_CRITICAL();\nhardware_register_access();\ntaskEXIT_CRITICAL();\n\n// 3. \u53ef\u80fd\u963b\u585e\u7684\u64cd\u4f5c\nvTaskSuspendAll();\nwait_for_external_event();  // \u274c \u5371\u9669\uff01\nxTaskResumeAll();\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_29","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#1","title":"1. \u4fdd\u6301\u6302\u8d77\u65f6\u95f4\u6700\u77ed","text":"<pre><code>// \u2705 \u597d\u7684\u505a\u6cd5\nvTaskSuspendAll();\n// \u53ea\u5305\u542b\u5fc5\u8981\u7684\u64cd\u4f5c\nessential_operations_only();\nxTaskResumeAll();\n\n// \u5c06\u975e\u5173\u952e\u64cd\u4f5c\u79fb\u5230\u5916\u9762\nnon_critical_operations();\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#2","title":"2. \u907f\u514d\u5728\u6302\u8d77\u671f\u95f4\u8c03\u7528\u963b\u585e\u51fd\u6570","text":"<pre><code>// \u274c \u5371\u9669\u505a\u6cd5\nvTaskSuspendAll();\nxQueueSend( xQueue, &amp;data, portMAX_DELAY );  // \u53ef\u80fd\u6c38\u8fdc\u963b\u585e\uff01\nxTaskResumeAll();\n\n// \u2705 \u5b89\u5168\u505a\u6cd5\nvTaskSuspendAll();\nBaseType_t xResult = xQueueSend( xQueue, &amp;data, 0 );  // \u4e0d\u963b\u585e\nxTaskResumeAll();\n\nif(xResult != pdPASS) {\n    // \u5904\u7406\u961f\u5217\u5df2\u6ee1\u7684\u60c5\u51b5\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#3","title":"3. \u4e0e\u4e2d\u65ad\u7684\u6b63\u786e\u534f\u4f5c","text":"<pre><code>// \u4efb\u52a1\u4e2d\u7684\u4ee3\u7801\nvTaskSuspendAll();\n// \u8bbe\u7f6e\u786c\u4ef6\uff0c\u542f\u52a8\u64cd\u4f5c\nSetupDMA_Transfer();\n// \u6b64\u65f6ISR\u4ecd\u7136\u53ef\u4ee5\u8fd0\u884c\u5e76\u5904\u7406DMA\u5b8c\u6210\nxTaskResumeAll();\n\n// ISR\u4e2d\u7684\u4ee3\u7801\uff08\u4e0d\u80fd\u8c03\u7528\u8c03\u5ea6\u5668\u63a7\u5236\u51fd\u6570\uff09\nvoid DMA_IRQHandler(void)\n{\n    BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n\n    // \u5904\u7406DMA\u5b8c\u6210\uff0c\u4f46\u4e0d\u4f1a\u5bfc\u81f4\u4efb\u52a1\u5207\u6362\n    if(xQueueSendFromISR(xDataQueue, &amp;data, &amp;xHigherPriorityTaskWoken) == pdPASS) {\n        // \u6210\u529f\u53d1\u9001\uff0c\u4f46xHigherPriorityTaskWoken\u53ef\u80fd\u88ab\u5ffd\u7565\n        // \u76f4\u5230\u8c03\u5ea6\u5668\u6062\u590d\u540e\u624d\u4f1a\u5b9e\u9645\u5207\u6362\u4efb\u52a1\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_30","title":"\u8c03\u8bd5\u548c\u76d1\u63a7","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#freertos_2","title":"FreeRTOS \u8c03\u8bd5\u6280\u5de7","text":"<pre><code>// \u68c0\u67e5\u8c03\u5ea6\u5668\u72b6\u6001\n#if ( configUSE_TRACE_FACILITY == 1 )\nvoid CheckSchedulerState(void)\n{\n    UBaseType_t uxSchedulerState = xTaskGetSchedulerState();\n\n    if(uxSchedulerState == taskSCHEDULER_SUSPENDED) {\n        printf(\"\u8c03\u5ea6\u5668\u5df2\u6302\u8d77\\n\");\n    } else if(uxSchedulerState == taskSCHEDULER_RUNNING) {\n        printf(\"\u8c03\u5ea6\u5668\u8fd0\u884c\u4e2d\\n\");\n    } else {\n        printf(\"\u8c03\u5ea6\u5668\u672a\u542f\u52a8\\n\");\n    }\n}\n#endif\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#cmsis-v2_1","title":"CMSIS v2 \u8c03\u8bd5\u6280\u5de7","text":"<pre><code>// \u76d1\u63a7\u5185\u6838\u72b6\u6001\nvoid MonitorKernelState(void)\n{\n    osKernelState_t state = osKernelGetState();\n\n    switch(state) {\n        case osKernelInactive:\n            printf(\"\u5185\u6838\u672a\u521d\u59cb\u5316\\n\");\n            break;\n        case osKernelReady:\n            printf(\"\u5185\u6838\u5c31\u7eea\\n\");\n            break;\n        case osKernelRunning:\n            printf(\"\u5185\u6838\u8fd0\u884c\u4e2d\\n\");\n            break;\n        case osKernelLocked:\n            printf(\"\u5185\u6838\u5df2\u9501\u5b9a\\n\");\n            break;\n        case osKernelSuspended:\n            printf(\"\u5185\u6838\u5df2\u6302\u8d77\\n\");\n            break;\n        case osKernelError:\n            printf(\"\u5185\u6838\u9519\u8bef\\n\");\n            break;\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_31","title":"\u603b\u7ed3","text":"<p>\u5173\u952e\u8981\u70b9\uff1a</p> <ol> <li>\u8c03\u5ea6\u5668\u6302\u8d77\u53ea\u5f71\u54cd\u4efb\u52a1\u5207\u6362\uff0c\u4e0d\u5f71\u54cd\u4e2d\u65ad</li> <li>\u4e24\u4e2a\u7cfb\u7edf\u90fd\u652f\u6301\u5b8c\u6574\u7684\u5d4c\u5957\u673a\u5236</li> <li>FreeRTOS \u63d0\u4f9b\u5207\u6362\u9700\u6c42\u4fe1\u606f\uff0cCMSIS v2 \u63d0\u4f9b\u9519\u8bef\u72b6\u6001</li> <li>\u9002\u5408\u4fdd\u62a4\u6269\u5c55\u64cd\u4f5c\uff0c\u4f46\u4e0d\u9002\u5408\u77ed\u65f6\u95f4\u4fdd\u62a4</li> <li>\u5fc5\u987b\u786e\u4fdd\u5728\u6240\u6709\u4ee3\u7801\u8def\u5f84\u4e0a\u90fd\u6062\u590d\u8c03\u5ea6\u5668</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_32","title":"\u6ce8\u610f","text":"<p>\u200b   \u6211\u53d1\u73b0\uff0cFreeRTOS\u7684\u8c03\u5ea6\u5668\u6062\u590dAPI\u4f1a\u5904\u7406\u662f\u5426\u9700\u8981\u4efb\u52a1\u5207\u6362\uff08\u5373\u5728\u9501\u6b7b\u671f\u95f4\u5982\u679c\u6709\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u5c31\u7eea\u4e86\uff0c\u9000\u51fa\u9501\u6b7b\u65f6\u9700\u8981\u9a6c\u4e0a\u5207\u6362\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u7b49\u5230\u4e0b\u4e00\u4e2a\u65f6\u95f4\u7247\u7531\u7cfb\u7edf\u53bb\u5207\u6362\uff09\uff0c\u800cCMSIS\u6ca1\u6709\u8fd9\u4e2a\u529f\u80fd\uff0c\u6211\u4eec\u5173\u6ce8\u89e3\u9501\u8fd9\u4e2a\u51fd\u6570\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_33","title":"\u5206\u6790","text":"<p><code>osKernelUnlock</code> \u786e\u5b9e\u5185\u90e8\u8c03\u7528\u4e86 <code>xTaskResumeAll()</code>\uff0c\u4f46\u5b83\u7684\u8fd4\u56de\u503c\u8bed\u4e49\u5b8c\u5168\u4e0d\u540c\uff1a</p> <pre><code>int32_t osKernelUnlock (void) {\n  int32_t lock;\n\n  // ... \u7b80\u5316\u4ee3\u7801 ...\n  if (xTaskResumeAll() != pdTRUE) {\n    if (xTaskGetSchedulerState() == taskSCHEDULER_SUSPENDED) {\n      lock = (int32_t)osError;\n    }\n  }\n  // ...\n  return (lock);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_34","title":"\u5173\u952e\u5dee\u5f02\uff1a\u8fd4\u56de\u503c\u542b\u4e49","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#xtaskresumeall","title":"<code>xTaskResumeAll()</code> \u7684\u8fd4\u56de\u503c\uff1a","text":"<ul> <li><code>pdTRUE</code> = \u9700\u8981\u4e0a\u4e0b\u6587\u5207\u6362</li> <li><code>pdFALSE</code> = \u4e0d\u9700\u8981\u4e0a\u4e0b\u6587\u5207\u6362</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#oskernelunlock_1","title":"<code>osKernelUnlock()</code> \u7684\u8fd4\u56de\u503c\uff1a","text":"<ul> <li><code>&gt; 0</code> = \u5269\u4f59\u7684\u9501\u8ba1\u6570\uff08\u9501\u72b6\u6001\uff09</li> <li><code>0</code> = \u8c03\u5ea6\u5668\u5df2\u5b8c\u5168\u89e3\u9501</li> <li><code>&lt; 0</code> = \u9519\u8bef\u72b6\u6001\uff08<code>osError</code>, <code>osErrorISR</code>\uff09</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_35","title":"\u4e3a\u4ec0\u4e48\u8bbe\u8ba1\u4e0d\u540c\uff1f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#1_1","title":"1. \u62bd\u8c61\u5c42\u6b21\u4e0d\u540c","text":"<p><code>osKernelUnlock</code> \u662f CMSIS-RTOS API\uff0c\u5173\u6ce8\u7684\u662f\u9501\u72b6\u6001\uff1a</p> <pre><code>// CMSIS-RTOS \u7684\u4f7f\u7528\u6a21\u5f0f\nint32_t lock_count = osKernelUnlock();\n// \u7528\u6237\u5173\u5fc3\u7684\u662f\uff1a\u9501\u662f\u5426\u5b8c\u5168\u89e3\u5f00\u4e86\uff1f\u8fd8\u5269\u51e0\u5c42\u9501\uff1f\n</code></pre> <p><code>xTaskResumeAll</code> \u662f FreeRTOS \u5185\u6838API\uff0c\u5173\u6ce8\u7684\u662f\u8c03\u5ea6\u51b3\u7b56\uff1a</p> <pre><code>// FreeRTOS \u5185\u6838\u7684\u4f7f\u7528\u6a21\u5f0f  \nif (xTaskResumeAll() == pdTRUE) {\n    taskYIELD(); // \u7acb\u5373\u6267\u884c\u8c03\u5ea6\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#2_1","title":"2. \u9519\u8bef\u5904\u7406\u673a\u5236","text":"<p><code>osKernelUnlock</code> \u6709\u5b8c\u6574\u7684\u9519\u8bef\u68c0\u67e5\uff1a - \u68c0\u67e5\u662f\u5426\u5728\u4e2d\u65ad\u4e2d\u8c03\u7528\uff08\u8fd4\u56de <code>osErrorISR</code>\uff09 - \u68c0\u67e5\u8c03\u5ea6\u5668\u72b6\u6001\uff08<code>taskSCHEDULER_NOT_STARTED</code> \u8fd4\u56de <code>osError</code>\uff09 - \u68c0\u67e5\u6062\u590d\u64cd\u4f5c\u662f\u5426\u6210\u529f</p> <p>\u800c <code>xTaskResumeAll</code> \u5047\u8bbe\u8c03\u7528\u73af\u5883\u6b63\u786e\uff0c\u4e13\u6ce8\u4e8e\u8c03\u5ea6\u903b\u8f91\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#3_1","title":"3. \u9501\u8ba1\u6570\u7ba1\u7406","text":"<p><code>osKernelUnlock</code> \u8fd4\u56de\u7684\u662f\u5269\u4f59\u7684\u9501\u8ba1\u6570\uff0c\u8fd9\u53cd\u6620\u4e86 CMSIS-RTOS \u5bf9\u5d4c\u5957\u9501\u7684\u652f\u6301\uff1a</p> <pre><code>osKernelLock();   // \u9501\u8ba1\u6570 = 1\nosKernelLock();   // \u9501\u8ba1\u6570 = 2  \nint32_t count = osKernelUnlock(); // count = 1, \u8c03\u5ea6\u5668\u4ecd\u88ab\u9501\u5b9a\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_36","title":"\u8c03\u5ea6\u5207\u6362\u7684\u5904\u7406","text":"<p>\u90a3\u4e48\uff0c\u5982\u679c <code>xTaskResumeAll()</code> \u8fd4\u56de <code>pdTRUE</code>\uff08\u9700\u8981\u5207\u6362\uff09\uff0c\u5728 <code>osKernelUnlock</code> \u4e2d\u8fd9\u4e2a\u4fe1\u606f\u53bb\u54ea\u4e86\uff1f</p> <p>\u7b54\u6848\uff1a\u8fd9\u4e2a\u4fe1\u606f\u88ab\u4e22\u5f03\u4e86\uff01</p> <pre><code>// \u5728 osKernelUnlock \u4e2d\uff1a\nif (xTaskResumeAll() != pdTRUE) {\n  // \u53ea\u5904\u7406 FALSE \u60c5\u51b5\uff08\u6062\u590d\u5931\u8d25\uff09\n  // \u5982\u679c\u8fd4\u56de TRUE\uff08\u9700\u8981\u5207\u6362\uff09\uff0c\u8fd9\u4e2a\u4fe1\u606f\u88ab\u5ffd\u7565\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#_37","title":"\u8fd9\u79cd\u8bbe\u8ba1\u7684\u5408\u7406\u6027","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#1-api","title":"1. API \u7b80\u6d01\u6027","text":"<p>CMSIS-RTOS \u9009\u62e9\u63d0\u4f9b\u7b80\u5355\u7684\u9501\u72b6\u6001\u7ba1\u7406\uff0c\u800c\u4e0d\u662f\u66b4\u9732\u5e95\u5c42\u7684\u8c03\u5ea6\u51b3\u7b56\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#2_2","title":"2. \u5ef6\u8fdf\u8c03\u5ea6\u53ef\u63a5\u53d7","text":"<p>\u5728\u5927\u591a\u6570\u5e94\u7528\u573a\u666f\u4e2d\uff0c\u7a0d\u5fae\u5ef6\u8fdf\u7684\u8c03\u5ea6\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u3002\u7cfb\u7edf\u4f1a\u5728\u4e0b\u4e00\u4e2a\u65f6\u95f4\u7247\u6216\u8c03\u5ea6\u70b9\u81ea\u7136\u5904\u7406\u5207\u6362\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/KernelPend/#3_2","title":"3. \u4f7f\u7528\u573a\u666f\u4e0d\u540c","text":"<pre><code>// \u9700\u8981\u7cbe\u786e\u63a7\u5236\u8c03\u5ea6\u7684\u573a\u666f - \u7528 FreeRTOS \u539f\u751f API\nvTaskSuspendAll();\n// \u7cbe\u786e\u7684\u4e34\u754c\u533a\u64cd\u4f5c\nif (xTaskResumeAll() == pdTRUE) {\n    portYIELD_WITHIN_API(); // \u7acb\u5373\u5207\u6362\n}\n\n// \u4e00\u822c\u7684\u4e34\u754c\u533a\u4fdd\u62a4 - \u7528 CMSIS-RTOS API  \nosKernelLock();\n// \u4e00\u822c\u7684\u5171\u4eab\u8d44\u6e90\u8bbf\u95ee\nosKernelUnlock(); // \u4e0d\u5173\u5fc3\u7acb\u5373\u5207\u6362\n</code></pre> <p>\u8fd9\u4f53\u73b0\u4e86\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u8bbe\u8ba1\u53d6\u820d\uff1aCMSIS-RTOS \u8ffd\u6c42\u6613\u7528\u6027\u548c\u53ef\u79fb\u690d\u6027\uff0c\u800c FreeRTOS \u5185\u6838\u63d0\u4f9b\u7cbe\u786e\u63a7\u5236\u548c\u6700\u9ad8\u6027\u80fd\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/","title":"List&ListItem","text":"<ul> <li>FreeRTOS \u5217\u8868\u548c\u5217\u8868\u9879\u7684\u7528\u9014<ul> <li>\u5185\u6838\u8c03\u5ea6\u7ba1\u7406<ul> <li>\u5c31\u7eea\u5217\u8868 (Ready Lists)</li> <li>\u5ef6\u8fdf\u5217\u8868 (Delayed List)</li> <li>\u6302\u8d77\u5217\u8868 (Pending Ready List)</li> <li>\u4e8b\u4ef6\u5217\u8868</li> </ul> </li> <li>\u8d44\u6e90\u7ba1\u7406<ul> <li>\u4efb\u52a1\u72b6\u6001\u8ddf\u8e2a</li> <li>\u5b9a\u65f6\u5668\u7ba1\u7406</li> <li>\u5185\u5b58\u7ba1\u7406</li> </ul> </li> </ul> </li> <li>\u5217\u8868\u548c\u5217\u8868\u9879\u7684\u6982\u5ff5<ul> <li>\u5217\u8868 (List)</li> <li>\u5217\u8868\u9879 (ListItem)</li> </ul> </li> <li>\u5217\u8868\u7ed3\u6784\u4f53\u5b9a\u4e49<ul> <li>List_t \u7ed3\u6784\u4f53</li> <li>ListItem_t \u7ed3\u6784\u4f53</li> <li>MiniListItem_t \u7ed3\u6784\u4f53<ul> <li>\u7528\u5904</li> </ul> </li> </ul> </li> <li>\u5217\u8868\u64cd\u4f5c API<ul> <li>\u5217\u8868\u521d\u59cb\u5316</li> <li>\u5217\u8868\u9879\u64cd\u4f5c</li> <li>\u5217\u8868\u904d\u5386</li> </ul> </li> <li>\u5217\u8868\u7684\u7279\u70b9<ul> <li>\u6392\u5e8f\u673a\u5236</li> <li>\u5b8c\u6574\u6027\u68c0\u67e5</li> <li>\u7ebf\u7a0b\u5b89\u5168</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#freertos","title":"FreeRTOS \u5217\u8868\u548c\u5217\u8868\u9879\u7684\u7528\u9014","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_1","title":"\u5185\u6838\u8c03\u5ea6\u7ba1\u7406","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#ready-lists","title":"\u5c31\u7eea\u5217\u8868 (Ready Lists)","text":"<p>\u6bcf\u4e2a\u4f18\u5148\u7ea7\u90fd\u6709\u4e00\u4e2a\u72ec\u7acb\u7684\u5c31\u7eea\u5217\u8868\uff0c\u7528\u4e8e\u7ba1\u7406\u5904\u4e8e\u5c31\u7eea\u72b6\u6001\u7684\u4efb\u52a1\u3002\u8c03\u5ea6\u5668\u901a\u8fc7\u904d\u5386\u8fd9\u4e9b\u5217\u8868\u6765\u9009\u62e9\u6700\u9ad8\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u4efb\u52a1\u8fd0\u884c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#delayed-list","title":"\u5ef6\u8fdf\u5217\u8868 (Delayed List)","text":"<p>\u7ba1\u7406\u88ab\u5ef6\u8fdf\u7684\u4efb\u52a1\uff0c\u6309\u5524\u9192\u65f6\u95f4\u6392\u5e8f\u3002\u5f53\u7cfb\u7edf\u65f6\u949f\u6ef4\u7b54\u53d1\u751f\u65f6\uff0c\u68c0\u67e5\u5e76\u79fb\u52a8\u5230\u671f\u4efb\u52a1\u5230\u5c31\u7eea\u5217\u8868\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#pending-ready-list","title":"\u6302\u8d77\u5217\u8868 (Pending Ready List)","text":"<p>\u5728\u8c03\u5ea6\u5668\u6302\u8d77\u671f\u95f4\u53d8\u4e3a\u5c31\u7eea\u7684\u4efb\u52a1\u4e34\u65f6\u5b58\u653e\u5728\u6b64\u5217\u8868\uff0c\u5f85\u8c03\u5ea6\u5668\u6062\u590d\u65f6\u518d\u8f6c\u79fb\u5230\u5c31\u7eea\u5217\u8868\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_2","title":"\u4e8b\u4ef6\u5217\u8868","text":"<p>\u7528\u4e8e\u7ba1\u7406\u7b49\u5f85\u4e8b\u4ef6\uff08\u5982\u4fe1\u53f7\u91cf\u3001\u961f\u5217\u3001\u4e8b\u4ef6\u7ec4\uff09\u7684\u4efb\u52a1\uff0c\u6309\u4f18\u5148\u7ea7\u6392\u5e8f\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_3","title":"\u8d44\u6e90\u7ba1\u7406","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_4","title":"\u4efb\u52a1\u72b6\u6001\u8ddf\u8e2a","text":"<p>\u901a\u8fc7\u5217\u8868\u7ba1\u7406\u4efb\u52a1\u7684\u4e0d\u540c\u72b6\u6001\u8f6c\u6362\uff0c\u786e\u4fdd\u4efb\u52a1\u5728\u9002\u5f53\u7684\u65f6\u5019\u88ab\u8c03\u5ea6\u6267\u884c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_5","title":"\u5b9a\u65f6\u5668\u7ba1\u7406","text":"<p>\u8f6f\u4ef6\u5b9a\u65f6\u5668\u4f7f\u7528\u5217\u8868\u6765\u7ba1\u7406\u5b9a\u65f6\u5668\u7684\u89e6\u53d1\u65f6\u95f4\u548c\u6267\u884c\u987a\u5e8f\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_6","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>\u67d0\u4e9b\u5185\u5b58\u5206\u914d\u65b9\u6848\u4f7f\u7528\u5217\u8868\u6765\u8ddf\u8e2a\u7a7a\u95f2\u5185\u5b58\u5757\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_7","title":"\u5217\u8868\u548c\u5217\u8868\u9879\u7684\u6982\u5ff5","text":"<p>\u5217\u8868\u662f FreeRTOS \u5185\u6838\u4e2d\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u7ba1\u7406\u4efb\u52a1\u548c\u5404\u79cd\u5185\u6838\u5bf9\u8c61\u3002\u6240\u6709\u5217\u8868\u90fd\u5b9e\u73b0\u4e3a\u53cc\u5411\u73af\u5f62\u94fe\u8868\uff0c\u786e\u4fdd\u9ad8\u6548\u7684\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#list","title":"\u5217\u8868 (List)","text":"<p>\u5217\u8868\u662f FreeRTOS \u4e2d\u7528\u4e8e\u7ec4\u7ec7\u548c\u7ba1\u7406\u591a\u4e2a\u76f8\u5173\u9879\u76ee\u7684\u6570\u636e\u7ed3\u6784\u3002\u6bcf\u4e2a\u5217\u8868\u5305\u542b\u5934\u5c3e\u6307\u9488\u548c\u9879\u76ee\u8ba1\u6570\uff0c\u4e3b\u8981\u7528\u4e8e\uff1a - \u5c31\u7eea\u4efb\u52a1\u5217\u8868 - \u5ef6\u8fdf\u4efb\u52a1\u5217\u8868 - \u6302\u8d77\u4efb\u52a1\u5217\u8868 - \u4e8b\u4ef6\u7b49\u5f85\u5217\u8868</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#listitem","title":"\u5217\u8868\u9879 (ListItem)","text":"<p>\u5217\u8868\u9879\u662f\u5217\u8868\u4e2d\u7684\u57fa\u672c\u5143\u7d20\uff0c\u6bcf\u4e2a\u5217\u8868\u9879\u5305\u542b\u524d\u540e\u6307\u9488\u548c\u6240\u5c5e\u5217\u8868\u6307\u9488\u3002\u5217\u8868\u9879\u8fd8\u5305\u542b\u4e00\u4e2a\u6392\u5e8f\u503c\uff0c\u7528\u4e8e\u5728\u5217\u8868\u4e2d\u6309\u5347\u5e8f\u6392\u5217\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_8","title":"\u5217\u8868\u7ed3\u6784\u4f53\u5b9a\u4e49","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#list_t","title":"List_t \u7ed3\u6784\u4f53","text":"<pre><code>typedef struct xLIST\n{\n    listFIRST_LIST_INTEGRITY_CHECK_VALUE\n    volatile UBaseType_t uxNumberOfItems;\n    ListItem_t * configLIST_VOLATILE pxIndex;\n    MiniListItem_t xListEnd;\n    listSECOND_LIST_INTEGRITY_CHECK_VALUE\n} List_t;\n</code></pre> <p>\u6210\u5458\u8bf4\u660e\uff1a - <code>uxNumberOfItems</code>\uff1a\u8bb0\u5f55\u5217\u8868\u4e2d\u5217\u8868\u9879\u7684\u6570\u91cf\uff0c\u4e0d\u542b <code>xListEnd</code> - <code>pxIndex</code>\uff1a\u7528\u4e8e\u904d\u5386\u5217\u8868\u7684\u6307\u9488\uff0c\u6307\u5411\u5f53\u524d\u88ab\u5f15\u7528\u7684\u5217\u8868\u9879 - <code>xListEnd</code>\uff1a\u5217\u8868\u5c3e\u6807\u8bb0\uff0c\u662f\u4e00\u4e2a\u8ff7\u4f60\u5217\u8868\u9879 - <code>listFIRST_LIST_INTEGRITY_CHECK_VALUE</code>\uff1a\u8fd9\u4e24\u4e2a\u5b8f\u662f\u786e\u5b9a\u7684\u5df2\u77e5\u5e38\u91cf\uff0cFreeRTOS\u901a\u8fc7\u68c0\u67e5\u8fd9\u4e24\u4e2a\u5e38\u91cf\u7684\u503c\uff0c\u6765\u5224\u65ad\u5217\u8868\u7684\u6570\u636e\u5728\u7a0b\u5e8f\u8fd0\u884c\u7684\u8fc7\u7a0b\u4e2d\uff0c\u662f\u5426\u906d\u5230\u7834\u574f\uff0c\u8be5\u529f\u80fd\u4e00\u822c\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4e0d\u5f00\u542f\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#listitem_t","title":"ListItem_t \u7ed3\u6784\u4f53","text":"<pre><code>struct xLIST_ITEM\n{\n    listFIRST_LIST_ITEM_INTEGRITY_CHECK_VALUE\n    configLIST_VOLATILE TickType_t xItemValue;\n    struct xLIST_ITEM * configLIST_VOLATILE pxNext;\n    struct xLIST_ITEM * configLIST_VOLATILE pxPrevious;\n    void * pvOwner;\n    struct xLIST * configLIST_VOLATILE pxContainer;\n    listSECOND_LIST_ITEM_INTEGRITY_CHECK_VALUE\n};\n</code></pre> <p>\u6210\u5458\u8bf4\u660e\uff1a - <code>xItemValue</code>\uff1a\u6392\u5e8f\u503c\uff0c\u7528\u4e8e\u786e\u5b9a\u5217\u8868\u9879\u5728\u5217\u8868\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4e3a0xFFFFFFFF(32\u4f4d) - <code>pxNext</code>\uff1a\u6307\u5411\u4e0b\u4e00\u4e2a\u5217\u8868\u9879\u7684\u6307\u9488 - <code>pxPrevious</code>\uff1a\u6307\u5411\u524d\u4e00\u4e2a\u5217\u8868\u9879\u7684\u6307\u9488 - <code>pvOwner</code>\uff1a\u6307\u5411\u62e5\u6709\u8be5\u5217\u8868\u9879\u7684\u5bf9\u8c61\uff08\u901a\u5e38\u662f\u4efb\u52a1\u63a7\u5236\u5757\uff09 - <code>pxContainer</code>\uff1a\u6307\u5411\u8be5\u5217\u8868\u9879\u6240\u5c5e\u7684\u5217\u8868</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#minilistitem_t","title":"MiniListItem_t \u7ed3\u6784\u4f53","text":"<pre><code>typedef struct xMINI_LIST_ITEM\n{\n    listFIRST_LIST_ITEM_INTEGRITY_CHECK_VALUE\n    configLIST_VOLATILE TickType_t xItemValue;\n    struct xLIST_ITEM * configLIST_VOLATILE pxNext;\n    struct xLIST_ITEM * configLIST_VOLATILE pxPrevious;\n} MiniListItem_t;\n</code></pre> <p>\u8ff7\u4f60\u5217\u8868\u9879\u662f\u7b80\u5316\u7248\u7684\u5217\u8868\u9879\uff0c\u4e3b\u8981\u7528\u4e8e\u5217\u8868\u5c3e\u6807\u8bb0\u4ee5\u53ca\u6302\u8f7d\u5176\u4ed6\u63d2\u5165\u5217\u8868\u4e2d\u7684\u5217\u8868\u9879\uff0c\u4e0d\u5305\u542b\u6240\u6709\u8005\u6307\u9488\u548c\u5bb9\u5668\u6307\u9488\u3002   </p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_9","title":"\u7528\u5904","text":"<p>\u5f53\u5217\u8868\u4e3a\u7a7a\u65f6\uff0c\u6211\u4eec\u600e\u4e48\u52a0\u5165\u65b0\u9879\uff1f\u8fd9\u65f6\u5019\u5c31\u9700\u8981\u8fd9\u4e2aMini\u5217\u8868\u9879\uff0c\u7528\u6765\u6302\u8f7d\u52a0\u5165\u5217\u8868\u7684\u65b0\u5143\u7d20\u3002\u5f53\u5217\u8868\u4e3a\u7a7a\u65f6\uff0c\u5b83\u7684\u4e24\u53ea\u624b\uff08Previous\u548cNext\uff09\u90fd\u6307\u5411\u81ea\u5df1\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#api","title":"\u5217\u8868\u64cd\u4f5c API","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_10","title":"\u5217\u8868\u521d\u59cb\u5316","text":"<ul> <li><code>vListInitialise(List_t* const pxList)</code> - \u521d\u59cb\u5316\u5217\u8868</li> <li><code>vListInitialiseItem(ListItem_t* const pxItem)</code> - \u521d\u59cb\u5316\u5217\u8868\u9879</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_11","title":"\u5217\u8868\u9879\u64cd\u4f5c","text":"<ul> <li><code>vListInsert(List_t* const pxList, ListItem_t* const pxNewListItem)</code> - \u6309\u6392\u5e8f\u503c\u63d2\u5165\u5217\u8868\u9879</li> <li><code>vListInsertEnd(List_t* const pxList, ListItem_t* const pxNewListItem)</code> - \u5728\u5217\u8868\u4e2d\u5f53\u524d\u6307\u5411\u5217\u8868\u9879\u524d\u63d2\u5165\uff0c\u5373\u63d2\u5165\u5df2\u904d\u5386\u7684\u6700\u672b\u7aef\uff0c\u662f\u4e00\u79cd\u65e0\u5e8f\u63d2\u5165\u65b9\u6cd5\u3002</li> <li><code>uxListRemove(ListItem_t* const pxItemToRemove)</code> - \u4ece\u5217\u8868\u4e2d\u79fb\u9664\u5217\u8868\u9879</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_12","title":"\u5217\u8868\u904d\u5386","text":"<ul> <li><code>listGET_OWNER_OF_NEXT_ENTRY()</code> - \u83b7\u53d6\u4e0b\u4e00\u4e2a\u5217\u8868\u9879\u7684\u6240\u6709\u8005</li> <li><code>listLIST_IS_EMPTY()</code> - \u68c0\u67e5\u5217\u8868\u662f\u5426\u4e3a\u7a7a</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_13","title":"\u5217\u8868\u7684\u7279\u70b9","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_14","title":"\u6392\u5e8f\u673a\u5236","text":"<p>\u5217\u8868\u9879\u6309 xItemValue \u503c\u5347\u5e8f\u6392\u5217\uff0c\u8fd9\u5bf9\u4e8e\u65f6\u95f4\u76f8\u5173\u7684\u64cd\u4f5c\uff08\u5982\u4efb\u52a1\u5ef6\u8fdf\uff09\u975e\u5e38\u6709\u7528\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_15","title":"\u5b8c\u6574\u6027\u68c0\u67e5","text":"<p>\u5f53 configUSE_LIST_DATA_INTEGRITY_CHECK_BYTES \u8bbe\u7f6e\u4e3a 1 \u65f6\uff0c\u5217\u8868\u548c\u5217\u8868\u9879\u4f1a\u5305\u542b\u5b8c\u6574\u6027\u68c0\u67e5\u5b57\u8282\uff0c\u7528\u4e8e\u68c0\u6d4b\u5185\u5b58\u635f\u574f\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/List%26ListItem/#_16","title":"\u7ebf\u7a0b\u5b89\u5168","text":"<p>\u5217\u8868\u64cd\u4f5c\u4e0d\u662f\u539f\u5b50\u6027\u7684\uff0c\u5728\u64cd\u4f5c\u5217\u8868\u65f6\u9700\u8981\u6302\u8d77\u8c03\u5ea6\u5668\u6216\u4f7f\u7528\u5173\u952e\u6bb5\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/","title":"Mem Mgmt","text":"<pre><code>\u6458\u81ea\u767e\u95ee\u7f51\u3002\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#_1","title":"\u4e3a\u4ec0\u4e48\u8981\u81ea\u5df1\u5b9e\u73b0\u5185\u5b58\u7ba1\u7406","text":"<p>\u540e\u7eed\u7684\u7ae0\u8282\u6d89\u53ca\u8fd9\u4e9b\u5185\u6838\u5bf9\u8c61\uff1atask\u3001queue\u3001semaphores\u548cevent group\u7b49\u3002\u4e3a\u4e86\u8ba9FreeRTOS\u66f4\u5bb9\u6613\u4f7f\u7528\uff0c\u8fd9\u4e9b\u5185\u6838\u5bf9\u8c61\u4e00\u822c\u90fd\u662f\u52a8\u6001\u5206\u914d\uff1a\u7528\u5230\u65f6\u5206\u914d\uff0c\u4e0d\u4f7f\u7528\u65f6\u91ca\u653e\u3002\u4f7f\u7528\u5185\u5b58\u7684\u52a8\u6001\u7ba1\u7406\u529f\u80fd\uff0c\u7b80\u5316\u4e86\u7a0b\u5e8f\u8bbe\u8ba1\uff1a\u4e0d\u518d\u9700\u8981\u5c0f\u5fc3\u7ffc\u7ffc\u5730\u63d0\u524d\u89c4\u5212\u5404\u7c7b\u5bf9\u8c61\uff0c\u7b80\u5316API\u51fd\u6570\u7684\u6d89\u53ca\uff0c\u751a\u81f3\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u7684\u4f7f\u7528\u3002</p> <p>\u5185\u5b58\u7684\u52a8\u6001\u7ba1\u7406\u662fC\u7a0b\u5e8f\u7684\u77e5\u8bc6\u8303\u7574\uff0c\u5e76\u4e0d\u5c5e\u4e8eFreeRTOS\u7684\u77e5\u8bc6\u8303\u7574\uff0c\u4f46\u662f\u5b83\u8ddfFreeRTOS\u5173\u7cfb\u662f\u5982\u6b64\u7d27\u5bc6\uff0c\u6240\u4ee5\u6211\u4eec\u5148\u8bb2\u89e3\u5b83\u3002</p> <p>\u5728C\u8bed\u8a00\u7684\u5e93\u51fd\u6570\u4e2d\uff0c\u6709mallc\u3001free\u7b49\u51fd\u6570\uff0c\u4f46\u662f\u5728FreeRTOS\u4e2d\uff0c\u5b83\u4eec\u4e0d\u9002\u7528\uff1a</p> <ul> <li>\u4e0d\u9002\u5408\u7528\u5728\u8d44\u6e90\u7d27\u7f3a\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d</li> <li>\u8fd9\u4e9b\u51fd\u6570\u7684\u5b9e\u73b0\u8fc7\u4e8e\u590d\u6742\u3001\u5360\u636e\u7684\u4ee3\u7801\u7a7a\u95f4\u592a\u5927</li> <li>\u5e76\u975e\u7ebf\u7a0b\u5b89\u5168\u7684(thread- safe)</li> <li>\u8fd0\u884c\u6709\u4e0d\u786e\u5b9a\u6027\uff1a\u6bcf\u6b21\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\u65f6\u82b1\u8d39\u7684\u65f6\u95f4\u53ef\u80fd\u90fd\u4e0d\u76f8\u540c</li> <li>\u5185\u5b58\u788e\u7247\u5316</li> <li>\u4f7f\u7528\u4e0d\u540c\u7684\u7f16\u8bd1\u5668\u65f6\uff0c\u9700\u8981\u8fdb\u884c\u590d\u6742\u7684\u914d\u7f6e</li> <li>\u6709\u65f6\u5019\u96be\u4ee5\u8c03\u8bd5</li> </ul> <p>\u6ce8\u610f\uff1a\u6211\u4eec\u7ecf\u5e38\"\u5806\u6808\"\u6df7\u5408\u7740\u8bf4\uff0c\u5176\u5b9e\u5b83\u4eec\u4e0d\u662f\u540c\u4e00\u4e2a\u4e1c\u897f\uff1a</p> <ul> <li>\u5806\uff0cheap\uff0c\u5c31\u662f\u4e00\u5757\u7a7a\u95f2\u7684\u5185\u5b58\uff0c\u9700\u8981\u63d0\u4f9b\u7ba1\u7406\u51fd\u6570<ul> <li>malloc\uff1a\u4ece\u5806\u91cc\u5212\u51fa\u4e00\u5757\u7a7a\u95f4\u7ed9\u7a0b\u5e8f\u4f7f\u7528</li> <li>free\uff1a\u7528\u5b8c\u540e\uff0c\u518d\u628a\u5b83\u6807\u8bb0\u4e3a\"\u7a7a\u95f2\"\u7684\uff0c\u53ef\u4ee5\u518d\u6b21\u4f7f\u7528</li> </ul> </li> <li>\u6808\uff0cstack\uff0c\u51fd\u6570\u8c03\u7528\u65f6\u5c40\u90e8\u53d8\u91cf\u4fdd\u5b58\u5728\u6808\u4e2d\uff0c\u5f53\u524d\u7a0b\u5e8f\u7684\u73af\u5883\u4e5f\u662f\u4fdd\u5b58\u5728\u6808\u4e2d<ul> <li>\u53ef\u4ee5\u4ece\u5806\u4e2d\u5206\u914d\u4e00\u5757\u7a7a\u95f4\u7528\u4f5c\u6808</li> </ul> </li> </ul> <p></p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#freertos5","title":"FreeRTOS\u76845\u4e2d\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5","text":"<p>FreeRTOS\u4e2d\u5185\u5b58\u7ba1\u7406\u7684\u63a5\u53e3\u51fd\u6570\u4e3a\uff1apvPortMalloc \u3001vPortFree\uff0c\u5bf9\u5e94\u4e8eC\u5e93\u7684malloc\u3001free\u3002 \u6587\u4ef6\u5728FreeRTOS/Source/portable/MemMang\u4e0b\uff0c\u5b83\u4e5f\u662f\u653e\u5728portable\u76ee\u5f55\u4e0b\uff0c\u8868\u793a\u4f60\u53ef\u4ee5\u63d0\u4f9b\u81ea\u5df1\u7684\u51fd\u6570\u3002</p> <p>\u6e90\u7801\u4e2d\u9ed8\u8ba4\u63d0\u4f9b\u4e865\u4e2a\u6587\u4ef6\uff0c\u5bf9\u5e94\u5185\u5b58\u7ba1\u7406\u76845\u79cd\u65b9\u6cd5\u3002</p> <p>\u53c2\u8003\u6587\u7ae0\uff1aFreeRTOS\u8bf4\u660e\u4e66\u5410\u8840\u6574\u7406\u3010\u9002\u5408\u65b0\u624b+\u5165\u95e8\u3011</p> \u6587\u4ef6 \u4f18\u70b9 \u7f3a\u70b9 heap_1.c \u5206\u914d\u7b80\u5355\uff0c\u65f6\u95f4\u786e\u5b9a \u53ea\u5206\u914d\u3001\u4e0d\u56de\u6536 heap_2.c \u52a8\u6001\u5206\u914d\u3001\u6700\u4f73\u5339\u914d \u788e\u7247\u3001\u65f6\u95f4\u4e0d\u5b9a heap_3.c \u8c03\u7528\u6807\u51c6\u5e93\u51fd\u6570 \u901f\u5ea6\u6162\u3001\u65f6\u95f4\u4e0d\u5b9a heap_4.c \u76f8\u90bb\u7a7a\u95f2\u5185\u5b58\u53ef\u5408\u5e76 \u53ef\u89e3\u51b3\u788e\u7247\u95ee\u9898\u3001\u65f6\u95f4\u4e0d\u5b9a heap_5.c \u5728heap_4\u57fa\u7840\u4e0a\u652f\u6301\u5206\u9694\u7684\u5185\u5b58\u5757 \u53ef\u89e3\u51b3\u788e\u7247\u95ee\u9898\u3001\u65f6\u95f4\u4e0d\u5b9a"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap_1","title":"Heap_1","text":"<p>\u5b83\u53ea\u5b9e\u73b0\u4e86pvPortMalloc\uff0c\u6ca1\u6709\u5b9e\u73b0vPortFree\u3002</p> <p>\u5982\u679c\u4f60\u7684\u7a0b\u5e8f\u4e0d\u9700\u8981\u5220\u9664\u5185\u6838\u5bf9\u8c61\uff0c\u90a3\u4e48\u53ef\u4ee5\u4f7f\u7528heap_1\uff1a</p> <ul> <li>\u5b9e\u73b0\u6700\u7b80\u5355</li> <li>\u6ca1\u6709\u788e\u7247\u95ee\u9898</li> <li>\u4e00\u4e9b\u8981\u6c42\u975e\u5e38\u4e25\u683c\u7684\u7cfb\u7edf\u91cc\uff0c\u4e0d\u5141\u8bb8\u4f7f\u7528\u52a8\u6001\u5185\u5b58\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528heap_1</li> </ul> <p>\u5b83\u7684\u5b9e\u73b0\u539f\u7406\u5f88\u7b80\u5355\uff0c\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u5927\u6570\u7ec4\uff1a</p> <pre><code>/* Allocate the memory for the heap. */\n##if ( configAPPLICATION_ALLOCATED_HEAP == 1 )\n\n/* The application writer has already defined the array used for the RTOS\n* heap -  probably so it can be placed in a special segment or address. */\n    extern uint8_t ucHeap[ configTOTAL_HEAP_SIZE ];\n##else\n    static uint8_t ucHeap[ configTOTAL_HEAP_SIZE ];\n##endif /* configAPPLICATION_ALLOCATED_HEAP */\n</code></pre> <p>\u7136\u540e\uff0c\u5bf9\u4e8epvPortMalloc\u8c03\u7528\u65f6\uff0c\u4ece\u8fd9\u4e2a\u6570\u7ec4\u4e2d\u5206\u914d\u7a7a\u95f4\u3002</p> <p>FreeRTOS\u5728\u521b\u5efa\u4efb\u52a1\u65f6\uff0c\u9700\u89812\u4e2a\u5185\u6838\u5bf9\u8c61\uff1atask control block(TCB)\u3001stack\u3002 \u4f7f\u7528heap_1\u65f6\uff0c\u5185\u5b58\u5206\u914d\u8fc7\u7a0b\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <ul> <li>A\uff1a\u521b\u5efa\u4efb\u52a1\u4e4b\u524d\u6574\u4e2a\u6570\u7ec4\u90fd\u662f\u7a7a\u95f2\u7684</li> <li>B\uff1a\u521b\u5efa\u7b2c1\u4e2a\u4efb\u52a1\u4e4b\u540e\uff0c\u84dd\u8272\u533a\u57df\u88ab\u5206\u914d\u51fa\u53bb\u4e86</li> <li>C\uff1a\u521b\u5efa3\u4e2a\u4efb\u52a1\u4e4b\u540e\u7684\u6570\u7ec4\u4f7f\u7528\u60c5\u51b5</li> </ul> <p></p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap_2","title":"Heap_2","text":"<p>Heap_2\u4e4b\u6240\u4ee5\u8fd8\u4fdd\u7559\uff0c\u53ea\u662f\u4e3a\u4e86\u517c\u5bb9\u4ee5\u524d\u7684\u4ee3\u7801\u3002\u65b0\u8bbe\u8ba1\u4e2d\u4e0d\u518d\u63a8\u8350\u4f7f\u7528Heap_2\u3002\u5efa\u8bae\u4f7f\u7528Heap_4\u6765\u66ff\u4ee3Heap_2\uff0c\u66f4\u52a0\u9ad8\u6548\u3002</p> <p>Heap_2\u4e5f\u662f\u5728\u6570\u7ec4\u4e0a\u5206\u914d\u5185\u5b58\uff0c\u8ddfHeap_1\u4e0d\u4e00\u6837\u7684\u5730\u65b9\u5728\u4e8e\uff1a</p> <ul> <li>Heap_2\u4f7f\u7528\u6700\u4f73\u5339\u914d\u7b97\u6cd5(best fit)\u6765\u5206\u914d\u5185\u5b58</li> <li>\u5b83\u652f\u6301vPortFree</li> </ul> <p>\u6700\u4f73\u5339\u914d\u7b97\u6cd5\uff1a</p> <ul> <li>\u5047\u8bbeheap\u67093\u5757\u7a7a\u95f2\u5185\u5b58\uff1a5\u5b57\u8282\u300125\u5b57\u8282\u3001100\u5b57\u8282</li> <li>pvPortMalloc\u60f3\u7533\u8bf720\u5b57\u8282</li> <li>\u627e\u51fa\u6700\u5c0f\u7684\u3001\u80fd\u6ee1\u8db3pvPortMalloc\u7684\u5185\u5b58\uff1a25\u5b57\u8282</li> <li>\u628a\u5b83\u5212\u5206\u4e3a20\u5b57\u8282\u30015\u5b57\u8282<ul> <li>\u8fd4\u56de\u8fd920\u5b57\u8282\u7684\u5730\u5740</li> <li>\u5269\u4e0b\u76845\u5b57\u8282\u4ecd\u7136\u662f\u7a7a\u95f2\u72b6\u6001\uff0c\u7559\u7ed9\u540e\u7eed\u7684pvPortMalloc\u4f7f\u7528</li> </ul> </li> </ul> <p>\u4e0eHeap_4\u76f8\u6bd4\uff0cHeap_2\u4e0d\u4f1a\u5408\u5e76\u76f8\u90bb\u7684\u7a7a\u95f2\u5185\u5b58\uff0c\u6240\u4ee5Heap_2\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\"\u788e\u7247\u5316\"\u95ee\u9898\u3002</p> <p>\u4f46\u662f\uff0c\u5982\u679c\u7533\u8bf7\u3001\u5206\u914d\u5185\u5b58\u65f6\u5927\u5c0f\u603b\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u7c7b\u573a\u666f\u4e0bHeap_2\u6ca1\u6709\u788e\u7247\u5316\u7684\u95ee\u9898\u3002\u6240\u4ee5\u5b83\u9002\u5408\u8fd9\u79cd\u573a\u666f\uff1a\u9891\u7e41\u5730\u521b\u5efa\u3001\u5220\u9664\u4efb\u52a1\uff0c\u4f46\u662f\u4efb\u52a1\u7684\u6808\u5927\u5c0f\u90fd\u662f\u76f8\u540c\u7684(\u521b\u5efa\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5206\u914dTCB\u548c\u6808\uff0cTCB\u603b\u662f\u4e00\u6837\u7684)\u3002</p> <p>\u867d\u7136\u4e0d\u518d\u63a8\u8350\u4f7f\u7528heap_2\uff0c\u4f46\u662f\u5b83\u7684\u6548\u7387\u8fd8\u662f\u8fdc\u9ad8\u4e8emalloc\u3001free\u3002</p> <p>\u4f7f\u7528heap_2\u65f6\uff0c\u5185\u5b58\u5206\u914d\u8fc7\u7a0b\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <ul> <li>A\uff1a\u521b\u5efa\u4e863\u4e2a\u4efb\u52a1</li> <li>B\uff1a\u5220\u9664\u4e86\u4e00\u4e2a\u4efb\u52a1\uff0c\u7a7a\u95f2\u5185\u5b58\u67093\u90e8\u5206\uff1a\u9876\u5c42\u7684\u3001\u88ab\u5220\u9664\u4efb\u52a1\u7684TCB\u7a7a\u95f4\u3001\u88ab\u5220\u9664\u4efb\u52a1\u7684Stack\u7a7a\u95f4</li> <li>C\uff1a\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u4efb\u52a1\uff0c\u56e0\u4e3aTCB\u3001\u6808\u5927\u5c0f\u8ddf\u524d\u9762\u88ab\u5220\u9664\u4efb\u52a1\u7684TCB\u3001\u6808\u5927\u5c0f\u4e00\u81f4\uff0c\u6240\u4ee5\u521a\u597d\u5206\u914d\u5230\u539f\u6765\u7684\u5185\u5b58</li> </ul> <p></p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap_3","title":"Heap_3","text":"<p>Heap_3\u4f7f\u7528\u6807\u51c6C\u5e93\u91cc\u7684malloc\u3001free\u51fd\u6570\uff0c\u6240\u4ee5\u5806\u5927\u5c0f\u7531\u94fe\u63a5\u5668\u7684\u914d\u7f6e\u51b3\u5b9a\uff0c\u914d\u7f6e\u9879configTOTAL_HEAP_SIZE\u4e0d\u518d\u8d77\u4f5c\u7528\u3002</p> <p>C\u5e93\u91cc\u7684malloc\u3001free\u51fd\u6570\u5e76\u975e\u7ebf\u7a0b\u5b89\u5168\u7684\uff0cHeap_3\u4e2d\u5148\u6682\u505cFreeRTOS\u7684\u8c03\u5ea6\u5668\uff0c\u518d\u53bb\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\uff0c\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ebf\u7a0b\u5b89\u5168\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap_4","title":"Heap_4","text":"<p>\u8ddfHeap_1\u3001Heap_2\u4e00\u6837\uff0cHeap_4\u4e5f\u662f\u4f7f\u7528\u5927\u6570\u7ec4\u6765\u5206\u914d\u5185\u5b58\u3002</p> <p>Heap_4\u4f7f\u7528\u00a0\u9996\u6b21\u9002\u5e94\u7b97\u6cd5(first fit)\u6765\u5206\u914d\u5185\u5b58\u00a0\u3002\u5b83\u8fd8\u4f1a\u628a\u76f8\u90bb\u7684\u7a7a\u95f2\u5185\u5b58\u5408\u5e76\u4e3a\u4e00\u4e2a\u66f4\u5927\u7684\u7a7a\u95f2\u5185\u5b58\uff0c\u8fd9\u6709\u52a9\u4e8e\u8f83\u5c11\u5185\u5b58\u7684\u788e\u7247\u95ee\u9898\u3002</p> <p>\u9996\u6b21\u9002\u5e94\u7b97\u6cd5\uff1a</p> <ul> <li>\u5047\u8bbe\u5806\u4e2d\u67093\u5757\u7a7a\u95f2\u5185\u5b58\uff1a5\u5b57\u8282\u3001200\u5b57\u8282\u3001100\u5b57\u8282</li> <li>pvPortMalloc\u60f3\u7533\u8bf720\u5b57\u8282</li> <li>\u627e\u51fa\u7b2c1\u4e2a\u80fd\u6ee1\u8db3pvPortMalloc\u7684\u5185\u5b58\uff1a200\u5b57\u8282</li> <li>\u628a\u5b83\u5212\u5206\u4e3a20\u5b57\u8282\u3001180\u5b57\u8282</li> <li>\u8fd4\u56de\u8fd920\u5b57\u8282\u7684\u5730\u5740</li> <li>\u5269\u4e0b\u7684180\u5b57\u8282\u4ecd\u7136\u662f\u7a7a\u95f2\u72b6\u6001\uff0c\u7559\u7ed9\u540e\u7eed\u7684pvPortMalloc\u4f7f\u7528</li> </ul> <p>Heap_4\u4f1a\u628a\u76f8\u90bb\u7a7a\u95f2\u5185\u5b58\u5408\u5e76\u4e3a\u4e00\u4e2a\u5927\u7684\u7a7a\u95f2\u5185\u5b58\uff0c\u53ef\u4ee5\u8f83\u5c11\u5185\u5b58\u7684\u788e\u7247\u5316\u95ee\u9898\u3002\u9002\u7528\u4e8e\u8fd9\u79cd\u573a\u666f\uff1a\u9891\u7e41\u5730\u5206\u914d\u3001\u91ca\u653e\u4e0d\u540c\u5927\u5c0f\u7684\u5185\u5b58\u3002</p> <p>Heap_4\u7684\u4f7f\u7528\u8fc7\u7a0b\u4e3e\u4f8b\u5982\u4e0b\uff1a</p> <ul> <li>A\uff1a\u521b\u5efa\u4e863\u4e2a\u4efb\u52a1</li> <li>B\uff1a\u5220\u9664\u4e86\u4e00\u4e2a\u4efb\u52a1\uff0c\u7a7a\u95f2\u5185\u5b58\u67092\u90e8\u5206\uff1a</li> <li>\u9876\u5c42\u7684</li> <li>\u88ab\u5220\u9664\u4efb\u52a1\u7684TCB\u7a7a\u95f4\u3001\u88ab\u5220\u9664\u4efb\u52a1\u7684Stack\u7a7a\u95f4\u5408\u5e76\u8d77\u6765\u7684</li> <li>C\uff1a\u5206\u914d\u4e86\u4e00\u4e2aQueue\uff0c\u4ece\u7b2c1\u4e2a\u7a7a\u95f2\u5757\u4e2d\u5206\u914d\u7a7a\u95f4</li> <li>D\uff1a\u5206\u914d\u4e86\u4e00\u4e2aUser\u6570\u636e\uff0c\u4eceQueue\u4e4b\u540e\u7684\u7a7a\u95f2\u5757\u4e2d\u5206\u914d</li> <li>E\uff1a\u91ca\u653e\u7684Queue\uff0cUser\u524d\u540e\u90fd\u6709\u4e00\u5757\u7a7a\u95f2\u5185\u5b58</li> <li>F\uff1a\u91ca\u653e\u4e86User\u6570\u636e\uff0cUser\u524d\u540e\u7684\u5185\u5b58\u3001User\u672c\u8eab\u5360\u636e\u7684\u5185\u5b58\uff0c\u5408\u5e76\u4e3a\u4e00\u4e2a\u5927\u7684\u7a7a\u95f2\u5185\u5b58</li> </ul> <p></p> <p>Heap_4\u6267\u884c\u7684\u65f6\u95f4\u662f\u4e0d\u786e\u5b9a\u7684\uff0c\u4f46\u662f\u5b83\u7684\u6548\u7387\u9ad8\u4e8e\u6807\u51c6\u5e93\u7684malloc\u3001free\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap_5","title":"Heap_5","text":"<p>Heap_5\u5206\u914d\u5185\u5b58\u3001\u91ca\u653e\u5185\u5b58\u7684\u7b97\u6cd5\u8ddfHeap_4\u662f\u4e00\u6837\u7684\u3002</p> <p>\u76f8\u6bd4\u4e8eHeap_4\uff0cHeap_5\u5e76\u4e0d\u5c40\u9650\u4e8e\u7ba1\u7406\u4e00\u4e2a\u5927\u6570\u7ec4\uff1a\u5b83\u53ef\u4ee5\u7ba1\u7406\u591a\u5757\u3001\u5206\u9694\u5f00\u7684\u5185\u5b58\u3002</p> <p>\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5185\u5b58\u7684\u5730\u5740\u53ef\u80fd\u5e76\u4e0d\u8fde\u7eed\uff0c\u8fd9\u79cd\u573a\u666f\u4e0b\u53ef\u4ee5\u4f7f\u7528Heap_5\u3002</p> <p>\u65e2\u7136\u5185\u5b58\u662f\u5206\u9694\u5f00\u7684\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u8fdb\u884c\u521d\u59cb\u5316\uff1a\u786e\u5b9a\u8fd9\u4e9b\u5185\u5b58\u5757\u5728\u54ea\u3001\u591a\u5927\uff1a</p> <ul> <li>\u5728\u4f7f\u7528pvPortMalloc\u4e4b\u524d\uff0c\u5fc5\u987b\u5148\u6307\u5b9a\u5185\u5b58\u5757\u7684\u4fe1\u606f</li> <li>\u4f7f\u7528vPortDefineHeapRegions\u6765\u6307\u5b9a\u8fd9\u4e9b\u4fe1\u606f</li> </ul> <p>\u600e\u4e48\u6307\u5b9a\u4e00\u5757\u5185\u5b58\uff1f\u4f7f\u7528\u5982\u4e0b\u7ed3\u6784\u4f53\uff1a</p> <pre><code>typedef struct HeapRegion\n{\n    uint8_t * pucStartAddress; // \u8d77\u59cb\u5730\u5740\n    size_t xSizeInBytes;       // \u5927\u5c0f\n} HeapRegion_t;\n</code></pre> <p>\u600e\u4e48\u6307\u5b9a\u591a\u5757\u5185\u5b58\uff1f\u4f7f\u7528\u4e00\u4e2aHeapRegion_t\u6570\u7ec4\uff0c\u5728\u8fd9\u4e2a\u6570\u7ec4\u4e2d\uff0c\u4f4e\u5730\u5740\u5728\u524d\u3001\u9ad8\u5730\u5740\u5728\u540e\u3002 \u6bd4\u5982\uff1a</p> <pre><code>HeapRegion_t xHeapRegions[] =\n{\n  { ( uint8_t * ) 0x80000000UL, 0x10000 }, // \u8d77\u59cb\u5730\u57400x80000000\uff0c\u5927\u5c0f0x10000\n  { ( uint8_t * ) 0x90000000UL, 0xa0000 }, // \u8d77\u59cb\u5730\u57400x90000000\uff0c\u5927\u5c0f0xa0000\n  { NULL, 0 } // \u8868\u793a\u6570\u7ec4\u7ed3\u675f\n };\n</code></pre> <p>vPortDefineHeapRegions\u51fd\u6570\u539f\u578b\u5982\u4e0b\uff1a</p> <pre><code>void vPortDefineHeapRegions( const HeapRegion_t * const pxHeapRegions );\n</code></pre> <p>\u628axHeapRegions\u6570\u7ec4\u4f20\u7ed9vPortDefineHeapRegions\u51fd\u6570\uff0c\u5373\u53ef\u521d\u59cb\u5316Heap_5\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#heap","title":"Heap\u76f8\u5173\u7684\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#pvportmallocvportfree","title":"<code>pvPortMalloc</code>/<code>vPortFree</code>","text":"<p>\u51fd\u6570\u539f\u578b\uff1a</p> <pre><code>void * pvPortMalloc( size_t xWantedSize );\nvoid vPortFree( void * pv );\n</code></pre> <p>\u4f5c\u7528\uff1a\u5206\u914d\u5185\u5b58\u3001\u91ca\u653e\u5185\u5b58\u3002</p> <p>\u5982\u679c\u5206\u914d\u5185\u5b58\u4e0d\u6210\u529f\uff0c\u5219\u8fd4\u56de\u503c\u4e3aNULL\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#xportgetfreeheapsize","title":"<code>xPortGetFreeHeapSize</code>","text":"<p>\u51fd\u6570\u539f\u578b\uff1a</p> <pre><code>size_t xPortGetFreeHeapSize( void );\n</code></pre> <p>\u5f53\u524d\u8fd8\u6709\u591a\u5c11\u7a7a\u95f2\u5185\u5b58\uff0c\u8fd9\u51fd\u6570\u53ef\u4ee5\u7528\u6765\u4f18\u5316\u5185\u5b58\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u6bd4\u5982\u5f53\u6240\u6709\u5185\u6838\u5bf9\u8c61\u90fd\u5206\u914d\u597d\u540e\uff0c\u6267\u884c\u6b64\u51fd\u6570\u8fd4\u56de2000\uff0c\u90a3\u4e48configTOTAL_HEAP_SIZE\u5c31\u53ef\u51cf\u5c0f2000\u3002</p> <p>\u6ce8\u610f\uff1a\u5728heap_3\u4e2d\u65e0\u6cd5\u4f7f\u7528\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#xportgetminimumeverfreeheapsize","title":"<code>xPortGetMinimumEverFreeHeapSize</code>","text":"<p>\u51fd\u6570\u539f\u578b\uff1a</p> <pre><code>size_t xPortGetMinimumEverFreeHeapSize( void );\n</code></pre> <p>\u8fd4\u56de\uff1a\u7a0b\u5e8f\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u7a7a\u95f2\u5185\u5b58\u5bb9\u91cf\u7684\u6700\u5c0f\u503c\u3002</p> <p>\u6ce8\u610f\uff1a\u53ea\u6709heap_4\u3001heap_5\u652f\u6301\u6b64\u51fd\u6570\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mem_Mgmt/#malloc","title":"malloc\u5931\u8d25\u7684\u94a9\u5b50\u51fd\u6570","text":"<p>\u5728pvPortMalloc\u51fd\u6570\u5185\u90e8\uff1a</p> <pre><code>void * pvPortMalloc( size_t xWantedSize )vPortDefineHeapRegions\n{\n    ......\n    #if ( configUSE_MALLOC_FAILED_HOOK == 1 )\n        {\n            if( pvReturn == NULL )\n            {\n                extern void vApplicationMallocFailedHook( void );\n                vApplicationMallocFailedHook();\n            }\n        }\n    #endif\n\n    return pvReturn;        \n}\n</code></pre> <p>\u6240\u4ee5\uff0c\u5982\u679c\u60f3\u4f7f\u7528\u8fd9\u4e2a\u94a9\u5b50\u51fd\u6570\uff1a - \u5728FreeRTOSConfig.h\u4e2d\uff0c\u628aconfigUSE_MALLOC_FAILED_HOOK\u5b9a\u4e49\u4e3a1 - \u63d0\u4f9bvApplicationMallocFailedHook\u51fd\u6570 - pvPortMalloc\u5931\u8d25\u65f6\uff0c\u624d\u4f1a\u8c03\u7528\u6b64\u51fd\u6570</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/","title":"\u4e92\u65a5\u91cf\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_2","title":"\u4ec0\u4e48\u662f\u4e92\u65a5\u91cf","text":"<p>\u4e92\u65a5\u91cf\uff08Mutex\uff09\u662f\u7528\u4e8e\u4fdd\u62a4\u5171\u4eab\u8d44\u6e90\u7684\u540c\u6b65\u673a\u5236\uff0c\u786e\u4fdd\u5728\u4efb\u4f55\u65f6\u523b\u53ea\u6709\u4e00\u4e2a\u4efb\u52a1\u53ef\u4ee5\u8bbf\u95ee\u4e34\u754c\u8d44\u6e90\u3002\u60f3\u8c61\u4e00\u4e0b\u536b\u751f\u95f4\u7684\u94a5\u5319\uff1a</p> <ul> <li>\u53ea\u6709\u4e00\u4e2a\u94a5\u5319\uff1a\u540c\u4e00\u65f6\u95f4\u53ea\u80fd\u6709\u4e00\u4e2a\u4eba\u4f7f\u7528\u536b\u751f\u95f4</li> <li>\u8c01\u62ff\u94a5\u5319\u8c01\u7528\uff1a\u62ff\u5230\u94a5\u5319\u7684\u4eba\u624d\u80fd\u8fdb\u5165</li> <li>\u7528\u5b8c\u8981\u5f52\u8fd8\uff1a\u4f7f\u7528\u5b8c\u6bd5\u540e\u5fc5\u987b\u5f52\u8fd8\u94a5\u5319\uff0c\u5176\u4ed6\u4eba\u624d\u80fd\u4f7f\u7528</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_3","title":"\u4e92\u65a5\u91cf\u7684\u6838\u5fc3\u7279\u6027","text":"<p>\u6240\u6709\u6743\u673a\u5236\uff1a - \u53ea\u6709\u83b7\u53d6\u4e92\u65a5\u91cf\u7684\u4efb\u52a1\u624d\u80fd\u91ca\u653e\u5b83 - \u7cfb\u7edf\u4f1a\u8bb0\u5f55\u5f53\u524d\u6301\u6709\u4e92\u65a5\u91cf\u7684\u4efb\u52a1</p> <p>\u4f18\u5148\u7ea7\u7ee7\u627f\uff1a - \u5f53\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u7b49\u5f85\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u6301\u6709\u7684\u4e92\u65a5\u91cf\u65f6 - \u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u4f1a\u4e34\u65f6\u63d0\u5347\u5230\u9ad8\u4f18\u5148\u7ea7 - \u9632\u6b62\"\u4f18\u5148\u7ea7\u53cd\u8f6c\"\u95ee\u9898</p> <p>\u9012\u5f52\u8bbf\u95ee\uff1a - \u540c\u4e00\u4e2a\u4efb\u52a1\u53ef\u4ee5\u591a\u6b21\u83b7\u53d6\u540c\u4e00\u4e2a\u4e92\u65a5\u91cf - \u9700\u8981\u76f8\u540c\u6b21\u6570\u7684\u91ca\u653e\u64cd\u4f5c\u624d\u80fd\u5f7b\u5e95\u91ca\u653e</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#vs","title":"\u4e92\u65a5\u91cf vs \u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_4","title":"\u5f62\u8c61\u5bf9\u6bd4","text":"\u7279\u6027 \u4e92\u65a5\u91cf\uff08Mutex\uff09 \u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\uff08Binary Semaphore\uff09 \u6bd4\u55bb \u536b\u751f\u95f4\u94a5\u5319 \u505c\u8f66\u573a\u7a7a\u4f4d\u4fe1\u53f7 \u6240\u6709\u6743 \u6709\u660e\u786e\u6240\u6709\u8005 \u65e0\u6240\u6709\u8005\u6982\u5ff5 \u91ca\u653e\u9650\u5236 \u53ea\u80fd\u7531\u83b7\u53d6\u8005\u91ca\u653e \u4efb\u4f55\u4efb\u52a1\u90fd\u53ef\u4ee5\u91ca\u653e \u4f18\u5148\u7ea7\u5904\u7406 \u652f\u6301\u4f18\u5148\u7ea7\u7ee7\u627f \u65e0\u4f18\u5148\u7ea7\u4fdd\u62a4 \u9002\u7528\u573a\u666f \u4fdd\u62a4\u5171\u4eab\u8d44\u6e90 \u4efb\u52a1\u540c\u6b65"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_5","title":"\u6280\u672f\u5bf9\u6bd4","text":"<pre><code>// \u4e92\u65a5\u91cf - \u7528\u4e8e\u8d44\u6e90\u4fdd\u62a4\nSemaphoreHandle_t xMutex = xSemaphoreCreateMutex();\n\nvoid vTaskAccessResource(void) {\n    if(xSemaphoreTake(xMutex, portMAX_DELAY) == pdTRUE) {\n        // \u8bbf\u95ee\u5171\u4eab\u8d44\u6e90\n        access_shared_resource();\n        // \u5fc5\u987b\u7531\u540c\u4e00\u4e2a\u4efb\u52a1\u91ca\u653e\n        xSemaphoreGive(xMutex);\n    }\n}\n\n// \u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf - \u7528\u4e8e\u4efb\u52a1\u540c\u6b65\nSemaphoreHandle_t xBinarySemaphore = xSemaphoreCreateBinary();\n\nvoid vProducerTask(void) {\n    // \u751f\u4ea7\u6570\u636e...\n    xSemaphoreGive(xBinarySemaphore);  // \u901a\u77e5\u6d88\u8d39\u8005\n}\n\nvoid vConsumerTask(void) {\n    xSemaphoreTake(xBinarySemaphore, portMAX_DELAY);  // \u7b49\u5f85\u751f\u4ea7\u8005\n    // \u6d88\u8d39\u6570\u636e...\n    // \u4e0d\u9700\u8981\u91ca\u653e\uff0c\u56e0\u4e3a\u7528\u4e8e\u540c\u6b65\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_6","title":"\u4f18\u5148\u7ea7\u53cd\u8f6c\u95ee\u9898\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_7","title":"\u4ec0\u4e48\u662f\u4f18\u5148\u7ea7\u53cd\u8f6c","text":"<p>\u4f18\u5148\u7ea7\u53cd\u8f6c\u662f\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u53d1\u751f\u5728\u4e0d\u540c\u4f18\u5148\u7ea7\u4efb\u52a1\u7ade\u4e89\u540c\u4e00\u8d44\u6e90\u65f6\uff1a</p> <p>\u95ee\u9898\u573a\u666f\uff1a 1. \u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1L\u83b7\u53d6\u4e86\u4e92\u65a5\u91cf 2. \u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1M\u5c31\u7eea\uff0c\u62a2\u5360\u4e86L 3. \u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u9700\u8981\u76f8\u540c\u4e92\u65a5\u91cf\uff0c\u88ab\u963b\u585e 4. \u7ed3\u679c\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u88ab\u8feb\u7b49\u5f85\u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1M</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_8","title":"\u4e92\u65a5\u91cf\u7684\u89e3\u51b3\u65b9\u6848","text":"<p>\u4e92\u65a5\u91cf\u901a\u8fc7\u4f18\u5148\u7ea7\u7ee7\u627f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a</p> <pre><code>// \u6ca1\u6709\u4f18\u5148\u7ea7\u7ee7\u627f\u7684\u60c5\u51b5\uff08\u4f7f\u7528\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\uff09\nvoid vLowPriorityTask(void) {\n    xSemaphoreTake(xBinarySemaphore, portMAX_DELAY);  // L\u83b7\u53d6\u4fe1\u53f7\u91cf\n    // \u6b64\u65f6\u88ab\u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1M\u62a2\u5360\n    // \u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u88ab\u963b\u585e\uff0c\u5fc5\u987b\u7b49\u5f85M\u548cL\u90fd\u5b8c\u6210\n    xSemaphoreGive(xBinarySemaphore);\n}\n\n// \u6709\u4f18\u5148\u7ea7\u7ee7\u627f\u7684\u60c5\u51b5\uff08\u4f7f\u7528\u4e92\u65a5\u91cf\uff09\nvoid vLowPriorityTask(void) {\n    xSemaphoreTake(xMutex, portMAX_DELAY);  // L\u83b7\u53d6\u4e92\u65a5\u91cf\n    // \u5f53\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u7b49\u5f85\u65f6\uff0cL\u4e34\u65f6\u63d0\u5347\u5230H\u7684\u4f18\u5148\u7ea7\n    // L\u4e0d\u4f1a\u88abM\u62a2\u5360\uff0c\u53ef\u4ee5\u5feb\u901f\u5b8c\u6210\u5e76\u91ca\u653e\u4e92\u65a5\u91cf\n    xSemaphoreGive(xMutex);  // L\u6062\u590d\u539f\u6709\u4f18\u5148\u7ea7\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#freertos","title":"FreeRTOS\u4e92\u65a5\u91cf\u521b\u5efa\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphorecreatemutex-","title":"xSemaphoreCreateMutex - \u521b\u5efa\u4e92\u65a5\u91cf","text":"<pre><code>SemaphoreHandle_t xSemaphoreCreateMutex(void);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u4e92\u65a5\u91cf\u53e5\u67c4 - \u5931\u8d25\uff1aNULL\uff08\u5185\u5b58\u4e0d\u8db3\u65f6\uff09</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u4fdd\u62a4UART\u7684\u4e92\u65a5\u91cf\nSemaphoreHandle_t xUartMutex;\n\nvoid vInitMutexes(void) {\n    xUartMutex = xSemaphoreCreateMutex();\n    if(xUartMutex == NULL) {\n        printf(\"ERROR: UART mutex creation failed!\\n\");\n    } else {\n        printf(\"UART mutex created successfully\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphorecreatemutexstatic-","title":"xSemaphoreCreateMutexStatic - \u9759\u6001\u521b\u5efa","text":"<pre><code>SemaphoreHandle_t xSemaphoreCreateMutexStatic(StaticSemaphore_t *pxMutexBuffer);\n</code></pre> <p>\u53c2\u6570\uff1a - <code>pxMutexBuffer</code>\uff1a\u6307\u5411\u9759\u6001\u5206\u914d\u7684\u5185\u5b58\u5757</p> <p>\u9759\u6001\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u9759\u6001\u5206\u914d\u4e92\u65a5\u91cf\u5185\u5b58\nstatic StaticSemaphore_t xUartMutexBuffer;\n\nvoid vInitStaticMutex(void) {\n    xUartMutex = xSemaphoreCreateMutexStatic(&amp;xUartMutexBuffer);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphorecreaterecursivemutex-","title":"xSemaphoreCreateRecursiveMutex - \u521b\u5efa\u9012\u5f52\u4e92\u65a5\u91cf","text":"<pre><code>SemaphoreHandle_t xSemaphoreCreateRecursiveMutex(void);\n</code></pre> <p>\u9012\u5f52\u4e92\u65a5\u91cf\u7279\u70b9\uff1a - \u540c\u4e00\u4e2a\u4efb\u52a1\u53ef\u4ee5\u591a\u6b21\u83b7\u53d6 - \u9700\u8981\u76f8\u540c\u6b21\u6570\u7684\u91ca\u653e\u64cd\u4f5c - \u9632\u6b62\u81ea\u6b7b\u9501</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_9","title":"\u4e92\u65a5\u91cf\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphoretake-","title":"xSemaphoreTake - \u83b7\u53d6\u4e92\u65a5\u91cf","text":"<pre><code>BaseType_t xSemaphoreTake(SemaphoreHandle_t xSemaphore, \n                         TickType_t xTicksToWait);\n</code></pre> <p>\u53c2\u6570\uff1a - <code>xSemaphore</code>\uff1a\u4e92\u65a5\u91cf\u53e5\u67c4 - <code>xTicksToWait</code>\uff1a\u7b49\u5f85\u8d85\u65f6\u65f6\u95f4</p> <p>\u8fd4\u56de\u503c\uff1a - <code>pdTRUE</code>\uff1a\u6210\u529f\u83b7\u53d6\u4e92\u65a5\u91cf - <code>pdFALSE</code>\uff1a\u8d85\u65f6\u672a\u83b7\u53d6\u5230</p> <p>\u4f7f\u7528\u6a21\u5f0f\uff1a</p> <pre><code>if(xSemaphoreTake(xMutex, portMAX_DELAY) == pdTRUE) {\n    // \u8fdb\u5165\u4e34\u754c\u533a\uff0c\u8bbf\u95ee\u5171\u4eab\u8d44\u6e90\n    access_shared_resource();\n\n    // \u9000\u51fa\u4e34\u754c\u533a\n    xSemaphoreGive(xMutex);\n} else {\n    // \u5904\u7406\u83b7\u53d6\u5931\u8d25\u7684\u60c5\u51b5\n    handle_mutex_timeout();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphoregive-","title":"xSemaphoreGive - \u91ca\u653e\u4e92\u65a5\u91cf","text":"<pre><code>BaseType_t xSemaphoreGive(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u91cd\u8981\u89c4\u5219\uff1a - \u5fc5\u987b\u7531\u83b7\u53d6\u4e92\u65a5\u91cf\u7684\u4efb\u52a1\u91ca\u653e - \u4e0d\u80fd\u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u4f7f\u7528 - \u9012\u5f52\u4e92\u65a5\u91cf\u9700\u8981\u5339\u914d\u7684\u91ca\u653e\u6b21\u6570</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_10","title":"\u9012\u5f52\u4e92\u65a5\u91cf\u64cd\u4f5c","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphoretakerecursive","title":"xSemaphoreTakeRecursive","text":"<pre><code>BaseType_t xSemaphoreTakeRecursive(SemaphoreHandle_t xMutex,\n                                  TickType_t xTicksToWait);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#xsemaphoregiverecursive","title":"xSemaphoreGiveRecursive","text":"<pre><code>BaseType_t xSemaphoreGiveRecursive(SemaphoreHandle_t xMutex);\n</code></pre> <p>\u9012\u5f52\u4e92\u65a5\u91cf\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>void vComplexOperation(void) {\n    // \u7b2c\u4e00\u6b21\u83b7\u53d6\n    if(xSemaphoreTakeRecursive(xRecursiveMutex, portMAX_DELAY) == pdTRUE) {\n        // \u8c03\u7528\u5176\u4ed6\u53ef\u80fd\u4e5f\u9700\u8981\u540c\u4e00\u4e92\u65a5\u91cf\u7684\u51fd\u6570\n        vSubFunction();\n\n        // \u9700\u8981\u5339\u914d\u7684\u91ca\u653e\u6b21\u6570\n        xSemaphoreGiveRecursive(xRecursiveMutex);\n    }\n}\n\nvoid vSubFunction(void) {\n    // \u7b2c\u4e8c\u6b21\u83b7\u53d6\uff08\u540c\u4e00\u4e2a\u4efb\u52a1\uff09\n    if(xSemaphoreTakeRecursive(xRecursiveMutex, portMAX_DELAY) == pdTRUE) {\n        // \u6267\u884c\u64cd\u4f5c...\n        xSemaphoreGiveRecursive(xRecursiveMutex);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_11","title":"\u4e92\u65a5\u91cf\u5220\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#vsemaphoredelete-","title":"vSemaphoreDelete - \u5220\u9664\u4e92\u65a5\u91cf","text":"<pre><code>void vSemaphoreDelete(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u4f7f\u7528\u6ce8\u610f\u4e8b\u9879\uff1a - \u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u6b63\u5728\u7b49\u5f85\u6216\u6301\u6709\u4e92\u65a5\u91cf - \u52a8\u6001\u521b\u5efa\u7684\u4e92\u65a5\u91cf\u624d\u4f1a\u91ca\u653e\u5185\u5b58 - \u9759\u6001\u521b\u5efa\u7684\u4e92\u65a5\u91cf\u53ea\u91cd\u7f6e\u72b6\u6001\uff0c\u4e0d\u91ca\u653e\u5185\u5b58</p> <p>\u5220\u9664\u793a\u4f8b\uff1a</p> <pre><code>void vCleanupMutexes(void) {\n    // \u68c0\u67e5\u662f\u5426\u6709\u4efb\u52a1\u6301\u6709\u4e92\u65a5\u91cf\n    if(uxSemaphoreGetCount(xUartMutex) == 0) {\n        vSemaphoreDelete(xUartMutex);\n        xUartMutex = NULL;\n        printf(\"UART mutex deleted\\n\");\n    } else {\n        printf(\"Warning: Cannot delete mutex, it's currently held\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_12","title":"\u4e92\u65a5\u91cf\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#uxsemaphoregetcount-","title":"uxSemaphoreGetCount - \u83b7\u53d6\u4fe1\u53f7\u91cf\u8ba1\u6570","text":"<pre><code>UBaseType_t uxSemaphoreGetCount(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u5bf9\u4e8e\u4e92\u65a5\u91cf\u7684\u8fd4\u56de\u503c\uff1a - 1\uff1a\u4e92\u65a5\u91cf\u53ef\u7528 - 0\uff1a\u4e92\u65a5\u91cf\u5df2\u88ab\u5360\u7528</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>void vCheckMutexStatus(void) {\n    UBaseType_t uxCount = uxSemaphoreGetCount(xUartMutex);\n\n    if(uxCount == 1) {\n        printf(\"Mutex is available\\n\");\n    } else {\n        printf(\"Mutex is currently held by another task\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_13","title":"\u4e92\u65a5\u91cf\u7efc\u5408\u5b9e\u9a8c\u6848\u4f8b\uff1a\u591a\u4efb\u52a1\u5171\u4eab\u8d44\u6e90\u4fdd\u62a4","text":"<pre><code>#include \"FreeRTOS.h\"\n#include \"task.h\"\n#include \"semphr.h\"\n#include \"stdio.h\"\n#include \"string.h\"\n\n// \u5171\u4eab\u8d44\u6e90 - \u5168\u5c40\u6570\u636e\ntypedef struct {\n    int temperature;\n    int humidity;\n    int pressure;\n    uint32_t update_count;\n} SharedSensorData_t;\n\n// \u5171\u4eab\u8d44\u6e90\u5b9e\u4f8b\nstatic SharedSensorData_t xSharedData = {0};\n\n// \u4e92\u65a5\u91cf\u53e5\u67c4\nSemaphoreHandle_t xDataMutex;        // \u4fdd\u62a4\u5171\u4eab\u6570\u636e\nSemaphoreHandle_t xDisplayMutex;     // \u4fdd\u62a4\u663e\u793a\u8f93\u51fa\nSemaphoreHandle_t xFileMutex;        // \u4fdd\u62a4\u6587\u4ef6\u64cd\u4f5c\n\n// \u4f20\u611f\u5668\u6570\u636e\u91c7\u96c6\u4efb\u52a1\nvoid vSensorTask(void *pvParameters) {\n    int task_id = (int)pvParameters;\n    int local_temp, local_humid, local_press;\n\n    printf(\"Sensor Task %d started\\n\", task_id);\n\n    while(1) {\n        // \u6a21\u62df\u4f20\u611f\u5668\u8bfb\u6570\n        local_temp = 20 + task_id + (rand() % 5);\n        local_humid = 40 + task_id * 5 + (rand() % 10);\n        local_press = 1000 + task_id * 10 + (rand() % 20);\n\n        // \u83b7\u53d6\u6570\u636e\u4e92\u65a5\u91cf\n        if(xSemaphoreTake(xDataMutex, 100 / portTICK_PERIOD_MS) == pdTRUE) {\n            // \u66f4\u65b0\u5171\u4eab\u6570\u636e\n            xSharedData.temperature = local_temp;\n            xSharedData.humidity = local_humid;\n            xSharedData.pressure = local_press;\n            xSharedData.update_count++;\n\n            printf(\"Sensor%d updated shared data (count: %lu)\\n\", \n                   task_id, xSharedData.update_count);\n\n            // \u91ca\u653e\u4e92\u65a5\u91cf\n            xSemaphoreGive(xDataMutex);\n        } else {\n            printf(\"Sensor%d: Failed to get data mutex!\\n\", task_id);\n        }\n\n        vTaskDelay(2000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u6570\u636e\u663e\u793a\u4efb\u52a1\nvoid vDisplayTask(void *pvParameters) {\n    SharedSensorData_t display_data;\n\n    printf(\"Display Task started\\n\");\n\n    while(1) {\n        // \u83b7\u53d6\u6570\u636e\u4e92\u65a5\u91cf\u8bfb\u53d6\u6570\u636e\n        if(xSemaphoreTake(xDataMutex, portMAX_DELAY) == pdTRUE) {\n            // \u62f7\u8d1d\u6570\u636e\u5230\u5c40\u90e8\u53d8\u91cf\n            memcpy(&amp;display_data, &amp;xSharedData, sizeof(SharedSensorData_t));\n            xSemaphoreGive(xDataMutex);\n\n            // \u83b7\u53d6\u663e\u793a\u4e92\u65a5\u91cf\u8fdb\u884c\u8f93\u51fa\n            if(xSemaphoreTake(xDisplayMutex, 50 / portTICK_PERIOD_MS) == pdTRUE) {\n                printf(\"\\n=== Current Sensor Readings ===\\n\");\n                printf(\"Temperature: %d\u00b0C\\n\", display_data.temperature);\n                printf(\"Humidity:    %d%%\\n\", display_data.humidity);\n                printf(\"Pressure:    %dhPa\\n\", display_data.pressure);\n                printf(\"Update Count: %lu\\n\", display_data.update_count);\n                printf(\"===============================\\n\");\n\n                xSemaphoreGive(xDisplayMutex);\n            }\n        }\n\n        vTaskDelay(3000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u6570\u636e\u8bb0\u5f55\u4efb\u52a1\uff08\u6a21\u62df\u6587\u4ef6\u64cd\u4f5c\uff09\nvoid vDataLoggerTask(void *pvParameters) {\n    SharedSensorData_t log_data;\n    static uint32_t log_count = 0;\n\n    printf(\"Data Logger Task started\\n\");\n\n    while(1) {\n        // \u83b7\u53d6\u6570\u636e\u4e92\u65a5\u91cf\n        if(xSemaphoreTake(xDataMutex, portMAX_DELAY) == pdTRUE) {\n            memcpy(&amp;log_data, &amp;xSharedData, sizeof(SharedSensorData_t));\n            xSemaphoreGive(xDataMutex);\n\n            // \u83b7\u53d6\u6587\u4ef6\u4e92\u65a5\u91cf\uff08\u6a21\u62df\u6587\u4ef6\u64cd\u4f5c\uff09\n            if(xSemaphoreTake(xFileMutex, 100 / portTICK_PERIOD_MS) == pdTRUE) {\n                // \u6a21\u62df\u6587\u4ef6\u5199\u5165\u64cd\u4f5c\n                printf(\"Log[%lu]: Writing data to file...\\n\", ++log_count);\n                vTaskDelay(100 / portTICK_PERIOD_MS);  // \u6a21\u62df\u5199\u5165\u65f6\u95f4\n                printf(\"Log[%lu]: Data written successfully\\n\", log_count);\n\n                xSemaphoreGive(xFileMutex);\n            } else {\n                printf(\"Logger: File busy, skipping log entry\\n\");\n            }\n        }\n\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u7cfb\u7edf\u76d1\u63a7\u4efb\u52a1\nvoid vSystemMonitorTask(void *pvParameters) {\n    printf(\"System Monitor Task started\\n\");\n\n    while(1) {\n        // \u68c0\u67e5\u5404\u4e2a\u4e92\u65a5\u91cf\u72b6\u6001\n        UBaseType_t data_status = uxSemaphoreGetCount(xDataMutex);\n        UBaseType_t display_status = uxSemaphoreGetCount(xDisplayMutex);\n        UBaseType_t file_status = uxSemaphoreGetCount(xFileMutex);\n\n        printf(\"\\n--- System Monitor ---\\n\");\n        printf(\"Data Mutex:    %s\\n\", data_status ? \"Available\" : \"In Use\");\n        printf(\"Display Mutex: %s\\n\", display_status ? \"Available\" : \"In Use\");\n        printf(\"File Mutex:    %s\\n\", file_status ? \"Available\" : \"In Use\");\n        printf(\"Shared Data Update Count: %lu\\n\", xSharedData.update_count);\n\n        vTaskDelay(10000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u9012\u5f52\u4e92\u65a5\u91cf\u6f14\u793a\u4efb\u52a1\nvoid vRecursiveMutexDemoTask(void *pvParameters) {\n    SemaphoreHandle_t xRecursiveMutex = xSemaphoreCreateRecursiveMutex();\n\n    printf(\"Recursive Mutex Demo Task started\\n\");\n\n    while(1) {\n        // \u7b2c\u4e00\u6b21\u83b7\u53d6\u9012\u5f52\u4e92\u65a5\u91cf\n        if(xSemaphoreTakeRecursive(xRecursiveMutex, portMAX_DELAY) == pdTRUE) {\n            printf(\"Recursive: First take\\n\");\n\n            // \u7b2c\u4e8c\u6b21\u83b7\u53d6\uff08\u540c\u4e00\u4e2a\u4efb\u52a1\uff09\n            if(xSemaphoreTakeRecursive(xRecursiveMutex, portMAX_DELAY) == pdTRUE) {\n                printf(\"Recursive: Second take\\n\");\n\n                // \u6267\u884c\u4e00\u4e9b\u64cd\u4f5c\n                vTaskDelay(100 / portTICK_PERIOD_MS);\n\n                // \u7b2c\u4e00\u6b21\u91ca\u653e\n                xSemaphoreGiveRecursive(xRecursiveMutex);\n                printf(\"Recursive: First give\\n\");\n            }\n\n            // \u7b2c\u4e8c\u6b21\u91ca\u653e\n            xSemaphoreGiveRecursive(xRecursiveMutex);\n            printf(\"Recursive: Second give - fully released\\n\");\n        }\n\n        vTaskDelay(8000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u4f18\u5148\u7ea7\u53cd\u8f6c\u6f14\u793a\u4efb\u52a1\nvoid vPriorityInversionDemoTask(void *pvParameters) {\n    int task_id = (int)pvParameters;\n\n    printf(\"Priority Demo Task %d started\\n\", task_id);\n\n    while(1) {\n        if(task_id == 1) {  // \u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\n            printf(\"LowPriority: Trying to get mutex\\n\");\n            if(xSemaphoreTake(xDataMutex, portMAX_DELAY) == pdTRUE) {\n                printf(\"LowPriority: Got mutex, working...\\n\");\n                vTaskDelay(3000 / portTICK_PERIOD_MS);  // \u957f\u65f6\u95f4\u5360\u7528\n                printf(\"LowPriority: Releasing mutex\\n\");\n                xSemaphoreGive(xDataMutex);\n            }\n            vTaskDelay(10000 / portTICK_PERIOD_MS);\n\n        } else if(task_id == 3) {  // \u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\n            vTaskDelay(1000 / portTICK_PERIOD_MS);  // \u8ba9\u4f4e\u4f18\u5148\u7ea7\u5148\u8fd0\u884c\n            printf(\"HighPriority: Trying to get mutex\\n\");\n            TickType_t start_time = xTaskGetTickCount();\n\n            if(xSemaphoreTake(xDataMutex, portMAX_DELAY) == pdTRUE) {\n                TickType_t end_time = xTaskGetTickCount();\n                printf(\"HighPriority: Got mutex after %lu ms\\n\", \n                       (end_time - start_time) * portTICK_PERIOD_MS);\n                xSemaphoreGive(xDataMutex);\n            }\n            vTaskDelay(15000 / portTICK_PERIOD_MS);\n        }\n    }\n}\n\n// \u7cfb\u7edf\u521d\u59cb\u5316\nvoid vInitMutexSystem(void) {\n    // \u521b\u5efa\u6240\u6709\u4e92\u65a5\u91cf\n    xDataMutex = xSemaphoreCreateMutex();\n    xDisplayMutex = xSemaphoreCreateMutex();\n    xFileMutex = xSemaphoreCreateMutex();\n\n    if(xDataMutex == NULL || xDisplayMutex == NULL || xFileMutex == NULL) {\n        printf(\"ERROR: Mutex creation failed!\\n\");\n        while(1);\n    }\n\n    printf(\"All mutexes created successfully\\n\");\n}\n\n// \u4e3b\u51fd\u6570\nint main(void) {\n    printf(\"Starting Mutex Demonstration System...\\n\");\n\n    // \u521d\u59cb\u5316\u4e92\u65a5\u91cf\u7cfb\u7edf\n    vInitMutexSystem();\n\n    // \u521b\u5efa\u4f20\u611f\u5668\u4efb\u52a1\uff083\u4e2a\uff09\n    xTaskCreate(vSensorTask, \"Sensor1\", 1024, (void*)1, 1, NULL);\n    xTaskCreate(vSensorTask, \"Sensor2\", 1024, (void*)2, 1, NULL);\n    xTaskCreate(vSensorTask, \"Sensor3\", 1024, (void*)3, 1, NULL);\n\n    // \u521b\u5efa\u5904\u7406\u4efb\u52a1\n    xTaskCreate(vDisplayTask, \"Display\", 1024, NULL, 2, NULL);\n    xTaskCreate(vDataLoggerTask, \"Logger\", 1024, NULL, 2, NULL);\n    xTaskCreate(vSystemMonitorTask, \"Monitor\", 1024, NULL, 1, NULL);\n    xTaskCreate(vRecursiveMutexDemoTask, \"Recursive\", 1024, NULL, 1, NULL);\n\n    // \u521b\u5efa\u4f18\u5148\u7ea7\u6f14\u793a\u4efb\u52a1\n    xTaskCreate(vPriorityInversionDemoTask, \"LowPriority\", 1024, (void*)1, 1, NULL);\n    xTaskCreate(vPriorityInversionDemoTask, \"MidPriority\", 1024, (void*)2, 2, NULL);\n    xTaskCreate(vPriorityInversionDemoTask, \"HighPriority\", 1024, (void*)3, 3, NULL);\n\n    // \u542f\u52a8\u8c03\u5ea6\u5668\n    printf(\"Starting FreeRTOS scheduler...\\n\");\n    vTaskStartScheduler();\n\n    return 0;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_14","title":"\u4e92\u65a5\u91cf\u4f7f\u7528\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_15","title":"\u8bbe\u8ba1\u539f\u5219","text":"<p>\u4fdd\u6301\u4e34\u754c\u533a\u7b80\u77ed\uff1a</p> <pre><code>// \u597d\u7684\u505a\u6cd5\uff1a\u53ea\u4fdd\u62a4\u5171\u4eab\u6570\u636e\u8bbf\u95ee\nif(xSemaphoreTake(xMutex, timeout) == pdTRUE) {\n    // \u5feb\u901f\u64cd\u4f5c\uff1a\u53ea\u62f7\u8d1d\u6570\u636e\u6216\u66f4\u65b0\u53d8\u91cf\n    shared_variable = new_value;\n    xSemaphoreGive(xMutex);\n}\n// \u8017\u65f6\u64cd\u4f5c\u653e\u5728\u4e34\u754c\u533a\u5916\nprocess_data(shared_variable);\n\n// \u4e0d\u597d\u505a\u6cd5\uff1a\u5728\u4e34\u754c\u533a\u5185\u6267\u884c\u8017\u65f6\u64cd\u4f5c\nif(xSemaphoreTake(xMutex, timeout) == pdTRUE) {\n    // \u8fd9\u4f1a\u957f\u65f6\u95f4\u963b\u585e\u5176\u4ed6\u4efb\u52a1\n    long_processing_operation();\n    xSemaphoreGive(xMutex);\n}\n</code></pre> <p>\u907f\u514d\u5d4c\u5957\u6b7b\u9501\uff1a</p> <pre><code>// \u5371\u9669\uff1a\u53ef\u80fd\u4ea7\u751f\u6b7b\u9501\nvoid vFunctionA(void) {\n    xSemaphoreTake(xMutex1, portMAX_DELAY);\n    xSemaphoreTake(xMutex2, portMAX_DELAY);  // \u5982\u679c\u5176\u4ed6\u4efb\u52a1\u4ee5\u76f8\u53cd\u987a\u5e8f\u83b7\u53d6\uff0c\u4f1a\u4ea7\u751f\u6b7b\u9501\n    // ...\n    xSemaphoreGive(xMutex2);\n    xSemaphoreGive(xMutex1);\n}\n\n// \u5b89\u5168\uff1a\u56fa\u5b9a\u83b7\u53d6\u987a\u5e8f\nvoid vFunctionA(void) {\n    xSemaphoreTake(xMutex1, portMAX_DELAY);\n    xSemaphoreTake(xMutex2, portMAX_DELAY);\n    // ...\n    xSemaphoreGive(xMutex2);\n    xSemaphoreGive(xMutex1);\n}\n\nvoid vFunctionB(void) {\n    xSemaphoreTake(xMutex1, portMAX_DELAY);  // \u540c\u6837\u987a\u5e8f\n    xSemaphoreTake(xMutex2, portMAX_DELAY);\n    // ...\n    xSemaphoreGive(xMutex2);\n    xSemaphoreGive(xMutex1);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Mutex/#_16","title":"\u9519\u8bef\u5904\u7406\u7b56\u7565","text":"<p>\u8d85\u65f6\u5904\u7406\uff1a</p> <pre><code>BaseType_t xResult = xSemaphoreTake(xMutex, reasonable_timeout);\nif(xResult != pdTRUE) {\n    // \u5904\u7406\u7b56\u7565\uff1a\n    // 1. \u4f7f\u7528\u9ed8\u8ba4\u503c\u7ee7\u7eed\u6267\u884c\n    // 2. \u8df3\u8fc7\u672c\u6b21\u64cd\u4f5c\n    // 3. \u5c1d\u8bd5\u6062\u590d\u6216\u91cd\u7f6e\u7cfb\u7edf\n    handle_mutex_timeout();\n    return;\n}\n</code></pre> <p>\u8d44\u6e90\u6e05\u7406\uff1a</p> <pre><code>void vCriticalOperation(void) {\n    if(xSemaphoreTake(xMutex, timeout) == pdTRUE) {\n        // \u786e\u4fdd\u5728\u4efb\u4f55\u9000\u51fa\u8def\u5f84\u90fd\u91ca\u653e\u4e92\u65a5\u91cf\n        if(operation_failed) {\n            xSemaphoreGive(xMutex);\n            return;\n        }\n\n        // \u6b63\u5e38\u64cd\u4f5c...\n        xSemaphoreGive(xMutex);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/","title":"\u4e0b\u8f7d\u5b98\u65b9\u6587\u4ef6","text":"<p>FreeRTOS</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_2","title":"\u6587\u4ef6\u8bf4\u660e","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_3","title":"\u5185\u6838\u6587\u4ef6\u5939\uff08\u91cd\u70b9\u5173\u6ce8\uff09","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_4","title":"\u6838\u5fc3\u4ee3\u7801","text":"<ol> <li>croutine.c\uff1a\u534f\u7a0b\u76f8\u5173\u6587\u4ef6\uff08\u4e0d\u7528\uff09</li> <li>event_groups.c\uff1a\u4e8b\u4ef6\u76f8\u5173\uff08\u6b21\u8981\uff09</li> <li>list.c\uff1a\u5217\u8868\uff08\u91cd\u8981\uff09</li> <li>queue.c\uff1a\u961f\u5217\uff08\u91cd\u8981\uff09</li> <li>stream_buffer.c\uff1a\u6d41\u5f0f\u7f13\u51b2\u533a\uff08\u6b21\u8981\uff09</li> <li>tasks.c\uff1a\u4efb\u52a1\u6587\u4ef6(\u91cd\u8981)</li> <li>timers.c\uff1a\u5b9a\u65f6\u5668\u76f8\u5173\uff08\u6b21\u8981\uff09</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#portable","title":"portable\u6587\u4ef6\u5939","text":"<p>\u7528\u4e8e\u79fb\u690d</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#memmang","title":"MemMang(\u5fc5\u987b)","text":"<p>\u5185\u5b58\u7ba1\u7406\uff0c\u4e00\u822c\u9009\u62e9\u7248\u672c4</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#keilgcc","title":"Keil\u6216GCC\uff08\u5fc5\u987b\uff09","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#rvdskeil","title":"RVDS(Keil\u5fc5\u987b)","text":"<p>Keil\u6216GCC\u7684\u5e95\u5c42\uff0c\u4e0d\u540c\u5185\u6838\u82af\u7247\u7684\u79fb\u690d\u6587\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_5","title":"\u79fb\u690d\u6b65\u9aa4","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#cubemx","title":"CubeMx\u4e00\u952e\u914d\u7f6e","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_6","title":"\u63a5\u53e3","text":"<p> \u9009\u62e9\u7248\u672cV2\uff0c\u66f4\u597d\u3002 - \u9009\u62e9 FreeRTOS \u540e\uff0c\u53f3\u4fa7\u4f1a\u591a\u51fa\u4e00\u4e2a\u00a0<code>FREERTOS</code>\u00a0\u7684\u914d\u7f6e\u9875\u7b7e\u3002</p> <ul> <li> <p>\u5728\u8fd9\u91cc\uff0c\u4f60\u53ef\u4ee5\u5b8c\u5168\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u914d\u7f6e\u6240\u6709 FreeRTOS \u53c2\u6570\uff0c\u65e0\u9700\u624b\u52a8\u4fee\u6539\u00a0<code>FreeRTOSConfig.h</code>\u00a0\u6587\u4ef6\uff1a</p> <ul> <li> <p>\u4efb\u52a1\u548c\u961f\u5217\uff1a\u8bbe\u7f6e\u5185\u6838\u5bf9\u8c61\u6570\u91cf\uff0c\u5982\u00a0<code>configTOTAL_HEAP_SIZE</code>\uff08\u975e\u5e38\u91cd\u8981\uff01\uff09\u3002</p> </li> <li> <p>\u94a9\u5b50\u51fd\u6570\uff1a\u4f7f\u80fd\u6216\u7981\u6b62\u7a7a\u95f2\u4efb\u52a1\u94a9\u5b50\u3001\u5b9a\u65f6\u5668\u94a9\u5b50\u7b49\u3002</p> </li> <li> <p>\u5185\u5b58\u7ba1\u7406\uff1a\u9009\u62e9\u5806\u5206\u914d\u65b9\u6848\uff08Heap 1~5\uff09\u3002</p> </li> <li> <p>Include parameters\uff1a\u914d\u7f6e\u00a0<code>configUSE_PREEMPTION</code>\uff08\u662f\u5426\u4f7f\u7528\u62a2\u5360\u5f0f\u8c03\u5ea6\uff09\u3001<code>configCPU_CLOCK_HZ</code>\uff08CPU \u9891\u7387\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u8bbe\u7f6e\u597d\uff09\u3001<code>configTICK_RATE_HZ</code>\uff08\u7cfb\u7edf\u65f6\u949f\u8282\u62cd\u9891\u7387\uff0c\u901a\u5e38\u8bbe\u4e3a 1000Hz\uff0c\u5373 1ms \u4e00\u4e2a\u8282\u62cd\uff09\u3002</p> </li> </ul> </li> <li> <p>\u9009\u62e9 FreeRTOS \u540e\uff0c\u53f3\u4fa7\u4f1a\u591a\u51fa\u4e00\u4e2a\u00a0<code>FREERTOS</code>\u00a0\u7684\u914d\u7f6e\u9875\u7b7e\u3002\u5728\u8fd9\u91cc\uff0c\u4f60\u53ef\u4ee5\u5b8c\u5168\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u914d\u7f6e\u6240\u6709 FreeRTOS \u53c2\u6570\uff0c\u65e0\u9700\u624b\u52a8\u4fee\u6539\u00a0<code>FreeRTOSConfig.h</code>\u00a0\u6587\u4ef6\u3002</p> </li> </ul> <p>#### preemptive scheduler</p> <p>\u914d\u7f6e\u00a0<code>configUSE_PREEMPTION</code>\uff08\u662f\u5426\u4f7f\u7528\u62a2\u5360\u5f0f\u8c03\u5ea6\uff09\u3001<code>configCPU_CLOCK_HZ</code>\uff08CPU \u9891\u7387\uff0c\u901a\u5e38\u4f1a\u81ea\u52a8\u8bbe\u7f6e\u597d\uff09\u3001<code>configTICK_RATE_HZ</code>\uff08\u7cfb\u7edf\u65f6\u949f\u8282\u62cd\u9891\u7387\uff0c\u901a\u5e38\u8bbe\u4e3a 1000Hz\uff0c\u5373 1ms \u4e00\u4e2a\u8282\u62cd\uff09\u3002</p> <p>#### \u4efb\u52a1\u548c\u961f\u5217</p> <p>\u8bbe\u7f6e\u5185\u6838\u5bf9\u8c61\u6570\u91cf\uff0c\u5982\u00a0<code>configTOTAL_HEAP_SIZE</code>\uff08\u975e\u5e38\u91cd\u8981\uff01\uff09\u3002</p> <p>#### \u5185\u5b58\u7ba1\u7406</p> <p>\u9009\u62e9\u5806\u5206\u914d\u65b9\u6848\uff08Heap 1~5\uff09\u3002</p> <p>#### USE_SB_COMPLETED_CALLBACK</p> <p>\"SB\" \u4ee3\u8868 \"Space Block\"\u3002\u8fd9\u4e2a\u9009\u9879\u7528\u4e8e\u542f\u7528\u6216\u7981\u7528\u00a0\"\u534a\u4f20\u8f93\u5b8c\u6210\u56de\u8c03\u51fd\u6570\"\u3002</p> <p>#### USE_MINI_LIST_ITEM <p>\u662f\u5426\u542f\u7528\u94fe\u8868\u4f18\u5316</p> <p>FreeRTOS \u5185\u6838\u5927\u91cf\u4f7f\u7528\u94fe\u8868\u6765\u7ba1\u7406\u5176\u5185\u6838\u5bf9\u8c61\uff0c\u4f8b\u5982\uff1a</p> <ul> <li>\u5c31\u7eea\u5217\u8868\uff1a\u7ba1\u7406\u6240\u6709\u51c6\u5907\u8fd0\u884c\u7684\u4efb\u52a1\u3002</li> <li>\u963b\u585e\u5217\u8868\uff1a\u7ba1\u7406\u6240\u6709\u6b63\u5728\u5ef6\u8fdf\u6216\u7b49\u5f85\u4e8b\u4ef6\uff08\u5982\u4fe1\u53f7\u91cf\u3001\u961f\u5217\uff09\u7684\u4efb\u52a1\u3002</li> <li>\u6302\u8d77\u5217\u8868\uff1a\u7ba1\u7406\u88ab\u6302\u8d77\u7684\u4efb\u52a1\u3002</li> </ul> <p>\u8fd9\u4e9b\u94fe\u8868\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u662f\u4e00\u4e2a <code>ListItem_t</code> \u7ed3\u6784\u4f53\u3002\u6bcf\u5f53\u4e00\u4e2a\u4efb\u52a1\u88ab\u521b\u5efa\u65f6\uff0c\u5b83\u90fd\u4f1a\u5305\u542b\u591a\u4e2a <code>ListItem_t</code> \u6210\u5458\uff08\u4f8b\u5982 <code>xStateListItem</code> \u7528\u4e8e\u653e\u5165\u5c31\u5e8f/\u963b\u585e/\u6302\u8d77\u5217\u8868\uff0c<code>xEventListItem</code> \u7528\u4e8e\u4e8b\u4ef6\u5217\u8868\uff09\u3002</p> <p>##### \u6807\u51c6\u7684 <code>ListItem_t</code></p> <p>\u4e00\u4e2a\u6807\u51c6\u7684 <code>ListItem_t</code> \u7ed3\u6784\u4f53\uff08\u5728 <code>list.h</code> \u4e2d\u5b9a\u4e49\uff09\u901a\u5e38\u5305\u542b\u4ee5\u4e0b\u6210\u5458\uff1a</p> <p><code>c   // \u6807\u51c6\u7684\u5217\u8868\u9879\u7ed3\u6784   struct xLIST_ITEM {       TickType_t xItemValue;               // \u4e3b\u8981\u503c\uff0c\u7528\u4e8e\u5728\u5217\u8868\u4e2d\u6392\u5e8f\uff08\u4f8b\u5982\uff0c\u5524\u9192\u65f6\u95f4\u6233\uff09       struct xLIST_ITEM * pxNext;          // \u6307\u5411\u94fe\u8868\u4e2d\u4e0b\u4e00\u4e2a\u5217\u8868\u9879\u7684\u6307\u9488       struct xLIST_ITEM * pxPrevious;      // \u6307\u5411\u94fe\u8868\u4e2d\u4e0a\u4e00\u4e2a\u5217\u8868\u9879\u7684\u6307\u9488       void * pvOwner;                      // \u6307\u5411\u62e5\u6709\u8fd9\u4e2a\u5217\u8868\u9879\u7684\u5bf9\u8c61\uff08\u901a\u5e38\u662f\u4efb\u52a1\u63a7\u5236\u5757 TCB\uff09       void * pvContainer;                  // \u6307\u5411\u8fd9\u4e2a\u5217\u8868\u9879\u6240\u5c5e\u7684\u5217\u8868   };   // \u5728 32 \u4f4d\u67b6\u6784\u4e0a\uff0c\u8fd9\u901a\u5e38\u662f 20 \u5b57\u8282\u3002</code></p> <p>##### <code>USE_MINI_LIST_ITEM</code> \u662f\u4ec0\u4e48\uff1f</p> <p><code>USE_MINI_LIST_ITEM</code> \u662f\u4e00\u4e2a\u5728 <code>FreeRTOSConfig.h</code> \u4e2d\u5b9a\u4e49\u7684\u7f16\u8bd1\u65f6\u914d\u7f6e\u9009\u9879\u3002</p> <ul> <li><code>#define USE_MINI_LIST_ITEM 0</code> \uff08\u9ed8\u8ba4\uff09\uff1a\u4f7f\u7528\u4e0a\u9762\u63cf\u8ff0\u7684\u6807\u51c6 <code>ListItem_t</code> \u7ed3\u6784\u4f53\u3002</li> <li><code>#define USE_MINI_LIST_ITEM 1</code>\uff1a\u4f7f\u7528\u4e00\u4e2a\u7cbe\u7b80\u7248\u7684 <code>ListItem_t</code> \u7ed3\u6784\u4f53\uff0c\u901a\u5e38\u88ab\u79f0\u4e3a <code>MiniListItem_t</code>\u3002</li> </ul> <p>##### \u7cbe\u7b80\u7248\u5217\u8868\u9879</p> <p><code>MiniListItem_t</code> \u79fb\u9664\u4e86\u67d0\u4e9b\u6210\u5458\uff0c\u4ee5\u8282\u7701\u5185\u5b58\u3002\u5b83\u901a\u5e38\u53ea\u5305\u542b\uff1a</p> <p><code>c   // \u7cbe\u7b80\u7248\u5217\u8868\u9879\u7ed3\u6784   struct xMINI_LIST_ITEM {       TickType_t xItemValue;               // \u4fdd\u7559\uff1a\u7528\u4e8e\u6392\u5e8f\u7684\u4e3b\u8981\u503c       struct xLIST_ITEM * pxNext;          // \u4fdd\u7559\uff1a\u6307\u5411\u4e0b\u4e00\u4e2a\u5217\u8868\u9879       struct xLIST_ITEM * pxPrevious;      // \u4fdd\u7559\uff1a\u6307\u5411\u4e0a\u4e00\u4e2a\u5217\u8868\u9879   };   // \u5728 32 \u4f4d\u67b6\u6784\u4e0a\uff0c\u8fd9\u901a\u5e38\u662f 12 \u5b57\u8282\uff08\u6bd4\u6807\u51c6\u7248\u8282\u7701\u4e86 8 \u5b57\u8282\uff09\u3002</code>   \u4f60\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\uff0c\u5b83\u79fb\u9664\u4e86\uff1a</p> <ul> <li><code>pvOwner</code>\uff1a\u6307\u5411\u62e5\u6709\u8005\u7684\u6307\u9488\u3002</li> <li><code>pvContainer</code>\uff1a\u6307\u5411\u6240\u5c5e\u5217\u8868\u7684\u6307\u9488\u3002</li> </ul> <p>##### \u7528\u9014\u548c\u9650\u5236</p> <p>\u4e3a\u4ec0\u4e48\u9700\u8981\u7cbe\u7b80\u7248\u5217\u8868\u9879\uff1f</p> <p>\u7cbe\u7b80\u7248\u5217\u8868\u9879\u5e76\u975e\u7528\u4e8e\u4efb\u52a1\uff0c\u800c\u662f\u4e13\u95e8\u7528\u4e8e\u5217\u8868\u7684\u201c\u672b\u5c3e\u201d\u6216\u201c\u7ed3\u675f\u201d\u6807\u8bb0\u3002</p> <p>\u5728 FreeRTOS \u7684\u94fe\u8868\u5b9e\u73b0\u4e2d\uff0c\u6bcf\u4e2a\u5217\u8868\u90fd\u6709\u4e00\u4e2a\u201c\u5217\u8868\u5c3e\u201d \u6216 \u201c\u7ed3\u675f\u6807\u8bb0\u201d\u3002\u8fd9\u4e2a\u6807\u8bb0\u672c\u8eab\u4e5f\u662f\u4e00\u4e2a <code>ListItem_t</code>\uff0c\u4f46\u5b83\u4e0d\u5f52\u5c5e\u4e8e\u4efb\u4f55\u4efb\u52a1\u6216\u5185\u6838\u5bf9\u8c61\u3002\u5b83\u7684\u552f\u4e00\u4f5c\u7528\u662f\uff1a</p> <ul> <li>\u8868\u793a\u5217\u8868\u7684\u7ed3\u675f\u3002</li> <li>\u5176 <code>xItemValue</code> \u88ab\u8bbe\u7f6e\u4e3a\u53ef\u80fd\u7684\u6700\u5927\u503c\uff08<code>portMAX_DELAY</code>\uff09\uff0c\u4ee5\u786e\u4fdd\u5b83\u5728\u6309\u503c\u6392\u5e8f\u7684\u5217\u8868\u4e2d\u59cb\u7ec8\u5904\u4e8e\u672b\u5c3e\u4f4d\u7f6e\u3002</li> </ul> <p>\u56e0\u4e3a\u8fd9\u4e2a\u201c\u7ed3\u675f\u6807\u8bb0\u201d\u5217\u8868\u9879\u6ca1\u6709\u201c\u62e5\u6709\u8005\u201d\uff0c\u4e5f\u4e0d\u5173\u5fc3\u81ea\u5df1\u5c5e\u4e8e\u54ea\u4e2a\u201c\u5bb9\u5668\u201d\uff08\u5b83\u672c\u8eab\u5c31\u662f\u5bb9\u5668\u7684\u4e00\u90e8\u5206\uff09\uff0c\u6240\u4ee5\u4f7f\u7528\u5b8c\u6574\u7684 <code>ListItem_t</code> \u662f\u5bf9\u5185\u5b58\u7684\u6d6a\u8d39\u3002\u56e0\u6b64\uff0cFreeRTOS \u4f7f\u7528 <code>MiniListItem_t</code> \u6765\u521b\u5efa\u8fd9\u4e2a\u7ed3\u675f\u6807\u8bb0\uff0c\u4ece\u800c\u8282\u7701\u5b9d\u8d35\u7684\u5185\u5b58\u3002</p> <p>#### MINIMAL_STACK_SIZE <code>MINIMAL_STACK_SIZE</code>\u00a0\u662f FreeRTOS \u4e2d\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u4e14\u57fa\u7840\u7684\u914d\u7f6e\u9009\u9879\u3002\u5b83\u5b9a\u4e49\u4e86\u7a7a\u95f2\u4efb\u52a1\u548c\uff08\u5728\u67d0\u4e9b\u914d\u7f6e\u4e0b\uff09\u5b9a\u65f6\u5668\u670d\u52a1\u4efb\u52a1\u6240\u4f7f\u7528\u7684\u6700\u5c0f\u6808\u6df1\u5ea6\u3002\u4f5c\u4e3a\u65b0\u624b\uff0c\u6211\u4eec\u53ea\u4e86\u89e3\u5b83\u7ecf\u9a8c\u503c\u4f4d128\u5b57\u3002</p> <p>#### IDLE_SHOULD_YIELD   \u5f53\u6709\u5176\u4ed6\u7528\u6237\u4efb\u52a1\u4e0e\u7a7a\u95f2\u4efb\u52a1\u5904\u4e8e\u540c\u4e00\u4f18\u5148\u7ea7\uff08\u5373\u4f18\u5148\u7ea7 0\uff09\u00a0\u65f6\uff0c\u7a7a\u95f2\u4efb\u52a1\u5728\u5b83\u7684\u65f6\u95f4\u7247\u7ed3\u675f\u540e\u662f\u5426\u5e94\u8be5\u7acb\u5373\u8ba9\u51fa\u00a0CPU\u3002</p> <p>#### ENABLE_BACKWARD_COMPATIBILITY </p> <p>\u662f\u5426\u542f\u7528\u5411\u540e\u7248\u672c\u517c\u5bb9\uff0c\u542f\u7528\u5219\u65e7\u7248\u672c\u7684\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u4f7f\u7528\u3002</p> <p>#### USE_TICKLESS_IDLE</p> <p>tickless\uff0c\u901a\u5e38\u88ab\u79f0\u4e3a \"\u65e0\u6ef4\u7b54\u7a7a\u95f2\u6a21\u5f0f\" \u6216 \"\u4f4e\u529f\u8017\u6ef4\u7b54\u6a21\u5f0f\"\u3002\u65b0\u624b\u4e0d\u9700\u8981\u542f\u7528\u8fd9\u4e2a\u3002</p> <p>#### USE_TASK_NOTIFICATIONS</p> <p>\u662f\u5426\u542f\u7528 \u4efb\u52a1\u901a\u77e5 \u529f\u80fd\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e2a\u9009\u9879\u662f\u542f\u7528\u7684\u3002 \u9664\u975e\u4f60\u51fa\u4e8e\u6781\u7279\u6b8a\u7684\u539f\u56e0\uff08\u6bd4\u5982\u8981\u8282\u7701 TCB \u4e2d\u90a3\u51e0\u4e2a\u5b57\u8282\u7684\u5185\u5b58\uff09\uff0c\u5426\u5219\u6c38\u8fdc\u4e0d\u8981\u7981\u7528\u5b83\u3002</p> <p>#### RECORD_STACK_HIGH_ADDRESS </p> <p>\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6808\u5206\u6790\u4fe1\u606f\u3002\u5148\u4e0d\u7ba1\u3002</p> <p>### Hook Function  \u94a9\u5b50\u51fd\u6570</p> <p>\u4f7f\u80fd\u6216\u7981\u6b62\u7a7a\u95f2\u4efb\u52a1\u94a9\u5b50\u3001\u5b9a\u65f6\u5668\u94a9\u5b50\u7b49\u3002</p> <p>\u94a9\u5b50\u51fd\u6570 \u662f FreeRTOS \u4e2d\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u7279\u6027\uff0c\u5b83\u5141\u8bb8\u4f60\u5728\u5185\u6838\u7684\u5173\u952e\u6267\u884c\u70b9\u63d2\u5165\u81ea\u5df1\u7684\u56de\u8c03\u51fd\u6570\u3002\u8fd9\u4e3a\u8c03\u8bd5\u3001\u6027\u80fd\u5206\u6790\u3001\u4f4e\u529f\u8017\u7ba1\u7406\u548c\u529f\u80fd\u6269\u5c55\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u7075\u6d3b\u6027\u3002</p> <p><code>c   configUSE_IDLE_HOOK //\u662f\u5426\u542f\u7528\u7a7a\u95f2\u4efb\u52a1\u94a9\u5b50\u51fd\u6570\u3002\u8fd9\u662f\u4e00\u4e2a\u7531\u7528\u6237\u5b9e\u73b0\u7684\u56de\u8c03\u51fd\u6570\uff0c\u5f53 FreeRTOS \u7684\u7a7a\u95f2\u4efb\u52a1\u8fd0\u884c\u65f6\uff0c\u8fd9\u4e2a\u51fd\u6570\u4f1a\u88ab\u81ea\u52a8\u4e14\u53cd\u590d\u5730\u8c03\u7528\u3002   configUSE_TICK_HOOK //\u9ad8\u7ea7\u529f\u80fd,\u5728 SysTick \u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f \u5185\u90e8\u88ab\u8c03\u7528\u3002   ...</code></p> <p>### Run time and task stats gathering  \u8fd0\u884c\u65f6\u548c\u4efb\u52a1\u7edf\u8ba1\u4fe1\u606f\u6536\u96c6</p> <p>#### 1. \u542f\u7528\u4efb\u52a1\u7edf\u8ba1 <code>configUSE_TRACE_FACILITY</code></p> <p>\u8fd9\u662f\u6700\u57fa\u7840\u7684\u914d\u7f6e\uff0c\u5b83\u4e3a\u4efb\u52a1\u63a7\u5236\u5757\uff08TCB\uff09\u6dfb\u52a0\u4e86\u7edf\u8ba1\u4fe1\u606f\u6240\u9700\u7684\u5b57\u6bb5\u3002</p> <p><code>c   #define configUSE_TRACE_FACILITY 1</code></p> <p>\u4f5c\u7528\uff1a</p> <ul> <li>\u5728 TCB \u4e2d\u6dfb\u52a0\u4e86 <code>ulRunTimeCounter</code> \u5b57\u6bb5\uff0c\u7528\u4e8e\u8bb0\u5f55\u4efb\u52a1\u7684\u7d2f\u8ba1\u8fd0\u884c\u65f6\u95f4\u3002</li> <li>\u4e3a\u8c03\u8bd5\u5668\u53ef\u89c6\u5316\u63d0\u4f9b\u66f4\u591a\u4efb\u52a1\u72b6\u6001\u4fe1\u606f\u3002</li> </ul> <p>#### 2. \u542f\u7528\u8fd0\u884c\u65f6\u7edf\u8ba1 <code>configGENERATE_RUN_TIME_STATS</code></p> <p>\u8fd9\u4e2a\u5b8f\u63a7\u5236\u662f\u5426\u8ba1\u7b97\u6bcf\u4e2a\u4efb\u52a1\u7684 CPU \u4f7f\u7528\u7387\u3002</p> <p><code>c   #define configGENERATE_RUN_TIME_STATS 1</code></p> <p>\u4f5c\u7528\uff1a</p> <ul> <li>\u542f\u7528\u4efb\u52a1\u8fd0\u884c\u65f6\u7edf\u8ba1\u8ba1\u7b97\u3002</li> <li>\u9700\u8981\u4f60\u5b9e\u73b0\u4e24\u4e2a\u5b8f\u6765\u63d0\u4f9b\u8ba1\u65f6\u529f\u80fd\u3002</li> </ul> <p>#### 3. \u542f\u7528\u4efb\u52a1\u72b6\u6001\u67e5\u8be2 <code>configUSE_STATS_FORMATTING_FUNCTIONS</code></p> <p>\u8fd9\u4e2a\u5b8f\u542f\u7528\u4e86\u4e00\u4e9b\u7528\u4e8e\u683c\u5f0f\u5316\u8f93\u51fa\u7edf\u8ba1\u4fe1\u606f\u7684\u5b9e\u7528\u51fd\u6570\u3002</p> <p><code>c   #define configUSE_STATS_FORMATTING_FUNCTIONS 1</code></p> <p>\u4f5c\u7528\uff1a</p> <ul> <li>\u542f\u7528\u4e86 <code>vTaskList()</code> \u548c <code>vTaskGetRunTimeStats()</code> \u7b49\u51fd\u6570\u3002</li> <li>\u8fd9\u4e9b\u51fd\u6570\u53ef\u4ee5\u5c06\u7edf\u8ba1\u4fe1\u606f\u683c\u5f0f\u5316\u4e3a\u53ef\u8bfb\u7684\u5b57\u7b26\u4e32\u3002</li> </ul> <p>### Software Timer</p> <p>\u8f6f\u4ef6\u5b9a\u65f6\u5668 \u662f FreeRTOS \u4e2d\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u529f\u80fd\uff0c\u5b83\u5141\u8bb8\u4f60\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u521b\u5efa\u548c\u7ba1\u7406\u591a\u4e2a\u865a\u62df\u5b9a\u65f6\u5668\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u786c\u4ef6\u5b9a\u65f6\u5668\u5916\u8bbe</p> <p>CubeMx\u7ed9\u6211\u4eec\u4f18\u5148\u7ea7\u662f2\uff0c\u53ea\u9ad8\u4e8e\u4f18\u5148\u7ea71\u548c\u7a7a\u95f2\u4efb\u52a1\uff08\u4f18\u5148\u7ea70\uff09\uff0c\u9002\u4e8e\u7b80\u5355\u5e94\u7528\u3002\u4e00\u822c\u8fd9\u4e2a\u4f18\u5148\u7ea7\u8981\u8c03\u9ad8\u4e00\u70b9\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_7","title":"\u534f\u5904\u7406\u5668","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_8","title":"\u5185\u5b58\u4fdd\u62a4","text":"<p>MPU \u662f\u4e00\u4e2a\u786c\u4ef6\u5355\u5143\uff0c\u5b83\u5141\u8bb8\u64cd\u4f5c\u7cfb\u7edf\uff08\u5982 FreeRTOS\uff09\u4e3a\u4e0d\u540c\u7684\u4efb\u52a1\u6216\u8fdb\u7a0b\u8bbe\u7f6e\u5185\u5b58\u8bbf\u95ee\u6743\u9650\u3002\u4f60\u53ef\u4ee5\u628a\u5b83\u60f3\u8c61\u6210\u4e00\u9897\u82af\u7247\u5185\u90e8\u7684\u201c\u5185\u5b58\u536b\u58eb\u201d\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_9","title":"\u6d6e\u70b9\u8fd0\u7b97","text":"<p>FPU \u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u6d6e\u70b9\u6570\uff08\u5c0f\u6570\uff09\u8ba1\u7b97\u7684\u786c\u4ef6\u5355\u5143\u3002\u6ca1\u6709 FPU \u7684 CPU \u53ea\u80fd\u901a\u8fc7\u8f6f\u4ef6\u5e93\u6765\u6a21\u62df\u6d6e\u70b9\u8fd0\u7b97\uff0c\u8fd9\u975e\u5e38\u7f13\u6162\u3002FPU \u5219\u50cf\u662f\u4e00\u4e2a\u6570\u5b66\u8ba1\u7b97\u5668\uff0c\u80fd\u76f4\u63a5\u5728\u786c\u4ef6\u4e0a\u9ad8\u901f\u6267\u884c\u00a0<code>+</code>,\u00a0<code>-</code>,\u00a0<code>*</code>,\u00a0<code>/</code>\u00a0\u751a\u81f3\u66f4\u590d\u6742\u7684\u4e09\u89d2\u51fd\u6570\u3001\u5bf9\u6570\u7b49\u8fd0\u7b97\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#kernal-settingsconfig","title":"Kernal Settings(Config)","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#_10","title":"\u624b\u52a8\u79fb\u690d","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#1-freertos","title":"1. \u6dfb\u52a0FreeRTOS\u6e90\u7801","text":"<p>\u5c06\u6e90\u7801\u6dfb\u52a0\u5165\u5de5\u7a0b\uff0c\u4e0d\u540cIDE\u4e0d\u4e00\u6837</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#2-freertosconfigh","title":"2. FreeRTOSConfig.h","text":"<p>\u914d\u7f6e\u6587\u4ef6</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#3-system","title":"3. \u4fee\u6539SYSTEM\u6587\u4ef6","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#4","title":"4. \u4fee\u6539\u4e2d\u65ad\u76f8\u5173\u6587\u4ef6","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Porting/#5","title":"5. \u6dfb\u52a0\u5e94\u7528\u7a0b\u5e8f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/","title":"Queue","text":"<ul> <li>\u961f\u5217\u7684\u57fa\u672c\u6982\u5ff5\u4e0e\u7279\u6027</li> <li>\u4f20\u8f93\u6570\u636e\u7684\u65b9\u6cd5</li> <li>\u961f\u5217\u7684\u963b\u585e\u8bbf\u95ee<ul> <li>\u5199\u961f\u5217</li> <li>\u4f18\u5148\u7ea7</li> <li>\u8bfb\u961f\u5217</li> </ul> </li> <li>\u961f\u5217\u57fa\u672c<ul> <li>QueueHandle_t</li> <li>Queue_t</li> <li>\u963b\u585e\u5524\u9192<ul> <li>\u6838\u5fc3\u601d\u60f3\uff1a\u786c\u4ef6\u5b9a\u65f6\u5668 + \u6392\u5e8f\u5217\u8868</li> <li>1. \u5173\u952e\u6570\u636e\u7ed3\u6784<ul> <li>a) \u4e24\u4e2a\u91cd\u8981\u7684\u5217\u8868</li> <li>b) \u6838\u5fc3\u53d8\u91cf</li> </ul> </li> <li>2. \u8d85\u65f6\u5524\u9192\u7684\u5b9e\u73b0\u6d41\u7a0b<ul> <li>\u9636\u6bb5\u4e00\uff1a\u4efb\u52a1\u8bbe\u7f6e\u963b\u585e\u5e76\u542f\u52a8\u8d85\u65f6\u5012\u8ba1\u65f6</li> <li>\u9636\u6bb5\u4e8c\uff1a\u7cfb\u7edf\u5fc3\u8df3\u4e2d\u65ad\u8fdb\u884c\u8d85\u65f6\u68c0\u67e5</li> </ul> </li> <li>3. \u8d85\u65f6\u4e0e\u4e8b\u4ef6\u540c\u65f6\u5230\u8fbe\u7684\u7ade\u4e89\u5904\u7406</li> <li>\u603b\u7ed3</li> </ul> </li> </ul> </li> <li>FreeRTOS<ul> <li>\u961f\u5217\u521b\u5efa\u51fd\u6570<ul> <li>xQueueCreate - \u52a8\u6001\u521b\u5efa\u961f\u5217</li> <li>xQueueCreateStatic - \u9759\u6001\u521b\u5efa\u961f\u5217</li> </ul> </li> <li>\u961f\u5217\u590d\u4f4d\u4e0e\u5220\u9664\u51fd\u6570<ul> <li>vQueueReset - \u961f\u5217\u590d\u4f4d</li> <li>vQueueDelete - \u961f\u5217\u5220\u9664</li> </ul> </li> <li>\u961f\u5217\u5199\u5165\u51fd\u6570<ul> <li>xQueueSend - \u540e\u7aef\u53d1\u9001\uff08\u6807\u51c6FIFO\uff09</li> <li>xQueueSendToFront - \u524d\u7aef\u53d1\u9001\uff08LIFO\uff09</li> <li>xQueueSendToBack - \u540e\u7aef\u53d1\u9001</li> <li>xQueueOverwrite - \u8986\u76d6\u5199\u5165</li> <li>\u4e2d\u65ad\u5b89\u5168\u5199\u5165\u51fd\u6570<ul> <li>xQueueSendFromISR</li> <li>xQueueSendToFrontFromISR</li> <li>xQueueOverwriteFromISR</li> </ul> </li> </ul> </li> <li>\u961f\u5217\u8bfb\u53d6\u51fd\u6570<ul> <li>xQueueReceive - \u63a5\u6536\u5e76\u79fb\u9664\u6570\u636e</li> <li>xQueuePeek - \u67e5\u770b\u4f46\u4e0d\u79fb\u9664\u6570\u636e</li> <li>\u4e2d\u65ad\u5b89\u5168\u8bfb\u53d6\u51fd\u6570<ul> <li>xQueueReceiveFromISR</li> <li>xQueuePeekFromISR</li> </ul> </li> </ul> </li> <li>\u961f\u5217\u67e5\u8be2\u51fd\u6570<ul> <li>uxQueueMessagesWaiting - \u67e5\u8be2\u6d88\u606f\u6570\u91cf</li> <li>uxQueueSpacesAvailable - \u67e5\u8be2\u5269\u4f59\u7a7a\u95f4</li> <li>\u4e2d\u65ad\u5b89\u5168\u67e5\u8be2\u51fd\u6570<ul> <li>uxQueueMessagesWaitingFromISR</li> </ul> </li> <li>\u67e5\u8be2\u51fd\u6570\u4f7f\u7528\u793a\u4f8b</li> </ul> </li> </ul> </li> <li>CMSISv2 API<ul> <li>osMessageQueueNew - \u521b\u5efa\u6d88\u606f\u961f\u5217</li> <li>osMessageQueuePut - \u53d1\u9001\u6d88\u606f</li> <li>osMessageQueueGet - \u63a5\u6536\u6d88\u606f</li> <li>\u5176\u4ed6\u961f\u5217\u7ba1\u7406\u51fd\u6570<ul> <li>osMessageQueueGetCount</li> <li>osMessageQueueGetSpace</li> <li>osMessageQueueDelete</li> <li>osMessageQueueReset</li> </ul> </li> </ul> </li> <li>\u961f\u5217\u4f7f\u7528\u6700\u4f73\u5b9e\u8df5\u603b\u7ed3<ul> <li>\u961f\u5217\u8bbe\u8ba1\u539f\u5219</li> <li>\u9519\u8bef\u5904\u7406\u7b56\u7565</li> <li>\u6027\u80fd\u4f18\u5316\u5efa\u8bae</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_1","title":"\u961f\u5217\u7684\u57fa\u672c\u6982\u5ff5\u4e0e\u7279\u6027","text":"<p>\u961f\u5217(queue)\u662fFreeRTOS\u4e2d\u6700\u57fa\u7840\u7684\u4efb\u52a1\u95f4\u901a\u4fe1\u673a\u5236\uff0c\u53ef\u4ee5\u7528\u4e8e\"\u4efb\u52a1\u5230\u4efb\u52a1\"\u3001\"\u4efb\u52a1\u5230\u4e2d\u65ad\"\u3001\"\u4e2d\u65ad\u5230\u4efb\u52a1\"\u76f4\u63a5\u4f20\u8f93\u4fe1\u606f\u3002\u5177\u6709\u4ee5\u4e0b\u6838\u5fc3\u7279\u6027\uff1a</p> <p>\u5148\u8fdb\u5148\u51fa(FIFO)\uff1a\u6570\u636e\u6309\u7167\u53d1\u9001\u7684\u987a\u5e8f\u4f9d\u6b21\u88ab\u63a5\u6536\uff0c\u4fdd\u8bc1\u6570\u636e\u987a\u5e8f\u6027</p> <p>\u7ebf\u7a0b\u5b89\u5168\uff1a\u591a\u4e2a\u4efb\u52a1\u53ef\u4ee5\u540c\u65f6\u5411\u961f\u5217\u8bfb\u5199\u6570\u636e\u800c\u4e0d\u4f1a\u4ea7\u751f\u6570\u636e\u7ade\u4e89</p> <p>\u6570\u636e\u62f7\u8d1d\uff1a\u961f\u5217\u5728\u4f20\u9012\u6570\u636e\u65f6\u8fdb\u884c\u5b8c\u6574\u7684\u6570\u636e\u590d\u5236\uff0c\u800c\u975e\u4f20\u9012\u6307\u9488</p> <p>\u963b\u585e\u673a\u5236\uff1a\u5f53\u961f\u5217\u7a7a\u65f6\u8bfb\u53d6\u4efb\u52a1\u53ef\u4ee5\u963b\u585e\u7b49\u5f85\uff0c\u961f\u5217\u6ee1\u65f6\u5199\u5165\u4efb\u52a1\u53ef\u4ee5\u963b\u585e\u7b49\u5f85</p> <p>\u591a\u4efb\u52a1\u540c\u6b65\uff1a\u5929\u7136\u652f\u6301\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u6a21\u5f0f\u7684\u4efb\u52a1\u540c\u6b65</p> <p>\u57fa\u7840</p> <ul> <li>\u961f\u5217\u53ef\u4ee5\u5305\u542b\u82e5\u5e72\u4e2a\u6570\u636e\uff1a\u961f\u5217\u4e2d\u6709\u82e5\u5e72\u9879\uff0c\u8fd9\u88ab\u79f0\u4e3a\"\u957f\u5ea6\"(length)</li> <li>\u6bcf\u4e2a\u6570\u636e\u5927\u5c0f\u56fa\u5b9a</li> <li>\u521b\u5efa\u961f\u5217\u65f6\u5c31\u8981\u6307\u5b9a\u957f\u5ea6\u3001\u6570\u636e\u5927\u5c0f</li> <li>\u6570\u636e\u7684\u64cd\u4f5c\u91c7\u7528\u5148\u8fdb\u5148\u51fa\u7684\u65b9\u6cd5(FIFO\uff0cFirst In First Out)\uff1a\u5199\u6570\u636e\u65f6\u653e\u5230\u5c3e\u90e8\uff0c\u8bfb\u6570\u636e\u65f6\u4ece\u5934\u90e8\u8bfb</li> <li>\u4e5f\u53ef\u4ee5\u5f3a\u5236\u5199\u961f\u5217\u5934\u90e8\uff1a\u8986\u76d6\u5934\u90e8\u6570\u636e</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_2","title":"\u4f20\u8f93\u6570\u636e\u7684\u65b9\u6cd5","text":"<p>\u4f7f\u7528\u961f\u5217\u4f20\u8f93\u6570\u636e\u65f6\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ul> <li>\u62f7\u8d1d\uff1a\u628a\u6570\u636e\u3001\u628a\u53d8\u91cf\u7684\u503c\u590d\u5236\u8fdb\u961f\u5217\u91cc</li> <li>\u5f15\u7528\uff1a\u628a\u6570\u636e\u3001\u628a\u53d8\u91cf\u7684\u5730\u5740\u590d\u5236\u8fdb\u961f\u5217\u91cc</li> </ul> <p>FreeRTOS\u4f7f\u7528\u62f7\u8d1d\u503c\u7684\u65b9\u6cd5\uff0c\u8fd9\u66f4\u7b80\u5355\uff1a</p> <ul> <li>\u5c40\u90e8\u53d8\u91cf\u7684\u503c\u53ef\u4ee5\u53d1\u9001\u5230\u961f\u5217\u4e2d\uff0c\u540e\u7eed\u5373\u4f7f\u51fd\u6570\u9000\u51fa\u3001\u5c40\u90e8\u53d8\u91cf\u88ab\u56de\u6536\uff0c\u4e5f\u4e0d\u4f1a\u5f71\u54cd\u961f\u5217\u4e2d\u7684\u6570\u636e</li> <li>\u65e0\u9700\u5206\u914dbuffer\u6765\u4fdd\u5b58\u6570\u636e\uff0c\u961f\u5217\u4e2d\u6709buffer</li> <li>\u5c40\u90e8\u53d8\u91cf\u53ef\u4ee5\u9a6c\u4e0a\u518d\u6b21\u4f7f\u7528</li> <li>\u53d1\u9001\u4efb\u52a1\u3001\u63a5\u6536\u4efb\u52a1\u89e3\u8026\uff1a\u63a5\u6536\u4efb\u52a1\u4e0d\u9700\u8981\u77e5\u9053\u8fd9\u6570\u636e\u662f\u8c01\u7684\u3001\u4e5f\u4e0d\u9700\u8981\u53d1\u9001\u4efb\u52a1\u6765\u91ca\u653e\u6570\u636e</li> <li>\u5982\u679c\u6570\u636e\u5b9e\u5728\u592a\u5927\uff0c\u4f60\u8fd8\u662f\u53ef\u4ee5\u4f7f\u7528\u961f\u5217\u4f20\u8f93\u5b83\u7684\u5730\u5740</li> <li>\u961f\u5217\u7684\u7a7a\u95f4\u6709FreeRTOS\u5185\u6838\u5206\u914d\uff0c\u65e0\u9700\u4efb\u52a1\u64cd\u5fc3</li> <li>\u5bf9\u4e8e\u6709\u5185\u5b58\u4fdd\u62a4\u529f\u80fd\u7684\u7cfb\u7edf\uff0c\u5982\u679c\u961f\u5217\u4f7f\u7528\u5f15\u7528\u65b9\u6cd5\uff0c\u4e5f\u5c31\u662f\u4f7f\u7528\u5730\u5740\uff0c\u5fc5\u987b\u786e\u4fdd\u53cc\u65b9\u4efb\u52a1\u5bf9\u8fd9\u4e2a\u5730\u5740\u90fd\u6709\u8bbf\u95ee\u6743\u9650\u3002\u4f7f\u7528\u62f7\u8d1d\u65b9\u6cd5\u65f6\uff0c\u5219\u65e0\u6b64\u9650\u5236\uff1a\u5185\u6838\u6709\u8db3\u591f\u7684\u6743\u9650\uff0c\u628a\u6570\u636e\u590d\u5236\u8fdb\u961f\u5217\u3001\u518d\u628a\u6570\u636e\u590d\u5236\u51fa\u961f\u5217\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_3","title":"\u961f\u5217\u7684\u963b\u585e\u8bbf\u95ee","text":"<p>\u53ea\u8981\u77e5\u9053\u961f\u5217\u7684\u53e5\u67c4\uff0c\u8c01\u90fd\u53ef\u4ee5\u8bfb\u3001\u5199\u8be5\u961f\u5217\u3002\u4efb\u52a1\u3001ISR\u90fd\u53ef\u8bfb\u3001\u5199\u961f\u5217\u3002\u53ef\u4ee5\u591a\u4e2a\u4efb\u52a1\u8bfb\u5199\u961f\u5217\u3002</p> <p>\u4efb\u52a1\u8bfb\u5199\u961f\u5217\u65f6\uff0c\u7b80\u5355\u5730\u8bf4\uff1a\u5982\u679c\u8bfb\u5199\u4e0d\u6210\u529f\uff0c\u5219\u963b\u585e\uff1b\u53ef\u4ee5\u6307\u5b9a\u8d85\u65f6\u65f6\u95f4\u3002\u53e3\u8bed\u5316\u5730\u8bf4\uff0c\u5c31\u662f\u53ef\u4ee5\u5b9a\u4e2a\u95f9\u949f\uff1a\u5982\u679c\u80fd\u8bfb\u5199\u4e86\u5c31\u9a6c\u4e0a\u8fdb\u5165\u5c31\u7eea\u6001\uff0c\u5426\u5219\u5c31\u963b\u585e\u76f4\u5230\u8d85\u65f6\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_4","title":"\u5199\u961f\u5217","text":"<p>\u67d0\u4e2a\u4efb\u52a1\u8bfb\u961f\u5217\u65f6\uff0c\u5982\u679c\u961f\u5217\u6ca1\u6709\u6570\u636e\uff0c\u5219\u8be5\u4efb\u52a1\u53ef\u4ee5\u8fdb\u5165\u963b\u585e\u72b6\u6001\uff1a\u8fd8\u53ef\u4ee5\u6307\u5b9a\u963b\u585e\u7684\u65f6\u95f4\u3002\u5982\u679c\u961f\u5217\u6709\u6570\u636e\u4e86\uff0c\u5219\u8be5\u963b\u585e\u7684\u4efb\u52a1\u4f1a\u53d8\u4e3a\u5c31\u7eea\u6001\u3002\u5982\u679c\u4e00\u76f4\u90fd\u6ca1\u6709\u6570\u636e\uff0c\u5219\u65f6\u95f4\u5230\u4e4b\u540e\u5b83\u4e5f\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_5","title":"\u4f18\u5148\u7ea7","text":"<p>\u65e2\u7136\u8bfb\u53d6\u961f\u5217\u7684\u4efb\u52a1\u4e2a\u6570\u6ca1\u6709\u9650\u5236\uff0c\u90a3\u4e48\u5f53\u591a\u4e2a\u4efb\u52a1\u8bfb\u53d6\u7a7a\u961f\u5217\u65f6\uff0c\u8fd9\u4e9b\u4efb\u52a1\u90fd\u4f1a\u8fdb\u5165\u963b\u585e\u72b6\u6001\uff1a\u6709\u591a\u4e2a\u4efb\u52a1\u5728\u7b49\u5f85\u540c\u4e00\u4e2a\u961f\u5217\u7684\u6570\u636e\u3002\u5f53\u961f\u5217\u4e2d\u6709\u6570\u636e\u65f6\uff0c\u54ea\u4e2a\u4efb\u52a1\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001\uff1f</p> <ul> <li>\u4f18\u5148\u7ea7\u6700\u9ad8\u7684\u4efb\u52a1</li> <li>\u5982\u679c\u5927\u5bb6\u7684\u4f18\u5148\u7ea7\u76f8\u540c\uff0c\u90a3\u7b49\u5f85\u65f6\u95f4\u6700\u4e45\u7684\u4efb\u52a1\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_6","title":"\u8bfb\u961f\u5217","text":"<p>\u8ddf\u8bfb\u961f\u5217\u7c7b\u4f3c\uff0c\u4e00\u4e2a\u4efb\u52a1\u8981\u5199\u961f\u5217\u65f6\uff0c\u5982\u679c\u961f\u5217\u6ee1\u4e86\uff0c\u8be5\u4efb\u52a1\u4e5f\u53ef\u4ee5\u8fdb\u5165\u963b\u585e\u72b6\u6001\uff1a\u8fd8\u53ef\u4ee5\u6307\u5b9a\u963b\u585e\u7684\u65f6\u95f4\u3002\u5982\u679c\u961f\u5217\u6709\u7a7a\u95f4\u4e86\uff0c\u5219\u8be5\u963b\u585e\u7684\u4efb\u52a1\u4f1a\u53d8\u4e3a\u5c31\u7eea\u6001\u3002\u5982\u679c\u4e00\u76f4\u90fd\u6ca1\u6709\u7a7a\u95f4\uff0c\u5219\u65f6\u95f4\u5230\u4e4b\u540e\u5b83\u4e5f\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001\u3002</p> <p>\u65e2\u7136\u5199\u961f\u5217\u7684\u4efb\u52a1\u4e2a\u6570\u6ca1\u6709\u9650\u5236\uff0c\u90a3\u4e48\u5f53\u591a\u4e2a\u4efb\u52a1\u5199\"\u6ee1\u961f\u5217\"\u65f6\uff0c\u8fd9\u4e9b\u4efb\u52a1\u90fd\u4f1a\u8fdb\u5165\u963b\u585e\u72b6\u6001\uff1a\u6709\u591a\u4e2a\u4efb\u52a1\u5728\u7b49\u5f85\u540c\u4e00\u4e2a\u961f\u5217\u7684\u7a7a\u95f4\u3002\u5f53\u961f\u5217\u4e2d\u6709\u7a7a\u95f4\u65f6\uff0c\u54ea\u4e2a\u4efb\u52a1\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001\uff1f</p> <ul> <li>\u4f18\u5148\u7ea7\u6700\u9ad8\u7684\u4efb\u52a1</li> <li>\u5982\u679c\u5927\u5bb6\u7684\u4f18\u5148\u7ea7\u76f8\u540c\uff0c\u90a3\u7b49\u5f85\u65f6\u95f4\u6700\u4e45\u7684\u4efb\u52a1\u4f1a\u8fdb\u5165\u5c31\u7eea\u6001</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_7","title":"\u961f\u5217\u57fa\u672c","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#queuehandle_t","title":"<code>QueueHandle_t</code>","text":"<p>\u961f\u5217\u53e5\u67c4\u7c7b\u578b\uff0c\u6307\u5411\u961f\u5217\u63a7\u5236\u5757\u7684\u6307\u9488\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#queue_t","title":"<code>Queue_t</code>","text":"<p>\u961f\u5217\u63a7\u5236\u5757\u7c7b\u578b\u3002</p> <pre><code>typedef struct QueueDefinition\n{\n    // ---------------- \u5b58\u50a8\u533a\u7ba1\u7406 ----------------\n    int8_t *pcHead;           // \u6307\u5411\u961f\u5217\u5b58\u50a8\u533a\u7684\u8d77\u59cb\u5730\u5740\uff08\u961f\u5217\u5f00\u59cb\uff09\n    int8_t *pcTail;           // \u6307\u5411\u961f\u5217\u5b58\u50a8\u533a\u7684\u7ed3\u675f\u5730\u5740\u7684\u4e0b\u4e00\u4e2a\u5b57\u8282\uff08\u961f\u5217\u7ed3\u675f\u4e4b\u540e\uff09\n    int8_t *pcWriteTo;        // \u6307\u5411\u4e0b\u4e00\u4e2a\u5199\u5165\u6570\u636e\u7684\u4f4d\u7f6e\n\n    // ---------------- \u8bfb\u53d6\u4f4d\u7f6e\u7ba1\u7406 ----------------\n    union\n    {\n        int8_t *pcReadFrom;   // \u5f53\u7528\u4f5c\u961f\u5217\u65f6\uff0c\u6307\u5411\u4e0b\u4e00\u4e2a\u8bfb\u53d6\u6570\u636e\u7684\u4f4d\u7f6e\n        UBaseType_t uxRecursiveCallCount; // \u5f53\u7528\u4f5c\u4e92\u65a5\u91cf\u65f6\uff0c\u7528\u4e8e\u9012\u5f52\u8c03\u7528\u8ba1\u6570\n    } u;\n\n    // ---------------- \u4efb\u52a1\u7b49\u5f85\u5217\u8868\uff08\u6838\u5fc3\u540c\u6b65\u673a\u5236\uff09 ----------------\n    // \u5f53\u4efb\u52a1\u56e0\u8bfb\u53d6\uff08\u63a5\u6536\uff09\u800c\u963b\u585e\u65f6\uff0c\u4f1a\u6302\u5230\u8fd9\u4e2a\u5217\u8868\u4e0a\uff08\u4f8b\u5982\uff1a\u961f\u5217\u7a7a\uff09\n    List_t xTasksWaitingToReceive;\n    // \u5f53\u4efb\u52a1\u56e0\u5199\u5165\uff08\u53d1\u9001\uff09\u800c\u963b\u585e\u65f6\uff0c\u4f1a\u6302\u5230\u8fd9\u4e2a\u5217\u8868\u4e0a\uff08\u4f8b\u5982\uff1a\u961f\u5217\u6ee1\uff09\n    List_t xTasksWaitingToSend;\n\n    // ---------------- \u961f\u5217\u72b6\u6001\u4fe1\u606f ----------------\n    volatile UBaseType_t uxMessagesWaiting; // \u5f53\u524d\u961f\u5217\u4e2d\u5df2\u6709\u7684\u6d88\u606f\u6570\u91cf\n    UBaseType_t uxLength;                   // \u961f\u5217\u521b\u5efa\u65f6\u8bbe\u5b9a\u7684\u6700\u5927\u957f\u5ea6\uff08\u9879\u76ee\u6570\uff09\n    UBaseType_t uxItemSize;                 // \u6bcf\u4e2a\u9879\u76ee\u7684\u5927\u5c0f\uff08\u5b57\u8282\u6570\uff09\n\n    // ---------------- \u7c7b\u578b\u6807\u8bc6 ----------------\n    // \u6807\u8bc6\u8fd9\u4e2a\u961f\u5217\u5bf9\u8c61\u7684\u7c7b\u578b\uff1a\u961f\u5217\u3001\u4e92\u65a5\u91cf\u3001\u4fe1\u53f7\u91cf\u7b49\u3002\n    // \u8fd9\u5141\u8bb8\u540c\u4e00\u4e2a\u5185\u6838\u51fd\u6570\uff08\u5982 xQueueGenericSend\uff09\u5904\u7406\u591a\u79cd\u5bf9\u8c61\u3002\n    int8_t cRxLock;          // \u4ece\u961f\u5217\u63a5\u6536\u65f6\u4f7f\u7528\u7684\u9501\u8ba1\u6570\uff08\u5728\u4e2d\u65ad\u4e2d\uff09\n    int8_t cTxLock;          // \u5411\u961f\u5217\u53d1\u9001\u65f6\u4f7f\u7528\u7684\u9501\u8ba1\u6570\uff08\u5728\u4e2d\u65ad\u4e2d\uff09\n\n    #if ( ( configSUPPORT_STATIC_ALLOCATION == 1 ) &amp;&amp; ( configSUPPORT_DYNAMIC_ALLOCATION == 1 ) )\n        uint8_t ucStaticallyAllocated; // \u6807\u8bc6\u961f\u5217\u5185\u5b58\u662f\u9759\u6001\u8fd8\u662f\u52a8\u6001\u5206\u914d\u7684\n    #endif\n\n    #if ( configUSE_QUEUE_SETS == 1 )\n        // \u5982\u679c\u542f\u7528\u961f\u5217\u96c6\uff0c\u8fd9\u4e2a\u6307\u9488\u7528\u4e8e\u5c06\u961f\u5217\u4e0e\u4e00\u4e2a\u961f\u5217\u96c6\u5173\u8054\n        struct QueueDefinition *pxQueueSetContainer;\n    #endif\n\n    #if ( configUSE_TRACE_FACILITY == 1 )\n        UBaseType_t uxQueueNumber; // \u7528\u4e8e\u8ddf\u8e2a\u8c03\u8bd5\u7684\u961f\u5217\u7f16\u53f7\n        uint8_t ucQueueType;       // \u7528\u4e8e\u8ddf\u8e2a\u8c03\u8bd5\u7684\u961f\u5217\u7c7b\u578b\n    #endif\n\n} Queue_t;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_8","title":"\u963b\u585e\u5524\u9192","text":"<p>\u975e\u5e38\u597d\uff01FreeRTOS \u7684\u8d85\u65f6\u5524\u9192\u673a\u5236\u662f\u5176\u5b9e\u65f6\u6027\u7684\u6838\u5fc3\u4fdd\u969c\u4e4b\u4e00\uff0c\u5b83\u7684\u5b9e\u73b0\u975e\u5e38\u7cbe\u5de7\u3002\u8fd9\u4e2a\u673a\u5236\u4e0d\u4ec5\u4ec5\u7528\u4e8e\u961f\u5217\uff0c\u800c\u662f\u7528\u4e8e\u6240\u6709\u53ef\u80fd\u5f15\u8d77\u4efb\u52a1\u963b\u585e\u7684\u573a\u5408\uff0c\u5982\u4fe1\u53f7\u91cf\u3001\u4e8b\u4ef6\u7ec4\u3001\u4efb\u52a1\u901a\u77e5\u7b49\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_9","title":"\u6838\u5fc3\u601d\u60f3\uff1a\u786c\u4ef6\u5b9a\u65f6\u5668 + \u6392\u5e8f\u5217\u8868","text":"<p>FreeRTOS \u5229\u7528\u4e00\u4e2a\u786c\u4ef6\u5b9a\u65f6\u5668\uff08\u901a\u5e38\u662f SysTick\uff09\u4f5c\u4e3a\u5fc3\u8df3\uff0c\u5e76\u7ef4\u62a4\u4e86\u4e00\u4e2a\u6309\u8d85\u65f6\u65f6\u95f4\u6392\u5e8f\u7684\u5217\u8868\uff0c\u79f0\u4e3a \"\u7b49\u5f85\u672a\u5230\u671f\u5217\u8868\" \u6216 \"\u5ef6\u65f6\u5217\u8868\"\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#1","title":"1. \u5173\u952e\u6570\u636e\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#a","title":"a) \u4e24\u4e2a\u91cd\u8981\u7684\u5217\u8868","text":"<ol> <li> <p><code>xDelayedTaskList1</code> \u548c <code>xDelayedTaskList2</code></p> <ul> <li>\u8fd9\u4e24\u4e2a\u5217\u8868\u6309\u5524\u9192\u65f6\u95f4\uff08\u8d85\u65f6\u5230\u671f\u65f6\u95f4\uff09\u6392\u5e8f\uff0c\u65f6\u95f4\u6700\u8fd1\u7684\u6392\u5728\u524d\u9762\u3002</li> <li>\u4e3a\u4ec0\u4e48\u8981\u4e24\u4e2a\u5217\u8868\uff1f\u8fd9\u662f\u4e3a\u4e86\u5728\u7cfb\u7edf\u5fc3\u8df3\u4e2d\u65ad\u4e2d\u8fdb\u884c\u5b89\u5168\u7684\u5217\u8868\u5207\u6362\uff0c\u4e00\u4e2a\u7ed9\u5185\u6838\u4f7f\u7528\uff08<code>pxDelayedTaskList</code>\uff09\uff0c\u53e6\u4e00\u4e2a\u7ed9\u4e2d\u65ad\u4f7f\u7528\uff08<code>pxOverflowDelayedTaskList</code>\uff09\uff0c\u7528\u4e8e\u5904\u7406\u7cfb\u7edf\u6ef4\u7b54\u8ba1\u6570\u5668\u6ea2\u51fa\u7684\u60c5\u51b5\u3002\u4ece\u5e94\u7528\u7a0b\u5e8f\u89c6\u89d2\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u89c6\u4e3a\u4e00\u4e2a\u6574\u4f53\u7684\u201c\u5ef6\u65f6\u5217\u8868\u201d\u3002</li> </ul> </li> <li> <p><code>xPendingReadyList</code></p> <ul> <li>\u8fd9\u662f\u4e00\u4e2a\u201c\u6302\u8d77\u5c31\u7eea\u5217\u8868\u201d\u3002\u5f53\u4efb\u52a1\u56e0\u4e3a\u8d85\u65f6\u6216\u4e8b\u4ef6\u53d1\u751f\u800c\u9700\u8981\u88ab\u89e3\u9664\u963b\u585e\uff0c\u4f46\u89e3\u9664\u64cd\u4f5c\u53d1\u751f\u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u65f6\uff0c\u4efb\u52a1\u4e0d\u4f1a\u88ab\u76f4\u63a5\u79fb\u5230\u5c31\u7eea\u5217\u8868\uff08\u56e0\u4e3a\u4e0d\u80fd\u5728\u4e2d\u65ad\u4e2d\u8fdb\u884c\u8c03\u5ea6\u51b3\u7b56\uff09\uff0c\u800c\u662f\u5148\u653e\u5230\u8fd9\u4e2a\u5217\u8868\u4e2d\u3002</li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#b","title":"b) \u6838\u5fc3\u53d8\u91cf","text":"<ul> <li><code>xTickCount</code>\uff1a</li> <li>\u8fd9\u662f\u7cfb\u7edf\u7684\u5fc3\u8df3\u8ba1\u6570\u5668\uff0c\u6bcf\u6b21\u786c\u4ef6\u5b9a\u65f6\u5668\u4e2d\u65ad\u53d1\u751f\uff0c\u5b83\u5c31\u52a0\u4e00\u3002\u5b83\u662f\u6574\u4e2a\u7cfb\u7edf\u7684\u65f6\u95f4\u57fa\u51c6\u3002</li> <li><code>xNextTaskUnblockTime</code>\uff1a</li> <li>\u8fd9\u662f\u4e00\u4e2a\u5168\u5c40\u4f18\u5316\u53d8\u91cf\uff0c\u8bb0\u5f55\u4e86\u6574\u4e2a\u7cfb\u7edf\u4e2d\u4e0b\u4e00\u4e2a\u5c06\u8981\u8d85\u65f6\u7684\u4efb\u52a1\u7684\u65f6\u95f4\u70b9\u3002</li> <li>\u8fd9\u4e2a\u53d8\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u544a\u8bc9\u8c03\u5ea6\u5668\uff1a\u201c\u5728 <code>xNextTaskUnblockTime</code> \u4e4b\u524d\uff0c\u7edd\u5bf9\u4e0d\u4f1a\u6709\u4efb\u52a1\u56e0\u8d85\u65f6\u800c\u9192\u6765\u201d\u3002\u8fd9\u610f\u5473\u7740\u5728 <code>xNextTaskUnblockTime</code> \u4e4b\u524d\uff0c\u8c03\u5ea6\u5668\u53ef\u4ee5\u653e\u5fc3\u5730\u8ba9 CPU \u8fdb\u5165\u4f4e\u529f\u8017\u7684 IDLE \u72b6\u6001\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#2","title":"2. \u8d85\u65f6\u5524\u9192\u7684\u5b9e\u73b0\u6d41\u7a0b","text":"<p>\u6574\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u5206\u4e3a \u201c\u8bbe\u7f6e\u963b\u585e\u201d \u548c \u201c\u8d85\u65f6\u68c0\u67e5\u201d \u4e24\u4e2a\u9636\u6bb5\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_10","title":"\u9636\u6bb5\u4e00\uff1a\u4efb\u52a1\u8bbe\u7f6e\u963b\u585e\u5e76\u542f\u52a8\u8d85\u65f6\u5012\u8ba1\u65f6","text":"<p>\u5f53\u4e00\u4e2a\u4efb\u52a1\uff08\u4f8b\u5982 <code>Task_A</code>\uff09\u8c03\u7528 <code>xQueueReceive(..., 100ms)</code> \u65f6\uff0c\u5982\u679c\u961f\u5217\u4e3a\u7a7a\uff0c\u5b83\u4f1a\u8fdb\u5165\u963b\u585e\u72b6\u6001\u3002\u5185\u6838\u4f1a\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a</p> <ol> <li> <p>\u8ba1\u7b97\u5524\u9192\u65f6\u95f4\u70b9\uff1a</p> <ul> <li><code>xWakeTime = xTickCount + pdMS_TO_TICKS(100)</code></li> <li>\u5c06\u5f53\u524d\u65f6\u95f4 <code>xTickCount</code> \u52a0\u4e0a\u8d85\u65f6\u65f6\u95f4\uff0c\u8ba1\u7b97\u51fa\u672a\u6765\u7684\u5524\u9192\u65f6\u95f4\u70b9 <code>xWakeTime</code>\u3002</li> </ul> </li> <li> <p>\u5c06\u4efb\u52a1\u6302\u5165\u5ef6\u65f6\u5217\u8868\uff1a</p> <ul> <li>\u5c06 <code>Task_A</code> \u4ece\u5c31\u7eea\u5217\u8868\u4e2d\u79fb\u9664\u3002</li> <li>\u6839\u636e\u8ba1\u7b97\u51fa\u7684 <code>xWakeTime</code>\uff0c\u5c06 <code>Task_A</code> \u6309\u7167\u987a\u5e8f\u63d2\u5165\u5230 \u5ef6\u65f6\u5217\u8868 \u4e2d\u3002\u8fd9\u4e2a\u5217\u8868\u662f\u6309 <code>xWakeTime</code> \u5347\u5e8f\u6392\u5217\u7684\u3002</li> </ul> </li> <li> <p>\u66f4\u65b0\u4e0b\u4e00\u4e2a\u5524\u9192\u65f6\u95f4\uff1a</p> <ul> <li>\u7531\u4e8e <code>Task_A</code> \u88ab\u63d2\u5165\u5230\u5ef6\u65f6\u5217\u8868\uff0c\u5185\u6838\u4f1a\u68c0\u67e5\u5b83\u662f\u5426\u6210\u4e3a\u4e86\u5217\u8868\u4e2d\u8d85\u65f6\u65f6\u95f4\u6700\u65e9\u7684\u4efb\u52a1\uff08\u5373\u662f\u5426\u5728\u5217\u8868\u9996\u90e8\uff09\u3002</li> <li>\u5982\u679c\u662f\uff0c\u5219\u66f4\u65b0\u5168\u5c40\u53d8\u91cf <code>xNextTaskUnblockTime</code> \u4e3a <code>Task_A</code> \u7684 <code>xWakeTime</code>\u3002</li> </ul> </li> </ol> <pre><code>flowchart TD\n    A[\"Task_A \u8c03\u7528&lt;br/&gt;xQueueReceive(..., 100ms)\"] --&gt; B{\"\u961f\u5217\u662f\u5426\u4e3a\u7a7a?\"}\n    B -- \u662f --&gt; C[\"\u8ba1\u7b97\u5524\u9192\u65f6\u95f4\u70b9&lt;br/&gt;xWakeTime = xTickCount + 100ms\"]\n    C --&gt; D[\"\u5c06 Task_A \u4ece\u5c31\u7eea\u5217\u8868\u79fb\u9664\"]\n    D --&gt; E[\"\u5c06 Task_A \u6309 xWakeTime&lt;br/&gt;\u6392\u5e8f\u63d2\u5165\u5ef6\u65f6\u5217\u8868\"]\n    E --&gt; F{\"Task_A \u662f\u5426\u6210\u4e3a&lt;br/&gt;\u6700\u65e9\u8d85\u65f6\u7684\u4efb\u52a1?\"}\n    F -- \u662f --&gt; G[\"\u66f4\u65b0 xNextTaskUnblockTime = xWakeTime\"]\n    F -- \u5426 --&gt; H[\"xNextTaskUnblockTime \u4e0d\u53d8\"]\n    G &amp; H --&gt; I[\"\u4efb\u52a1\u5207\u6362: \u8c03\u5ea6\u5176\u4ed6\u5c31\u7eea\u4efb\u52a1\"]\n    B -- \u5426 --&gt; J[\"\u7acb\u5373\u6210\u529f\u8fd4\u56de\"]\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_11","title":"\u9636\u6bb5\u4e8c\uff1a\u7cfb\u7edf\u5fc3\u8df3\u4e2d\u65ad\u8fdb\u884c\u8d85\u65f6\u68c0\u67e5","text":"<p>\u6bcf\u6b21 SysTick \u5b9a\u65f6\u5668\u4e2d\u65ad\u53d1\u751f\u65f6\uff0c<code>xTaskIncrementTick()</code> \u51fd\u6570\u4f1a\u88ab\u8c03\u7528\u3002\u5b83\u8d1f\u8d23\uff1a</p> <ol> <li> <p>\u9012\u589e\u65f6\u95f4\u57fa\u51c6\uff1a</p> <ul> <li><code>xTickCount = xTickCount + 1</code></li> </ul> </li> <li> <p>\u68c0\u67e5\u662f\u5426\u5230\u8fbe\u8d85\u65f6\u65f6\u95f4\uff1a</p> <ul> <li>\u5b83\u6bd4\u8f83\u5f53\u524d\u7684 <code>xTickCount</code> \u548c <code>xNextTaskUnblockTime</code>\u3002</li> <li>\u5982\u679c <code>xTickCount &lt; xNextTaskUnblockTime</code>\uff1a\u8bf4\u660e\u8fd8\u6ca1\u6709\u4efb\u52a1\u9700\u8981\u88ab\u5524\u9192\uff0c\u76f4\u63a5\u8fd4\u56de\u3002</li> <li>\u5982\u679c <code>xTickCount &gt;= xNextTaskUnblockTime</code>\uff1a\u8bf4\u660e\u81f3\u5c11\u6709\u4e00\u4e2a\u4efb\u52a1\u8d85\u65f6\u65f6\u95f4\u5df2\u5230\u3002</li> </ul> </li> <li> <p>\u5524\u9192\u8d85\u65f6\u4efb\u52a1\uff1a</p> <ul> <li>\u5185\u6838\u904d\u5386\u5ef6\u65f6\u5217\u8868\uff0c\u5c06\u6240\u6709 <code>xWakeTime &lt;= xTickCount</code> \u7684\u4efb\u52a1\u90fd\u4ece\u5217\u8868\u4e2d\u79fb\u9664\u3002</li> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u88ab\u79fb\u9664\u7684\u4efb\u52a1\uff1a<ul> <li>\u5982\u679c\u5b83\u5728\u7b49\u5f85\u4e00\u4e2a\u8d44\u6e90\uff08\u5982\u961f\u5217\uff09\uff1a\u8bf4\u660e\u5b83\u662f\u56e0\u4e3a\u8d85\u65f6\u800c\u9192\u6765\uff0c\u800c\u4e0d\u662f\u56e0\u4e3a\u7b49\u5230\u4e86\u6570\u636e\u3002\u56e0\u6b64\uff0c\u5185\u6838\u4f1a\u5c06\u5b83\u4ece\u8be5\u8d44\u6e90\u7684\u7b49\u5f85\u5217\u8868\uff08\u5982\u961f\u5217\u7684 <code>xTasksWaitingToReceive</code>\uff09\u4e2d\u4e5f\u79fb\u9664\u6389\u3002</li> <li>\u5c06\u4efb\u52a1\u91cd\u65b0\u653e\u56de\u5c31\u7eea\u5217\u8868\u3002</li> </ul> </li> </ul> </li> <li> <p>\u66f4\u65b0\u4e0b\u4e00\u4e2a\u5524\u9192\u65f6\u95f4\uff1a</p> <ul> <li>\u5728\u5904\u7406\u5b8c\u6240\u6709\u8d85\u65f6\u4efb\u52a1\u540e\uff0c\u5ef6\u65f6\u5217\u8868\u7684\u65b0\u5934\u90e8\uff08\u5982\u679c\u5217\u8868\u4e0d\u4e3a\u7a7a\uff09\u5c31\u662f\u4e0b\u4e00\u4e2a\u8981\u8d85\u65f6\u7684\u4efb\u52a1\u3002</li> <li>\u66f4\u65b0 <code>xNextTaskUnblockTime</code> \u4e3a\u8fd9\u4e2a\u65b0\u5934\u90e8\u7684\u8d85\u65f6\u65f6\u95f4\u3002\u5982\u679c\u5217\u8868\u4e3a\u7a7a\uff0c\u5219 <code>xNextTaskUnblockTime</code> \u88ab\u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u6700\u5927\u503c\uff0c\u8868\u793a\u201c\u6ca1\u6709\u4efb\u52a1\u5728\u7b49\u5f85\u8d85\u65f6\u201d\u3002</li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#3","title":"3. \u8d85\u65f6\u4e0e\u4e8b\u4ef6\u540c\u65f6\u5230\u8fbe\u7684\u7ade\u4e89\u5904\u7406","text":"<p>\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u662f\uff1a\u5982\u679c\u4e00\u4e2a\u4efb\u52a1\u5728\u7b49\u5f85\u961f\u5217\u6570\u636e\uff0c\u540c\u65f6\u8d85\u65f6\u65f6\u95f4\u4e5f\u5230\u4e86\uff0c\u4f1a\u600e\u6837\uff1f</p> <ul> <li>\u4e8b\u4ef6\uff08\u6570\u636e\u5230\u8fbe\uff09\u5148\u4e8e\u8d85\u65f6\uff1a<ul> <li>\u53e6\u4e00\u4e2a\u4efb\u52a1\u53d1\u9001\u4e86\u6570\u636e\uff0c\u5185\u6838\u5c06 <code>Task_A</code> \u4ece\u961f\u5217\u7684\u7b49\u5f85\u5217\u8868\u548c\u5ef6\u65f6\u5217\u8868\u4e2d\u540c\u65f6\u79fb\u9664\uff0c\u5e76\u653e\u5165\u5c31\u7eea\u5217\u8868\u3002<code>Task_A</code> \u6210\u529f\u83b7\u5f97\u6570\u636e\u3002</li> </ul> </li> <li>\u8d85\u65f6\u5148\u4e8e\u4e8b\u4ef6\uff1a<ul> <li>SysTick \u4e2d\u65ad\u53d1\u73b0 <code>Task_A</code> \u8d85\u65f6\uff0c\u5c06\u5176\u4ece\u5ef6\u65f6\u5217\u8868\u548c\u961f\u5217\u7684\u7b49\u5f85\u5217\u8868\u4e2d\u79fb\u9664\uff0c\u5e76\u653e\u5165\u5c31\u7eea\u5217\u8868\u3002\u5f53 <code>Task_A</code> \u8fd0\u884c\u65f6\uff0c<code>xQueueReceive</code> \u4f1a\u8fd4\u56de <code>errQUEUE_EMPTY</code> \u6216 <code>pdFALSE</code>\uff0c\u544a\u77e5\u8c03\u7528\u8005\u8d85\u65f6\u53d1\u751f\u4e86\u3002</li> </ul> </li> </ul> <p>\u5185\u6838\u4fdd\u8bc1\u4e86\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u6240\u4ee5\u4e00\u4e2a\u4efb\u52a1\u53ea\u4f1a\u56e0\u4e3a\u4e00\u4e2a\u539f\u56e0\uff08\u8981\u4e48\u4e8b\u4ef6\uff0c\u8981\u4e48\u8d85\u65f6\uff09\u88ab\u5524\u9192\uff0c\u4e0d\u4f1a\u51fa\u73b0\u6df7\u4e71\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_12","title":"\u603b\u7ed3","text":"<p>FreeRTOS \u7684\u8d85\u65f6\u5524\u9192\u673a\u5236\u53ef\u4ee5\u6982\u62ec\u4e3a\uff1a</p> \u7ec4\u4ef6 \u4f5c\u7528 SysTick \u5b9a\u65f6\u5668 \u63d0\u4f9b\u7cfb\u7edf\u5fc3\u8df3\uff0c\u9a71\u52a8\u65f6\u95f4\u524d\u8fdb\u3002 <code>xTickCount</code> \u7cfb\u7edf\u65f6\u95f4\u57fa\u51c6\u3002 \u5ef6\u65f6\u5217\u8868 \u4e00\u4e2a\u6309\u8d85\u65f6\u65f6\u95f4\u6392\u5e8f\u7684\u5217\u8868\uff0c\u7ba1\u7406\u6240\u6709\u963b\u585e\u4e2d\u7684\u4efb\u52a1\u3002 <code>xNextTaskUnblockTime</code> \u6027\u80fd\u5173\u952e\uff1a\u8bb0\u5f55\u4e0b\u4e00\u4e2a\u8d85\u65f6\u70b9\uff0c\u8ba9\u8c03\u5ea6\u5668\u5728\u7a7a\u95f2\u65f6\u80fd\u8fdb\u5165\u4f4e\u529f\u8017\u6a21\u5f0f\u3002 <code>xTaskIncrementTick()</code> \u6838\u5fc3\u51fd\u6570\uff1a\u5728\u6bcf\u6b21\u5fc3\u8df3\u4e2d\u65ad\u4e2d\u88ab\u8c03\u7528\uff0c\u68c0\u67e5\u5e76\u5904\u7406\u8d85\u65f6\u4efb\u52a1\u3002 <p>\u8fd9\u79cd\u57fa\u4e8e\u6392\u5e8f\u5217\u8868\u548c\u5168\u5c40\u6700\u5c0f\u8d85\u65f6\u65f6\u95f4\u7684\u8bbe\u8ba1\uff0c\u4f7f\u5f97\u8d85\u65f6\u68c0\u67e5\u975e\u5e38\u9ad8\u6548\uff08O(1) \u7684\u5feb\u901f\u68c0\u67e5\uff09\uff0c\u662f FreeRTOS \u9002\u5408\u8d44\u6e90\u53d7\u9650\u4f46\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u539f\u56e0\u4e4b\u4e00\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#freertos","title":"FreeRTOS","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_13","title":"\u961f\u5217\u521b\u5efa\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuecreate-","title":"xQueueCreate - \u52a8\u6001\u521b\u5efa\u961f\u5217","text":"<pre><code>QueueHandle_t xQueueCreate(UBaseType_t uxQueueLength, \n                          UBaseType_t uxItemSize);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>uxQueueLength</code>\uff1a\u961f\u5217\u80fd\u591f\u5b58\u50a8\u7684\u6700\u5927\u9879\u76ee\u6570\u91cf - <code>uxItemSize</code>\uff1a\u6bcf\u4e2a\u9879\u76ee\u7684\u5927\u5c0f\uff0c\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u8fd4\u56de\u961f\u5217\u53e5\u67c4 - \u5931\u8d25\uff1a\u8fd4\u56deNULL\uff08\u5185\u5b58\u4e0d\u8db3\u65f6\uff09</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u5b58\u50a810\u4e2a\u6574\u6570\u7684\u961f\u5217\nQueueHandle_t xIntQueue = xQueueCreate(10, sizeof(int));\n\n// \u521b\u5efa\u5b58\u50a85\u4e2a\u6d6e\u70b9\u6570\u7684\u961f\u5217  \nQueueHandle_t xFloatQueue = xQueueCreate(5, sizeof(float));\n\n// \u521b\u5efa\u5b58\u50a8\u81ea\u5b9a\u4e49\u7ed3\u6784\u4f53\u7684\u961f\u5217\ntypedef struct {\n    uint8_t command;\n    uint32_t parameter;\n    TickType_t timestamp;\n} Command_t;\nQueueHandle_t xCommandQueue = xQueueCreate(8, sizeof(Command_t));\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuecreatestatic-","title":"xQueueCreateStatic - \u9759\u6001\u521b\u5efa\u961f\u5217","text":"<pre><code>QueueHandle_t xQueueCreateStatic(UBaseType_t uxQueueLength,\n                                UBaseType_t uxItemSize,\n                                uint8_t *pucQueueStorageBuffer,\n                                StaticQueue_t *pxQueueBuffer);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>pucQueueStorageBuffer</code>\uff1a\u6307\u5411\u961f\u5217\u5b58\u50a8\u533a\u7684\u6307\u9488 - <code>pxQueueBuffer</code>\uff1a\u6307\u5411\u961f\u5217\u63a7\u5236\u7ed3\u6784\u4f53\u7684\u6307\u9488</p> <p>\u9759\u6001\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u9884\u5206\u914d\u5b58\u50a8\u533a\u548c\u63a7\u5236\u7ed3\u6784\n#define QUEUE_LENGTH 5\n#define ITEM_SIZE sizeof(int)\n\nstatic uint8_t ucQueueStorage[QUEUE_LENGTH * ITEM_SIZE];\nstatic StaticQueue_t xQueueBuffer;\n\n// \u9759\u6001\u521b\u5efa\u961f\u5217\nQueueHandle_t xStaticQueue = xQueueCreateStatic(QUEUE_LENGTH,\n                                               ITEM_SIZE,\n                                               ucQueueStorage,\n                                               &amp;xQueueBuffer);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_14","title":"\u961f\u5217\u590d\u4f4d\u4e0e\u5220\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#vqueuereset-","title":"vQueueReset - \u961f\u5217\u590d\u4f4d","text":"<pre><code>void vQueueReset(QueueHandle_t xQueue);\n</code></pre> <p>\u529f\u80fd\uff1a\u5c06\u961f\u5217\u91cd\u7f6e\u4e3a\u7a7a\u72b6\u6001\uff0c\u6e05\u9664\u6240\u6709\u6570\u636e</p> <p>\u6ce8\u610f\u4e8b\u9879\uff1a - \u590d\u4f4d\u65f6\u4e0d\u4f1a\u5524\u9192\u6b63\u5728\u7b49\u5f85\u961f\u5217\u7684\u4efb\u52a1 - \u9700\u8981\u5728\u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u4f7f\u7528\u961f\u5217\u65f6\u8c03\u7528 - \u8c28\u614e\u4f7f\u7528\uff0c\u53ef\u80fd\u9020\u6210\u6570\u636e\u4e22\u5931</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#vqueuedelete-","title":"vQueueDelete - \u961f\u5217\u5220\u9664","text":"<pre><code>void vQueueDelete(QueueHandle_t xQueue);\n</code></pre> <p>\u4f7f\u7528\u573a\u666f\uff1a - \u52a8\u6001\u521b\u5efa\u7684\u961f\u5217\u4e0d\u518d\u9700\u8981\u65f6\u91ca\u653e\u5185\u5b58 - \u7cfb\u7edf\u91cd\u65b0\u914d\u7f6e\u65f6\u6e05\u7406\u8d44\u6e90 - \u5e94\u7528\u7a0b\u5e8f\u5173\u95ed\u65f6\u6e05\u7406</p> <p>\u5220\u9664\u793a\u4f8b\uff1a</p> <pre><code>void vCleanupResources(void) {\n    if(xCommandQueue != NULL) {\n        vQueueDelete(xCommandQueue);\n        xCommandQueue = NULL;\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_15","title":"\u961f\u5217\u5199\u5165\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuesend-fifo","title":"xQueueSend - \u540e\u7aef\u53d1\u9001\uff08\u6807\u51c6FIFO\uff09","text":"<pre><code>BaseType_t xQueueSend(QueueHandle_t xQueue,\n                     const void *pvItemToQueue,\n                     TickType_t xTicksToWait);\n</code></pre> <p>\u963b\u585e\u884c\u4e3a\uff1a - <code>xTicksToWait = 0</code>\uff1a\u975e\u963b\u585e\uff0c\u961f\u5217\u6ee1\u7acb\u5373\u8fd4\u56de - <code>xTicksToWait = portMAX_DELAY</code>\uff1a\u65e0\u9650\u963b\u585e\uff0c\u76f4\u5230\u961f\u5217\u6709\u7a7a\u4f4d - <code>xTicksToWait = N</code>\uff1a\u963b\u585eN\u4e2a\u65f6\u949f\u8282\u62cd</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>int data = 42;\n// \u5c1d\u8bd5\u53d1\u9001\uff0c\u7b49\u5f85\u6700\u591a100ms\nif(xQueueSend(xQueue, &amp;data, 100 / portTICK_PERIOD_MS) != pdPASS) {\n    printf(\"Send failed: queue full\\n\");\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuesendtofront-lifo","title":"xQueueSendToFront - \u524d\u7aef\u53d1\u9001\uff08LIFO\uff09","text":"<pre><code>BaseType_t xQueueSendToFront(QueueHandle_t xQueue,\n                            const void *pvItemToQueue,\n                            TickType_t xTicksToWait);\n</code></pre> <p>\u7279\u70b9\uff1a\u6570\u636e\u63d2\u5165\u961f\u5217\u524d\u7aef\uff0c\u5b9e\u73b0\u540e\u8fdb\u5148\u51fa\u884c\u4e3a</p> <p>\u5e94\u7528\u573a\u666f\uff1a - \u7d27\u6025\u6d88\u606f\u9700\u8981\u4f18\u5148\u5904\u7406 - \u5b9e\u73b0\u6808\u6570\u636e\u7ed3\u6784</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuesendtoback-","title":"xQueueSendToBack - \u540e\u7aef\u53d1\u9001","text":"<p>\u4e0e<code>xQueueSend</code>\u529f\u80fd\u5b8c\u5168\u76f8\u540c\uff0c\u90fd\u662f\u6807\u51c6FIFO\u53d1\u9001\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueueoverwrite-","title":"xQueueOverwrite - \u8986\u76d6\u5199\u5165","text":"<pre><code>BaseType_t xQueueOverwrite(QueueHandle_t xQueue,\n                          const void *pvItemToQueue);\n</code></pre> <p>\u7279\u70b9\uff1a - \u4e13\u4e3a\u957f\u5ea6\u4e3a1\u7684\u961f\u5217\u8bbe\u8ba1 - \u5982\u679c\u961f\u5217\u5df2\u6ee1\uff0c\u81ea\u52a8\u8986\u76d6\u6700\u8001\u7684\u6570\u636e - \u6c38\u8fdc\u4e0d\u4f1a\u963b\u585e\uff0c\u603b\u662f\u6210\u529f</p> <p>\u9002\u7528\u573a\u666f\uff1a - \u53ea\u9700\u8981\u6700\u65b0\u6570\u636e\u7684\u4f20\u611f\u5668\u91c7\u6837 - \u72b6\u6001\u66f4\u65b0\uff0c\u53ea\u5173\u5fc3\u6700\u65b0\u72b6\u6001</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_16","title":"\u4e2d\u65ad\u5b89\u5168\u5199\u5165\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuesendfromisr","title":"xQueueSendFromISR","text":"<pre><code>BaseType_t xQueueSendFromISR(QueueHandle_t xQueue,\n                            const void *pvItemToQueue,\n                            BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuesendtofrontfromisr","title":"xQueueSendToFrontFromISR","text":"<pre><code>BaseType_t xQueueSendToFrontFromISR(QueueHandle_t xQueue,\n                                   const void *pvItemToQueue,\n                                   BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueueoverwritefromisr","title":"xQueueOverwriteFromISR","text":"<pre><code>BaseType_t xQueueOverwriteFromISR(QueueHandle_t xQueue,\n                                 const void *pvItemToQueue,\n                                 BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre> <p>pxHigherPriorityTaskWoken\u53c2\u6570\uff1a - \u5982\u679c\u5199\u5165\u64cd\u4f5c\u5524\u9192\u4e86\u66f4\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\uff0c\u8be5\u53c2\u6570\u4f1a\u88ab\u8bbe\u7f6e\u4e3apdTRUE - \u5728\u4e2d\u65ad\u9000\u51fa\u524d\u9700\u8981\u68c0\u67e5\u6b64\u53c2\u6570\uff0c\u5fc5\u8981\u65f6\u624b\u52a8\u8fdb\u884c\u4e0a\u4e0b\u6587\u5207\u6362</p> <p>\u4e2d\u65ad\u4e2d\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>void UART_IRQHandler(void) {\n    BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n    char received_char;\n\n    // \u4eceUART\u8bfb\u53d6\u5b57\u7b26\n    received_char = UART-&gt;DR;\n\n    // \u53d1\u9001\u5230\u961f\u5217\n    xQueueSendFromISR(xUartQueue, &amp;received_char, &amp;xHigherPriorityTaskWoken);\n\n    // \u5fc5\u8981\u65f6\u8fdb\u884c\u4efb\u52a1\u5207\u6362\n    portYIELD_FROM_ISR(xHigherPriorityTaskWoken);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_17","title":"\u961f\u5217\u8bfb\u53d6\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuereceive-","title":"xQueueReceive - \u63a5\u6536\u5e76\u79fb\u9664\u6570\u636e","text":"<pre><code>BaseType_t xQueueReceive(QueueHandle_t xQueue,\n                        void *pvBuffer,\n                        TickType_t xTicksToWait);\n</code></pre> <p>\u529f\u80fd\uff1a\u4ece\u961f\u5217\u524d\u7aef\u8bfb\u53d6\u6570\u636e\uff0c\u8bfb\u53d6\u540e\u6570\u636e\u4ece\u961f\u5217\u4e2d\u79fb\u9664</p> <p>\u963b\u585e\u884c\u4e3a\uff1a - <code>xTicksToWait = 0</code>\uff1a\u975e\u963b\u585e\uff0c\u961f\u5217\u7a7a\u7acb\u5373\u8fd4\u56de - <code>xTicksToWait = portMAX_DELAY</code>\uff1a\u65e0\u9650\u963b\u585e\uff0c\u76f4\u5230\u961f\u5217\u6709\u6570\u636e - <code>xTicksToWait = N</code>\uff1a\u963b\u585eN\u4e2a\u65f6\u949f\u8282\u62cd</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>int received_data;\n\n// \u7b49\u5f85\u6570\u636e\uff0c\u6700\u591a\u7b49\u5f85500ms\nif(xQueueReceive(xQueue, &amp;received_data, 500 / portTICK_PERIOD_MS) == pdPASS) {\n    printf(\"Received: %d\\n\", received_data);\n} else {\n    printf(\"No data received within timeout\\n\");\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuepeek-","title":"xQueuePeek - \u67e5\u770b\u4f46\u4e0d\u79fb\u9664\u6570\u636e","text":"<pre><code>BaseType_t xQueuePeek(QueueHandle_t xQueue,\n                     void *pvBuffer,\n                     TickType_t xTicksToWait);\n</code></pre> <p>\u7279\u70b9\uff1a - \u8bfb\u53d6\u961f\u5217\u524d\u7aef\u6570\u636e\u4f46\u4e0d\u4ece\u961f\u5217\u4e2d\u79fb\u9664 - \u591a\u4e2a\u4efb\u52a1\u53ef\u4ee5\u540c\u65f6\u67e5\u770b\u76f8\u540c\u6570\u636e - \u6570\u636e\u4ecd\u7136\u4fdd\u7559\u5728\u961f\u5217\u4e2d\u4f9b\u5176\u4ed6\u4efb\u52a1\u8bfb\u53d6</p> <p>\u5e94\u7528\u573a\u666f\uff1a - \u591a\u4e2a\u4efb\u52a1\u9700\u8981\u76d1\u63a7\u76f8\u540c\u6570\u636e - \u6570\u636e\u5e7f\u64ad\u573a\u666f</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_18","title":"\u4e2d\u65ad\u5b89\u5168\u8bfb\u53d6\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuereceivefromisr","title":"xQueueReceiveFromISR","text":"<pre><code>BaseType_t xQueueReceiveFromISR(QueueHandle_t xQueue,\n                               void *pvBuffer,\n                               BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#xqueuepeekfromisr","title":"xQueuePeekFromISR","text":"<pre><code>BaseType_t xQueuePeekFromISR(QueueHandle_t xQueue,\n                            void *pvBuffer);\n</code></pre> <p>\u6ce8\u610f\uff1a<code>xQueuePeekFromISR</code>\u6ca1\u6709\u8d85\u65f6\u53c2\u6570\uff0c\u56e0\u4e3a\u4e2d\u65ad\u4e2d\u4e0d\u5e94\u8be5\u963b\u585e\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_19","title":"\u961f\u5217\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#uxqueuemessageswaiting-","title":"uxQueueMessagesWaiting - \u67e5\u8be2\u6d88\u606f\u6570\u91cf","text":"<pre><code>UBaseType_t uxQueueMessagesWaiting(QueueHandle_t xQueue);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a\u961f\u5217\u4e2d\u5f53\u524d\u5b58\u50a8\u7684\u6d88\u606f\u6570\u91cf</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#uxqueuespacesavailable-","title":"uxQueueSpacesAvailable - \u67e5\u8be2\u5269\u4f59\u7a7a\u95f4","text":"<pre><code>UBaseType_t uxQueueSpacesAvailable(QueueHandle_t xQueue);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a\u961f\u5217\u4e2d\u5269\u4f59\u53ef\u7528\u7684\u7a7a\u95f4\u6570\u91cf</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_20","title":"\u4e2d\u65ad\u5b89\u5168\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#uxqueuemessageswaitingfromisr","title":"uxQueueMessagesWaitingFromISR","text":"<pre><code>UBaseType_t uxQueueMessagesWaitingFromISR(QueueHandle_t xQueue);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_21","title":"\u67e5\u8be2\u51fd\u6570\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>void vQueueStatusMonitor(QueueHandle_t xQueue) {\n    UBaseType_t uxMessages = uxQueueMessagesWaiting(xQueue);\n    UBaseType_t uxSpaces = uxQueueSpacesAvailable(xQueue);\n    UBaseType_t uxTotalLength = uxMessages + uxSpaces;\n\n    printf(\"Queue Status: %d/%d messages (%.1f%% full)\\n\",\n           uxMessages, uxTotalLength,\n           (float)uxMessages / uxTotalLength * 100);\n\n    // \u6839\u636e\u961f\u5217\u72b6\u6001\u8c03\u6574\u884c\u4e3a\n    if(uxSpaces == 0) {\n        printf(\"Warning: Queue is full!\\n\");\n    } else if(uxMessages == 0) {\n        printf(\"Info: Queue is empty\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#cmsisv2-api","title":"CMSISv2 API","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeuenew-","title":"osMessageQueueNew - \u521b\u5efa\u6d88\u606f\u961f\u5217","text":"<pre><code>osMessageQueueId_t osMessageQueueNew(uint32_t msg_count, \n                                    uint32_t msg_size, \n                                    const osMessageQueueAttr_t *attr);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <ul> <li><code>msg_count</code>: \u961f\u5217\u5bb9\u91cf\uff08\u6d88\u606f\u6570\u91cf\uff09</li> <li><code>msg_size</code>: \u6bcf\u4e2a\u6d88\u606f\u7684\u5927\u5c0f\uff08\u5b57\u8282\uff09</li> <li><code>attr</code>: \u961f\u5217\u5c5e\u6027\uff08\u540d\u79f0\u3001\u5185\u5b58\u533a\u57df\u7b49\uff09</li> </ul> <p>\u521b\u5efa\u793a\u4f8b:</p> <pre><code>// \u9759\u6001\u5206\u914d\u65b9\u5f0f\nstatic uint8_t queue_mem[10 * sizeof(my_data_t)];\nstatic osMessageQueueId_t my_queue;\n\nvoid create_queue_static(void) {\n    osMessageQueueAttr_t attr = {\n        .name = \"StaticQueue\",\n        .cb_mem = &amp;queue_mem,\n        .cb_size = sizeof(queue_mem)\n    };\n    my_queue = osMessageQueueNew(10, sizeof(my_data_t), &amp;attr);\n}\n\n// \u52a8\u6001\u5206\u914d\u65b9\u5f0f\nvoid create_queue_dynamic(void) {\n    osMessageQueueAttr_t attr = {\n        .name = \"DynamicQueue\"\n    };\n    my_queue = osMessageQueueNew(10, sizeof(my_data_t), &amp;attr);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeueput-","title":"osMessageQueuePut - \u53d1\u9001\u6d88\u606f","text":"<pre><code>osStatus_t osMessageQueuePut(osMessageQueueId_t mq_id,\n                            const void *msg_ptr,\n                            uint8_t msg_prio,\n                            uint32_t timeout);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <ul> <li><code>mq_id</code>: \u6d88\u606f\u961f\u5217ID</li> <li><code>msg_ptr</code>: \u6307\u5411\u8981\u53d1\u9001\u7684\u6d88\u606f</li> <li><code>msg_prio</code>: \u6d88\u606f\u4f18\u5148\u7ea7\uff08\u901a\u5e38\u4e3a0\uff09</li> <li><code>timeout</code>: \u8d85\u65f6\u65f6\u95f4\uff08osWaitForever, 0, \u6216\u5177\u4f53\u6beb\u79d2\u6570\uff09</li> </ul> <p>\u4f7f\u7528\u793a\u4f8b:</p> <pre><code>typedef struct {\n    int sensor_id;\n    float value;\n    uint32_t timestamp;\n} sensor_data_t;\n\nvoid send_sensor_data(void) {\n    sensor_data_t data = {1, 25.5f, osKernelGetTickCount()};\n\n    osStatus_t status = osMessageQueuePut(sensor_queue, &amp;data, 0, osWaitForever);\n    if(status == osOK) {\n        printf(\"Data sent successfully\\n\");\n    } else {\n        printf(\"Failed to send data: %d\\n\", status);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeueget-","title":"osMessageQueueGet - \u63a5\u6536\u6d88\u606f","text":"<pre><code>osStatus_t osMessageQueueGet(osMessageQueueId_t mq_id,\n                            void *msg_ptr,\n                            uint8_t *msg_prio,\n                            uint32_t timeout);\n</code></pre> <p>\u4f7f\u7528\u793a\u4f8b:</p> <pre><code>void receive_sensor_data(void) {\n    sensor_data_t received_data;\n    uint8_t priority;\n\n    osStatus_t status = osMessageQueueGet(sensor_queue, &amp;received_data, &amp;priority, 1000);\n    if(status == osOK) {\n        printf(\"Received: Sensor%d=%.1f at %lu\\n\", \n               received_data.sensor_id, received_data.value, received_data.timestamp);\n    } else if(status == osErrorTimeout) {\n        printf(\"No data received within timeout\\n\");\n    } else {\n        printf(\"Error receiving data: %d\\n\", status);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_22","title":"\u5176\u4ed6\u961f\u5217\u7ba1\u7406\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeuegetcount","title":"osMessageQueueGetCount","text":"<pre><code>uint32_t osMessageQueueGetCount(osMessageQueueId_t mq_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeuegetspace","title":"osMessageQueueGetSpace","text":"<pre><code>uint32_t osMessageQueueGetSpace(osMessageQueueId_t mq_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeuedelete","title":"osMessageQueueDelete","text":"<pre><code>osStatus_t osMessageQueueDelete(osMessageQueueId_t mq_id);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#osmessagequeuereset","title":"osMessageQueueReset","text":"<pre><code>osStatus_t osMessageQueueReset(osMessageQueueId_t mq_id);\n</code></pre> \u64cd\u4f5c FreeRTOS CMSIS-RTOS v2 \u521b\u5efa\u961f\u5217 <code>xQueueCreate()</code> <code>osMessageQueueNew()</code> \u53d1\u9001\u961f\u5217 <code>xQueueSend()</code> <code>osMessageQueuePut()</code> \u63a5\u6536\u961f\u5217 <code>xQueueReceive()</code> <code>osMessageQueueGet()</code> \u67e5\u8be2\u6d88\u606f\u6570 <code>uxQueueMessagesWaiting()</code> <code>osMessageQueueGetCount()</code> \u67e5\u8be2\u7a7a\u95f4 <code>uxQueueSpacesAvailable()</code> <code>osMessageQueueGetSpace()</code> \u5220\u9664\u961f\u5217 <code>vQueueDelete()</code> <code>osMessageQueueDelete()</code> \u91cd\u7f6e\u961f\u5217 <code>vQueueReset()</code> <code>osMessageQueueReset()</code>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_23","title":"\u961f\u5217\u4f7f\u7528\u6700\u4f73\u5b9e\u8df5\u603b\u7ed3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_24","title":"\u961f\u5217\u8bbe\u8ba1\u539f\u5219","text":"<p>\u5408\u7406\u8bbe\u7f6e\u961f\u5217\u957f\u5ea6\uff1a - \u6839\u636e\u6570\u636e\u4ea7\u751f\u901f\u5ea6\u548c\u6d88\u8d39\u901f\u5ea6\u8ba1\u7b97 - \u8003\u8651\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6570\u636e\u79ef\u538b - \u5e73\u8861\u5185\u5b58\u4f7f\u7528\u548c\u7cfb\u7edf\u53ef\u9760\u6027</p> <p>\u9009\u62e9\u9002\u5f53\u7684\u6570\u636e\u7c7b\u578b\uff1a - \u7b80\u5355\u6570\u636e\u76f4\u63a5\u4f20\u9012\u503c - \u590d\u6742\u6570\u636e\u4f7f\u7528\u7ed3\u6784\u4f53\u5c01\u88c5 - \u5927\u6570\u636e\u91cf\u8003\u8651\u4f20\u9012\u6307\u9488\uff08\u9700\u81ea\u884c\u7ba1\u7406\u5185\u5b58\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_25","title":"\u9519\u8bef\u5904\u7406\u7b56\u7565","text":"<p>\u53d1\u9001\u5931\u8d25\u5904\u7406\uff1a</p> <pre><code>BaseType_t xResult = xQueueSend(xQueue, &amp;data, timeout);\nif(xResult != pdPASS) {\n    // \u5904\u7406\u7b56\u7565\uff1a\u91cd\u8bd5\u3001\u4e22\u5f03\u3001\u8f6c\u5b58\u7b49\n    handle_send_failure(data);\n}\n</code></pre> <p>\u63a5\u6536\u8d85\u65f6\u5904\u7406\uff1a</p> <pre><code>if(xQueueReceive(xQueue, &amp;data, timeout) != pdPASS) {\n    // \u5904\u7406\u7b56\u7565\uff1a\u4f7f\u7528\u9ed8\u8ba4\u503c\u3001\u5c1d\u8bd5\u6062\u590d\u7b49\n    handle_receive_timeout();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue/#_26","title":"\u6027\u80fd\u4f18\u5316\u5efa\u8bae","text":"<ol> <li>\u51cf\u5c11\u6570\u636e\u62f7\u8d1d\uff1a\u5bf9\u4e8e\u5927\u578b\u6570\u636e\uff0c\u8003\u8651\u4f20\u9012\u6307\u9488</li> <li>\u5408\u7406\u4f7f\u7528\u8986\u76d6\u5199\u5165\uff1a\u5bf9\u4e8e\u53ea\u9700\u8981\u6700\u65b0\u6570\u636e\u7684\u573a\u666f</li> <li>\u4f18\u5316\u961f\u5217\u957f\u5ea6\uff1a\u907f\u514d\u8fc7\u957f\u961f\u5217\u589e\u52a0\u641c\u7d22\u65f6\u95f4</li> <li>\u4f7f\u7528\u9759\u6001\u5206\u914d\uff1a\u5728\u5185\u5b58\u53d7\u9650\u7cfb\u7edf\u4e2d\u51cf\u5c11\u52a8\u6001\u5206\u914d\u5f00\u9500</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/","title":"QueueSet","text":"<p>\u961f\u5217\u96c6\u53ef\u4ee5\u4f20\u8f93\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u4fe1\u606f</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_1","title":"\u961f\u5217\u96c6\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_2","title":"\u4ec0\u4e48\u662f\u961f\u5217\u96c6","text":"<p>\u961f\u5217\u96c6\uff08Queue Set\uff09\u662f\u4e00\u79cd\u9ad8\u7ea7\u540c\u6b65\u673a\u5236\uff0c\u5141\u8bb8\u5355\u4e2a\u4efb\u52a1\u540c\u65f6\u76d1\u89c6\u591a\u4e2a\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u3002\u60f3\u8c61\u4e00\u4e2a\u76d1\u63a7\u4e2d\u5fc3\uff1a</p> <ul> <li>\u591a\u4e2a\u4f20\u611f\u5668\uff1a\u6bcf\u4e2a\u961f\u5217\u5c31\u50cf\u4e00\u4e2a\u4f20\u611f\u5668\uff0c\u4ea7\u751f\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e</li> <li>\u4e2d\u592e\u76d1\u63a7\u53f0\uff1a\u961f\u5217\u96c6\u5c31\u662f\u76d1\u63a7\u53f0\uff0c\u53ef\u4ee5\u540c\u65f6\u663e\u793a\u6240\u6709\u4f20\u611f\u5668\u7684\u72b6\u6001</li> <li>\u503c\u73ed\u4eba\u5458\uff1a\u4efb\u52a1\u5c31\u50cf\u503c\u73ed\u4eba\u5458\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u76d1\u63a7\u53f0\uff0c\u4e0d\u9700\u8981\u8f6e\u6d41\u68c0\u67e5\u6bcf\u4e2a\u4f20\u611f\u5668</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_3","title":"\u961f\u5217\u96c6\u7684\u6838\u5fc3\u4ef7\u503c","text":"<p>\u89e3\u51b3\u591a\u8def\u76d1\u542c\u95ee\u9898\uff1a\u5728\u6ca1\u6709\u961f\u5217\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u4efb\u52a1\u9700\u8981\u8f6e\u6d41\u68c0\u67e5\u591a\u4e2a\u961f\u5217\uff1a</p> <pre><code>// \u4f4e\u6548\u7684\u65b9\u5f0f\uff1a\u8f6e\u6d41\u68c0\u67e5\u6bcf\u4e2a\u961f\u5217\nvoid vTaskInefficient(void) {\n    while(1) {\n        if(xQueueReceive(xQueue1, &amp;data1, 0) == pdPASS) {\n            process_data1(data1);\n        }\n        if(xQueueReceive(xQueue2, &amp;data2, 0) == pdPASS) {\n            process_data2(data2);\n        }\n        if(xQueueReceive(xQueue3, &amp;data3, 0) == pdPASS) {\n            process_data3(data3);\n        }\n        vTaskDelay(10); // \u5fd9\u7b49\u5f85\uff0c\u6d6a\u8d39CPU\n    }\n}\n</code></pre> <p>\u961f\u5217\u96c6\u7684\u4f18\u52bf\uff1a - \u4efb\u52a1\u53ef\u4ee5\u963b\u585e\u7b49\u5f85\u4efb\u610f\u961f\u5217\u6709\u6570\u636e - \u907f\u514d\u5fd9\u7b49\u5f85\uff0c\u63d0\u9ad8CPU\u6548\u7387 - \u7b80\u5316\u591a\u961f\u5217\u76d1\u542c\u903b\u8f91</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#freertos","title":"FreeRTOS \u961f\u5217\u96c6\u7279\u6027","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_4","title":"\u8bbe\u8ba1\u7279\u70b9","text":"<p>\u7edf\u4e00\u76d1\u542c\u63a5\u53e3\uff1a\u4efb\u52a1\u53ea\u9700\u8981\u7b49\u5f85\u961f\u5217\u96c6\uff0c\u4e0d\u9700\u8981\u5173\u5fc3\u5177\u4f53\u54ea\u4e2a\u961f\u5217</p> <p>\u652f\u6301\u6df7\u5408\u7c7b\u578b\uff1a\u53ef\u4ee5\u540c\u65f6\u5305\u542b\u961f\u5217\u548c\u4fe1\u53f7\u91cf</p> <p>\u975e\u7834\u574f\u6027\u67e5\u770b\uff1a\u4ece\u961f\u5217\u96c6\u8bfb\u53d6\u4e0d\u4f1a\u79fb\u9664\u539f\u59cb\u961f\u5217\u4e2d\u7684\u6570\u636e</p> <p>\u5bb9\u91cf\u9650\u5236\uff1a\u961f\u5217\u96c6\u672c\u8eab\u6709\u5bb9\u91cf\u9650\u5236\uff0c\u9700\u8981\u5408\u7406\u8bbe\u8ba1</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_5","title":"\u9002\u7528\u573a\u666f","text":"<p>\u591a\u6570\u636e\u6e90\u5904\u7406\uff1a\u76d1\u63a7\u591a\u4e2a\u4f20\u611f\u5668\u6570\u636e\u6d41</p> <p>\u4e8b\u4ef6\u9a71\u52a8\u7cfb\u7edf\uff1a\u54cd\u5e94\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u4e8b\u4ef6</p> <p>\u534f\u8bae\u5904\u7406\uff1a\u5904\u7406\u6765\u81ea\u591a\u4e2a\u901a\u4fe1\u901a\u9053\u7684\u6570\u636e</p> <p>GUI\u4e8b\u4ef6\u5904\u7406\uff1a\u76d1\u542c\u7528\u6237\u8f93\u5165\u3001\u5b9a\u65f6\u5668\u3001\u7f51\u7edc\u4e8b\u4ef6\u7b49</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#freertos_1","title":"FreeRTOS \u961f\u5217\u96c6\u51fd\u6570\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#xqueuecreateset-","title":"xQueueCreateSet - \u521b\u5efa\u961f\u5217\u96c6","text":"<pre><code>QueueSetHandle_t xQueueCreateSet(const UBaseType_t uxEventQueueLength);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>uxEventQueueLength</code>\uff1a\u961f\u5217\u96c6\u7684\u5bb9\u91cf\uff0c\u5373\u80fd\u591f\u540c\u65f6\u8bb0\u5f55\u7684\u6700\u5927\u4e8b\u4ef6\u6570</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u961f\u5217\u96c6\u53e5\u67c4 - \u5931\u8d25\uff1aNULL</p> <p>\u5bb9\u91cf\u8bbe\u8ba1\u539f\u5219\uff1a</p> <pre><code>// \u5982\u679c\u67093\u4e2a\u961f\u5217\u9700\u8981\u76d1\u63a7\uff0c\u6bcf\u4e2a\u961f\u5217\u53ef\u80fd\u540c\u65f6\u6709\u591a\u4e2a\u6570\u636e\n// \u5efa\u8bae\u5bb9\u91cf = \u76d1\u63a7\u7684\u961f\u5217\u6570 \u00d7 \u6bcf\u4e2a\u961f\u5217\u7684\u9884\u671f\u6700\u5927\u5e76\u53d1\u6570\u636e\nQueueSetHandle_t xQueueSet = xQueueCreateSet(10); // \u5bb9\u91cf\u4e3a10\u4e2a\u4e8b\u4ef6\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#xqueueaddtoset-","title":"xQueueAddToSet - \u6dfb\u52a0\u6210\u5458\u5230\u961f\u5217\u96c6","text":"<pre><code>BaseType_t xQueueAddToSet(QueueSetMemberHandle_t xQueueOrSemaphore,\n                         QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xQueueOrSemaphore</code>\uff1a\u8981\u6dfb\u52a0\u7684\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u53e5\u67c4 - <code>xQueueSet</code>\uff1a\u76ee\u6807\u961f\u5217\u96c6\u53e5\u67c4</p> <p>\u8fd4\u56de\u503c\uff1a - <code>pdPASS</code>\uff1a\u6dfb\u52a0\u6210\u529f - <code>pdFAIL</code>\uff1a\u6dfb\u52a0\u5931\u8d25</p> <p>\u6dfb\u52a0\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u961f\u5217\u548c\u961f\u5217\u96c6\nQueueHandle_t xSensorQueue = xQueueCreate(5, sizeof(SensorData_t));\nQueueHandle_t xCommandQueue = xQueueCreate(3, sizeof(Command_t));\nQueueSetHandle_t xMainQueueSet = xQueueCreateSet(8);\n\n// \u6dfb\u52a0\u961f\u5217\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xSensorQueue, xMainQueueSet);\nxQueueAddToSet(xCommandQueue, xMainQueueSet);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#xqueueremovefromset-","title":"xQueueRemoveFromSet - \u4ece\u961f\u5217\u96c6\u4e2d\u79fb\u9664\u6210\u5458","text":"<pre><code>BaseType_t xQueueRemoveFromSet(QueueSetMemberHandle_t xQueueOrSemaphore,\n                              QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u4f7f\u7528\u573a\u666f\uff1a - \u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u7cfb\u7edf - \u4e34\u65f6\u7981\u7528\u67d0\u4e9b\u6570\u636e\u6e90 - \u7cfb\u7edf\u5173\u95ed\u65f6\u6e05\u7406\u8d44\u6e90</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#xqueueselectfromset-","title":"xQueueSelectFromSet - \u4ece\u961f\u5217\u96c6\u9009\u62e9\u5c31\u7eea\u6210\u5458","text":"<pre><code>QueueSetMemberHandle_t xQueueSelectFromSet(QueueSetHandle_t xQueueSet,\n                                          TickType_t const xTicksToWait);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xQueueSet</code>\uff1a\u8981\u76d1\u542c\u7684\u961f\u5217\u96c6 - <code>xTicksToWait</code>\uff1a\u963b\u585e\u8d85\u65f6\u65f6\u95f4</p> <p>\u8fd4\u56de\u503c\uff1a - \u975eNULL\uff1a\u5c31\u7eea\u7684\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u53e5\u67c4 - NULL\uff1a\u8d85\u65f6\u6ca1\u6709\u5c31\u7eea\u7684\u6210\u5458</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#xqueueselectfromsetfromisr-","title":"xQueueSelectFromSetFromISR - \u4e2d\u65ad\u5b89\u5168\u7248\u672c","text":"<pre><code>QueueSetMemberHandle_t xQueueSelectFromSetFromISR(QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u4e2d\u65ad\u4e2d\u4f7f\u7528\u6ce8\u610f\u4e8b\u9879\uff1a - \u6ca1\u6709\u8d85\u65f6\u53c2\u6570 - \u9700\u8981\u68c0\u67e5\u8fd4\u56de\u503c\u662f\u5426\u4e3aNULL - \u901a\u5e38\u914d\u5408<code>portYIELD_FROM_ISR</code>\u4f7f\u7528</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#freertos_2","title":"FreeRTOS \u961f\u5217\u96c6\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_6","title":"\u57fa\u672c\u4f7f\u7528\u6d41\u7a0b","text":"<pre><code>// 1. \u521b\u5efa\u961f\u5217\u96c6\u548c\u6210\u5458\u961f\u5217\nQueueSetHandle_t xQueueSet = xQueueCreateSet(10);\nQueueHandle_t xQueue1 = xQueueCreate(5, sizeof(int));\nQueueHandle_t xQueue2 = xQueueCreate(5, sizeof(float));\n\n// 2. \u6dfb\u52a0\u961f\u5217\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xQueue1, xQueueSet);\nxQueueAddToSet(xQueue2, xQueueSet);\n\n// 3. \u4efb\u52a1\u4e2d\u76d1\u542c\u961f\u5217\u96c6\nvoid vMonitorTask(void *pvParameters) {\n    QueueSetMemberHandle_t xActivatedMember;\n\n    while(1) {\n        // \u7b49\u5f85\u4efb\u610f\u961f\u5217\u6709\u6570\u636e\n        xActivatedMember = xQueueSelectFromSet(xQueueSet, portMAX_DELAY);\n\n        if(xActivatedMember == xQueue1) {\n            int data;\n            xQueueReceive(xQueue1, &amp;data, 0);\n            process_queue1_data(data);\n        }\n        else if(xActivatedMember == xQueue2) {\n            float data;\n            xQueueReceive(xQueue2, &amp;data, 0);\n            process_queue2_data(data);\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_7","title":"\u4fe1\u53f7\u91cf\u4e0e\u961f\u5217\u6df7\u5408\u76d1\u542c","text":"<pre><code>// \u521b\u5efa\u961f\u5217\u96c6\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u6210\u5458\nQueueSetHandle_t xEventSet = xQueueCreateSet(8);\nQueueHandle_t xDataQueue = xQueueCreate(5, sizeof(Data_t));\nSemaphoreHandle_t xTimerSemaphore = xSemaphoreCreateBinary();\nSemaphoreHandle_t xButtonSemaphore = xSemaphoreCreateBinary();\n\n// \u6dfb\u52a0\u6240\u6709\u6210\u5458\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xDataQueue, xEventSet);\nxQueueAddToSet(xTimerSemaphore, xEventSet);\nxQueueAddToSet(xButtonSemaphore, xEventSet);\n\n// \u7edf\u4e00\u4e8b\u4ef6\u5904\u7406\nvoid vEventHandler(void *pvParameters) {\n    QueueSetMemberHandle_t xEventSource;\n\n    while(1) {\n        xEventSource = xQueueSelectFromSet(xEventSet, portMAX_DELAY);\n\n        if(xEventSource == xDataQueue) {\n            Data_t data;\n            xQueueReceive(xDataQueue, &amp;data, 0);\n            handle_data_event(data);\n        }\n        else if(xEventSource == xTimerSemaphore) {\n            xSemaphoreTake(xTimerSemaphore, 0);\n            handle_timer_event();\n        }\n        else if(xEventSource == xButtonSemaphore) {\n            xSemaphoreTake(xButtonSemaphore, 0);\n            handle_button_event();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_8","title":"\u52a8\u6001\u6210\u5458\u7ba1\u7406","text":"<pre><code>// \u52a8\u6001\u6dfb\u52a0\u548c\u79fb\u9664\u961f\u5217\u96c6\u6210\u5458\nvoid vDynamicQueueManagement(void) {\n    QueueHandle_t xTempQueue = xQueueCreate(3, sizeof(float));\n\n    // \u4e34\u65f6\u6dfb\u52a0\u76d1\u63a7\n    if(xQueueAddToSet(xTempQueue, xMainQueueSet) == pdPASS) {\n        printf(\"Temporary queue added to set\\n\");\n\n        // \u76d1\u63a7\u4e00\u6bb5\u65f6\u95f4...\n        vTaskDelay(10000 / portTICK_PERIOD_MS);\n\n        // \u79fb\u9664\u76d1\u63a7\n        xQueueRemoveFromSet(xTempQueue, xMainQueueSet);\n    }\n\n    vQueueDelete(xTempQueue);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#cmsis-rtos-v2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#cmsis-rtos-v2_1","title":"CMSIS-RTOS v2 \u7684\u7b49\u6548\u673a\u5236","text":"<p>\u5728CMSIS-RTOS v2\u4e2d\uff0c\u961f\u5217\u96c6\u7684\u529f\u80fd\u7531\u4e8b\u4ef6\u6807\u5fd7\uff08Event Flags) \u5b9e\u73b0\u3002\u4e8b\u4ef6\u6807\u5fd7\u63d0\u4f9b\u7c7b\u4f3c\u7684\u529f\u80fd\uff0c\u4f46\u5b9e\u73b0\u65b9\u5f0f\u4e0d\u540c\uff1aCMSISv2\u63d0\u4f9bEvent_Group</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#rtos-vs-cmsis-rtos-v2","title":"RTOS \u961f\u5217\u96c6 vs CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u673a\u5236\u5bf9\u6bd4","text":"\u7279\u6027 FreeRTOS \u961f\u5217\u96c6 CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7 \u5b9e\u73b0\u57fa\u7840 \u961f\u5217\u96c6\u5408\u76d1\u542c \u4f4d\u6807\u5fd7\u64cd\u4f5c \u6570\u636e\u4f20\u9012 \u652f\u6301\u6570\u636e\u4f20\u8f93 \u4ec5\u4e8b\u4ef6\u901a\u77e5\uff0c\u65e0\u6570\u636e \u6210\u5458\u7c7b\u578b \u961f\u5217\u3001\u4fe1\u53f7\u91cf\u6df7\u5408 \u7edf\u4e00\u7684\u4e8b\u4ef6\u6807\u5fd7\u4f4d \u8d44\u6e90\u5f00\u9500 \u8f83\u9ad8\uff08\u9700\u8981\u961f\u5217\u96c6\u548c\u6210\u5458\u961f\u5217\uff09 \u8f83\u4f4e\uff08\u5355\u4e2a\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61\uff09 \u7075\u6d3b\u6027 \u52a8\u6001\u6dfb\u52a0\u79fb\u9664\u6210\u5458 \u56fa\u5b9a\u7684\u6807\u5fd7\u4f4d\u5b9a\u4e49"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_9","title":"\u9002\u7528\u573a\u666f\u5bf9\u6bd4","text":"<p>FreeRTOS\u961f\u5217\u96c6\u66f4\u9002\u5408\uff1a - \u9700\u8981\u4f20\u8f93\u5b9e\u9645\u6570\u636e\u7684\u573a\u666f - \u6df7\u5408\u7c7b\u578b\u7684\u540c\u6b65\u5bf9\u8c61\uff08\u961f\u5217+\u4fe1\u53f7\u91cf\uff09 - \u52a8\u6001\u53d8\u5316\u7684\u76d1\u542c\u96c6\u5408</p> <p>CMSIS\u4e8b\u4ef6\u6807\u5fd7\u66f4\u9002\u5408\uff1a - \u7eaf\u4e8b\u4ef6\u901a\u77e5\u573a\u666f - \u56fa\u5b9a\u7684\u4e8b\u4ef6\u7c7b\u578b\u96c6\u5408 - \u8d44\u6e90\u53d7\u9650\u7684\u7cfb\u7edf</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_10","title":"\u6027\u80fd\u8003\u8651","text":"<p>FreeRTOS\u961f\u5217\u96c6\uff1a - \u5185\u5b58\u5f00\u9500\uff1a\u961f\u5217\u96c6\u7ed3\u6784 + \u6240\u6709\u6210\u5458\u961f\u5217 - CPU\u5f00\u9500\uff1a\u9009\u62e9\u64cd\u4f5c\u9700\u8981\u904d\u5386\u6210\u5458 - \u7075\u6d3b\u6027\uff1a\u8fd0\u884c\u65f6\u52a8\u6001\u914d\u7f6e</p> <p>CMSIS\u4e8b\u4ef6\u6807\u5fd7\uff1a - \u5185\u5b58\u5f00\u9500\uff1a\u5355\u4e2a\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 - CPU\u5f00\u9500\uff1a\u4f4d\u64cd\u4f5c\uff0c\u6548\u7387\u9ad8 - \u7075\u6d3b\u6027\uff1a\u7f16\u8bd1\u65f6\u786e\u5b9a\u4e8b\u4ef6\u7c7b\u578b</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_11","title":"\u8bbe\u8ba1\u6ce8\u610f\u4e8b\u9879","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#freertos_3","title":"FreeRTOS \u961f\u5217\u96c6\u8bbe\u8ba1\u8981\u70b9","text":"<p>\u5bb9\u91cf\u89c4\u5212\uff1a</p> <pre><code>// \u9519\u8bef\uff1a\u5bb9\u91cf\u8fc7\u5c0f\u53ef\u80fd\u5bfc\u81f4\u4e8b\u4ef6\u4e22\u5931\nQueueSetHandle_t xSet = xQueueCreateSet(2); // \u592a\u5c0f\uff01\n\n// \u6b63\u786e\uff1a\u6839\u636e\u5e76\u53d1\u9700\u6c42\u8bbe\u8ba1\u5bb9\u91cf\n// \u5047\u8bbe\u67093\u4e2a\u961f\u5217\uff0c\u6bcf\u4e2a\u961f\u5217\u53ef\u80fd\u540c\u65f6\u67092\u4e2a\u6570\u636e\nQueueSetHandle_t xSet = xQueueCreateSet(3 * 2); // \u5bb9\u91cf6\n</code></pre> <p>\u6210\u5458\u7ba1\u7406\uff1a - \u4e00\u4e2a\u961f\u5217\u53ea\u80fd\u5c5e\u4e8e\u4e00\u4e2a\u961f\u5217\u96c6 - \u6dfb\u52a0\u524d\u786e\u4fdd\u961f\u5217\u5df2\u521b\u5efa - \u79fb\u9664\u524d\u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u6b63\u5728\u7b49\u5f85</p> <p>\u9519\u8bef\u5904\u7406\uff1a</p> <pre><code>QueueSetMemberHandle_t xReadyMember = xQueueSelectFromSet(xSet, timeout);\nif(xReadyMember != NULL) {\n    if(xReadyMember == xQueue1) {\n        // \u5904\u7406\u961f\u52171\u6570\u636e\n        if(xQueueReceive(xQueue1, &amp;data, 0) != pdPASS) {\n            // \u6570\u636e\u53ef\u80fd\u5df2\u88ab\u5176\u4ed6\u4efb\u52a1\u53d6\u8d70\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#cmsis-rtos-v2_2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u8bbe\u8ba1\u8981\u70b9","text":"<p>\u6807\u5fd7\u4f4d\u89c4\u5212\uff1a</p> <pre><code>// \u4f7f\u7528\u4f4d\u57df\u6e05\u6670\u5b9a\u4e49\u4e8b\u4ef6\ntypedef enum {\n    EVT_SENSOR1_READY = (1UL &lt;&lt; 0),\n    EVT_SENSOR2_READY = (1UL &lt;&lt; 1),\n    EVT_USER_INPUT   = (1UL &lt;&lt; 2),\n    EVT_SYSTEM_ERROR = (1UL &lt;&lt; 3),\n    EVT_ALL_SENSORS  = EVT_SENSOR1_READY | EVT_SENSOR2_READY\n} system_events_t;\n</code></pre> <p>\u7b49\u5f85\u7b56\u7565\uff1a</p> <pre><code>// \u7b49\u5f85\u4efb\u610f\u4f20\u611f\u5668\u6570\u636e\nflags = osEventFlagsWait(events, EVT_ALL_SENSORS, osFlagsWaitAny, timeout);\n\n// \u7b49\u5f85\u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u5c31\u7eea\nflags = osEventFlagsWait(events, EVT_ALL_SENSORS, osFlagsWaitAll, timeout);\n\n// \u7b49\u5f85\u4f46\u4e0d\u6e05\u9664\u6807\u5fd7\uff08\u7528\u4e8e\u76d1\u63a7\uff09\nflags = osEventFlagsWait(events, EVT_SYSTEM_ERROR, osFlagsWaitAny | osFlagsNoClear, timeout);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/QueueSet/#_12","title":"\u5171\u540c\u7684\u6700\u4f73\u5b9e\u8df5","text":"<p>\u8d85\u65f6\u8bbe\u7f6e\uff1a - \u5173\u952e\u4efb\u52a1\uff1a<code>portMAX_DELAY</code> / <code>osWaitForever</code> - \u975e\u5173\u952e\u4efb\u52a1\uff1a\u5408\u7406\u8d85\u65f6\uff0c\u907f\u514d\u6c38\u4e45\u963b\u585e - \u76d1\u63a7\u4efb\u52a1\uff1a\u77ed\u8d85\u65f6\uff0c\u5b9a\u671f\u6267\u884c\u5176\u4ed6\u5de5\u4f5c</p> <p>\u8d44\u6e90\u6e05\u7406\uff1a - \u5220\u9664\u524d\u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u5728\u7b49\u5f85 - \u6309\u521b\u5efa\u987a\u5e8f\u9006\u5e8f\u5220\u9664 - \u5904\u7406\u5220\u9664\u5931\u8d25\u7684\u60c5\u51b5</p> <p>\u9519\u8bef\u6062\u590d\uff1a - \u68c0\u67e5\u6240\u6709API\u8fd4\u56de\u503c - \u5b9e\u73b0\u4f18\u96c5\u964d\u7ea7\u7b56\u7565 - \u8bb0\u5f55\u9519\u8bef\u4fe1\u606f\u7528\u4e8e\u8c03\u8bd5</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/","title":"Queue Set","text":"<p>\u961f\u5217\u96c6\u53ef\u4ee5\u4f20\u8f93\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u4fe1\u606f</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_1","title":"\u961f\u5217\u96c6\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_2","title":"\u4ec0\u4e48\u662f\u961f\u5217\u96c6","text":"<p>\u961f\u5217\u96c6\uff08Queue Set\uff09\u662f\u4e00\u79cd\u9ad8\u7ea7\u540c\u6b65\u673a\u5236\uff0c\u5141\u8bb8\u5355\u4e2a\u4efb\u52a1\u540c\u65f6\u76d1\u89c6\u591a\u4e2a\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u3002\u60f3\u8c61\u4e00\u4e2a\u76d1\u63a7\u4e2d\u5fc3\uff1a</p> <ul> <li>\u591a\u4e2a\u4f20\u611f\u5668\uff1a\u6bcf\u4e2a\u961f\u5217\u5c31\u50cf\u4e00\u4e2a\u4f20\u611f\u5668\uff0c\u4ea7\u751f\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e</li> <li>\u4e2d\u592e\u76d1\u63a7\u53f0\uff1a\u961f\u5217\u96c6\u5c31\u662f\u76d1\u63a7\u53f0\uff0c\u53ef\u4ee5\u540c\u65f6\u663e\u793a\u6240\u6709\u4f20\u611f\u5668\u7684\u72b6\u6001</li> <li>\u503c\u73ed\u4eba\u5458\uff1a\u4efb\u52a1\u5c31\u50cf\u503c\u73ed\u4eba\u5458\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u76d1\u63a7\u53f0\uff0c\u4e0d\u9700\u8981\u8f6e\u6d41\u68c0\u67e5\u6bcf\u4e2a\u4f20\u611f\u5668</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_3","title":"\u961f\u5217\u96c6\u7684\u6838\u5fc3\u4ef7\u503c","text":"<p>\u89e3\u51b3\u591a\u8def\u76d1\u542c\u95ee\u9898\uff1a\u5728\u6ca1\u6709\u961f\u5217\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u4efb\u52a1\u9700\u8981\u8f6e\u6d41\u68c0\u67e5\u591a\u4e2a\u961f\u5217\uff1a</p> <pre><code>// \u4f4e\u6548\u7684\u65b9\u5f0f\uff1a\u8f6e\u6d41\u68c0\u67e5\u6bcf\u4e2a\u961f\u5217\nvoid vTaskInefficient(void) {\n    while(1) {\n        if(xQueueReceive(xQueue1, &amp;data1, 0) == pdPASS) {\n            process_data1(data1);\n        }\n        if(xQueueReceive(xQueue2, &amp;data2, 0) == pdPASS) {\n            process_data2(data2);\n        }\n        if(xQueueReceive(xQueue3, &amp;data3, 0) == pdPASS) {\n            process_data3(data3);\n        }\n        vTaskDelay(10); // \u5fd9\u7b49\u5f85\uff0c\u6d6a\u8d39CPU\n    }\n}\n</code></pre> <p>\u961f\u5217\u96c6\u7684\u4f18\u52bf\uff1a - \u4efb\u52a1\u53ef\u4ee5\u963b\u585e\u7b49\u5f85\u4efb\u610f\u961f\u5217\u6709\u6570\u636e - \u907f\u514d\u5fd9\u7b49\u5f85\uff0c\u63d0\u9ad8CPU\u6548\u7387 - \u7b80\u5316\u591a\u961f\u5217\u76d1\u542c\u903b\u8f91</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#freertos","title":"FreeRTOS \u961f\u5217\u96c6\u7279\u6027","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_4","title":"\u8bbe\u8ba1\u7279\u70b9","text":"<p>\u7edf\u4e00\u76d1\u542c\u63a5\u53e3\uff1a\u4efb\u52a1\u53ea\u9700\u8981\u7b49\u5f85\u961f\u5217\u96c6\uff0c\u4e0d\u9700\u8981\u5173\u5fc3\u5177\u4f53\u54ea\u4e2a\u961f\u5217</p> <p>\u652f\u6301\u6df7\u5408\u7c7b\u578b\uff1a\u53ef\u4ee5\u540c\u65f6\u5305\u542b\u961f\u5217\u548c\u4fe1\u53f7\u91cf</p> <p>\u975e\u7834\u574f\u6027\u67e5\u770b\uff1a\u4ece\u961f\u5217\u96c6\u8bfb\u53d6\u4e0d\u4f1a\u79fb\u9664\u539f\u59cb\u961f\u5217\u4e2d\u7684\u6570\u636e</p> <p>\u5bb9\u91cf\u9650\u5236\uff1a\u961f\u5217\u96c6\u672c\u8eab\u6709\u5bb9\u91cf\u9650\u5236\uff0c\u9700\u8981\u5408\u7406\u8bbe\u8ba1</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_5","title":"\u9002\u7528\u573a\u666f","text":"<p>\u591a\u6570\u636e\u6e90\u5904\u7406\uff1a\u76d1\u63a7\u591a\u4e2a\u4f20\u611f\u5668\u6570\u636e\u6d41</p> <p>\u4e8b\u4ef6\u9a71\u52a8\u7cfb\u7edf\uff1a\u54cd\u5e94\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u4e8b\u4ef6</p> <p>\u534f\u8bae\u5904\u7406\uff1a\u5904\u7406\u6765\u81ea\u591a\u4e2a\u901a\u4fe1\u901a\u9053\u7684\u6570\u636e</p> <p>GUI\u4e8b\u4ef6\u5904\u7406\uff1a\u76d1\u542c\u7528\u6237\u8f93\u5165\u3001\u5b9a\u65f6\u5668\u3001\u7f51\u7edc\u4e8b\u4ef6\u7b49</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#freertos_1","title":"FreeRTOS \u961f\u5217\u96c6\u51fd\u6570\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#xqueuecreateset-","title":"xQueueCreateSet - \u521b\u5efa\u961f\u5217\u96c6","text":"<pre><code>QueueSetHandle_t xQueueCreateSet(const UBaseType_t uxEventQueueLength);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>uxEventQueueLength</code>\uff1a\u961f\u5217\u96c6\u7684\u5bb9\u91cf\uff0c\u5373\u80fd\u591f\u540c\u65f6\u8bb0\u5f55\u7684\u6700\u5927\u4e8b\u4ef6\u6570</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u961f\u5217\u96c6\u53e5\u67c4 - \u5931\u8d25\uff1aNULL</p> <p>\u5bb9\u91cf\u8bbe\u8ba1\u539f\u5219\uff1a</p> <pre><code>// \u5982\u679c\u67093\u4e2a\u961f\u5217\u9700\u8981\u76d1\u63a7\uff0c\u6bcf\u4e2a\u961f\u5217\u53ef\u80fd\u540c\u65f6\u6709\u591a\u4e2a\u6570\u636e\n// \u5efa\u8bae\u5bb9\u91cf = \u76d1\u63a7\u7684\u961f\u5217\u6570 \u00d7 \u6bcf\u4e2a\u961f\u5217\u7684\u9884\u671f\u6700\u5927\u5e76\u53d1\u6570\u636e\nQueueSetHandle_t xQueueSet = xQueueCreateSet(10); // \u5bb9\u91cf\u4e3a10\u4e2a\u4e8b\u4ef6\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#xqueueaddtoset-","title":"xQueueAddToSet - \u6dfb\u52a0\u6210\u5458\u5230\u961f\u5217\u96c6","text":"<pre><code>BaseType_t xQueueAddToSet(QueueSetMemberHandle_t xQueueOrSemaphore,\n                         QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xQueueOrSemaphore</code>\uff1a\u8981\u6dfb\u52a0\u7684\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u53e5\u67c4 - <code>xQueueSet</code>\uff1a\u76ee\u6807\u961f\u5217\u96c6\u53e5\u67c4</p> <p>\u8fd4\u56de\u503c\uff1a - <code>pdPASS</code>\uff1a\u6dfb\u52a0\u6210\u529f - <code>pdFAIL</code>\uff1a\u6dfb\u52a0\u5931\u8d25</p> <p>\u6dfb\u52a0\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u961f\u5217\u548c\u961f\u5217\u96c6\nQueueHandle_t xSensorQueue = xQueueCreate(5, sizeof(SensorData_t));\nQueueHandle_t xCommandQueue = xQueueCreate(3, sizeof(Command_t));\nQueueSetHandle_t xMainQueueSet = xQueueCreateSet(8);\n\n// \u6dfb\u52a0\u961f\u5217\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xSensorQueue, xMainQueueSet);\nxQueueAddToSet(xCommandQueue, xMainQueueSet);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#xqueueremovefromset-","title":"xQueueRemoveFromSet - \u4ece\u961f\u5217\u96c6\u4e2d\u79fb\u9664\u6210\u5458","text":"<pre><code>BaseType_t xQueueRemoveFromSet(QueueSetMemberHandle_t xQueueOrSemaphore,\n                              QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u4f7f\u7528\u573a\u666f\uff1a - \u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u7cfb\u7edf - \u4e34\u65f6\u7981\u7528\u67d0\u4e9b\u6570\u636e\u6e90 - \u7cfb\u7edf\u5173\u95ed\u65f6\u6e05\u7406\u8d44\u6e90</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#xqueueselectfromset-","title":"xQueueSelectFromSet - \u4ece\u961f\u5217\u96c6\u9009\u62e9\u5c31\u7eea\u6210\u5458","text":"<pre><code>QueueSetMemberHandle_t xQueueSelectFromSet(QueueSetHandle_t xQueueSet,\n                                          TickType_t const xTicksToWait);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xQueueSet</code>\uff1a\u8981\u76d1\u542c\u7684\u961f\u5217\u96c6 - <code>xTicksToWait</code>\uff1a\u963b\u585e\u8d85\u65f6\u65f6\u95f4</p> <p>\u8fd4\u56de\u503c\uff1a - \u975eNULL\uff1a\u5c31\u7eea\u7684\u961f\u5217\u6216\u4fe1\u53f7\u91cf\u53e5\u67c4 - NULL\uff1a\u8d85\u65f6\u6ca1\u6709\u5c31\u7eea\u7684\u6210\u5458</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#xqueueselectfromsetfromisr-","title":"xQueueSelectFromSetFromISR - \u4e2d\u65ad\u5b89\u5168\u7248\u672c","text":"<pre><code>QueueSetMemberHandle_t xQueueSelectFromSetFromISR(QueueSetHandle_t xQueueSet);\n</code></pre> <p>\u4e2d\u65ad\u4e2d\u4f7f\u7528\u6ce8\u610f\u4e8b\u9879\uff1a - \u6ca1\u6709\u8d85\u65f6\u53c2\u6570 - \u9700\u8981\u68c0\u67e5\u8fd4\u56de\u503c\u662f\u5426\u4e3aNULL - \u901a\u5e38\u914d\u5408<code>portYIELD_FROM_ISR</code>\u4f7f\u7528</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#freertos_2","title":"FreeRTOS \u961f\u5217\u96c6\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_6","title":"\u57fa\u672c\u4f7f\u7528\u6d41\u7a0b","text":"<pre><code>// 1. \u521b\u5efa\u961f\u5217\u96c6\u548c\u6210\u5458\u961f\u5217\nQueueSetHandle_t xQueueSet = xQueueCreateSet(10);\nQueueHandle_t xQueue1 = xQueueCreate(5, sizeof(int));\nQueueHandle_t xQueue2 = xQueueCreate(5, sizeof(float));\n\n// 2. \u6dfb\u52a0\u961f\u5217\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xQueue1, xQueueSet);\nxQueueAddToSet(xQueue2, xQueueSet);\n\n// 3. \u4efb\u52a1\u4e2d\u76d1\u542c\u961f\u5217\u96c6\nvoid vMonitorTask(void *pvParameters) {\n    QueueSetMemberHandle_t xActivatedMember;\n\n    while(1) {\n        // \u7b49\u5f85\u4efb\u610f\u961f\u5217\u6709\u6570\u636e\n        xActivatedMember = xQueueSelectFromSet(xQueueSet, portMAX_DELAY);\n\n        if(xActivatedMember == xQueue1) {\n            int data;\n            xQueueReceive(xQueue1, &amp;data, 0);\n            process_queue1_data(data);\n        }\n        else if(xActivatedMember == xQueue2) {\n            float data;\n            xQueueReceive(xQueue2, &amp;data, 0);\n            process_queue2_data(data);\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_7","title":"\u4fe1\u53f7\u91cf\u4e0e\u961f\u5217\u6df7\u5408\u76d1\u542c","text":"<pre><code>// \u521b\u5efa\u961f\u5217\u96c6\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u6210\u5458\nQueueSetHandle_t xEventSet = xQueueCreateSet(8);\nQueueHandle_t xDataQueue = xQueueCreate(5, sizeof(Data_t));\nSemaphoreHandle_t xTimerSemaphore = xSemaphoreCreateBinary();\nSemaphoreHandle_t xButtonSemaphore = xSemaphoreCreateBinary();\n\n// \u6dfb\u52a0\u6240\u6709\u6210\u5458\u5230\u961f\u5217\u96c6\nxQueueAddToSet(xDataQueue, xEventSet);\nxQueueAddToSet(xTimerSemaphore, xEventSet);\nxQueueAddToSet(xButtonSemaphore, xEventSet);\n\n// \u7edf\u4e00\u4e8b\u4ef6\u5904\u7406\nvoid vEventHandler(void *pvParameters) {\n    QueueSetMemberHandle_t xEventSource;\n\n    while(1) {\n        xEventSource = xQueueSelectFromSet(xEventSet, portMAX_DELAY);\n\n        if(xEventSource == xDataQueue) {\n            Data_t data;\n            xQueueReceive(xDataQueue, &amp;data, 0);\n            handle_data_event(data);\n        }\n        else if(xEventSource == xTimerSemaphore) {\n            xSemaphoreTake(xTimerSemaphore, 0);\n            handle_timer_event();\n        }\n        else if(xEventSource == xButtonSemaphore) {\n            xSemaphoreTake(xButtonSemaphore, 0);\n            handle_button_event();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_8","title":"\u52a8\u6001\u6210\u5458\u7ba1\u7406","text":"<pre><code>// \u52a8\u6001\u6dfb\u52a0\u548c\u79fb\u9664\u961f\u5217\u96c6\u6210\u5458\nvoid vDynamicQueueManagement(void) {\n    QueueHandle_t xTempQueue = xQueueCreate(3, sizeof(float));\n\n    // \u4e34\u65f6\u6dfb\u52a0\u76d1\u63a7\n    if(xQueueAddToSet(xTempQueue, xMainQueueSet) == pdPASS) {\n        printf(\"Temporary queue added to set\\n\");\n\n        // \u76d1\u63a7\u4e00\u6bb5\u65f6\u95f4...\n        vTaskDelay(10000 / portTICK_PERIOD_MS);\n\n        // \u79fb\u9664\u76d1\u63a7\n        xQueueRemoveFromSet(xTempQueue, xMainQueueSet);\n    }\n\n    vQueueDelete(xTempQueue);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#cmsis-rtos-v2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#cmsis-rtos-v2_1","title":"CMSIS-RTOS v2 \u7684\u7b49\u6548\u673a\u5236","text":"<p>\u5728CMSIS-RTOS v2\u4e2d\uff0c\u961f\u5217\u96c6\u7684\u529f\u80fd\u7531\u4e8b\u4ef6\u6807\u5fd7\uff08Event Flags) \u5b9e\u73b0\u3002\u4e8b\u4ef6\u6807\u5fd7\u63d0\u4f9b\u7c7b\u4f3c\u7684\u529f\u80fd\uff0c\u4f46\u5b9e\u73b0\u65b9\u5f0f\u4e0d\u540c\uff1aCMSISv2\u63d0\u4f9bEvent_Group</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#rtos-vs-cmsis-rtos-v2","title":"RTOS \u961f\u5217\u96c6 vs CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u673a\u5236\u5bf9\u6bd4","text":"\u7279\u6027 FreeRTOS \u961f\u5217\u96c6 CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7 \u5b9e\u73b0\u57fa\u7840 \u961f\u5217\u96c6\u5408\u76d1\u542c \u4f4d\u6807\u5fd7\u64cd\u4f5c \u6570\u636e\u4f20\u9012 \u652f\u6301\u6570\u636e\u4f20\u8f93 \u4ec5\u4e8b\u4ef6\u901a\u77e5\uff0c\u65e0\u6570\u636e \u6210\u5458\u7c7b\u578b \u961f\u5217\u3001\u4fe1\u53f7\u91cf\u6df7\u5408 \u7edf\u4e00\u7684\u4e8b\u4ef6\u6807\u5fd7\u4f4d \u8d44\u6e90\u5f00\u9500 \u8f83\u9ad8\uff08\u9700\u8981\u961f\u5217\u96c6\u548c\u6210\u5458\u961f\u5217\uff09 \u8f83\u4f4e\uff08\u5355\u4e2a\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61\uff09 \u7075\u6d3b\u6027 \u52a8\u6001\u6dfb\u52a0\u79fb\u9664\u6210\u5458 \u56fa\u5b9a\u7684\u6807\u5fd7\u4f4d\u5b9a\u4e49"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_9","title":"\u9002\u7528\u573a\u666f\u5bf9\u6bd4","text":"<p>FreeRTOS\u961f\u5217\u96c6\u66f4\u9002\u5408\uff1a - \u9700\u8981\u4f20\u8f93\u5b9e\u9645\u6570\u636e\u7684\u573a\u666f - \u6df7\u5408\u7c7b\u578b\u7684\u540c\u6b65\u5bf9\u8c61\uff08\u961f\u5217+\u4fe1\u53f7\u91cf\uff09 - \u52a8\u6001\u53d8\u5316\u7684\u76d1\u542c\u96c6\u5408</p> <p>CMSIS\u4e8b\u4ef6\u6807\u5fd7\u66f4\u9002\u5408\uff1a - \u7eaf\u4e8b\u4ef6\u901a\u77e5\u573a\u666f - \u56fa\u5b9a\u7684\u4e8b\u4ef6\u7c7b\u578b\u96c6\u5408 - \u8d44\u6e90\u53d7\u9650\u7684\u7cfb\u7edf</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_10","title":"\u6027\u80fd\u8003\u8651","text":"<p>FreeRTOS\u961f\u5217\u96c6\uff1a - \u5185\u5b58\u5f00\u9500\uff1a\u961f\u5217\u96c6\u7ed3\u6784 + \u6240\u6709\u6210\u5458\u961f\u5217 - CPU\u5f00\u9500\uff1a\u9009\u62e9\u64cd\u4f5c\u9700\u8981\u904d\u5386\u6210\u5458 - \u7075\u6d3b\u6027\uff1a\u8fd0\u884c\u65f6\u52a8\u6001\u914d\u7f6e</p> <p>CMSIS\u4e8b\u4ef6\u6807\u5fd7\uff1a - \u5185\u5b58\u5f00\u9500\uff1a\u5355\u4e2a\u4e8b\u4ef6\u6807\u5fd7\u5bf9\u8c61 - CPU\u5f00\u9500\uff1a\u4f4d\u64cd\u4f5c\uff0c\u6548\u7387\u9ad8 - \u7075\u6d3b\u6027\uff1a\u7f16\u8bd1\u65f6\u786e\u5b9a\u4e8b\u4ef6\u7c7b\u578b</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_11","title":"\u8bbe\u8ba1\u6ce8\u610f\u4e8b\u9879","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#freertos_3","title":"FreeRTOS \u961f\u5217\u96c6\u8bbe\u8ba1\u8981\u70b9","text":"<p>\u5bb9\u91cf\u89c4\u5212\uff1a</p> <pre><code>// \u9519\u8bef\uff1a\u5bb9\u91cf\u8fc7\u5c0f\u53ef\u80fd\u5bfc\u81f4\u4e8b\u4ef6\u4e22\u5931\nQueueSetHandle_t xSet = xQueueCreateSet(2); // \u592a\u5c0f\uff01\n\n// \u6b63\u786e\uff1a\u6839\u636e\u5e76\u53d1\u9700\u6c42\u8bbe\u8ba1\u5bb9\u91cf\n// \u5047\u8bbe\u67093\u4e2a\u961f\u5217\uff0c\u6bcf\u4e2a\u961f\u5217\u53ef\u80fd\u540c\u65f6\u67092\u4e2a\u6570\u636e\nQueueSetHandle_t xSet = xQueueCreateSet(3 * 2); // \u5bb9\u91cf6\n</code></pre> <p>\u6210\u5458\u7ba1\u7406\uff1a - \u4e00\u4e2a\u961f\u5217\u53ea\u80fd\u5c5e\u4e8e\u4e00\u4e2a\u961f\u5217\u96c6 - \u6dfb\u52a0\u524d\u786e\u4fdd\u961f\u5217\u5df2\u521b\u5efa - \u79fb\u9664\u524d\u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u6b63\u5728\u7b49\u5f85</p> <p>\u9519\u8bef\u5904\u7406\uff1a</p> <pre><code>QueueSetMemberHandle_t xReadyMember = xQueueSelectFromSet(xSet, timeout);\nif(xReadyMember != NULL) {\n    if(xReadyMember == xQueue1) {\n        // \u5904\u7406\u961f\u52171\u6570\u636e\n        if(xQueueReceive(xQueue1, &amp;data, 0) != pdPASS) {\n            // \u6570\u636e\u53ef\u80fd\u5df2\u88ab\u5176\u4ed6\u4efb\u52a1\u53d6\u8d70\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#cmsis-rtos-v2_2","title":"CMSIS-RTOS v2 \u4e8b\u4ef6\u6807\u5fd7\u8bbe\u8ba1\u8981\u70b9","text":"<p>\u6807\u5fd7\u4f4d\u89c4\u5212\uff1a</p> <pre><code>// \u4f7f\u7528\u4f4d\u57df\u6e05\u6670\u5b9a\u4e49\u4e8b\u4ef6\ntypedef enum {\n    EVT_SENSOR1_READY = (1UL &lt;&lt; 0),\n    EVT_SENSOR2_READY = (1UL &lt;&lt; 1),\n    EVT_USER_INPUT   = (1UL &lt;&lt; 2),\n    EVT_SYSTEM_ERROR = (1UL &lt;&lt; 3),\n    EVT_ALL_SENSORS  = EVT_SENSOR1_READY | EVT_SENSOR2_READY\n} system_events_t;\n</code></pre> <p>\u7b49\u5f85\u7b56\u7565\uff1a</p> <pre><code>// \u7b49\u5f85\u4efb\u610f\u4f20\u611f\u5668\u6570\u636e\nflags = osEventFlagsWait(events, EVT_ALL_SENSORS, osFlagsWaitAny, timeout);\n\n// \u7b49\u5f85\u6240\u6709\u4f20\u611f\u5668\u6570\u636e\u5c31\u7eea\nflags = osEventFlagsWait(events, EVT_ALL_SENSORS, osFlagsWaitAll, timeout);\n\n// \u7b49\u5f85\u4f46\u4e0d\u6e05\u9664\u6807\u5fd7\uff08\u7528\u4e8e\u76d1\u63a7\uff09\nflags = osEventFlagsWait(events, EVT_SYSTEM_ERROR, osFlagsWaitAny | osFlagsNoClear, timeout);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Queue_Set/#_12","title":"\u5171\u540c\u7684\u6700\u4f73\u5b9e\u8df5","text":"<p>\u8d85\u65f6\u8bbe\u7f6e\uff1a - \u5173\u952e\u4efb\u52a1\uff1a<code>portMAX_DELAY</code> / <code>osWaitForever</code> - \u975e\u5173\u952e\u4efb\u52a1\uff1a\u5408\u7406\u8d85\u65f6\uff0c\u907f\u514d\u6c38\u4e45\u963b\u585e - \u76d1\u63a7\u4efb\u52a1\uff1a\u77ed\u8d85\u65f6\uff0c\u5b9a\u671f\u6267\u884c\u5176\u4ed6\u5de5\u4f5c</p> <p>\u8d44\u6e90\u6e05\u7406\uff1a - \u5220\u9664\u524d\u786e\u4fdd\u6ca1\u6709\u4efb\u52a1\u5728\u7b49\u5f85 - \u6309\u521b\u5efa\u987a\u5e8f\u9006\u5e8f\u5220\u9664 - \u5904\u7406\u5220\u9664\u5931\u8d25\u7684\u60c5\u51b5</p> <p>\u9519\u8bef\u6062\u590d\uff1a - \u68c0\u67e5\u6240\u6709API\u8fd4\u56de\u503c - \u5b9e\u73b0\u4f18\u96c5\u964d\u7ea7\u7b56\u7565 - \u8bb0\u5f55\u9519\u8bef\u4fe1\u606f\u7528\u4e8e\u8c03\u8bd5</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/","title":"Scheduling","text":"<ul> <li>\u542f\u52a8\u4efb\u52a1\u8c03\u5ea6\u5668<ul> <li>CMSIS\u7ea7\u522b\u64cd\u4f5c<ul> <li>SVC_Setup()<ul> <li>\u8be6\u7ec6\u89e3\u91ca\uff1a<ul> <li>1. SVC \u5f02\u5e38\u7684\u4f5c\u7528</li> <li>2. \u4e3a\u4ec0\u4e48\u8981\u8bbe\u7f6e SVC \u4f18\u5148\u7ea7</li> <li>3. SVC_Setup() \u7684\u5177\u4f53\u5de5\u4f5c</li> <li>4. \u5728 osKernelStart() \u4e2d\u7684\u610f\u4e49</li> </ul> </li> </ul> </li> </ul> </li> <li>FreeRTOS\u7ea7\u522b\u64cd\u4f5c<ul> <li>vTaskStartScheduler()</li> <li>xPortStartScheduler()<ul> <li>\u51fd\u6570\u6982\u8ff0</li> <li>\u4ee3\u7801\u5206\u6790<ul> <li>1. \u5f02\u5e38\u4f18\u5148\u7ea7\u914d\u7f6e</li> <li>2. \u7cfb\u7edf\u5b9a\u65f6\u5668\u521d\u59cb\u5316</li> <li>3. \u8c03\u5ea6\u5668\u72b6\u6001\u521d\u59cb\u5316</li> <li>4. \u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1</li> </ul> </li> <li>\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1\u7684\u6838\u5fc3\u673a\u5236<ul> <li>1. \u5806\u6808\u6307\u9488\u521d\u59cb\u5316</li> <li>2. \u4e2d\u65ad\u4f7f\u80fd</li> <li>3. \u89e6\u53d1 SVC \u5f02\u5e38\u542f\u52a8\u4efb\u52a1</li> </ul> </li> <li>SVC \u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u7684\u5173\u952e\u64cd\u4f5c<ul> <li>1. \u4efb\u52a1\u4e0a\u4e0b\u6587\u6062\u590d</li> <li>2. \u5207\u6362\u5230\u4efb\u52a1\u6a21\u5f0f</li> </ul> </li> <li>\u8bbe\u8ba1\u539f\u7406\u5206\u6790<ul> <li>1. \u4e3a\u4ec0\u4e48\u4f7f\u7528 SVC \u5f02\u5e38\u542f\u52a8\u4efb\u52a1\uff1f</li> <li>2. \u4f18\u5148\u7ea7\u914d\u7f6e\u7684\u91cd\u8981\u6027</li> <li>3. \u72b6\u6001\u673a\u8f6c\u6362</li> </ul> </li> <li>\u9519\u8bef\u5904\u7406\u673a\u5236<ul> <li>\u542f\u52a8\u5931\u8d25\u7684\u53ef\u80fd\u539f\u56e0</li> <li>\u8fd4\u56de\u503c\u8bed\u4e49</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>\u4efb\u52a1\u5207\u6362<ul> <li>\u57fa\u672c\u6982\u5ff5</li> <li>\u7c7b\u6bd4\u7406\u89e3</li> <li>\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u6df1\u5165\u7406\u89e3<ul> <li>\u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587</li> <li>\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u8fc7\u7a0b</li> </ul> </li> <li>FreeRTOS \u4efb\u52a1\u5207\u6362\u7684\u89e6\u53d1\u6761\u4ef6<ul> <li>\u4e3b\u52a8\u89e6\u53d1\u65b9\u5f0f</li> <li>\u88ab\u52a8\u89e6\u53d1\u65b9\u5f0f</li> </ul> </li> <li>FreeRTOS \u4efb\u52a1\u5207\u6362\u7684\u786c\u4ef6\u673a\u5236<ul> <li>ARM Cortex-M \u5f02\u5e38\u7cfb\u7edf</li> <li>\u4f18\u5148\u7ea7\u914d\u7f6e\u7b56\u7565</li> </ul> </li> <li>\u4efb\u52a1\u5207\u6362\u7684\u8be6\u7ec6\u6267\u884c\u6d41\u7a0b<ul> <li>\u5b8c\u6574\u7684\u5207\u6362\u5e8f\u5217</li> </ul> </li> <li>\u4efb\u52a1\u63a7\u5236\u5757\uff08TCB\uff09\u7684\u5173\u952e\u4f5c\u7528<ul> <li>TCB \u6570\u636e\u7ed3\u6784</li> <li>\u5806\u6808\u5e03\u5c40\u8bbe\u8ba1</li> </ul> </li> <li>\u65f6\u95f4\u7247\u8f6e\u8f6c\u8c03\u5ea6\u539f\u7406<ul> <li>\u540c\u4f18\u5148\u7ea7\u4efb\u52a1\u8c03\u5ea6</li> </ul> </li> <li>\u6027\u80fd\u4f18\u5316\u6280\u672f<ul> <li>\u5feb\u901f\u4efb\u52a1\u9009\u62e9\u7b97\u6cd5</li> <li>\u4e2d\u65ad\u5ef6\u8fdf\u4f18\u5316</li> </ul> </li> <li>\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5206\u6790<ul> <li>\u573a\u666f\u4e00\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u62a2\u5360</li> <li>\u573a\u666f\u4e8c\uff1a\u8d44\u6e90\u5171\u4eab\u4e0e\u540c\u6b65</li> </ul> </li> <li>\u8c03\u8bd5\u4e0e\u6027\u80fd\u5206\u6790<ul> <li>\u4e0a\u4e0b\u6587\u5207\u6362\u5f00\u9500\u6d4b\u91cf</li> <li>\u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848</li> </ul> </li> </ul> </li> <li>\u5176\u4ed6API\u51fd\u6570<ul> <li>FreeRTOS</li> <li>CMSIS-RTOS v2</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_1","title":"\u542f\u52a8\u4efb\u52a1\u8c03\u5ea6\u5668","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#cmsis","title":"CMSIS\u7ea7\u522b\u64cd\u4f5c","text":"<pre><code>osStatus_t osKernelStart (void) {\n  osStatus_t stat;\n\n  if (IS_IRQ()) {\n    stat = osErrorISR;\n  }\n  else {\n    if (KernelState == osKernelReady) {\n      /* Ensure SVC priority is at the reset value */\n      SVC_Setup();\n      /* Change state to enable IRQ masking check */\n      KernelState = osKernelRunning;\n      /* Start the kernel scheduler */\n      vTaskStartScheduler();\n      stat = osOK;\n    } else {\n      stat = osError;\n    }\n  }\n\n  return (stat);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#svc_setup","title":"<code>SVC_Setup()</code>","text":"<p>\u8fd9\u4e00\u53e5 <code>SVC_Setup();</code> \u5728 CMSIS-RTOS2 \u7684 <code>osKernelStart()</code> \u51fd\u6570\u4e2d\uff0c\u4e3b\u8981\u76ee\u7684\u662f\u786e\u4fdd SVC\uff08Supervisor Call\uff09\u5f02\u5e38\u7684\u4f18\u5148\u7ea7\u88ab\u8bbe\u7f6e\u4e3a\u590d\u4f4d\u540e\u7684\u9ed8\u8ba4\u503c\u3002 \u5173\u4e8e\u5f02\u5e38\u53ef\u770bInterrupt_Mgmt</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_2","title":"\u8be6\u7ec6\u89e3\u91ca\uff1a","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#1-svc","title":"1. SVC \u5f02\u5e38\u7684\u4f5c\u7528","text":"<ul> <li>SVC \u662f ARM Cortex-M \u5904\u7406\u5668\u7684\u4e00\u4e2a\u7cfb\u7edf\u5f02\u5e38</li> <li>RTOS \u4f7f\u7528 SVC \u5f02\u5e38\u6765\u5b9e\u73b0\u4ece\u7528\u6237\u6a21\u5f0f\uff08\u7ebf\u7a0b\u6a21\u5f0f\uff09\u5230\u7279\u6743\u6a21\u5f0f\uff08\u5904\u7406\u5668\u6a21\u5f0f\uff09\u7684\u5b89\u5168\u5207\u6362</li> <li>\u7cfb\u7edf\u8c03\u7528\uff08\u5982\u521b\u5efa\u7ebf\u7a0b\u3001\u4fe1\u53f7\u91cf\u64cd\u4f5c\u7b49\uff09\u901a\u5e38\u901a\u8fc7 SVC \u6307\u4ee4\u89e6\u53d1</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#2-svc","title":"2. \u4e3a\u4ec0\u4e48\u8981\u8bbe\u7f6e SVC \u4f18\u5148\u7ea7","text":"<ul> <li>\u5728\u7cfb\u7edf\u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u67d0\u4e9b\u521d\u59cb\u5316\u4ee3\u7801\u6216\u7b2c\u4e09\u65b9\u5e93\u53ef\u80fd\u4f1a\u4fee\u6539 SVC \u7684\u4f18\u5148\u7ea7</li> <li>\u5982\u679c SVC \u4f18\u5148\u7ea7\u8bbe\u7f6e\u4e0d\u5f53\uff0c\u53ef\u80fd\u5bfc\u81f4\uff1a</li> <li>\u7cfb\u7edf\u8c03\u7528\u65e0\u6cd5\u6b63\u786e\u6267\u884c</li> <li>\u4efb\u52a1\u8c03\u5ea6\u51fa\u73b0\u95ee\u9898</li> <li>\u7cfb\u7edf\u7a33\u5b9a\u6027\u964d\u4f4e</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#3-svc_setup","title":"3. <code>SVC_Setup()</code> \u7684\u5177\u4f53\u5de5\u4f5c","text":"<pre><code>static void SVC_Setup (void) {\n  #if   (defined(__ARM_ARCH_7M__) &amp;&amp; (__ARM_ARCH_7M__ != 0)) || \\\n        (defined(__ARM_ARCH_7EM__) &amp;&amp; (__ARM_ARCH_7EM__ != 0))\n  SCB-&gt;SHCSR |= SCB_SHCSR_SVCALLUSED_Msk;  // \u542f\u7528 SVC \u5f02\u5e38\n  NVIC_SetPriority(SVCall_IRQn, 0xFE);     // \u8bbe\u7f6e\u8f83\u4f4e\u7684\u4f18\u5148\u7ea7\n  #endif\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#4-oskernelstart","title":"4. \u5728 <code>osKernelStart()</code> \u4e2d\u7684\u610f\u4e49","text":"<ul> <li>\u786e\u4fdd\u5185\u6838\u542f\u52a8\u524d SVC \u5f02\u5e38\u5904\u4e8e\u5df2\u77e5\u7684\u3001\u6b63\u786e\u7684\u72b6\u6001</li> <li>\u4e3a\u540e\u7eed\u7684\u7cfb\u7edf\u8c03\u7528\u548c\u4efb\u52a1\u8c03\u5ea6\u63d0\u4f9b\u53ef\u9760\u7684\u57fa\u7840</li> <li>\u8fd9\u662f RTOS \u542f\u52a8\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u5b89\u5168\u63aa\u65bd</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#freertos","title":"FreeRTOS\u7ea7\u522b\u64cd\u4f5c","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#vtaskstartscheduler","title":"<code>vTaskStartScheduler()</code>","text":"<pre><code>void vTaskStartScheduler( void )\n{\n    /* Add the idle task at the lowest priority. */\n    //1.\u521b\u5efa\u7a7a\u95f2\u4efb\u52a1\n    xTaskCreate() or xTaskCreateStatic()\n\n    //2.\u521b\u5efa\u8f6f\u4ef6\u5b9a\u65f6\u5668\u4efb\u52a1\uff08\u5982\u679c\u4f7f\u80fd\uff09\n    xTimerCreateTimerTask()\n\n    //3.\u5173\u4e2d\u65ad\uff0c\u9632\u6b62\u8c03\u5ea6\u5668\u5f00\u542f\u4e4b\u524d\u6216\u4e4b\u4e2d\u53d7\u4e2d\u65ad\u5e72\u6270\n    portDISABLE_INTERRUPTS();\n\n    xNextTaskUnblockTime = portMAX_DELAY; //\u8bbe\u7f6e\u7b2c\u4e00\u4e2a\u4efb\u52a1\u7684\u963b\u585e\u8d85\u65f6\u65f6\u95f4\u4e3a\u5f88\u59270xffffffffUL\n    xSchedulerRunning = pdTRUE; //\u6807\u8bb0\u4efb\u52a1\u8c03\u5ea6\u5668\u5f00\u59cb\u8fd0\u884c\n    xTickCount = ( TickType_t ) configINITIAL_TICK_COUNT; //\u521d\u59cb\u5316\u5fc3\u8df3\u8ba1\u6570\u5668\u4e3a0\n\n    portCONFIGURE_TIMER_FOR_RUN_TIME_STATS();//\u7edf\u8ba1\u4efb\u52a1\u8fd0\u884c\u65f6\u95f4\u7684\u5b9e\u73b0\u63a5\u53e3\uff08\u7528\u6237\u5b9e\u73b0\uff09\n\n    traceTASK_SWITCHED_IN()//\u8c03\u8bd5\u63a5\u53e3\uff0c\u9700\u8981\u5b9e\u73b0\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#xportstartscheduler","title":"<code>xPortStartScheduler()</code>","text":"<p><code>xPortStartScheduler()</code> \u662f FreeRTOS \u8c03\u5ea6\u5668\u7684\u6838\u5fc3\u542f\u52a8\u51fd\u6570\uff0c\u8d1f\u8d23\u521d\u59cb\u5316\u786c\u4ef6\u8d44\u6e90\u5e76\u542f\u52a8\u591a\u4efb\u52a1\u8c03\u5ea6\u73af\u5883\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_3","title":"\u51fd\u6570\u6982\u8ff0","text":"<p><code>xPortStartScheduler()</code> \u4f4d\u4e8e FreeRTOS \u7684\u79fb\u690d\u5c42\uff08\u901a\u5e38\u662f <code>port.c</code> \u6587\u4ef6\uff09\uff0c\u4e3b\u8981\u804c\u8d23\u5305\u62ec\uff1a</p> <ul> <li>\u914d\u7f6e\u7cfb\u7edf\u5173\u952e\u786c\u4ef6\uff08\u5982 SysTick \u5b9a\u65f6\u5668\uff09</li> <li>\u8bbe\u7f6e\u5f02\u5e38\u4f18\u5148\u7ea7</li> <li>\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1</li> <li>\u6c38\u4e0d\u8fd4\u56de\uff08\u9664\u975e\u53d1\u751f\u4e25\u91cd\u9519\u8bef\uff09</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_4","title":"\u4ee3\u7801\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#1","title":"1. \u5f02\u5e38\u4f18\u5148\u7ea7\u914d\u7f6e","text":"<pre><code>BaseType_t xPortStartScheduler( void )\n{\n    /* \u914d\u7f6e PendSV \u548c SysTick \u5f02\u5e38\u7684\u4f18\u5148\u7ea7 */\n    portNVIC_SYSPRI2_REG |= portNVIC_PENDSV_PRI;\n    portNVIC_SYSPRI2_REG |= portNVIC_SYSTICK_PRI;\n</code></pre> <p>\u5173\u952e\u8bbe\u8ba1\u8981\u70b9\uff1a</p> <ul> <li>PendSV \u4f18\u5148\u7ea7\u8bbe\u7f6e\u4e3a\u6700\u4f4e\uff0c\u786e\u4fdd\u9ad8\u4f18\u5148\u7ea7\u4e2d\u65ad\u80fd\u591f\u7acb\u5373\u54cd\u5e94</li> <li>\u8fd9\u79cd\u8bbe\u8ba1\u907f\u514d\u4e86\u4efb\u52a1\u5207\u6362\u963b\u585e\u65f6\u95f4\u654f\u611f\u7684\u4e2d\u65ad\u5904\u7406</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#2","title":"2. \u7cfb\u7edf\u5b9a\u65f6\u5668\u521d\u59cb\u5316","text":"<pre><code>    /* \u542f\u52a8 SysTick \u5b9a\u65f6\u5668 - \u63d0\u4f9b\u4efb\u52a1\u8c03\u5ea6\u7684\u65f6\u95f4\u57fa\u51c6 */\n    vPortSetupTimerInterrupt();\n</code></pre> <p>SysTick \u5b9a\u65f6\u5668\u7684\u4f5c\u7528\uff1a - \u4ea7\u751f\u56fa\u5b9a\u7684\u65f6\u95f4\u8282\u62cd\uff08tick\uff09 - \u9a71\u52a8\u65f6\u95f4\u7247\u8f6e\u8f6c\u8c03\u5ea6 - \u7ba1\u7406\u963b\u585e\u4efb\u52a1\u7684\u8d85\u65f6\u673a\u5236</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#3","title":"3. \u8c03\u5ea6\u5668\u72b6\u6001\u521d\u59cb\u5316","text":"<pre><code>    /* \u521d\u59cb\u5316\u8c03\u5ea6\u5668\u5173\u952e\u53d8\u91cf */\n    xNextTaskUnblockTime = portMAX_DELAY;\n    xSchedulerRunning = pdTRUE;\n    xTickCount = 0;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#4","title":"4. \u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1","text":"<pre><code>    /* \u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1 */\n    if( xPortStartFirstTask() != pdFALSE )\n    {\n        /* \u5982\u679c\u6267\u884c\u5230\u8fd9\u91cc\uff0c\u8bf4\u660e\u4efb\u52a1\u542f\u52a8\u5931\u8d25 */\n        return pdFALSE;\n    }\n\n    /* \u6b63\u5e38\u60c5\u51b5\u4e0b\u4e0d\u4f1a\u5230\u8fbe\u8fd9\u91cc */\n    return pdFALSE;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_5","title":"\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1\u7684\u6838\u5fc3\u673a\u5236","text":"<p><code>xPortStartFirstTask()</code> \u901a\u5e38\u7528\u6c47\u7f16\u8bed\u8a00\u5b9e\u73b0\uff0c\u4e3b\u8981\u6b65\u9aa4\u5305\u62ec\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#1_1","title":"1. \u5806\u6808\u6307\u9488\u521d\u59cb\u5316","text":"<pre><code>xPortStartFirstTask:\n    /* \u4ece\u5411\u91cf\u8868\u521d\u59cb\u5316\u4e3b\u5806\u6808\u6307\u9488 */\n    ldr r0, =0xE000ED08    /* VTOR \u5bc4\u5b58\u5668\u5730\u5740 */\n    ldr r0, [r0]\n    ldr r0, [r0]           /* \u83b7\u53d6\u521d\u59cb MSP \u503c */\n    msr msp, r0            /* \u8bbe\u7f6e\u4e3b\u5806\u6808\u6307\u9488 */\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#2_1","title":"2. \u4e2d\u65ad\u4f7f\u80fd","text":"<pre><code>    /* \u4f7f\u80fd\u5168\u5c40\u4e2d\u65ad */\n    cpsie i                /* \u4f7f\u80fd IRQ \u4e2d\u65ad */\n    cpsie f                /* \u4f7f\u80fd Fault \u4e2d\u65ad */\n    dsb\n    isb\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#3-svc","title":"3. \u89e6\u53d1 SVC \u5f02\u5e38\u542f\u52a8\u4efb\u52a1","text":"<pre><code>    /* \u901a\u8fc7 SVC \u5f02\u5e38\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1 */\n    svc 0                  /* \u89e6\u53d1 SVC \u5f02\u5e38 */\n\n    /* SVC \u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u4f1a\u914d\u7f6e\u4efb\u52a1\u73af\u5883\u5e76\u5f00\u59cb\u6267\u884c */\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#svc","title":"SVC \u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u7684\u5173\u952e\u64cd\u4f5c","text":"<p>\u5728 SVC \u5f02\u5e38\u5904\u7406\u7a0b\u5e8f\u4e2d\u5b8c\u6210\u4ee5\u4e0b\u5173\u952e\u64cd\u4f5c\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#1_2","title":"1. \u4efb\u52a1\u4e0a\u4e0b\u6587\u6062\u590d","text":"<pre><code>void vPortSVCHandler( void )\n{\n    /* \u4ece\u5f53\u524d\u4efb\u52a1\u7684 TCB \u4e2d\u6062\u590d\u5806\u6808\u6307\u9488 */\n    pxCurrentTCB = \u83b7\u53d6\u6700\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u7684 TCB;\n    __asm volatile (\n        \"ldr r3, [%0]          \\n\" /* \u4ece TCB \u83b7\u53d6\u5806\u6808\u6307\u9488 */\n        \"ldmdb r3!, {r0-r2}    \\n\" /* \u6062\u590d\u90e8\u5206\u5bc4\u5b58\u5668 */\n        \"msr control, r0       \\n\" /* \u8bbe\u7f6e\u63a7\u5236\u5bc4\u5b58\u5668 */\n        \"msr psp, r3           \\n\" /* \u8bbe\u7f6e\u8fdb\u7a0b\u5806\u6808\u6307\u9488 */\n        \"isb                   \\n\"\n        :: \"r\" (&amp;pxCurrentTCB)\n    );\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#2_2","title":"2. \u5207\u6362\u5230\u4efb\u52a1\u6a21\u5f0f","text":"<pre><code>    /* \u4f7f\u7528\u8fdb\u7a0b\u5806\u6808\u6267\u884c\u4efb\u52a1 */\n    __asm volatile (\n        \"mov r0, #2            \\n\" /* \u5207\u6362\u5230\u7ebf\u7a0b\u6a21\u5f0f\u5e76\u4f7f\u7528 PSP */\n        \"msr control, r0       \\n\"\n        \"isb                   \\n\"\n        \"bx lr                 \\n\" /* \u8fd4\u56de\u5230\u4efb\u52a1\u4ee3\u7801 */\n    );\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_6","title":"\u8bbe\u8ba1\u539f\u7406\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#1-svc_1","title":"1. \u4e3a\u4ec0\u4e48\u4f7f\u7528 SVC \u5f02\u5e38\u542f\u52a8\u4efb\u52a1\uff1f","text":"<ul> <li>\u6743\u9650\u5207\u6362\uff1a\u4ece Handler \u6a21\u5f0f\uff08\u7279\u6743\u7ea7\uff09\u5207\u6362\u5230 Thread \u6a21\u5f0f\uff08\u53ef\u80fd\u4e3a\u975e\u7279\u6743\u7ea7\uff09</li> <li>\u73af\u5883\u9694\u79bb\uff1a\u786e\u4fdd\u4efb\u52a1\u5728\u53d7\u63a7\u7684\u73af\u5883\u4e2d\u5f00\u59cb\u6267\u884c</li> <li>\u5806\u6808\u5206\u79bb\uff1a\u5b9e\u73b0\u4e3b\u5806\u6808\uff08MSP\uff09\u548c\u8fdb\u7a0b\u5806\u6808\uff08PSP\uff09\u7684\u5206\u79bb</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#2_3","title":"2. \u4f18\u5148\u7ea7\u914d\u7f6e\u7684\u91cd\u8981\u6027","text":"<ul> <li>SysTick\uff1a\u9700\u8981\u9002\u5f53\u7684\u4f18\u5148\u7ea7\u6765\u4fdd\u8bc1\u65f6\u95f4\u57fa\u51c6\u7684\u51c6\u786e\u6027</li> <li>PendSV\uff1a\u6700\u4f4e\u4f18\u5148\u7ea7\u786e\u4fdd\u4e0d\u4f1a\u5ef6\u8fdf\u9ad8\u4f18\u5148\u7ea7\u4e2d\u65ad\u7684\u5904\u7406</li> <li>SVC\uff1a\u7528\u4e8e\u7cfb\u7edf\u8c03\u7528\uff0c\u9700\u8981\u5408\u7406\u7684\u4f18\u5148\u7ea7\u8bbe\u7f6e</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#3_1","title":"3. \u72b6\u6001\u673a\u8f6c\u6362","text":"<p>\u8c03\u5ea6\u5668\u542f\u52a8\u8fc7\u7a0b\u7684\u72b6\u6001\u8f6c\u6362\uff1a 1. \u521d\u59cb\u5316\u72b6\u6001\uff1a\u914d\u7f6e\u786c\u4ef6\uff0c\u8bbe\u7f6e\u4f18\u5148\u7ea7 2. \u51c6\u5907\u72b6\u6001\uff1a\u521d\u59cb\u5316\u53d8\u91cf\uff0c\u542f\u52a8\u5b9a\u65f6\u5668 3. \u542f\u52a8\u72b6\u6001\uff1a\u901a\u8fc7 SVC \u5f02\u5e38\u5207\u6362\u5230\u7b2c\u4e00\u4e2a\u4efb\u52a1 4. \u8fd0\u884c\u72b6\u6001\uff1a\u591a\u4efb\u52a1\u73af\u5883\u6b63\u5f0f\u8fd0\u884c</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_7","title":"\u9519\u8bef\u5904\u7406\u673a\u5236","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_8","title":"\u542f\u52a8\u5931\u8d25\u7684\u53ef\u80fd\u539f\u56e0","text":"<ul> <li>\u5806\u6808\u6307\u9488\u521d\u59cb\u5316\u5931\u8d25</li> <li>\u5411\u91cf\u8868\u914d\u7f6e\u9519\u8bef</li> <li>\u4f18\u5148\u7ea7\u914d\u7f6e\u51b2\u7a81</li> <li>\u5185\u5b58\u5206\u914d\u95ee\u9898</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_9","title":"\u8fd4\u56de\u503c\u8bed\u4e49","text":"<ul> <li>pdTRUE\uff1a\u542f\u52a8\u6210\u529f\uff08\u5b9e\u9645\u4e0a\u4e0d\u4f1a\u8fd4\u56de\uff09</li> <li>pdFALSE\uff1a\u542f\u52a8\u5931\u8d25\uff0c\u9700\u8981\u9519\u8bef\u5904\u7406</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_10","title":"\u4efb\u52a1\u5207\u6362","text":"<p>\u4efb\u52a1\u5207\u6362\u7684\u672c\u8d28\uff1aCPU\u5bc4\u5b58\u5668\u7684\u5207\u6362</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_11","title":"\u57fa\u672c\u6982\u5ff5","text":"<p>\u4efb\u52a1\u5207\u6362\u662f\u591a\u4efb\u52a1\u64cd\u4f5c\u7cfb\u7edf\u7684\u6838\u5fc3\u673a\u5236\uff0c\u6307 CPU \u4ece\u4e00\u4e2a\u6b63\u5728\u8fd0\u884c\u7684\u4efb\u52a1\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\u4efb\u52a1\u7684\u8fc7\u7a0b\u3002\u5728\u5355\u6838\u5904\u7406\u5668\u7cfb\u7edf\u4e2d\uff0c\u591a\u4e2a\u4efb\u52a1\u901a\u8fc7\u5feb\u901f\u5207\u6362\u521b\u9020\"\u540c\u65f6\u8fd0\u884c\"\u7684\u5047\u8c61\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_12","title":"\u7c7b\u6bd4\u7406\u89e3","text":"<p>\u60f3\u8c61\u4e00\u540d\u53a8\u5e08\u540c\u65f6\u5904\u7406\u591a\u9053\u83dc\u54c1\uff1a - \u6bcf\u9053\u83dc\u5c31\u662f\u4e00\u4e2a\u4efb\u52a1 - \u53a8\u5e08\u5728\u4e0d\u540c\u83dc\u54c1\u95f4\u5feb\u901f\u5207\u6362\u6ce8\u610f\u529b - \u6bcf\u6b21\u5207\u6362\u65f6\u9700\u8981\u8bb0\u4f4f\u5f53\u524d\u83dc\u54c1\u7684\u8fdb\u5ea6 - \u5207\u6362\u5230\u53e6\u4e00\u9053\u83dc\u65f6\u9700\u8981\u56de\u5fc6\u8be5\u83dc\u54c1\u7684\u4e0a\u6b21\u72b6\u6001</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_13","title":"\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u6df1\u5165\u7406\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_14","title":"\u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587","text":"<p>\u4e0a\u4e0b\u6587\u662f\u6307\u4efb\u52a1\u5728\u67d0\u4e2a\u65f6\u95f4\u70b9\u7684\u5b8c\u6574\u6267\u884c\u72b6\u6001\uff0c\u5305\u62ec\uff1a</p> <p>\u786c\u4ef6\u4e0a\u4e0b\u6587\uff1a - \u6240\u6709\u5bc4\u5b58\u5668\u503c\uff08R0-R15\uff09 - \u7a0b\u5e8f\u72b6\u6001\u5bc4\u5b58\u5668\uff08xPSR\uff09 - \u5806\u6808\u6307\u9488\uff08SP\uff09 - \u7a0b\u5e8f\u8ba1\u6570\u5668\uff08PC\uff09</p> <p>\u8f6f\u4ef6\u4e0a\u4e0b\u6587\uff1a - \u4efb\u52a1\u63a7\u5236\u5757\uff08TCB\uff09\u4e2d\u7684\u72b6\u6001\u4fe1\u606f - \u5806\u6808\u4e2d\u7684\u5c40\u90e8\u53d8\u91cf\u548c\u8fd4\u56de\u5730\u5740 - \u4efb\u52a1\u7279\u6709\u7684\u914d\u7f6e\u548c\u6570\u636e</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_15","title":"\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u8fc7\u7a0b","text":"<pre><code>// \u4f2a\u4ee3\u7801\u8868\u793a\u4e0a\u4e0b\u6587\u5207\u6362\u6d41\u7a0b\nvoid context_switch(Task* current, Task* next) {\n    // 1. \u4fdd\u5b58\u5f53\u524d\u4efb\u52a1\u4e0a\u4e0b\u6587\n    save_registers(current-&gt;stack_pointer);\n    save_processor_state(current-&gt;tcb);\n\n    // 2. \u66f4\u65b0\u8c03\u5ea6\u5668\u72b6\u6001\n    scheduler-&gt;current_task = next;\n\n    // 3. \u6062\u590d\u65b0\u4efb\u52a1\u4e0a\u4e0b\u6587  \n    restore_processor_state(next-&gt;tcb);\n    restore_registers(next-&gt;stack_pointer);\n\n    // 4. \u8df3\u8f6c\u5230\u65b0\u4efb\u52a1\n    jump_to_task(next-&gt;program_counter);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#freertos_1","title":"FreeRTOS \u4efb\u52a1\u5207\u6362\u7684\u89e6\u53d1\u6761\u4ef6","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_16","title":"\u4e3b\u52a8\u89e6\u53d1\u65b9\u5f0f","text":"<p>\u4efb\u52a1\u4e3b\u52a8\u8ba9\u51fa CPU <code>portYIELD()/taskYIELD()</code>\uff1a</p> <p>\u901a\u8fc7\u6302\u8d77PendSV\u542f\u7528PendSv\u5f02\u5e38\uff0c\u5982\u4f55\u6302\u8d77\u89c1KernelPend</p> <pre><code>void vTaskA( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u6267\u884c\u4e00\u4e9b\u5de5\u4f5c\n        perform_work();\n\n        // \u4e3b\u52a8\u8ba9\u51fa CPU \u7ed9\u5176\u4ed6\u4efb\u52a1\n        taskYIELD();\n    }\n}\n</code></pre> <p>\u7b49\u5f85\u8d44\u6e90\u65f6\u81ea\u52a8\u5207\u6362\uff1a</p> <pre><code>void vTaskB( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u7b49\u5f85\u4fe1\u53f7\u91cf\uff0c\u671f\u95f4\u4f1a\u81ea\u52a8\u5207\u6362\u4efb\u52a1\n        xSemaphoreTake( xBinarySemaphore, portMAX_DELAY );\n\n        // \u83b7\u53d6\u4fe1\u53f7\u91cf\u540e\u7ee7\u7eed\u6267\u884c\n        process_data();\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_17","title":"\u88ab\u52a8\u89e6\u53d1\u65b9\u5f0f","text":"<p>\u65f6\u95f4\u7247\u5230\u671f\uff1a</p> <pre><code>// \u5728 SysTick \u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\nvoid xPortSysTickHandler( void )\n{\n    // \u66f4\u65b0\u65f6\u95f4\u8ba1\u6570\n    if( xTaskIncrementTick() != pdFALSE )\n    {\n        // \u89e6\u53d1\u4efb\u52a1\u5207\u6362\n        portNVIC_INT_CTRL_REG = portNVIC_PENDSVSET_BIT;\n    }\n}\n</code></pre> <p>\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u5524\u9192\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\uff1a</p> <pre><code>void vAnInterruptHandler( void )\n{\n    BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n\n    // \u91ca\u653e\u4fe1\u53f7\u91cf\uff0c\u53ef\u80fd\u5524\u9192\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\n    xSemaphoreGiveFromISR( xSemaphore, &amp;xHigherPriorityTaskWoken );\n\n    // \u5982\u679c\u9700\u8981\u7acb\u5373\u5207\u6362\n    portYIELD_FROM_ISR( xHigherPriorityTaskWoken );\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#freertos_2","title":"FreeRTOS \u4efb\u52a1\u5207\u6362\u7684\u786c\u4ef6\u673a\u5236","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#arm-cortex-m","title":"ARM Cortex-M \u5f02\u5e38\u7cfb\u7edf","text":"<p>FreeRTOS \u5145\u5206\u5229\u7528 ARM Cortex-M \u7684\u5f02\u5e38\u673a\u5236\u6765\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u5207\u6362\uff1a</p> <p>SysTick \u5f02\u5e38\uff1a - \u7528\u9014\uff1a\u7cfb\u7edf\u65f6\u949f\u8282\u62cd\uff0c\u63d0\u4f9b\u65f6\u95f4\u57fa\u51c6 - \u4f18\u5148\u7ea7\uff1a\u914d\u7f6e\u4e3a\u8f83\u4f4e\u4f18\u5148\u7ea7 - \u529f\u80fd\uff1a\u9a71\u52a8\u65f6\u95f4\u7247\u8f6e\u8f6c\u548c\u4efb\u52a1\u8d85\u65f6\u7ba1\u7406</p> <p>PendSV \u5f02\u5e38\uff1a</p> <ul> <li>\u7528\u9014\uff1a\u53ef\u6302\u8d77\u7684\u7cfb\u7edf\u670d\u52a1\uff0c\u6267\u884c\u5b9e\u9645\u4e0a\u4e0b\u6587\u5207\u6362</li> <li>\u4f18\u5148\u7ea7\uff1a\u914d\u7f6e\u4e3a\u6700\u4f4e\u4f18\u5148\u7ea7</li> <li>\u7279\u70b9\uff1a\u53ef\u5ef6\u8fdf\u6267\u884c\uff0c\u4e0d\u963b\u585e\u9ad8\u4f18\u5148\u7ea7\u4e2d\u65ad</li> </ul> <p>SVC \u5f02\u5e38\uff1a - \u7528\u9014\uff1a\u7cfb\u7edf\u8c03\u7528\u63a5\u53e3 - \u529f\u80fd\uff1a\u542f\u52a8\u8c03\u5ea6\u5668\u548c\u6267\u884c\u7279\u6743\u64cd\u4f5c</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_18","title":"\u4f18\u5148\u7ea7\u914d\u7f6e\u7b56\u7565","text":"<pre><code>// FreeRTOSConfig.h \u4e2d\u7684\u5178\u578b\u914d\u7f6e\n#define configKERNEL_INTERRUPT_PRIORITY    255\n#define configMAX_SYSCALL_INTERRUPT_PRIORITY  191\n\n// \u5b9e\u9645\u4f18\u5148\u7ea7\u8bbe\u7f6e\n#define portNVIC_PENDSV_PRI          ( ( ( uint32_t ) configKERNEL_INTERRUPT_PRIORITY ) &lt;&lt; 16UL )\n#define portNVIC_SYSTICK_PRI         ( ( ( uint32_t ) configKERNEL_INTERRUPT_PRIORITY ) &lt;&lt; 24UL )\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_19","title":"\u4efb\u52a1\u5207\u6362\u7684\u8be6\u7ec6\u6267\u884c\u6d41\u7a0b","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_20","title":"\u5b8c\u6574\u7684\u5207\u6362\u5e8f\u5217","text":"<p>\u9636\u6bb5\u4e00\uff1a\u89e6\u53d1\u5207\u6362</p> <pre><code>// \u4ece\u4efb\u52a1\u4e2d\u89e6\u53d1\u5207\u6362\ntaskYIELD();\n    \u2193\n// \u8bbe\u7f6e PendSV \u6302\u8d77\u4f4d\nportNVIC_INT_CTRL_REG = portNVIC_PENDSVSET_BIT;\n    \u2193\n// CPU \u5728\u9002\u5f53\u65f6\u5019\u8fdb\u5165 PendSV \u5f02\u5e38\n</code></pre> <p>\u9636\u6bb5\u4e8c\uff1a\u4fdd\u5b58\u5f53\u524d\u4efb\u52a1\u4e0a\u4e0b\u6587</p> <pre><code>vPortPendSVHandler:\n    /* \u4fdd\u5b58\u5f53\u524d\u4efb\u52a1\u72b6\u6001 */\n    mrs r0, psp                 // \u83b7\u53d6\u5f53\u524d\u4efb\u52a1\u7684\u5806\u6808\u6307\u9488\n    isb                         // \u6307\u4ee4\u540c\u6b65\u5c4f\u969c\n\n    // \u5c06\u5bc4\u5b58\u5668 r4-r11 \u4fdd\u5b58\u5230\u4efb\u52a1\u5806\u6808\uff08\u624b\u52a8\u538b\u6808\uff09\n    stmdb r0!, {r4-r11}\n\n    // \u66f4\u65b0\u4efb\u52a1\u63a7\u5236\u5757\u4e2d\u7684\u5806\u6808\u6307\u9488\n    ldr r2, =pxCurrentTCB\n    ldr r1, [r2]\n    str r0, [r1]\n</code></pre> <p>\u9636\u6bb5\u4e09\uff1a\u9009\u62e9\u4e0b\u4e00\u4e2a\u4efb\u52a1</p> <pre><code>// \u8c03\u7528\u8c03\u5ea6\u5668\u9009\u62e9\u65b0\u4efb\u52a1\nvTaskSwitchContext();\n    \u2193\n// \u8c03\u5ea6\u5668\u51b3\u7b56\u8fc7\u7a0b\nvoid vTaskSwitchContext( void )\n{\n    // \u68c0\u67e5\u8c03\u5ea6\u5668\u662f\u5426\u6302\u8d77\n    if( uxSchedulerSuspended != ( UBaseType_t ) pdFALSE )\n    {\n        xYieldPending = pdTRUE;\n        return;\n    }\n\n    // \u5bfb\u627e\u6700\u9ad8\u4f18\u5148\u7ea7\u7684\u5c31\u7eea\u4efb\u52a1\n    #if ( configUSE_PORT_OPTIMISED_TASK_SELECTION == 1 )\n        // \u4f7f\u7528\u4f4d\u56fe\u7b97\u6cd5\u5feb\u901f\u67e5\u627e\n        uxTopReadyPriority = portGET_HIGHEST_PRIORITY();\n    #else\n        // \u904d\u5386\u5c31\u7eea\u5217\u8868\u67e5\u627e\n        listGET_OWNER_OF_NEXT_ENTRY( pxCurrentTCB, \n                                   &amp;( pxReadyTasksLists[ uxTopReadyPriority ] ) );\n    #endif\n}\n</code></pre> <p>\u9636\u6bb5\u56db\uff1a\u6062\u590d\u65b0\u4efb\u52a1\u4e0a\u4e0b\u6587</p> <pre><code>    /* \u6062\u590d\u65b0\u4efb\u52a1\u72b6\u6001 */\n    ldr r1, =pxCurrentTCB      // \u83b7\u53d6\u65b0\u4efb\u52a1\u7684 TCB\n    ldr r0, [r1]\n    ldr r0, [r0]               // \u83b7\u53d6\u65b0\u4efb\u52a1\u7684\u5806\u6808\u6307\u9488\n\n    // \u4ece\u5806\u6808\u6062\u590d\u5bc4\u5b58\u5668 r4-r11\n    ldmia r0!, {r4-r11}\n\n    // \u66f4\u65b0\u8fdb\u7a0b\u5806\u6808\u6307\u9488\n    msr psp, r0\n    isb\n\n    // \u8fd4\u56de\u5230\u65b0\u4efb\u52a1\u7ee7\u7eed\u6267\u884c\n    bx r14\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#tcb","title":"\u4efb\u52a1\u63a7\u5236\u5757\uff08TCB\uff09\u7684\u5173\u952e\u4f5c\u7528","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#tcb_1","title":"TCB \u6570\u636e\u7ed3\u6784","text":"<pre><code>typedef struct tskTaskControlBlock\n{\n    // \u5806\u6808\u7ba1\u7406\n    volatile StackType_t *pxTopOfStack;    // \u5f53\u524d\u5806\u6808\u9876\n    StackType_t *pxStack;                  // \u5806\u6808\u8d77\u59cb\u5730\u5740\n\n    // \u4efb\u52a1\u72b6\u6001\u7ba1\u7406\n    ListItem_t xStateListItem;             // \u72b6\u6001\u5217\u8868\u9879\n    UBaseType_t uxPriority;                // \u4efb\u52a1\u4f18\u5148\u7ea7\n    StackType_t *pxEndOfStack;             // \u5806\u6808\u7ed3\u675f\u5730\u5740\n\n    // \u4efb\u52a1\u6807\u8bc6\n    char pcTaskName[ configMAX_TASK_NAME_LEN ];\n\n    // \u8c03\u8bd5\u548c\u8ddf\u8e2a\u4fe1\u606f\n    #if ( configUSE_TRACE_FACILITY == 1 )\n        UBaseType_t uxTCBNumber;\n    #endif\n\n    // \u4e92\u65a5\u91cf\u7ee7\u627f\u76f8\u5173\n    #if ( configUSE_MUTEXES == 1 )\n        UBaseType_t uxBasePriority;\n        UBaseType_t uxMutexesHeld;\n    #endif\n} tskTCB;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_21","title":"\u5806\u6808\u5e03\u5c40\u8bbe\u8ba1","text":"<pre><code>\u4efb\u52a1\u5806\u6808\u5728\u4e0a\u4e0b\u6587\u4fdd\u5b58\u540e\u7684\u5e03\u5c40\uff1a\n\u9ad8\u5730\u5740 -&gt; | \u5f02\u5e38\u81ea\u52a8\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668(\u81ea\u52a8\u538b\u6808\uff0c\u81ea\u52a8\u51fa\u6808) |\n         | R0-R3, R12, LR, PC, xPSR |\n         |---------------------------|\n         | \u624b\u52a8\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668 R4-R11 (\u624b\u52a8\u538b\u6808\uff0c\u624b\u52a8\u51fa\u6808)  | &lt;- PSP \u6307\u5411\u8fd9\u91cc\n         |---------------------------|\n         | \u4efb\u52a1\u5c40\u90e8\u53d8\u91cf\u548c\u8c03\u7528\u6808      |\n\u4f4e\u5730\u5740 -&gt; | \u5806\u6808\u8d77\u59cb\u4f4d\u7f6e            |\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_22","title":"\u65f6\u95f4\u7247\u8f6e\u8f6c\u8c03\u5ea6\u539f\u7406","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_23","title":"\u540c\u4f18\u5148\u7ea7\u4efb\u52a1\u8c03\u5ea6","text":"<pre><code>// \u5728 SysTick \u4e2d\u65ad\u4e2d\u5904\u7406\u65f6\u95f4\u7247\nBaseType_t xTaskIncrementTick( void )\n{\n    TickType_t xSwitchRequired = pdFALSE;\n\n    if( xSchedulerRunning != pdFALSE )\n    {\n        // \u66f4\u65b0\u7cfb\u7edf\u8282\u62cd\u8ba1\u6570\n        const TickType_t xConstTickCount = xTickCount + 1;\n        xTickCount = xConstTickCount;\n\n        // \u68c0\u67e5\u662f\u5426\u6709\u4efb\u52a1\u8d85\u65f6\n        if( xConstTickCount == 0 )\n        {\n            taskSWITCH_DELAYED_LISTS();\n        }\n\n        // \u65f6\u95f4\u7247\u8f6e\u8f6c\u51b3\u7b56\n        #if ( configUSE_PREEMPTION == 1 ) &amp;&amp; ( configUSE_TIME_SLICING == 1 )\n        {\n            // \u5982\u679c\u5f53\u524d\u4f18\u5148\u7ea7\u6709\u591a\u4e2a\u5c31\u7eea\u4efb\u52a1\uff0c\u89e6\u53d1\u5207\u6362\n            if( listCURRENT_LIST_LENGTH( \n                &amp;( pxReadyTasksLists[ pxCurrentTCB-&gt;uxPriority ] ) ) &gt; ( UBaseType_t ) 1 )\n            {\n                xSwitchRequired = pdTRUE;\n            }\n        }\n        #endif\n    }\n\n    return xSwitchRequired;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_24","title":"\u6027\u80fd\u4f18\u5316\u6280\u672f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_25","title":"\u5feb\u901f\u4efb\u52a1\u9009\u62e9\u7b97\u6cd5","text":"<p>\u4f4d\u56fe\u8c03\u5ea6\u7b97\u6cd5\uff1a</p> <pre><code>#if configUSE_PORT_OPTIMISED_TASK_SELECTION == 1\n\n// \u4f7f\u7528\u524d\u5bfc\u96f6\u8ba1\u6570\u6307\u4ee4\u5feb\u901f\u627e\u5230\u6700\u9ad8\u4f18\u5148\u7ea7\n#define portGET_HIGHEST_PRIORITY( uxTopPriority, uxReadyPriorities ) \\\n    __asm volatile( \"clz %0, %1\" : \"=r\" ( uxTopPriority ) : \"r\" ( uxReadyPriorities ) )\n\n#endif\n</code></pre> <p>\u60f0\u6027\u5806\u6808\u4fdd\u5b58\uff1a - \u53ea\u4fdd\u5b58\u88ab\u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\uff08R4-R11\uff09 - \u8c03\u7528\u8005\u4fdd\u5b58\u7684\u5bc4\u5b58\u5668\uff08R0-R3, R12\uff09\u7531 C-ABI \u4fdd\u8bc1 - \u786c\u4ef6\u81ea\u52a8\u4fdd\u5b58\u5f02\u5e38\u5e27\uff08R0-R3, R12, LR, PC, PSR\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_26","title":"\u4e2d\u65ad\u5ef6\u8fdf\u4f18\u5316","text":"<p>PendSV \u7684\u4f4e\u4f18\u5148\u7ea7\u8bbe\u8ba1\uff1a</p> <pre><code>// PendSV \u8bbe\u7f6e\u4e3a\u6700\u4f4e\u4f18\u5148\u7ea7\uff0c\u786e\u4fdd\uff1a\n// 1. \u9ad8\u4f18\u5148\u7ea7\u4e2d\u65ad\u53ef\u4ee5\u7acb\u5373\u54cd\u5e94\n// 2. \u591a\u4e2a\u4e2d\u65ad\u53ef\u4ee5\u5408\u5e76\u5904\u7406\n// 3. \u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u4e0a\u4e0b\u6587\u5207\u6362\n\n#define portPendSVHandler_PRIORITY ( 255UL &lt;&lt; 16UL )\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_27","title":"\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_28","title":"\u573a\u666f\u4e00\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u62a2\u5360","text":"<pre><code>void vHighPriorityTask( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u7b49\u5f85\u4e8b\u4ef6\n        xQueueReceive( xHighPriorityQueue, ... );\n\n        // \u7acb\u5373\u62a2\u5360\u5f53\u524d\u8fd0\u884c\u7684\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\n        process_critical_work();\n    }\n}\n\nvoid vLowPriorityTask( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u6267\u884c\u975e\u5173\u952e\u5de5\u4f5c\n        background_processing();\n\n        // \u53ef\u80fd\u5728\u4efb\u4f55\u65f6\u523b\u88ab\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u62a2\u5360\n        taskYIELD();\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_29","title":"\u573a\u666f\u4e8c\uff1a\u8d44\u6e90\u5171\u4eab\u4e0e\u540c\u6b65","text":"<pre><code>void vProducerTask( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u751f\u4ea7\u6570\u636e\n        generate_data();\n\n        // \u53d1\u9001\u6570\u636e\u5230\u961f\u5217\uff0c\u53ef\u80fd\u5524\u9192\u6d88\u8d39\u8005\u4efb\u52a1\n        xQueueSend( xDataQueue, &amp;data, portMAX_DELAY );\n\n        // \u5982\u679c\u6d88\u8d39\u8005\u4f18\u5148\u7ea7\u66f4\u9ad8\uff0c\u4f1a\u53d1\u751f\u4efb\u52a1\u5207\u6362\n    }\n}\n\nvoid vConsumerTask( void *pvParameters )\n{\n    for( ;; )\n    {\n        // \u7b49\u5f85\u6570\u636e\uff0c\u5982\u679c\u6ca1\u6709\u6570\u636e\u4f1a\u963b\u585e\n        xQueueReceive( xDataQueue, &amp;data, portMAX_DELAY );\n\n        // \u88ab\u751f\u4ea7\u8005\u5524\u9192\u540e\u7ee7\u7eed\u6267\u884c\n        process_data( data );\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_30","title":"\u8c03\u8bd5\u4e0e\u6027\u80fd\u5206\u6790","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_31","title":"\u4e0a\u4e0b\u6587\u5207\u6362\u5f00\u9500\u6d4b\u91cf","text":"<pre><code>// \u6d4b\u91cf\u5207\u6362\u65f6\u95f4\u7684\u7b80\u5355\u65b9\u6cd5\nuint32_t measure_switch_time( void )\n{\n    uint32_t start_time, end_time;\n\n    // \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4\n    start_time = DWT-&gt;CYCCNT;\n\n    // \u89e6\u53d1\u4efb\u52a1\u5207\u6362\n    taskYIELD();\n\n    // \u83b7\u53d6\u7ed3\u675f\u65f6\u95f4\n    end_time = DWT-&gt;CYCCNT;\n\n    return end_time - start_time;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#_32","title":"\u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848","text":"<p>\u95ee\u9898\u4e00\uff1a\u8fc7\u591a\u7684\u4e0a\u4e0b\u6587\u5207\u6362 - \u75c7\u72b6\uff1aCPU \u5927\u90e8\u5206\u65f6\u95f4\u5728\u5207\u6362\u4efb\u52a1\u800c\u975e\u6267\u884c\u4efb\u52a1 - \u89e3\u51b3\u65b9\u6848\uff1a\u8c03\u6574\u65f6\u95f4\u7247\u5927\u5c0f\uff0c\u5408\u5e76\u5c0f\u4efb\u52a1</p> <p>\u95ee\u9898\u4e8c\uff1a\u4f18\u5148\u7ea7\u53cd\u8f6c - \u75c7\u72b6\uff1a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u88ab\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u963b\u585e - \u89e3\u51b3\u65b9\u6848\uff1a\u4f7f\u7528\u4f18\u5148\u7ea7\u7ee7\u627f\u534f\u8bae\u6216\u4f18\u5148\u7ea7\u5929\u82b1\u677f</p> <p>\u95ee\u9898\u4e09\uff1a\u5806\u6808\u6ea2\u51fa - \u75c7\u72b6\uff1a\u4efb\u52a1\u5d29\u6e83\u6216\u7cfb\u7edf\u4e0d\u7a33\u5b9a - \u89e3\u51b3\u65b9\u6848\uff1a\u589e\u52a0\u5806\u6808\u5927\u5c0f\uff0c\u4f7f\u7528\u5806\u6808\u68c0\u67e5\u529f\u80fd</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#api","title":"\u5176\u4ed6API\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#freertos_3","title":"FreeRTOS","text":"<p>\u8fd9\u4e9b\u51fd\u6570\u4ee5 <code>vTask</code>\u3001<code>xTask</code> \u7b49\u4e3a\u524d\u7f00\uff0c\u662f FreeRTOS \u5185\u6838\u7684\u76f4\u63a5\u63a5\u53e3\u3002</p> \u5206\u7c7b \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 \u53c2\u6570\u8bf4\u660e \u8fd4\u56de\u503c/\u6ce8\u610f\u4e8b\u9879 <code>taskYIELD()</code> \u4e3b\u52a8\u8bf7\u6c42\u4efb\u52a1\u5207\u6362\uff08\u5f3a\u5236\u8c03\u5ea6\uff09\u3002 \u65e0 \u662f\u4e00\u4e2a\u5b8f\uff0c\u901a\u5e38\u89e6\u53d1 PendSV \u5f02\u5e38\u3002 \u4f18\u5148\u7ea7\u7ba1\u7406 <code>vTaskPrioritySet()</code> \u8bbe\u7f6e\u4efb\u52a1\u7684\u4f18\u5148\u7ea7\u3002 <code>xTask</code>: \u4efb\u52a1\u53e5\u67c4<code>uxNewPriority</code>: \u65b0\u7684\u4f18\u5148\u7ea7 \u4f18\u5148\u7ea7\u5fc5\u987b\u5728 <code>0</code> \u5230 <code>configMAX_PRIORITIES-1</code> \u8303\u56f4\u5185\u3002 <code>uxTaskPriorityGet()</code> \u83b7\u53d6\u4efb\u52a1\u7684\u5f53\u524d\u4f18\u5148\u7ea7\u3002 <code>xTask</code>: \u4efb\u52a1\u53e5\u67c4 \u8fd4\u56de\u4efb\u52a1\u7684\u5f53\u524d\u4f18\u5148\u7ea7\u3002 \u72b6\u6001\u67e5\u8be2 <code>eTaskGetState()</code> \u83b7\u53d6\u4efb\u52a1\u7684\u72b6\u6001\uff08\u8fd0\u884c\u3001\u5c31\u7eea\u3001\u963b\u585e\u3001\u6302\u8d77\u3001\u5220\u9664\uff09\u3002 <code>xTask</code>: \u4efb\u52a1\u53e5\u67c4 \u8fd4\u56de\u4e00\u4e2a <code>eTaskState</code> \u679a\u4e3e\u503c\u3002 <code>uxTaskGetStackHighWaterMark()</code> \u83b7\u53d6\u4efb\u52a1\u5806\u6808\u7684\u5386\u53f2\u6700\u5c0f\u5269\u4f59\u7a7a\u95f4\uff08\u9ad8\u6c34\u4f4d\u7ebf\uff09\u3002 <code>xTask</code>: \u4efb\u52a1\u53e5\u67c4 \u8fd4\u56de\u503c\u8d8a\u5c0f\uff0c\u8bf4\u660e\u5806\u6808\u4f7f\u7528\u7387\u8d8a\u9ad8\u30020 \u8868\u793a\u5806\u6808\u5df2\u6ea2\u51fa\u3002\u7528\u4e8e\u8bc4\u4f30\u5806\u6808\u5927\u5c0f\u662f\u5426\u5408\u7406\u3002 \u5176\u4ed6 <code>pcTaskGetName()</code> \u83b7\u53d6\u4efb\u52a1\u7684\u540d\u79f0\u5b57\u7b26\u4e32\u3002 <code>xTaskToQuery</code>: \u4efb\u52a1\u53e5\u67c4 \u8fd4\u56de\u4efb\u52a1\u540d\u79f0\u5b57\u7b26\u4e32\u7684\u6307\u9488\u3002 <code>xTaskGetTickCount()</code> \u83b7\u53d6\u7cfb\u7edf\u8282\u62cd\u8ba1\u6570\u5668\u81ea\u542f\u52a8\u4ee5\u6765\u7684\u503c\u3002 \u65e0 \u5728\u4efb\u52a1\u4e2d\u4f7f\u7528\u3002\u6ce8\u610f\u8282\u62cd\u8ba1\u6570\u5668\u6ea2\u51fa\u3002 <code>xTaskGetTickCountFromISR()</code> \u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u83b7\u53d6\u7cfb\u7edf\u8282\u62cd\u8ba1\u6570\u5668\u3002 \u65e0 \u5728 ISR \u4e2d\u4f7f\u7528\u3002"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Scheduling/#cmsis-rtos-v2","title":"CMSIS-RTOS v2","text":"<p>\u8fd9\u4e9b\u51fd\u6570\u4ee5 <code>osThread</code> \u4e3a\u524d\u7f00\uff0c\u662f FreeRTOS \u539f\u751f API \u7684\u5c01\u88c5\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u63a5\u53e3\u3002</p> \u5206\u7c7b \u51fd\u6570\u540d \u529f\u80fd\u63cf\u8ff0 \u53c2\u6570\u8bf4\u660e \u8fd4\u56de\u503c/\u6ce8\u610f\u4e8b\u9879 <code>osThreadGetId()</code> \u83b7\u53d6\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u7684\u7ebf\u7a0b\u7684 ID\u3002 \u65e0 \u8fd4\u56de\u5f53\u524d\u7ebf\u7a0b\u7684 ID\u3002 <code>osThreadGetName()</code> \u83b7\u53d6\u6307\u5b9a\u7ebf\u7a0b\u7684\u540d\u79f0\u5b57\u7b26\u4e32\u3002 <code>thread_id</code>: \u7ebf\u7a0b ID \u8fd4\u56de\u7ebf\u7a0b\u540d\u79f0\u5b57\u7b26\u4e32\u7684\u6307\u9488\u3002 \u4efb\u52a1\u63a7\u5236 <code>osDelay()</code> \u5c06\u5f53\u524d\u7ebf\u7a0b\u5ef6\u8fdf\uff08\u963b\u585e\uff09\u6307\u5b9a\u7684\u65f6\u95f4\u3002 <code>ticks</code>: \u5ef6\u8fdf\u7684\u65f6\u95f4\u8282\u62cd\u6570 \u76f8\u5f53\u4e8e FreeRTOS \u7684 <code>vTaskDelay</code>\u3002 <code>osDelayUntil()</code> \u7cbe\u786e\u7684\u5468\u671f\u6027\u5ef6\u8fdf\u3002 <code>ticks</code>: \u4e0b\u4e00\u6b21\u5524\u9192\u7684\u7edd\u5bf9\u8282\u62cd\u65f6\u95f4 \u76f8\u5f53\u4e8e FreeRTOS \u7684 <code>vTaskDelayUntil</code>\u3002 <code>osThreadYield()</code> \u5c06 CPU \u63a7\u5236\u6743\u4ea4\u7ed9\u53e6\u4e00\u4e2a\u5c31\u7eea\u6001\u7684\u540c\u4f18\u5148\u7ea7\u7ebf\u7a0b\u3002 \u65e0 \u76f8\u5f53\u4e8e FreeRTOS \u7684 <code>taskYIELD()</code>\u3002 \u72b6\u6001\u4e0e\u4f18\u5148\u7ea7 <code>osThreadGetState()</code> \u83b7\u53d6\u5f53\u524d\u7ebf\u7a0b\u7684\u72b6\u6001\u3002 <code>thread_id</code>: \u7ebf\u7a0b ID \u8fd4\u56de\u4e00\u4e2a <code>osThreadState_t</code> \u679a\u4e3e\u503c\uff08\u5c31\u7eea\u3001\u8fd0\u884c\u3001\u963b\u585e\u3001\u7ec8\u6b62\u7b49\uff09\u3002 <code>osThreadSetPriority()</code> \u8bbe\u7f6e\u6307\u5b9a\u7ebf\u7a0b\u7684\u4f18\u5148\u7ea7\u3002 <code>thread_id</code>: \u7ebf\u7a0b ID<code>priority</code>: \u65b0\u7684\u4f18\u5148\u7ea7 \u4f18\u5148\u7ea7\u5728 <code>osPriority_t</code> \u679a\u4e3e\u4e2d\u5b9a\u4e49\u3002 <code>osThreadGetPriority()</code> \u83b7\u53d6\u6307\u5b9a\u7ebf\u7a0b\u7684\u5f53\u524d\u4f18\u5148\u7ea7\u3002 <code>thread_id</code>: \u7ebf\u7a0b ID \u8fd4\u56de\u7ebf\u7a0b\u7684\u5f53\u524d\u4f18\u5148\u7ea7\u3002 \u7cfb\u7edf\u4fe1\u606f <code>osThreadGetStackSpace()</code> \uff08\u53ef\u9009\uff09\u83b7\u53d6\u7ebf\u7a0b\u7684\u5269\u4f59\u5806\u6808\u7a7a\u95f4\u3002 <code>thread_id</code>: \u7ebf\u7a0b ID \u8fd4\u56de\u503c\u4f9d\u8d56\u4e8e\u5177\u4f53\u5b9e\u73b0\uff0c\u53ef\u80fd\u8fd4\u56de\u5b57\u8282\u6570\u6216\u5b57\u6570\u7684\u5269\u4f59\u7a7a\u95f4\u3002 <code>osKernelGetTickCount()</code> \u83b7\u53d6\u5185\u6838\u8282\u62cd\u8ba1\u6570\u5668\u3002 \u65e0 \u76f8\u5f53\u4e8e FreeRTOS \u7684 <code>xTaskGetTickCount</code>\uff0c\u53ef\u5728\u4efb\u52a1\u548c\u4e2d\u65ad\u4e2d\u4f7f\u7528\u3002 <code>osKernelGetSysTimerCount()</code> \u83b7\u53d6\u7cfb\u7edf\u8ba1\u65f6\u5668\u7684\u7cbe\u786e\u8ba1\u6570\u503c\u3002 \u65e0 \u7528\u4e8e\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u6d4b\u91cf\u3002"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/","title":"Semaphore","text":"<ul> <li>\u4fe1\u53f7\u91cf\u7684\u57fa\u672c\u6982\u5ff5\u4e0e\u7279\u6027</li> <li>\u4fe1\u53f7\u91cf\u57fa\u672c\u64cd\u4f5c<ul> <li>\u7c7b\u6bd4\uff1a\u6e38\u4e50\u573a\u95e8\u7968\u7ba1\u7406\u5458</li> <li>\u4fe1\u53f7\u91cf\u7684\u4e24\u5927\u6838\u5fc3\u64cd\u4f5c<ul> <li>1. take() \u64cd\u4f5c - \u201c\u83b7\u53d6\u95e8\u7968\u201d</li> <li>2. give() \u64cd\u4f5c - \u201c\u5f52\u8fd8\u95e8\u7968\u201d</li> </ul> </li> <li>\u5176\u4ed6\u76f8\u5173\u64cd\u4f5c</li> <li>\u603b\u7ed3\u8868\u683c</li> </ul> </li> <li>\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u8be6\u89e3<ul> <li>\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u7279\u6027</li> <li>\u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf<ul> <li>xSemaphoreCreateBinary</li> <li>xSemaphoreCreateBinaryStatic</li> </ul> </li> </ul> </li> <li>\u8ba1\u6570\u4fe1\u53f7\u91cf\u8be6\u89e3<ul> <li>\u8ba1\u6570\u4fe1\u53f7\u91cf\u7279\u6027</li> <li>\u521b\u5efa\u8ba1\u6570\u4fe1\u53f7\u91cf<ul> <li>xSemaphoreCreateCounting</li> <li>xSemaphoreCreateCountingStatic</li> </ul> </li> </ul> </li> <li>\u4fe1\u53f7\u91cf\u57fa\u672c\u64cd\u4f5c\u51fd\u6570<ul> <li>xSemaphoreTake - \u83b7\u53d6\u4fe1\u53f7\u91cf</li> <li>xSemaphoreGive - \u91ca\u653e\u4fe1\u53f7\u91cf</li> <li>\u4e2d\u65ad\u5b89\u5168\u64cd\u4f5c\u51fd\u6570<ul> <li>xSemaphoreTakeFromISR</li> <li>xSemaphoreGiveFromISR</li> </ul> </li> </ul> </li> <li>\u4fe1\u53f7\u91cf\u5220\u9664\u51fd\u6570<ul> <li>vSemaphoreDelete</li> </ul> </li> <li>\u4fe1\u53f7\u91cf\u67e5\u8be2\u51fd\u6570<ul> <li>uxSemaphoreGetCount - \u83b7\u53d6\u4fe1\u53f7\u91cf\u8ba1\u6570</li> </ul> </li> <li>\u4fe1\u53f7\u91cf\u7efc\u5408\u5b9e\u9a8c\u6848\u4f8b\uff1a\u505c\u8f66\u573a\u7ba1\u7406\u7cfb\u7edf</li> <li>\u4fe1\u53f7\u91cf\u4f7f\u7528\u6700\u4f73\u5b9e\u8df5<ul> <li>\u4fe1\u53f7\u91cf\u7c7b\u578b\u9009\u62e9\u6307\u5357</li> <li>\u9519\u8bef\u5904\u7406\u7b56\u7565<ul> <li>\u83b7\u53d6\u8d85\u65f6\u5904\u7406</li> <li>\u4fe1\u53f7\u91cf\u521b\u5efa\u68c0\u67e5</li> </ul> </li> <li>\u6027\u80fd\u4f18\u5316\u5efa\u8bae</li> </ul> </li> <li>\u4f18\u5148\u7ea7\u53cd\u8f6c<ul> <li>\u6838\u5fc3\u6982\u5ff5\uff1a\u4ec0\u4e48\u662f\u4f18\u5148\u7ea7\u53cd\u8f6c\uff1f</li> <li>\u4e00\u4e2a\u7ecf\u5178\u7684\u4f18\u5148\u7ea7\u53cd\u8f6c\u573a\u666f\uff08\u4e3e\u4f8b\u8bf4\u660e\uff09</li> <li>\u4f18\u5148\u7ea7\u53cd\u8f6c\u7684\u5371\u5bb3</li> <li>\u89e3\u51b3\u65b9\u6848<ul> <li>\u65b9\u6848\u4e00\uff1a\u4f18\u5148\u7ea7\u7ee7\u627f</li> <li>\u65b9\u6848\u4e8c\uff1a\u4f18\u5148\u7ea7\u5929\u82b1\u677f</li> </ul> </li> <li>\u603b\u7ed3</li> <li>\u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_1","title":"\u4fe1\u53f7\u91cf\u7684\u57fa\u672c\u6982\u5ff5\u4e0e\u7279\u6027","text":"<p>\u4fe1\u53f7\u91cf\u548c\u961f\u5217\u6781\u5176\u7c7b\u4f3c\uff0c\u4e0d\u540c\u5c31\u5728\u4e8e\u4fe1\u53f7\u91cf\u4e0d\u8fdb\u884c\u6570\u636e\u8bfb\u5199\u64cd\u4f5c\uff0c\u53ea\u50a8\u5b58\u8ba1\u6570\u503c\u3002</p> <p>\u4fe1\u53f7\u91cf\u662f\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u6700\u57fa\u7840\u7684\u540c\u6b65\u673a\u5236\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u8ba1\u6570\u5668\uff0c\u7528\u4e8e\u63a7\u5236\u5bf9\u5171\u4eab\u8d44\u6e90\u7684\u8bbf\u95ee\u548c\u4efb\u52a1\u95f4\u7684\u540c\u6b65\u3002</p> <p>\u6838\u5fc3\u7279\u6027\uff1a - \u8ba1\u6570\u5668\u673a\u5236\uff1a\u901a\u8fc7\u8ba1\u6570\u503c\u7ba1\u7406\u8d44\u6e90\u53ef\u7528\u6027 - \u7ebf\u7a0b\u5b89\u5168\uff1a\u539f\u5b50\u64cd\u4f5c\uff0c\u4e0d\u4f1a\u51fa\u73b0\u7ade\u4e89\u6761\u4ef6 - \u963b\u585e\u7b49\u5f85\uff1a\u4efb\u52a1\u53ef\u4ee5\u963b\u585e\u7b49\u5f85\u4fe1\u53f7\u91cf\u53ef\u7528 - \u65e0\u6570\u636e\u4f20\u9012\uff1a\u53ea\u4f20\u9012\u4fe1\u53f7\uff0c\u4e0d\u4f20\u9012\u5b9e\u9645\u6570\u636e</p> <p>\u4fe1\u53f7\u91cf vs \u961f\u5217\uff1a</p> \u7279\u6027 \u4fe1\u53f7\u91cf \u961f\u5217 \u6570\u636e\u4f20\u9012 \u4e0d\u4f20\u9012\u6570\u636e \u4f20\u9012\u6570\u636e \u4e3b\u8981\u7528\u9014 \u540c\u6b65\u3001\u4e92\u65a5 \u6570\u636e\u4f20\u8f93 \u5185\u5b58\u5f00\u9500 \u8f83\u5c0f \u8f83\u5927 \u7075\u6d3b\u6027 \u76f8\u5bf9\u8f83\u4f4e \u8f83\u9ad8"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_2","title":"\u4fe1\u53f7\u91cf\u57fa\u672c\u64cd\u4f5c","text":"<p>\u53ef\u4ee5\u628a\u4fe1\u53f7\u91cf\u60f3\u8c61\u6210\u4e00\u4e2a\u6e38\u4e50\u573a\u7684\u95e8\u7968\u7ba1\u7406\u5458\uff0c\u4ed6\u7ba1\u7406\u7740\u4e00\u4e2a\u70ed\u95e8\u6e38\u4e50\u8bbe\u65bd\uff08\u5171\u4eab\u8d44\u6e90\uff09\u3002</p> <p>\u8fd9\u4e2a\u7ba1\u7406\u5458\u624b\u91cc\u638c\u63e1\u7740\u4e00\u5b9a\u6570\u91cf\u7684\u95e8\u7968\uff08\u4fe1\u53f7\u91cf\u7684\u8ba1\u6570\u503c\uff09\uff0c\u4ed6\u7684\u89c4\u5219\u975e\u5e38\u7b80\u5355\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_3","title":"\u7c7b\u6bd4\uff1a\u6e38\u4e50\u573a\u95e8\u7968\u7ba1\u7406\u5458","text":"<ul> <li>\u5171\u4eab\u8d44\u6e90\uff1a\u4e00\u4e2a\u53ea\u80fd\u540c\u65f6\u5bb9\u7eb3\u56fa\u5b9a\u4eba\u6570\u7684\u70ed\u95e8\u6e38\u4e50\u8bbe\u65bd\uff08\u6bd4\u5982\u201c\u592a\u7a7a\u6f29\u6da1\u201d\uff09\u3002</li> <li>\u4fe1\u53f7\u91cf\uff1a\u7ba1\u7406\u8fd9\u4e2a\u8bbe\u65bd\u7684\u7ba1\u7406\u5458\uff0c\u4ee5\u53ca\u4ed6\u624b\u91cc\u7684\u95e8\u7968\u3002</li> <li>\u4fe1\u53f7\u91cf\u8ba1\u6570\u503c\uff1a\u7ba1\u7406\u5458\u624b\u91cc\u5269\u4f59\u7684\u53ef\u76f4\u63a5\u5165\u573a\u95e8\u7968\u7684\u6570\u91cf\u3002<ul> <li>\u521d\u59cb\u503c = \u8bbe\u65bd\u7684\u5bb9\u91cf\uff08\u6bd4\u5982 3 \u5f20\u7968\uff0c\u8868\u793a\u6700\u591a3\u4e2a\u4eba\u540c\u65f6\u73a9\uff09\u3002</li> </ul> </li> <li>\u4efb\u52a1\uff1a\u60f3\u8981\u73a9\u201c\u592a\u7a7a\u6f29\u6da1\u201d\u7684\u6e38\u5ba2\u4eec\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_4","title":"\u4fe1\u53f7\u91cf\u7684\u4e24\u5927\u6838\u5fc3\u64cd\u4f5c","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#1-take-","title":"1. <code>take()</code> \u64cd\u4f5c - \u201c\u83b7\u53d6\u95e8\u7968\u201d","text":"<p>\u8fd9\u4e2a\u540d\u5b57\u6709\u5f88\u591a\u7b49\u4ef7\u53eb\u6cd5\uff1a<code>wait</code>\uff0c <code>P</code> \u64cd\u4f5c\uff0c <code>acquire</code>\uff0c <code>take</code>\u3002 \u5c31\u50cf\u6e38\u5ba2\u5bf9\u7ba1\u7406\u5458\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u5f20\u7968\uff01\u201d</p> <p>\u5f62\u8c61\u5316\u7684\u6b65\u9aa4\uff1a</p> <ol> <li>\u4f60\u8d70\u8fc7\u53bb\u95ee\u7ba1\u7406\u5458\uff1a\u201c\u6211\u8981\u4e00\u5f20\u7968\u201d\uff08\u8c03\u7528 <code>wait()</code>\uff09\u3002</li> <li>\u7ba1\u7406\u5458\u68c0\u67e5\u624b\u91cc\uff1a<ul> <li>\u60c5\u51b5A\uff08\u6709\u7968\uff09\uff1a\u5982\u679c\u8ba1\u6570\u503c &gt; 0\uff08\u6bd4\u5982\u8fd8\u67091\u5f20\u7968\uff09\uff0c\u4ed6\u4e8c\u8bdd\u4e0d\u8bf4\uff0c\u76f4\u63a5\u9012\u7ed9\u4f60\u4e00\u5f20\u7968\uff0c\u540c\u65f6\u628a\u624b\u91cc\u7684\u7968\u6570\u51cf1\uff08<code>\u8ba1\u6570\u503c--</code>\uff09\u3002\u4f60\u53ef\u4ee5\u76f4\u63a5\u8fdb\u53bb\u73a9\u4e86\uff01</li> <li>\u60c5\u51b5B\uff08\u6ca1\u7968\uff09\uff1a\u5982\u679c\u8ba1\u6570\u503c = 0\uff08\u7968\u53d1\u5b8c\u4e86\uff09\uff0c\u7ba1\u7406\u5458\u4f1a\u8ba9\u4f60\u53bb\u65c1\u8fb9\u6392\u961f\u7b49\u5f85\uff08\u4efb\u52a1\u8fdb\u5165\u963b\u585e\u72b6\u6001\uff09\u3002\u4f60\u4f1a\u88ab\u6302\u5728\u90a3\u4e2a\u8bbe\u65bd\u7684\u7b49\u5f85\u961f\u5217\u91cc\u3002</li> </ul> </li> <li>\u4ec0\u4e48\u65f6\u5019\u80fd\u9192\u6765\uff1f<ul> <li>\u5f53\u91cc\u9762\u6709\u4e00\u4e2a\u4eba\u73a9\u5b8c\u4e86\u51fa\u6765\uff08\u6267\u884c\u4e86 <code>signal()</code> \u64cd\u4f5c\uff09\uff0c\u8fd8\u56de\u4e00\u5f20\u7968\u65f6\uff0c\u7ba1\u7406\u5458\u4f1a\u53eb\u9192\u6392\u961f\u961f\u4f0d\u91cc\u7684\u7b2c\u4e00\u4e2a\u4eba\uff08\u6bd4\u5982\u6309\u7167\u4f18\u5148\u7ea7\u6216\u5148\u6765\u540e\u5230\uff09\uff0c\u628a\u7968\u7ed9\u4ed6\u3002</li> </ul> </li> </ol> <p><code>wait()</code> \u7684\u6838\u5fc3\u76ee\u7684\u5c31\u662f\u7533\u8bf7\u8d44\u6e90\u7684\u8bbf\u95ee\u6743\u3002\u5982\u679c\u7533\u8bf7\u4e0d\u5230\uff0c\u5c31\u4e56\u4e56\u6392\u961f\u7b49\u7740\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#2-give-","title":"2. <code>give()</code> \u64cd\u4f5c - \u201c\u5f52\u8fd8\u95e8\u7968\u201d","text":"<p>\u8fd9\u4e2a\u540d\u5b57\u4e5f\u6709\u5f88\u591a\u7b49\u4ef7\u53eb\u6cd5\uff1a<code>signal</code>, <code>V</code> \u64cd\u4f5c, <code>release</code>, <code>give</code>\u3002 \u5c31\u50cf\u6e38\u5ba2\u73a9\u5b8c\u4e86\u51fa\u6765\u5bf9\u7ba1\u7406\u5458\u8bf4\uff1a\u201c\u6211\u51fa\u6765\u4e86\uff0c\u7968\u8fd8\u4f60\uff01\u201d</p> <p>\u5f62\u8c61\u5316\u7684\u6b65\u9aa4\uff1a</p> <ol> <li>\u4f60\u73a9\u5b8c\u4e86\uff0c\u4ece\u51fa\u53e3\u51fa\u6765\uff08\u4e34\u754c\u533a\u4ee3\u7801\u6267\u884c\u5b8c\u6bd5\uff09\u3002</li> <li>\u4f60\u5bf9\u7ba1\u7406\u5458\u8bf4\uff1a\u201c\u6211\u8fd8\u7968\u201d\uff08\u8c03\u7528 <code>signal()</code>\uff09\u3002</li> <li>\u7ba1\u7406\u5458\u68c0\u67e5\u7b49\u5f85\u961f\u5217\uff1a<ul> <li>\u60c5\u51b5A\uff08\u6709\u4eba\u5728\u7b49\uff09\uff1a\u5982\u679c\u7b49\u5f85\u961f\u5217\u91cc\u6709\u5176\u4ed6\u6e38\u5ba2\u5728\u7b49\u7968\uff0c\u7ba1\u7406\u5458\u4e0d\u4f1a\u628a\u7968\u6536\u56de\u76d2\u5b50\uff0c\u800c\u662f\u76f4\u63a5\u62cd\u62cd\u961f\u4f0d\u7b2c\u4e00\u4e2a\u4eba\u7684\u80a9\u8180\uff0c\u628a\u8fd9\u5f20\u7968\u7ed9\u4ed6\u3002\u90a3\u4e2a\u88ab\u53eb\u9192\u7684\u4eba\u5c31\u53ef\u4ee5\u8fdb\u53bb\u73a9\u4e86\u3002\u6b64\u65f6\uff0c\u7ba1\u7406\u5458\u624b\u91cc\u7684\u7968\u6570\uff08\u8ba1\u6570\u503c\uff09\u6ca1\u6709\u53d8\u5316\uff08\u8fd8\u662f0\uff09\uff0c\u4ed6\u53ea\u662f\u5b8c\u6210\u4e86\u4e00\u6b21\u201c\u4ea4\u63a5\u201d\u3002</li> <li>\u60c5\u51b5B\uff08\u6ca1\u4eba\u5728\u7b49\uff09\uff1a\u5982\u679c\u7b49\u5f85\u961f\u5217\u662f\u7a7a\u7684\uff0c\u7ba1\u7406\u5458\u5c31\u628a\u8fd9\u5f20\u7968\u653e\u56de\u81ea\u5df1\u624b\u91cc\u7684\u7968\u5806\uff0c\u8ba9\u53ef\u7528\u7968\u6570\u52a01\uff08<code>\u8ba1\u6570\u503c++</code>\uff09\u3002</li> </ul> </li> </ol> <p><code>signal()</code> \u7684\u6838\u5fc3\u76ee\u7684\u662f\u91ca\u653e\u8d44\u6e90\u7684\u8bbf\u95ee\u6743\uff0c\u5e76\u901a\u77e5\u7b49\u5f85\u7684\u4efb\u52a1\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_5","title":"\u5176\u4ed6\u76f8\u5173\u64cd\u4f5c","text":"<p>\u9664\u4e86\u6838\u5fc3\u7684 <code>take</code> \u548c <code>give</code>\uff0c\u901a\u5e38\u8fd8\u4f1a\u6709\uff1a</p> <ul> <li> <p>\u521d\u59cb\u5316\uff1a\u5728\u6e38\u4e50\u573a\u5f00\u95e8\u524d\uff0c\u8bbe\u5b9a\u8fd9\u4e2a\u8bbe\u65bd\u521d\u59cb\u6709\u591a\u5c11\u5f20\u7968\u53ef\u7528\uff08\u8bbe\u7f6e\u4fe1\u53f7\u91cf\u7684\u521d\u59cb\u8ba1\u6570\u503c\uff09\u3002</p> <ul> <li>\u521d\u59cb\u503c = 1\uff1a\u8fd9\u5c31\u662f\u6211\u4eec\u5e38\u8bf4\u7684\u4e92\u65a5\u9501\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u8bbe\u65bd\u4e00\u6b21\u53ea\u5141\u8bb8\u4e00\u4e2a\u4eba\u8fdb\u53bb\uff08\u6bd4\u5982\u4e00\u4e2a\u5355\u4eba\u95f4\u9b3c\u5c4b\uff09\u3002</li> <li>\u521d\u59cb\u503c = N (N&gt;1)\uff1a\u8fd9\u79f0\u4e3a\u8ba1\u6570\u4fe1\u53f7\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u8bbf\u95ee\u6570\u91cf\u6709\u9650\u7684\u8d44\u6e90\u6c60\uff08\u6bd4\u5982\u67093\u8f86\u7684\u78b0\u78b0\u8f66\u573a\uff0c\u670910\u4e2a\u8fde\u63a5\u7684\u6570\u636e\u5e93\u8fde\u63a5\u6c60\uff09\u3002</li> </ul> </li> <li> <p>\u975e\u963b\u585e\u7b49\u5f85\uff1a<code>tryWait()</code></p> <ul> <li>\u4f60\u8d70\u8fc7\u53bb\u95ee\u7ba1\u7406\u5458\uff1a\u201c\u6709\u7968\u5417\uff1f\u7acb\u523b\u544a\u8bc9\u6211\uff01\u6211\u4e0d\u60f3\u6392\u961f\u3002\u201d</li> <li>\u5982\u679c\u6ca1\u7968\uff0c\u4ed6\u4e0d\u4f1a\u8ba9\u4f60\u6392\u961f\uff0c\u800c\u662f\u76f4\u63a5\u544a\u8bc9\u4f60\uff1a\u201c\u6ca1\u7968\uff01\u201d\uff0c\u7136\u540e\u4f60\u5c31\u53ef\u4ee5\u8f6c\u8eab\u53bb\u73a9\u522b\u7684\u4e86\uff08\u51fd\u6570\u7acb\u5373\u8fd4\u56de\u9519\u8bef\u7801\uff09\u3002</li> </ul> </li> <li> <p>\u5e26\u8d85\u65f6\u7684\u7b49\u5f85\uff1a<code>timedWait()</code></p> <ul> <li>\u4f60\u8d70\u8fc7\u53bb\u8bf4\uff1a\u201c\u6211\u8981\u4e00\u5f20\u7968\uff0c\u4f46\u6211\u53ea\u63925\u5206\u949f\u7684\u961f\u3002\u201d</li> <li>\u5982\u679c5\u5206\u949f\u5185\u6709\u4eba\u8fd8\u7968\uff0c\u4f60\u5c31\u80fd\u8fdb\u53bb\u3002</li> <li>\u5982\u679c5\u5206\u949f\u5230\u4e86\u8fd8\u6ca1\u7968\uff0c\u4f60\u5c31\u4e0d\u6392\u4e86\uff0c\u81ea\u5df1\u79bb\u5f00\uff08\u4efb\u52a1\u4ece\u963b\u585e\u72b6\u6001\u8d85\u65f6\u8fd4\u56de\uff09\u3002</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_6","title":"\u603b\u7ed3\u8868\u683c","text":"\u64cd\u4f5c \u7f16\u7a0b\u51fd\u6570\u540d \u5f62\u8c61\u5316\u52a8\u4f5c \u5bf9\u8ba1\u6570\u503c\u7684\u5f71\u54cd \u5bf9\u4efb\u52a1\u7684\u5f71\u54cd \u521d\u59cb\u5316 <code>sem_init(&amp;sem, N)</code> \u7ba1\u7406\u5458\u51c6\u5907N\u5f20\u95e8\u7968 \u8bbe\u7f6e\u4e3aN \u65e0 \u7533\u8bf7\u8d44\u6e90 <code>sem_wait()</code> \u201c\u6211\u8981\u4e00\u5f20\u7968\uff01\u201d \u5982\u679c\u6709\u7968\uff0c\u5219\u51cf1 \u5982\u679c\u6ca1\u7968\uff0c\u5c31\u6392\u961f\u963b\u585e \u91ca\u653e\u8d44\u6e90 <code>sem_post()</code> \u201c\u6211\u51fa\u6765\u4e86\uff0c\u8fd8\u7968\uff01\u201d \u5982\u679c\u6ca1\u4eba\u7b49\uff0c\u5219\u52a01\u5982\u679c\u6709\u4eba\u7b49\uff0c\u4e0d\u53d8 \u53eb\u9192\u4e00\u4e2a\u6392\u961f\u7684\u4eba \u5c1d\u8bd5\u7533\u8bf7 <code>sem_trywait()</code> \u201c\u6709\u73b0\u7968\u5417\uff1f\u6ca1\u6709\u6211\u8d70\u4e86\u201d \u6709\u7968\u5219\u51cf1\uff0c\u65e0\u7968\u4e0d\u53d8 \u4e0d\u963b\u585e\uff0c\u7acb\u5373\u8fd4\u56de \u9650\u65f6\u7533\u8bf7 <code>sem_timedwait()</code> \u201c\u6211\u63925\u5206\u949f\u961f\uff0c\u7b49\u4e0d\u5230\u5c31\u8d70\u201d \u8d85\u65f6\u5185\u7b49\u5230\u7968\u5219\u51cf1 \u8d85\u65f6\u5219\u7ed3\u675f\u963b\u585e"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_7","title":"\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_8","title":"\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u7279\u6027","text":"<p>\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u662f\u6700\u7b80\u5355\u7684\u4fe1\u53f7\u91cf\u7c7b\u578b\uff1a - \u8ba1\u6570\u503c\u8303\u56f4\uff1a\u53ea\u67090\u548c1\u4e24\u79cd\u72b6\u6001 - \u884c\u4e3a\u6a21\u5f0f\uff1a\u7c7b\u4f3c\u4e8e\u9501\u673a\u5236 - \u5e94\u7528\u573a\u666f\uff1a\u4efb\u52a1\u540c\u6b65\u3001\u4e92\u65a5\u8bbf\u95ee</p> <p>\u72b6\u6001\u8bf4\u660e\uff1a - 1\uff1a\u4fe1\u53f7\u91cf\u53ef\u7528\uff0c\u4efb\u52a1\u53ef\u4ee5\u83b7\u53d6 - 0\uff1a\u4fe1\u53f7\u91cf\u4e0d\u53ef\u7528\uff0c\u4efb\u52a1\u9700\u8981\u7b49\u5f85</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_9","title":"\u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphorecreatebinary","title":"xSemaphoreCreateBinary","text":"<pre><code>SemaphoreHandle_t xSemaphoreCreateBinary(void);\n</code></pre> <p>\u91cd\u8981\u7279\u6027\uff1a - \u521b\u5efa\u540e\u521d\u59cb\u72b6\u6001\u4e3a0\uff08\u4e0d\u53ef\u7528\uff09 - \u9700\u8981\u624b\u52a8\u8c03\u7528<code>xSemaphoreGive</code>\u4f7f\u5176\u53ef\u7528 - \u9002\u7528\u4e8e\u52a8\u6001\u5185\u5b58\u5206\u914d</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xBinarySemaphore;\n\nvoid vInitSemaphores(void) {\n    xBinarySemaphore = xSemaphoreCreateBinary();\n\n    if(xBinarySemaphore != NULL) {\n        // \u521b\u5efa\u6210\u529f\u540e\u7acb\u5373\u4f7f\u5176\u53ef\u7528\n        xSemaphoreGive(xBinarySemaphore);\n        printf(\"Binary semaphore created and made available\\n\");\n    } else {\n        printf(\"Failed to create binary semaphore\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphorecreatebinarystatic","title":"xSemaphoreCreateBinaryStatic","text":"<p>\u9759\u6001\u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\uff1a</p> <pre><code>SemaphoreHandle_t xSemaphoreCreateBinaryStatic(StaticSemaphore_t *pxSemaphoreBuffer);\n</code></pre> <p>\u9759\u6001\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u9759\u6001\u5206\u914d\u4fe1\u53f7\u91cf\u63a7\u5236\u5757\nstatic StaticSemaphore_t xBinarySemaphoreBuffer;\n\nvoid vInitStaticSemaphores(void) {\n    // \u9759\u6001\u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\n    SemaphoreHandle_t xStaticBinarySemaphore;\n    xStaticBinarySemaphore = xSemaphoreCreateBinaryStatic(&amp;xBinarySemaphoreBuffer);\n\n    if(xStaticBinarySemaphore != NULL) {\n        xSemaphoreGive(xStaticBinarySemaphore);\n        printf(\"Static binary semaphore created successfully\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_10","title":"\u8ba1\u6570\u4fe1\u53f7\u91cf\u8be6\u89e3","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_11","title":"\u8ba1\u6570\u4fe1\u53f7\u91cf\u7279\u6027","text":"<p>\u8ba1\u6570\u4fe1\u53f7\u91cf\u7528\u4e8e\u7ba1\u7406\u591a\u4e2a\u76f8\u540c\u7684\u8d44\u6e90\uff1a - \u8ba1\u6570\u503c\u8303\u56f4\uff1a0\u5230\u6700\u5927\u8ba1\u6570\u503c - \u8d44\u6e90\u7ba1\u7406\uff1a\u8ba1\u6570\u503c\u8868\u793a\u53ef\u7528\u8d44\u6e90\u6570\u91cf - \u5e94\u7528\u573a\u666f\uff1a\u8d44\u6e90\u6c60\u7ba1\u7406\u3001\u9650\u6d41\u63a7\u5236</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_12","title":"\u521b\u5efa\u8ba1\u6570\u4fe1\u53f7\u91cf","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphorecreatecounting","title":"xSemaphoreCreateCounting","text":"<pre><code>SemaphoreHandle_t xSemaphoreCreateCounting(UBaseType_t uxMaxCount,\n                                         UBaseType_t uxInitialCount);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>uxMaxCount</code>\uff1a\u4fe1\u53f7\u91cf\u6700\u5927\u8ba1\u6570\u503c - <code>uxInitialCount</code>\uff1a\u4fe1\u53f7\u91cf\u521d\u59cb\u8ba1\u6570\u503c</p> <p>\u521b\u5efa\u793a\u4f8b\uff1a</p> <pre><code>// \u521b\u5efa\u5404\u79cd\u8ba1\u6570\u4fe1\u53f7\u91cf\u793a\u4f8b\nSemaphoreHandle_t xParkingSemaphore;      // \u505c\u8f66\u573a\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xBufferSemaphore;       // \u7f13\u51b2\u533a\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xConnectionSemaphore;   // \u8fde\u63a5\u6570\u4fe1\u53f7\u91cf\n\nvoid vCreateCountingSemaphores(void) {\n    // \u505c\u8f66\u573a\uff1a10\u4e2a\u8f66\u4f4d\uff0c\u521d\u59cb\u5168\u90e8\u7a7a\u7740\n    xParkingSemaphore = xSemaphoreCreateCounting(10, 10);\n\n    // \u7f13\u51b2\u533a\uff1a\u6700\u591a5\u4e2a\u7f13\u51b2\u5757\uff0c\u521d\u59cb\u67093\u4e2a\u53ef\u7528\n    xBufferSemaphore = xSemaphoreCreateCounting(5, 3);\n\n    // \u6570\u636e\u5e93\u8fde\u63a5\uff1a\u6700\u591a8\u4e2a\u8fde\u63a5\uff0c\u521d\u59cb\u5168\u90e8\u53ef\u7528\n    xConnectionSemaphore = xSemaphoreCreateCounting(8, 8);\n\n    // \u68c0\u67e5\u521b\u5efa\u7ed3\u679c\n    if(xParkingSemaphore == NULL || xBufferSemaphore == NULL || xConnectionSemaphore == NULL) {\n        printf(\"Error: Failed to create counting semaphores\\n\");\n    } else {\n        printf(\"All counting semaphores created successfully\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphorecreatecountingstatic","title":"xSemaphoreCreateCountingStatic","text":"<p>\u9759\u6001\u521b\u5efa\u8ba1\u6570\u4fe1\u53f7\u91cf\uff1a</p> <pre><code>SemaphoreHandle_t xSemaphoreCreateCountingStatic(UBaseType_t uxMaxCount,\n                                                UBaseType_t uxInitialCount,\n                                                StaticSemaphore_t *pxSemaphoreBuffer);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_13","title":"\u4fe1\u53f7\u91cf\u57fa\u672c\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphoretake-","title":"xSemaphoreTake - \u83b7\u53d6\u4fe1\u53f7\u91cf","text":"<pre><code>BaseType_t xSemaphoreTake(SemaphoreHandle_t xSemaphore, \n                         TickType_t xTicksToWait);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xSemaphore</code>\uff1a\u4fe1\u53f7\u91cf\u53e5\u67c4 - <code>xTicksToWait</code>\uff1a\u963b\u585e\u8d85\u65f6\u65f6\u95f4</p> <p>\u963b\u585e\u884c\u4e3a\uff1a - <code>0</code>\uff1a\u975e\u963b\u585e\uff0c\u7acb\u5373\u8fd4\u56de - <code>portMAX_DELAY</code>\uff1a\u65e0\u9650\u963b\u585e - <code>N</code>\uff1a\u963b\u585eN\u4e2a\u65f6\u949f\u8282\u62cd</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>void vTaskUsingSemaphore(void *pvParameters) {\n    TickType_t xLastWakeTime = xTaskGetTickCount();\n\n    while(1) {\n        // \u5c1d\u8bd5\u83b7\u53d6\u4fe1\u53f7\u91cf\uff0c\u7b49\u5f85100ms\n        if(xSemaphoreTake(xBinarySemaphore, 100 / portTICK_PERIOD_MS) == pdTRUE) {\n            // \u6210\u529f\u83b7\u53d6\u4fe1\u53f7\u91cf\uff0c\u6267\u884c\u53d7\u4fdd\u62a4\u7684\u64cd\u4f5c\n            printf(\"Semaphore acquired, performing critical operation\\n\");\n            vPerformCriticalOperation();\n\n            // \u91ca\u653e\u4fe1\u53f7\u91cf\n            xSemaphoreGive(xBinarySemaphore);\n            printf(\"Semaphore released\\n\");\n        } else {\n            // \u83b7\u53d6\u4fe1\u53f7\u91cf\u8d85\u65f6\n            printf(\"Failed to acquire semaphore within timeout\\n\");\n        }\n\n        // \u4efb\u52a1\u5468\u671f\u6267\u884c\n        vTaskDelayUntil(&amp;xLastWakeTime, 1000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphoregive-","title":"xSemaphoreGive - \u91ca\u653e\u4fe1\u53f7\u91cf","text":"<p>\u4efb\u4f55\u4efb\u52a1\u90fd\u53ef\u4ee5\u91ca\u653e</p> <pre><code>BaseType_t xSemaphoreGive(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u8fd4\u56de\u503c\uff1a - <code>pdTRUE</code>\uff1a\u6210\u529f\u91ca\u653e\u4fe1\u53f7\u91cf - <code>pdFALSE</code>\uff1a\u91ca\u653e\u5931\u8d25\uff08\u4fe1\u53f7\u91cf\u5df2\u6ee1\uff09</p> <p>\u91ca\u653e\u793a\u4f8b\uff1a</p> <pre><code>void vProducerTask(void *pvParameters) {\n    while(1) {\n        // \u751f\u4ea7\u6570\u636e\n        vProduceData();\n\n        // \u91ca\u653e\u4fe1\u53f7\u91cf\u901a\u77e5\u6d88\u8d39\u8005\n        if(xSemaphoreGive(xDataReadySemaphore) == pdTRUE) {\n            printf(\"Signal sent to consumer\\n\");\n        } else {\n            printf(\"Failed to give semaphore\\n\");\n        }\n\n        vTaskDelay(500 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_14","title":"\u4e2d\u65ad\u5b89\u5168\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphoretakefromisr","title":"xSemaphoreTakeFromISR","text":"<pre><code>BaseType_t xSemaphoreTakeFromISR(SemaphoreHandle_t xSemaphore,\n                                BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#xsemaphoregivefromisr","title":"xSemaphoreGiveFromISR","text":"<pre><code>BaseType_t xSemaphoreGiveFromISR(SemaphoreHandle_t xSemaphore,\n                                BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre> <p>\u4e2d\u65ad\u4e2d\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>// \u5916\u90e8\u4e2d\u65ad\u5904\u7406\u51fd\u6570\nvoid EXTI0_IRQHandler(void) {\n    BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n\n    // \u6e05\u9664\u4e2d\u65ad\u6807\u5fd7\n    EXTI-&gt;PR = EXTI_PR_PR0;\n\n    // \u5728\u4e2d\u65ad\u4e2d\u91ca\u653e\u4fe1\u53f7\u91cf\n    if(xSemaphoreGiveFromISR(xInterruptSemaphore, &amp;xHigherPriorityTaskWoken) == pdTRUE) {\n        printf(\"Semaphore given from ISR\\n\");\n    }\n\n    // \u5fc5\u8981\u65f6\u8fdb\u884c\u4efb\u52a1\u5207\u6362\n    portYIELD_FROM_ISR(xHigherPriorityTaskWoken);\n}\n\n// \u5904\u7406\u4e2d\u65ad\u7684\u4efb\u52a1\nvoid vInterruptHandlerTask(void *pvParameters) {\n    while(1) {\n        // \u7b49\u5f85\u4e2d\u65ad\u4fe1\u53f7\n        if(xSemaphoreTake(xInterruptSemaphore, portMAX_DELAY) == pdTRUE) {\n            printf(\"Interrupt received, processing...\\n\");\n            vProcessInterrupt();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_15","title":"\u4fe1\u53f7\u91cf\u5220\u9664\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#vsemaphoredelete","title":"vSemaphoreDelete","text":"<pre><code>void vSemaphoreDelete(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u5220\u9664\u4fe1\u53f7\u91cf\u793a\u4f8b\uff1a</p> <pre><code>void vCleanupResources(void) {\n    // \u5220\u9664\u4e0d\u518d\u4f7f\u7528\u7684\u4fe1\u53f7\u91cf\n    if(xBinarySemaphore != NULL) {\n        vSemaphoreDelete(xBinarySemaphore);\n        xBinarySemaphore = NULL;\n        printf(\"Binary semaphore deleted\\n\");\n    }\n\n    if(xCountingSemaphore != NULL) {\n        vSemaphoreDelete(xCountingSemaphore);\n        xCountingSemaphore = NULL;\n        printf(\"Counting semaphore deleted\\n\");\n    }\n\n    if(xMutex != NULL) {\n        vSemaphoreDelete(xMutex);\n        xMutex = NULL;\n        printf(\"Mutex deleted\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_16","title":"\u4fe1\u53f7\u91cf\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#uxsemaphoregetcount-","title":"uxSemaphoreGetCount - \u83b7\u53d6\u4fe1\u53f7\u91cf\u8ba1\u6570","text":"<pre><code>UBaseType_t uxSemaphoreGetCount(SemaphoreHandle_t xSemaphore);\n</code></pre> <p>\u67e5\u8be2\u793a\u4f8b\uff1a</p> <pre><code>void vMonitorSemaphoreStatus(void) {\n    UBaseType_t uxBinaryCount, uxCountingCount, uxMutexCount;\n\n    // \u83b7\u53d6\u5404\u79cd\u4fe1\u53f7\u91cf\u7684\u5f53\u524d\u8ba1\u6570\u503c\n    uxBinaryCount = uxSemaphoreGetCount(xBinarySemaphore);\n    uxCountingCount = uxSemaphoreGetCount(xCountingSemaphore);\n    uxMutexCount = uxSemaphoreGetCount(xMutex);\n\n    printf(\"Semaphore Status:\\n\");\n    printf(\"  Binary: %lu\\n\", uxBinaryCount);\n    printf(\"  Counting: %lu\\n\", uxCountingCount);\n    printf(\"  Mutex: %lu\\n\", uxMutexCount);\n\n    // \u6839\u636e\u72b6\u6001\u91c7\u53d6\u884c\u52a8\n    if(uxCountingCount == 0) {\n        printf(\"Warning: No resources available in counting semaphore!\\n\");\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_17","title":"\u4fe1\u53f7\u91cf\u7efc\u5408\u5b9e\u9a8c\u6848\u4f8b\uff1a\u505c\u8f66\u573a\u7ba1\u7406\u7cfb\u7edf","text":"<pre><code>#include \"FreeRTOS.h\"\n#include \"task.h\"\n#include \"semphr.h\"\n#include \"stdio.h\"\n#include \"string.h\"\n\n// \u7cfb\u7edf\u4fe1\u53f7\u91cf\u5b9a\u4e49\nSemaphoreHandle_t xParkingSpaces;      // \u505c\u8f66\u4f4d\u8ba1\u6570\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xEntryGate;          // \u5165\u53e3\u95f8\u673a\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xExitGate;           // \u51fa\u53e3\u95f8\u673a\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\nSemaphoreHandle_t xDisplayMutex;       // \u663e\u793a\u5c4f\u4e92\u65a5\u91cf\n\n// \u7cfb\u7edf\u72b6\u6001\u53d8\u91cf\nstatic uint32_t ulCarsInParking = 0;\nstatic uint32_t ulTotalCarsServed = 0;\n\n// \u663e\u793a\u5c4f\u8f93\u51fa\u51fd\u6570\uff08\u53d7\u4e92\u65a5\u91cf\u4fdd\u62a4\uff09\nvoid vSafePrintf(const char *format, ...) {\n    va_list args;\n\n    // \u83b7\u53d6\u663e\u793a\u5c4f\u4e92\u65a5\u91cf\n    if(xSemaphoreTake(xDisplayMutex, 100 / portTICK_PERIOD_MS) == pdTRUE) {\n        va_start(args, format);\n        vprintf(format, args);\n        va_end(args);\n\n        // \u91ca\u653e\u4e92\u65a5\u91cf\n        xSemaphoreGive(xDisplayMutex);\n    }\n}\n\n// \u8f66\u8f86\u5165\u573a\u4efb\u52a1\nvoid vCarEntryTask(void *pvParameters) {\n    uint8_t car_id = (uint8_t)pvParameters;\n    char car_plate[10];\n\n    // \u751f\u6210\u8f66\u724c\u53f7\n    snprintf(car_plate, sizeof(car_plate), \"CAR%03d\", car_id);\n\n    vSafePrintf(\"[ENTRY] Car %s arrived at entrance\\n\", car_plate);\n\n    while(1) {\n        // \u7b49\u5f85\u5165\u53e3\u95f8\u673a\u53ef\u7528\uff08\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\uff09\n        if(xSemaphoreTake(xEntryGate, 2000 / portTICK_PERIOD_MS) == pdTRUE) {\n            vSafePrintf(\"[ENTRY] Car %s is at gate\\n\", car_plate);\n\n            // \u5c1d\u8bd5\u83b7\u53d6\u505c\u8f66\u4f4d\uff08\u8ba1\u6570\u4fe1\u53f7\u91cf\uff09\n            if(xSemaphoreTake(xParkingSpaces, 1000 / portTICK_PERIOD_MS) == pdTRUE) {\n                // \u6210\u529f\u83b7\u53d6\u505c\u8f66\u4f4d\n                ulCarsInParking++;\n                ulTotalCarsServed++;\n\n                vSafePrintf(\"[ENTRY] Car %s entered. Parking: %lu/%lu, Total: %lu\\n\",\n                           car_plate, ulCarsInParking, \n                           uxSemaphoreGetCount(xParkingSpaces) + ulCarsInParking,\n                           ulTotalCarsServed);\n\n                // \u91ca\u653e\u5165\u53e3\u95f8\u673a\n                xSemaphoreGive(xEntryGate);\n\n                // \u6a21\u62df\u505c\u8f66\u65f6\u95f4\uff081-10\u79d2\uff09\n                vTaskDelay((1000 + (rand() % 9000)) / portTICK_PERIOD_MS);\n\n                // \u8f66\u8f86\u51c6\u5907\u79bb\u5f00\n                vSafePrintf(\"[EXIT]  Car %s preparing to leave\\n\", car_plate);\n\n                // \u7b49\u5f85\u51fa\u53e3\u95f8\u673a\n                if(xSemaphoreTake(xExitGate, 2000 / portTICK_PERIOD_MS) == pdTRUE) {\n                    // \u91ca\u653e\u505c\u8f66\u4f4d\n                    xSemaphoreGive(xParkingSpaces);\n                    ulCarsInParking--;\n\n                    vSafePrintf(\"[EXIT]  Car %s left. Parking: %lu/%lu\\n\",\n                               car_plate, ulCarsInParking,\n                               uxSemaphoreGetCount(xParkingSpaces) + ulCarsInParking);\n\n                    // \u91ca\u653e\u51fa\u53e3\u95f8\u673a\n                    xSemaphoreGive(xExitGate);\n                }\n\n            } else {\n                // \u6ca1\u6709\u505c\u8f66\u4f4d\u53ef\u7528\n                vSafePrintf(\"[ENTRY] Car %s left - no parking space available\\n\", car_plate);\n                xSemaphoreGive(xEntryGate);\n            }\n        } else {\n            vSafePrintf(\"[ENTRY] Car %s gave up waiting for gate\\n\", car_plate);\n        }\n\n        // \u8f66\u8f86\u79bb\u5f00\u540e\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u518d\u56de\u6765\n        vTaskDelay((5000 + (rand() % 10000)) / portTICK_PERIOD_MS);\n        vSafePrintf(\"[ENTRY] Car %s arrived again\\n\", car_plate);\n    }\n}\n\n// \u505c\u8f66\u573a\u76d1\u63a7\u4efb\u52a1\nvoid vParkingMonitorTask(void *pvParameters) {\n    UBaseType_t uxAvailableSpaces;\n    UBaseType_t uxMaxSpaces = 10;  // \u5047\u8bbe\u603b\u517110\u4e2a\u8f66\u4f4d\n\n    vSafePrintf(\"[MONITOR] Parking monitor started\\n\");\n\n    while(1) {\n        // \u83b7\u53d6\u5f53\u524d\u53ef\u7528\u8f66\u4f4d\u6570\u91cf\n        uxAvailableSpaces = uxSemaphoreGetCount(xParkingSpaces);\n\n        vSafePrintf(\"\\n=== Parking Status ===\\n\");\n        vSafePrintf(\"Available spaces: %lu\\n\", uxAvailableSpaces);\n        vSafePrintf(\"Cars in parking:  %lu\\n\", ulCarsInParking);\n        vSafePrintf(\"Total served:     %lu\\n\", ulTotalCarsServed);\n        vSafePrintf(\"Utilization:      %.1f%%\\n\", \n                   (float)(uxMaxSpaces - uxAvailableSpaces) / uxMaxSpaces * 100);\n\n        // \u72b6\u6001\u8b66\u544a\n        if(uxAvailableSpaces == 0) {\n            vSafePrintf(\"WARNING: Parking lot FULL!\\n\");\n        } else if(uxAvailableSpaces &lt;= 2) {\n            vSafePrintf(\"WARNING: Parking lot almost full\\n\");\n        }\n\n        vSafePrintf(\"=====================\\n\\n\");\n\n        // \u6bcf5\u79d2\u76d1\u63a7\u4e00\u6b21\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n    }\n}\n\n// \u7d27\u6025\u8f66\u8f86\u4f18\u5148\u4efb\u52a1\nvoid vEmergencyVehicleTask(void *pvParameters) {\n    vSafePrintf(\"[EMERGENCY] Emergency vehicle service started\\n\");\n\n    while(1) {\n        // \u968f\u673a\u6a21\u62df\u7d27\u6025\u8f66\u8f86\u5230\u8fbe\n        vTaskDelay((30000 + (rand() % 30000)) / portTICK_PERIOD_MS);\n\n        vSafePrintf(\"[EMERGENCY] Emergency vehicle arriving!\\n\");\n\n        // \u7d27\u6025\u8f66\u8f86\u4f18\u5148\u5165\u573a\n        if(xSemaphoreTake(xEntryGate, portMAX_DELAY) == pdTRUE) {\n            vSafePrintf(\"[EMERGENCY] Emergency vehicle entering\\n\");\n\n            // \u5f3a\u5236\u83b7\u53d6\u505c\u8f66\u4f4d\uff08\u53ef\u80fd\u7b49\u5f85\uff09\n            if(xSemaphoreTake(xParkingSpaces, portMAX_DELAY) == pdTRUE) {\n                ulCarsInParking++;\n                ulTotalCarsServed++;\n\n                vSafePrintf(\"[EMERGENCY] Emergency vehicle parked\\n\");\n\n                // \u91ca\u653e\u5165\u53e3\u95f8\u673a\n                xSemaphoreGive(xEntryGate);\n\n                // \u7d27\u6025\u5904\u7406\u65f6\u95f4\u8f83\u77ed\n                vTaskDelay(2000 / portTICK_PERIOD_MS);\n\n                // \u7d27\u6025\u79bb\u5f00\n                if(xSemaphoreTake(xExitGate, portMAX_DELAY) == pdTRUE) {\n                    xSemaphoreGive(xParkingSpaces);\n                    ulCarsInParking--;\n\n                    vSafePrintf(\"[EMERGENCY] Emergency vehicle departed\\n\");\n                    xSemaphoreGive(xExitGate);\n                }\n            }\n        }\n    }\n}\n\n// \u7cfb\u7edf\u521d\u59cb\u5316\u51fd\u6570\nvoid vInitializeParkingSystem(void) {\n    printf(\"Initializing Parking Management System...\\n\");\n\n    // \u521b\u5efa\u8ba1\u6570\u4fe1\u53f7\u91cf\uff1a10\u4e2a\u505c\u8f66\u4f4d\uff0c\u521d\u59cb\u5168\u90e8\u53ef\u7528\n    xParkingSpaces = xSemaphoreCreateCounting(10, 10);\n\n    // \u521b\u5efa\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\uff1a\u5165\u53e3\u548c\u51fa\u53e3\u95f8\u673a\uff0c\u521d\u59cb\u53ef\u7528\n    xEntryGate = xSemaphoreCreateBinary();\n    xExitGate = xSemaphoreCreateBinary();\n\n    // \u521b\u5efa\u4e92\u65a5\u91cf\uff1a\u4fdd\u62a4\u663e\u793a\u5c4f\u8f93\u51fa\n    xDisplayMutex = xSemaphoreCreateMutex();\n\n    // \u68c0\u67e5\u4fe1\u53f7\u91cf\u521b\u5efa\u7ed3\u679c\n    if(xParkingSpaces == NULL || xEntryGate == NULL || \n       xExitGate == NULL || xDisplayMutex == NULL) {\n        printf(\"ERROR: Failed to create semaphores!\\n\");\n        while(1); // \u7cfb\u7edf\u505c\u6b62\n    }\n\n    // \u4f7f\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u521d\u59cb\u53ef\u7528\n    xSemaphoreGive(xEntryGate);\n    xSemaphoreGive(xExitGate);\n\n    printf(\"Parking system initialized successfully\\n\");\n    printf(\"Total parking spaces: 10\\n\");\n}\n\n// \u4e3b\u51fd\u6570\nint main(void) {\n    // \u521d\u59cb\u5316\u505c\u8f66\u573a\u7cfb\u7edf\n    vInitializeParkingSystem();\n\n    // \u521b\u5efa\u8f66\u8f86\u4efb\u52a1\uff085\u8f86\u5e38\u89c4\u8f66\u8f86\uff09\n    for(int i = 1; i &lt;= 5; i++) {\n        xTaskCreate(vCarEntryTask, \"CarEntry\", 1024, (void*)i, 2, NULL);\n    }\n\n    // \u521b\u5efa\u76d1\u63a7\u4efb\u52a1\n    xTaskCreate(vParkingMonitorTask, \"Monitor\", 1024, NULL, 1, NULL);\n\n    // \u521b\u5efa\u7d27\u6025\u8f66\u8f86\u4efb\u52a1\uff08\u8f83\u9ad8\u4f18\u5148\u7ea7\uff09\n    xTaskCreate(vEmergencyVehicleTask, \"Emergency\", 1024, NULL, 3, NULL);\n\n    printf(\"Starting FreeRTOS scheduler...\\n\");\n    vTaskStartScheduler();\n\n    // \u5982\u679c\u8c03\u5ea6\u5668\u542f\u52a8\u5931\u8d25\n    while(1) {\n        printf(\"ERROR: Scheduler failed to start!\\n\");\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_18","title":"\u4fe1\u53f7\u91cf\u4f7f\u7528\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_19","title":"\u4fe1\u53f7\u91cf\u7c7b\u578b\u9009\u62e9\u6307\u5357","text":"<p>\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u9002\u7528\u573a\u666f\uff1a - \u4efb\u52a1\u95f4\u540c\u6b65 - \u7b80\u5355\u7684\u4e92\u65a5\u4fdd\u62a4 - \u4e8b\u4ef6\u901a\u77e5\u673a\u5236</p> <p>\u8ba1\u6570\u4fe1\u53f7\u91cf\u9002\u7528\u573a\u666f\uff1a - \u8d44\u6e90\u6c60\u7ba1\u7406\uff08\u8fde\u63a5\u6c60\u3001\u7f13\u51b2\u533a\u7b49\uff09 - \u9650\u6d41\u63a7\u5236 - \u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u6a21\u5f0f\u4e2d\u7684\u8d44\u6e90\u8ba1\u6570</p> <p>\u4e92\u65a5\u91cf\u9002\u7528\u573a\u666f\uff1a - \u5171\u4eab\u8d44\u6e90\u4fdd\u62a4 - \u53ef\u80fd\u53d1\u751f\u4f18\u5148\u7ea7\u53cd\u8f6c\u7684\u60c5\u51b5 - \u9700\u8981\u9012\u5f52\u8bbf\u95ee\u7684\u5171\u4eab\u8d44\u6e90</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_20","title":"\u9519\u8bef\u5904\u7406\u7b56\u7565","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_21","title":"\u83b7\u53d6\u8d85\u65f6\u5904\u7406","text":"<pre><code>BaseType_t xResult = xSemaphoreTake(xSemaphore, timeout);\nif(xResult != pdTRUE) {\n    // \u5904\u7406\u7b56\u7565\uff1a\u91cd\u8bd5\u3001\u4f7f\u7528\u5907\u7528\u65b9\u6848\u3001\u62a5\u544a\u9519\u8bef\u7b49\n    handle_semaphore_timeout();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_22","title":"\u4fe1\u53f7\u91cf\u521b\u5efa\u68c0\u67e5","text":"<pre><code>xSemaphore = xSemaphoreCreateBinary();\nif(xSemaphore == NULL) {\n    // \u5185\u5b58\u4e0d\u8db3\u5904\u7406\n    handle_memory_allocation_failure();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_23","title":"\u6027\u80fd\u4f18\u5316\u5efa\u8bae","text":"<ol> <li>\u5408\u7406\u8bbe\u7f6e\u8d85\u65f6\uff1a\u907f\u514d\u4efb\u52a1\u6c38\u4e45\u963b\u585e</li> <li>\u4f7f\u7528\u9759\u6001\u5206\u914d\uff1a\u5728\u786e\u5b9a\u6027\u7cfb\u7edf\u4e2d\u51cf\u5c11\u52a8\u6001\u5206\u914d</li> <li>\u907f\u514d\u4fe1\u53f7\u91cf\u6ee5\u7528\uff1a\u7b80\u5355\u7684\u6807\u5fd7\u4f4d\u53ef\u4ee5\u4f7f\u7528\u4efb\u52a1\u901a\u77e5\u66ff\u4ee3</li> <li>\u6ce8\u610f\u4f18\u5148\u7ea7\u5b89\u6392\uff1a\u9632\u6b62\u4f18\u5148\u7ea7\u53cd\u8f6c\u548c\u6b7b\u9501</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_24","title":"\u4f18\u5148\u7ea7\u53cd\u8f6c","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u5728\u5b9e\u65f6\u7cfb\u7edf\u6216\u591a\u4efb\u52a1\u7cfb\u7edf\u4e2d\u975e\u5e38\u91cd\u8981\u4e14\u7ecf\u5178\u7684\u95ee\u9898\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_25","title":"\u6838\u5fc3\u6982\u5ff5\uff1a\u4ec0\u4e48\u662f\u4f18\u5148\u7ea7\u53cd\u8f6c\uff1f","text":"<p>\u4f18\u5148\u7ea7\u53cd\u8f6c\u662f\u6307\uff1a\u5728\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u7b49\u5f85\u4e00\u4e2a\u88ab\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u5360\u6709\u7684\u8d44\u6e90\u65f6\uff0c\u88ab\u4e00\u4e2a\u4e2d\u95f4\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u62a2\u5360\uff0c\u4ece\u800c\u5bfc\u81f4\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u88ab\u8feb\u7b49\u5f85\u65e0\u9650\u671f\u5ef6\u957f\u7684\u73b0\u8c61\u3002</p> <p>\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u201c\u5361\u4f4f\u4e86\u201d\uff0c\u53cd\u800c\u8ba9\u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1\u5148\u8fd0\u884c\uff0c\u8fd9\u4e25\u91cd\u8fdd\u80cc\u4e86\u4f18\u5148\u7ea7\u8c03\u5ea6\u673a\u5236\u7684\u521d\u8877\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_26","title":"\u4e00\u4e2a\u7ecf\u5178\u7684\u4f18\u5148\u7ea7\u53cd\u8f6c\u573a\u666f\uff08\u4e3e\u4f8b\u8bf4\u660e\uff09","text":"<p>\u5047\u8bbe\u4e00\u4e2a\u7cfb\u7edf\u4e2d\u6709\u4e09\u4e2a\u4efb\u52a1\uff0c\u4f18\u5148\u7ea7\u4ece\u9ad8\u5230\u4f4e\u6392\u5217\uff1a</p> <ul> <li>\u4efb\u52a1H\uff1a\u9ad8\u4f18\u5148\u7ea7</li> <li>\u4efb\u52a1M\uff1a\u4e2d\u4f18\u5148\u7ea7</li> <li>\u4efb\u52a1L\uff1a\u4f4e\u4f18\u5148\u7ea7</li> </ul> <p>\u7cfb\u7edf\u4e2d\u6709\u4e00\u4e2a\u8d44\u6e90R\uff08\u4f8b\u5982\u4e00\u4e2a\u6253\u5370\u673a\u3001\u4e00\u6bb5\u5171\u4eab\u5185\u5b58\uff09\uff0c\u7531\u4fe1\u53f7\u91cfS \u6765\u4fdd\u62a4\u3002</p> <p>\u95ee\u9898\u53d1\u751f\u7684\u6b65\u9aa4\uff1a</p> <ol> <li>\u65f6\u523b1\uff1a\u4efb\u52a1L\u5f00\u59cb\u8fd0\u884c\uff0c\u5e76\u6210\u529f\u83b7\u53d6\u4e86\u4fe1\u53f7\u91cfS\uff0c\u5f00\u59cb\u4f7f\u7528\u8d44\u6e90R\u3002</li> <li>\u65f6\u523b2\uff1a\u4efb\u52a1H\u5c31\u7eea\u3002\u7531\u4e8e\u5b83\u7684\u4f18\u5148\u7ea7\u6700\u9ad8\uff0c\u7cfb\u7edf\u4f1a\u62a2\u5360\u4efb\u52a1L\uff0c\u5f00\u59cb\u8fd0\u884c\u4efb\u52a1H\u3002</li> <li>\u65f6\u523b3\uff1a\u4efb\u52a1H\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4e5f\u5c1d\u8bd5\u53bb\u83b7\u53d6\u4fe1\u53f7\u91cfS\uff0c\u4ee5\u4f7f\u7528\u8d44\u6e90R\u3002<ul> <li>\u4f46\u6b64\u65f6\u4fe1\u53f7\u91cfS\u5df2\u7ecf\u88ab\u4efb\u52a1L\u6301\u6709\u3002</li> <li>\u56e0\u6b64\uff0c\u4efb\u52a1H\u88ab\u963b\u585e\uff0c\u8fdb\u5165\u7b49\u5f85\u72b6\u6001\uff0c\u5e76\u91ca\u653eCPU\u3002</li> </ul> </li> <li>\u65f6\u523b4\uff1a\u7cfb\u7edf\u9700\u8981\u8c03\u5ea6\u4e00\u4e2a\u65b0\u4efb\u52a1\u3002\u6b64\u65f6\uff0c\u4efb\u52a1L\uff08\u6301\u6709\u4fe1\u53f7\u91cf\uff09\u548c\u4efb\u52a1M\uff08\u5c31\u7eea\uff09\u90fd\u5728\u7b49\u5f85CPU\u3002<ul> <li>\u6309\u7167\u4f18\u5148\u7ea7\u89c4\u5219\uff0c\u4efb\u52a1M\u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u4efb\u52a1L\u3002</li> <li>\u56e0\u6b64\uff0c\u7cfb\u7edf\u8c03\u5ea6\u4efb\u52a1M\u5f00\u59cb\u8fd0\u884c\u3002\uff08\u8fd9\u662f\u95ee\u9898\u7684\u5173\u952e\uff01\uff09</li> </ul> </li> <li>\u65f6\u523b5\uff1a\u4efb\u52a1M\u5f00\u59cb\u957f\u65f6\u95f4\u8fd0\u884c\uff08\u4f8b\u5982\uff0c\u4e00\u4e2a\u5927\u7684\u8ba1\u7b97\u5faa\u73af\uff09\uff0c\u56e0\u4e3a\u5b83\u4e0d\u9700\u8981\u8d44\u6e90R\uff0c\u6240\u4ee5\u5b83\u4e0d\u4f1a\u91ca\u653e\u4fe1\u53f7\u91cfS\u3002</li> <li>\u7ed3\u679c\uff1a<ul> <li>\u4efb\u52a1L\uff08\u6301\u6709\u4fe1\u53f7\u91cfS\uff09\u65e0\u6cd5\u8fd0\u884c\uff0c\u56e0\u4e3a\u5b83\u88ab\u4efb\u52a1M\u62a2\u5360\u4e86\u3002</li> <li>\u4efb\u52a1L\u65e0\u6cd5\u8fd0\u884c\uff0c\u5c31\u65e0\u6cd5\u91ca\u653e\u4fe1\u53f7\u91cfS\u3002</li> <li>\u4efb\u52a1H\uff08\u9ad8\u4f18\u5148\u7ea7\uff09\u867d\u7136\u5728\u7b49\u5f85\u4fe1\u53f7\u91cfS\uff0c\u4f46\u5b83\u53ea\u80fd\u5e72\u7b49\u7740\u4efb\u52a1L\u91ca\u653e\u4fe1\u53f7\u91cf\u3002</li> <li>\u800c\u4efb\u52a1L\u53c8\u5728\u7b49\u7740\u4efb\u52a1M\u91ca\u653eCPU\u3002</li> </ul> </li> </ol> <p>\u6700\u7ec8\u5f62\u6210\u4e86\u4e00\u4e2a\u5c34\u5c2c\u7684\u94fe\u6761\uff1a<code>H</code> \u5728\u7b49 <code>L</code>\uff0c<code>L</code> \u5728\u7b49 <code>M</code>\uff0c\u800c <code>M</code> \u4e0e <code>H</code>\u3001<code>L</code> \u90fd\u65e0\u5173\u3002 \u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u7684\u884c\u4e3a\uff0c\u5b9e\u9645\u4e0a\u88ab\u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1M\u201c\u7ed1\u67b6\u201d\u4e86\u3002\u8fd9\u5c31\u662f\u4f18\u5148\u7ea7\u53cd\u8f6c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_27","title":"\u4f18\u5148\u7ea7\u53cd\u8f6c\u7684\u5371\u5bb3","text":"<ul> <li>\u7834\u574f\u5b9e\u65f6\u6027\uff1a\u5728\u4e25\u683c\u7684\u5b9e\u65f6\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u5fc5\u987b\u5728\u89c4\u5b9a\u7684\u65f6\u95f4\u5185\u5b8c\u6210\u3002\u4f18\u5148\u7ea7\u53cd\u8f6c\u4f1a\u5bfc\u81f4\u5176\u54cd\u5e94\u65f6\u95f4\u65e0\u6cd5\u9884\u6d4b\uff0c\u751a\u81f3\u65e0\u9650\u671f\u5ef6\u8fdf\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5d29\u6e83\u6216\u4e25\u91cd\u6545\u969c\u3002</li> <li>\u903b\u8f91\u9519\u8bef\uff1a\u5728\u666e\u901a\u7cfb\u7edf\u4e2d\uff0c\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3001\u6b7b\u9501-like\u7684\u73b0\u8c61\uff0c\u96be\u4ee5\u8c03\u8bd5\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_28","title":"\u89e3\u51b3\u65b9\u6848","text":"<p>\u4e3a\u4e86\u89e3\u51b3\u4f18\u5148\u7ea7\u53cd\u8f6c\u95ee\u9898\uff0c\u5de5\u7a0b\u5e08\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u6709\u6548\u7684\u65b9\u6848\uff1a</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_29","title":"\u65b9\u6848\u4e00\uff1a\u4f18\u5148\u7ea7\u7ee7\u627f","text":"<p>\u8fd9\u662f\u6700\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002</p> <ul> <li>\u6838\u5fc3\u601d\u60f3\uff1a\u5f53\u4e00\u4e2a\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1H\u7b49\u5f85\u4e00\u4e2a\u88ab\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1L\u5360\u6709\u7684\u4fe1\u53f7\u91cf\u65f6\uff0c\u4e34\u65f6\u5c06\u4efb\u52a1L\u7684\u4f18\u5148\u7ea7\u63d0\u5347\u5230\u4e0e\u4efb\u52a1H\u76f8\u540c\u3002(\u63d0\u62d4)</li> <li>\u5982\u4f55\u5de5\u4f5c\uff1a<ul> <li>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u5f53\u4efb\u52a1H\u5728\u65f6\u523b3\u5c1d\u8bd5\u83b7\u53d6\u4fe1\u53f7\u91cf\u5931\u8d25\u65f6\uff0c\u7cfb\u7edf\u4f1a\u7acb\u5373\u5c06\u4efb\u52a1L\u7684\u4f18\u5148\u7ea7\u63d0\u5347\u5230\u4e0e\u4efb\u52a1H\u4e00\u6837\u9ad8\u3002</li> <li>\u8fd9\u6837\uff0c\u5728\u65f6\u523b4\u8fdb\u884c\u8c03\u5ea6\u65f6\uff0c\u4efb\u52a1L\u7684\u4f18\u5148\u7ea7\uff08\u5df2\u88ab\u4e34\u65f6\u63d0\u5347\u4e3a\u9ad8\uff09\u5c31\u9ad8\u4e8e\u4efb\u52a1M\u3002</li> <li>\u56e0\u6b64\uff0cCPU\u4f1a\u7acb\u523b\u5206\u914d\u7ed9\u4efb\u52a1L\uff0c\u8ba9\u5b83\u7ee7\u7eed\u6267\u884c\u3002</li> <li>\u4efb\u52a1L\u5f97\u4ee5\u5feb\u901f\u6267\u884c\u5b8c\u4e34\u754c\u533a\u4ee3\u7801\uff0c\u91ca\u653e\u4fe1\u53f7\u91cfS\u3002</li> <li>\u4e00\u65e6\u4efb\u52a1L\u91ca\u653e\u4e86\u4fe1\u53f7\u91cf\uff1a<ol> <li>\u5b83\u7684\u4f18\u5148\u7ea7\u4f1a\u6062\u590d\u4e3a\u539f\u6765\u7684\u4f4e\u4f18\u5148\u7ea7\u3002</li> <li>\u7b49\u5f85\u8be5\u4fe1\u53f7\u91cf\u7684\u4efb\u52a1H\u7acb\u5373\u83b7\u53d6\u5230\u4fe1\u53f7\u91cf\uff0c\u5e76\u56e0\u5176\u672c\u8eab\u7684\u9ad8\u4f18\u5148\u7ea7\u800c\u5f00\u59cb\u8fd0\u884c\u3002</li> </ol> </li> </ul> </li> <li>\u6548\u679c\uff1a\u6709\u6548\u6253\u7834\u4e86\u201cH\u7b49L\uff0cL\u88abM\u963b\u585e\u201d\u7684\u94fe\u6761\uff0c\u5c06\u53cd\u8f6c\u7684\u5f71\u54cd\u964d\u5230\u6700\u4f4e\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_30","title":"\u65b9\u6848\u4e8c\uff1a\u4f18\u5148\u7ea7\u5929\u82b1\u677f","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u66f4\u6fc0\u8fdb\u3001\u4e5f\u66f4\u786e\u5b9a\u7684\u65b9\u6848\u3002</p> <ul> <li>\u6838\u5fc3\u601d\u60f3\uff1a\u4e3a\u6bcf\u4e2a\u4fe1\u53f7\u91cf\u9884\u8bbe\u4e00\u4e2a\u201c\u5929\u82b1\u677f\u201d\u4f18\u5148\u7ea7\uff0c\u8fd9\u4e2a\u4f18\u5148\u7ea7\u901a\u5e38\u7b49\u4e8e\u6240\u6709\u53ef\u80fd\u8bbf\u95ee\u8be5\u4fe1\u53f7\u91cf\u7684\u4efb\u52a1\u4e2d\u7684\u6700\u9ad8\u4f18\u5148\u7ea7\u3002\u5f53\u4e00\u4e2a\u4efb\u52a1\u6210\u529f\u83b7\u53d6\u8be5\u4fe1\u53f7\u91cf\u65f6\uff0c\u5b83\u7684\u4f18\u5148\u7ea7\u4f1a\u81ea\u52a8\u88ab\u63d0\u5347\u5230\u8fd9\u4e2a\u5929\u82b1\u677f\u4f18\u5148\u7ea7\u3002</li> <li>\u5982\u4f55\u5de5\u4f5c\uff1a<ul> <li>\u5728\u4f8b\u5b50\u4e2d\uff0c\u5047\u8bbe\u4fe1\u53f7\u91cfS\u7684\u5929\u82b1\u677f\u4f18\u5148\u7ea7\u88ab\u8bbe\u4e3a\u4efb\u52a1H\u7684\u4f18\u5148\u7ea7\u3002</li> <li>\u5728\u65f6\u523b1\uff0c\u4efb\u52a1L\u4e00\u83b7\u53d6\u4fe1\u53f7\u91cfS\uff0c\u5b83\u7684\u4f18\u5148\u7ea7\u7acb\u523b\u88ab\u63d0\u5347\u5230\u5929\u82b1\u677f\u4f18\u5148\u7ea7\uff08\u5373H\u7684\u4f18\u5148\u7ea7\uff09\u3002</li> <li>\u8fd9\u6837\uff0c\u5728\u65f6\u523b2\uff0c\u4efb\u52a1H\u5c31\u7eea\u65f6\uff0c\u5b83\u65e0\u6cd5\u62a2\u5360\u4efb\u52a1L\uff0c\u56e0\u4e3a\u6b64\u65f6\u5b83\u4eec\u4f18\u5148\u7ea7\u76f8\u540c\uff08\u6216\u8005\u6839\u636e\u5177\u4f53\u5b9e\u73b0\uff0c\u53ef\u80fd\u4ecd\u7531L\u7ee7\u7eed\u8fd0\u884c\uff09\u3002</li> <li>\u4efb\u52a1L\u4f1a\u4e0d\u53d7\u5e72\u6270\u5730\u3001\u5feb\u901f\u5730\u8fd0\u884c\u5b8c\u4e34\u754c\u533a\u4ee3\u7801\uff0c\u7136\u540e\u91ca\u653e\u4fe1\u53f7\u91cf\uff0c\u540c\u65f6\u4f18\u5148\u7ea7\u6062\u590d\u539f\u72b6\u3002</li> <li>\u4e4b\u540e\uff0c\u4efb\u52a1H\u624d\u80fd\u62a2\u5360\u5e76\u8fd0\u884c\u3002</li> </ul> </li> <li>\u6548\u679c\uff1a\u5b83\u751a\u81f3\u9632\u6b62\u4e86\u4f18\u5148\u7ea7\u53cd\u8f6c\u7684\u53d1\u751f\uff0c\u800c\u4e0d\u662f\u7b49\u53d1\u751f\u4e86\u518d\u53bb\u89e3\u51b3\u3002\u4f46\u5b83\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u4f18\u5148\u7ea7\u63d0\u5347\uff0c\u7a0d\u5fae\u964d\u4f4e\u7cfb\u7edf\u7075\u6d3b\u6027\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_31","title":"\u603b\u7ed3","text":"\u7279\u6027 \u95ee\u9898\uff1a\u4f18\u5148\u7ea7\u53cd\u8f6c \u89e3\u51b3\u65b9\u68481\uff1a\u4f18\u5148\u7ea7\u7ee7\u627f \u89e3\u51b3\u65b9\u68482\uff1a\u4f18\u5148\u7ea7\u5929\u82b1\u677f \u6838\u5fc3\u601d\u60f3 \u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u88ab\u4e2d\u4f18\u5148\u7ea7\u4efb\u52a1\u95f4\u63a5\u963b\u585e \u4e34\u65f6\u63d0\u5347\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u4f18\u5148\u7ea7 \u5728\u6301\u6709\u9501\u65f6\u76f4\u63a5\u63d0\u5347\u5230\u9884\u8bbe\u6700\u9ad8\u4f18\u5148\u7ea7 \u884c\u4e3a\u65f6\u673a \u81ea\u7136\u53d1\u751f \u53d1\u751f\u963b\u585e\u65f6\u89e6\u53d1\u63d0\u5347 \u83b7\u53d6\u4fe1\u53f7\u91cf\u65f6\u7acb\u5373\u63d0\u5347 \u4f18\u70b9 - \u52a8\u6001\u3001\u9ad8\u6548\u3001\u8d44\u6e90\u5360\u7528\u5c11 \u80fd\u9632\u6b62\u6b7b\u9501\uff0c\u884c\u4e3a\u66f4\u786e\u5b9a \u7f3a\u70b9 \u7834\u574f\u7cfb\u7edf\u5b9e\u65f6\u6027\uff0c\u96be\u4ee5\u8c03\u8bd5 \u5b9e\u73b0\u7a0d\u590d\u6742\uff0c\u5b58\u5728\u94fe\u5f0f\u7ee7\u627f\u95ee\u9898 \u53ef\u80fd\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u4f18\u5148\u7ea7\u63d0\u5347 <p>\u66f4\u591a\u8bf7\u89c1Mutex</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Semaphore/#_32","title":"\u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848","text":"<p>\u6b7b\u9501\u9884\u9632\uff1a - \u6309\u7167\u56fa\u5b9a\u987a\u5e8f\u83b7\u53d6\u591a\u4e2a\u4fe1\u53f7\u91cf - \u8bbe\u7f6e\u5408\u7406\u7684\u8d85\u65f6\u65f6\u95f4 - \u4f7f\u7528\u6b7b\u9501\u68c0\u6d4b\u673a\u5236</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4efb\u52a1\u95f4\u7684\u540c\u6b65\u4e0e\u901a\u4fe1","text":"<p>\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u591a\u4e2a\u4efb\u52a1\u540c\u65f6\u8fd0\u884c\u65f6\uff0c\u5b83\u4eec\u4e4b\u95f4\u5f80\u5f80\u4e0d\u662f\u5b8c\u5168\u72ec\u7acb\u7684\uff0c\u800c\u662f\u9700\u8981\u76f8\u4e92\u534f\u4f5c\u3002\u8fd9\u79cd\u534f\u4f5c\u5173\u7cfb\u4e3b\u8981\u4f53\u73b0\u5728\uff1a</p> <ul> <li> <p>\u5171\u4eab\u8d44\u6e90\u4fdd\u62a4\uff1a\u5f53\u591a\u4e2a\u4efb\u52a1\u9700\u8981\u8bbf\u95ee\u540c\u4e00\u4e2a\u786c\u4ef6\u8d44\u6e90\uff08\u5982\u4e32\u53e3\u3001\u663e\u793a\u5c4f\uff09\u6216\u8f6f\u4ef6\u8d44\u6e90\uff08\u5982\u5168\u5c40\u53d8\u91cf\uff09\u65f6\uff0c\u9700\u8981\u786e\u4fdd\u540c\u4e00\u65f6\u95f4\u53ea\u6709\u4e00\u4e2a\u4efb\u52a1\u80fd\u591f\u8bbf\u95ee\uff0c\u907f\u514d\u6570\u636e\u635f\u574f\u6216\u884c\u4e3a\u5f02\u5e38\u3002</p> </li> <li> <p>\u6267\u884c\u987a\u5e8f\u534f\u8c03\uff1a\u67d0\u4e9b\u4efb\u52a1\u9700\u8981\u7b49\u5f85\u5176\u4ed6\u4efb\u52a1\u5b8c\u6210\u7279\u5b9a\u64cd\u4f5c\u540e\u624d\u80fd\u5f00\u59cb\u6267\u884c\uff0c\u6bd4\u5982\u4efb\u52a1B\u5fc5\u987b\u7b49\u5f85\u4efb\u52a1A\u51c6\u5907\u597d\u6570\u636e\u540e\u624d\u80fd\u8fdb\u884c\u5904\u7406\u3002</p> </li> <li> <p>\u6570\u636e\u4f20\u9012\uff1a\u4efb\u52a1\u4e4b\u95f4\u7ecf\u5e38\u9700\u8981\u4f20\u9012\u6570\u636e\uff0c\u6bd4\u5982\u4f20\u611f\u5668\u91c7\u96c6\u4efb\u52a1\u9700\u8981\u5c06\u6570\u636e\u4f20\u9012\u7ed9\u6570\u636e\u5904\u7406\u4efb\u52a1\u3002</p> </li> </ul> <p>\u5982\u679c\u6ca1\u6709\u5408\u9002\u7684\u540c\u6b65\u4e0e\u901a\u4fe1\u673a\u5236\uff0c\u5c31\u4f1a\u51fa\u73b0\u6570\u636e\u7ade\u4e89\u3001\u6b7b\u9501\u3001\u4f18\u5148\u7ea7\u53cd\u8f6c\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_2","title":"\u540c\u6b65\u4e0e\u4e92\u65a5\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_3","title":"\u540c\u6b65","text":"<p>\u540c\u6b65\u6307\u7684\u662f\u4efb\u52a1\u4e4b\u95f4\u6309\u7167\u67d0\u79cd\u9884\u5b9a\u7684\u987a\u5e8f\u6267\u884c\uff0c\u4e00\u4e2a\u4efb\u52a1\u6267\u884c\u5230\u67d0\u4e2a\u70b9\u65f6\u9700\u8981\u7b49\u5f85\u53e6\u4e00\u4e2a\u4efb\u52a1\u53d1\u51fa\u4fe1\u53f7\u540e\u624d\u80fd\u7ee7\u7eed\u3002\u5c31\u50cf\u4e24\u4e2a\u4eba\u5408\u4f5c\u5b8c\u6210\u4e00\u9879\u5de5\u4f5c\uff0c\u9700\u8981\u4e92\u76f8\u914d\u5408\u3001\u6b65\u8c03\u4e00\u81f4\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_4","title":"\u4e92\u65a5","text":"<p>\u4e92\u65a5\u6307\u7684\u662f\u4fdd\u62a4\u5171\u4eab\u8d44\u6e90\uff0c\u786e\u4fdd\u5728\u540c\u4e00\u65f6\u523b\u53ea\u6709\u4e00\u4e2a\u4efb\u52a1\u80fd\u591f\u8bbf\u95ee\u8be5\u8d44\u6e90\u3002\u5f53\u67d0\u4e2a\u4efb\u52a1\u6b63\u5728\u4f7f\u7528\u5171\u4eab\u8d44\u6e90\u65f6\uff0c\u5176\u4ed6\u4efb\u52a1\u5fc5\u987b\u7b49\u5f85\uff0c\u76f4\u5230\u8be5\u8d44\u6e90\u88ab\u91ca\u653e\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_5","title":"\u4e34\u754c\u533a","text":"<p>\u4e34\u754c\u533a\u662f\u6307\u8bbf\u95ee\u5171\u4eab\u8d44\u6e90\u7684\u4ee3\u7801\u6bb5\uff0c\u8fd9\u6bb5\u4ee3\u7801\u5728\u6267\u884c\u65f6\u4e0d\u80fd\u88ab\u4e2d\u65ad\u6216\u5176\u4ed6\u4efb\u52a1\u6253\u65ad\u3002\u5bf9\u4e34\u754c\u533a\u7684\u4fdd\u62a4\u662f\u5b9e\u73b0\u4e92\u65a5\u7684\u5173\u952e\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_6","title":"\u6570\u636e\u4f20\u8f93\u7684\u65b9\u6cd5","text":"\u6570\u636e\u5bb9\u91cf \u4e92\u65a5\u63aa\u65bd \u963b\u585e-\u5524\u9192 \u5168\u5c40\u53d8\u91cf 1\u4e2a \u65e0 \u65e0 \u73af\u5f62\u7f13\u51b2\u533a \u591a\u4e2a \u65e0 \u65e0 \u961f\u5217 \u591a\u4e2a \u6709 \u6709"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_7","title":"\u73af\u5f62\u7f13\u51b2\u533a","text":"<p>\u987e\u540d\u601d\u4e49\uff0c\u73af\u5f62\u7684\u7f13\u5b58\uff0c\u6bd4\u5982</p> <pre><code>int buf[SIZE];\nint read_pos,write_pos;\n//\u7a7a\u72b6\u6001\nread_pos == write_pos;\n//\u5199\u5165(\u672a\u6ee1\u624d\u80fd\u5199\uff0c\u5b9a\u4e49\u6570\u636e\u91cf\u4e3aSIZE-1\u65f6\u6ee1\uff0c\u5373\u6b64\u65f6write_pos+1==read_pos)\nif((write_pos+1)%SIZE != read_pos){\n    buf[write_pos++]=data;\n    write_pos%=SIZE;\n}\n//\u8bfb\u53d6(\u4e0d\u7a7a)\nif(read_pos!=write_pos){\n    val = buf[read_pos++];\n    read_pos%=SIZE;\n}\n</code></pre> <p>\u601d\u8003\uff1a\u4e3a\u4ec0\u4e48\u4e0d\u9002\u7528\u53d8\u91cfnum\u50a8\u5b58\u6570\u636e\u4e2a\u6570\u6765\u5224\u65ad\u662f\u5426\u6ee1\u548c\u7a7a\uff1f</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#buffer","title":"\u6709\u7f3a\u9677\u7684\u73af\u5f62buffer","text":"<p>num\u4e3a\u5168\u5c40\u53d8\u91cf\uff0c\u5728\u5199\u5165\u548c\u8bfb\u51fa\u65f6\u90fd\u4f1a\u88ab\u4fee\u6539\u3002</p> <p>num++,num--\u90fd\u53ef\u80fd\u4f1a\u88ab\u4efb\u52a1\u5207\u6362\u6253\u65ad\uff0c\u5bfc\u81f4\u66f4\u65b0\u9519\u4e71\uff1a</p> <pre><code>//\u6b63\u5e38\u6d41\u7a0b num=10\nnum -&gt; R0\nR0++\nR0-&gt;num //num=11\n//\u6253\u65ad\u6d41\u7a0b\nnum -&gt; R0\n    &lt;-\u88ab\u6253\u65ad\uff0c\u5207\u6362\u5230B\u6267\u884c\u4e86num--\n    &lt;-\u6062\u590d\nR0++\nR0-&gt;num //num=11\n    \u7136\u800c\u6b63\u5e38\u7684num\u5e94\u8be5\u4e3a10\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_8","title":"\u95ee\u9898\u6240\u5728","text":"<p>\u8ba9\u591a\u4e2a\u4efb\u52a1\u4fee\u6539\u540c\u4e00\u4e2a\u53d8\u91cf\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_9","title":"\u9002\u7528","text":"<p>\u53ef\u89c1\uff0c\u5728\u4e24\u4e2a\u4efb\u52a1\u95f4\u7684\u901a\u4fe1\uff0c\u5982\u679c\u4e0d\u8981\u6c42\u6548\u7387\uff08\u5373\u963b\u585e-\u5524\u9192\u673a\u5236\uff09\uff0c\u4f7f\u7528\u6b63\u786e\u7684\u73af\u5f62\u7f13\u51b2\u533a\u5c31\u53ef\u4ee5\u907f\u514d\u51b2\u7a81\u95ee\u9898\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_10","title":"\u961f\u5217\u7684\u672c\u8d28","text":"<p>\u5c31\u662f\u73af\u5f62\u7f13\u51b2\u533a\uff0c\u589e\u52a0\u4e86\u4fdd\u62a4\u63aa\u65bd\uff08\u6240\u4ee5\u53ef\u4ee5\u6709\u6570\u636e\u4e2a\u6570\uff1b\u8bfb\u4f4d\u7f6e\uff0c\u5199\u4f4d\u7f6e\uff09\uff1b\u5982\u679c\u53ea\u662f\u6539\u53d8\u8ba1\u6570\u503c\uff0c\u90a3\u5c31\u53d8\u6210\u4e86\u4fe1\u53f7\u91cf(semaphore)\uff1b\u4fe1\u53f7\u91cf\u518d\u9650\u5236\u8ba1\u6570\u503c\u4e0a\u9650\u4e3a1\uff0c\u5c31\u53d8\u6210\u4e86\u4e92\u65a5\u91cf(mutex);</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#freertos","title":"FreeRTOS\u7684\u961f\u5217","text":"<p>Queue</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#freertos_1","title":"FreeRTOS\u7684\u961f\u5217\u96c6","text":"<p>Queue_Set</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#freertos_2","title":"FreeRTOS\u7684\u4fe1\u53f7\u91cf","text":"<p>Semaphore</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#freertos_3","title":"FreeRTOS\u7684\u4e92\u65a5\u91cf","text":"<p>Mutex</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#freertos_4","title":"FreeRTOS\u7684\u4e8b\u4ef6\u7ec4","text":"<p>Event_Group</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_11","title":"\u4efb\u52a1\u901a\u77e5","text":"<p>TaskNote</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_12","title":"\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9009\u62e9\u5efa\u8bae","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_13","title":"\u5404\u79cd\u901a\u4fe1\u673a\u5236\u7684\u6bd4\u8f83","text":"\u673a\u5236 \u4f7f\u7528\u573a\u666f \u4f18\u70b9 \u7f3a\u70b9 \u961f\u5217 \u4efb\u52a1\u95f4\u6570\u636e\u4f20\u8f93 \u7ebf\u7a0b\u5b89\u5168\uff0c\u652f\u6301\u963b\u585e \u5185\u5b58\u5f00\u9500\u8f83\u5927 \u4fe1\u53f7\u91cf \u540c\u6b65\u548c\u8d44\u6e90\u7ba1\u7406 \u7b80\u5355\u9ad8\u6548 \u4e0d\u80fd\u4f20\u9012\u6570\u636e \u4e92\u65a5\u91cf \u5171\u4eab\u8d44\u6e90\u4fdd\u62a4 \u4f18\u5148\u7ea7\u7ee7\u627f\uff0c\u9632\u6b62\u4f18\u5148\u7ea7\u53cd\u8f6c \u53ea\u80fd\u7528\u4e8e\u4e92\u65a5 \u4e8b\u4ef6\u7ec4 \u591a\u4e8b\u4ef6\u7b49\u5f85 \u53ef\u4ee5\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6 \u4e0d\u80fd\u4f20\u9012\u6570\u636e \u4efb\u52a1\u901a\u77e5 \u8f7b\u91cf\u7ea7\u901a\u4fe1 \u901f\u5ea6\u6700\u5feb\uff0c\u5185\u5b58\u5f00\u9500\u6700\u5c0f \u53ea\u80fd\u4e00\u5bf9\u4e00\u901a\u4fe1"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_14","title":"\u9009\u62e9\u6307\u5357","text":"<ol> <li>\u9700\u8981\u4f20\u9012\u6570\u636e \u2192 \u4f7f\u7528\u961f\u5217</li> <li>\u7b80\u5355\u7684\u4efb\u52a1\u540c\u6b65 \u2192 \u4f7f\u7528\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u6216\u4efb\u52a1\u901a\u77e5</li> <li>\u7ba1\u7406\u591a\u4e2a\u76f8\u540c\u8d44\u6e90 \u2192 \u4f7f\u7528\u8ba1\u6570\u4fe1\u53f7\u91cf</li> <li>\u4fdd\u62a4\u5171\u4eab\u8d44\u6e90 \u2192 \u4f7f\u7528\u4e92\u65a5\u91cf\uff08\u7279\u522b\u662f\u53ef\u80fd\u53d1\u751f\u4f18\u5148\u7ea7\u53cd\u8f6c\u65f6\uff09</li> <li>\u7b49\u5f85\u591a\u4e2a\u4e8b\u4ef6 \u2192 \u4f7f\u7528\u4e8b\u4ef6\u7ec4\u6216\u4efb\u52a1\u901a\u77e5\uff08\u8bbe\u7f6e\u4f4d\u65b9\u5f0f\uff09</li> <li>\u8ffd\u6c42\u6700\u9ad8\u6027\u80fd \u2192 \u4f18\u5148\u8003\u8651\u4efb\u52a1\u901a\u77e5</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Sync_Comms/#_15","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ul> <li>\u907f\u514d\u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u6267\u884c\u8017\u65f6\u64cd\u4f5c\uff0c\u4f7f\u7528FromISR\u7248\u672c\u51fd\u6570</li> <li>\u5408\u7406\u8bbe\u7f6e\u963b\u585e\u8d85\u65f6\u65f6\u95f4\uff0c\u907f\u514d\u4efb\u52a1\u6c38\u4e45\u963b\u585e</li> <li>\u6ce8\u610f\u4f18\u5148\u7ea7\u5b89\u6392\uff0c\u9632\u6b62\u4f18\u5148\u7ea7\u53cd\u8f6c\u548c\u9965\u997f\u73b0\u8c61</li> <li>\u4f7f\u7528\u4e92\u65a5\u91cf\u4fdd\u62a4\u5171\u4eab\u8d44\u6e90\u65f6\uff0c\u4fdd\u6301\u4e34\u754c\u533a\u4ee3\u7801\u5c3d\u53ef\u80fd\u77ed</li> <li>\u5b9a\u671f\u68c0\u67e5API\u51fd\u6570\u7684\u8fd4\u56de\u503c\uff0c\u5904\u7406\u5f02\u5e38\u60c5\u51b5</li> </ul> <p>\u901a\u8fc7\u5408\u7406\u9009\u62e9\u548c\u4f7f\u7528\u8fd9\u4e9b\u540c\u6b65\u4e0e\u901a\u4fe1\u673a\u5236\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u7a33\u5b9a\u3001\u9ad8\u6548\u7684FreeRTOS\u5e94\u7528\u7a0b\u5e8f\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/","title":"TaksPend&Resume","text":"<ul> <li>\u539f\u751fAPI<ul> <li>vTaskSuspend()</li> <li>vTaskResume()</li> <li>xTaskResumeFromISR</li> <li>\u91cd\u8981\u6ce8\u610f\u4e8b\u9879</li> <li>\u603b\u7ed3</li> </ul> </li> <li>CMSISv2 API<ul> <li>1. \u6302\u8d77\u7ebf\u7a0b osThreadSuspend()</li> <li>2. \u6062\u590d\u7ebf\u7a0b osThreadResume()</li> <li>\u5982\u4f55\u83b7\u53d6\u7ebf\u7a0b ID\uff1f</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#api","title":"\u539f\u751fAPI","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#vtasksuspend","title":"<code>vTaskSuspend()</code>","text":"<pre><code>void vTaskSuspend( TaskHandle_t xTaskToSuspend );\n</code></pre> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>xTaskToSuspend</code>\uff1a \u8981\u6302\u8d77\u7684\u4efb\u52a1\u7684\u53e5\u67c4\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#vtaskresume","title":"<code>vTaskResume()</code>","text":"<pre><code>void vTaskResume( TaskHandle_t xTaskToResume );\n</code></pre> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>xTaskToResume</code>\uff1a \u8981\u6062\u590d\u7684\u4efb\u52a1\u7684\u53e5\u67c4\u3002</li> </ul> <p>[!IMPORTANT]</p> <p>\u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u4f7f\u7528 <code>vTaskResume</code> \u662f\u4e0d\u5b89\u5168\u7684\uff0c\u56e0\u6b64 FreeRTOS \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e26\u4e2d\u65ad\u4fdd\u62a4\u7248\u672c\u7684\u51fd\u6570\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#xtaskresumefromisr","title":"<code>xTaskResumeFromISR</code>","text":"<pre><code>BaseType_t xTaskResumeFromISR( TaskHandle_t xTaskToResume );\n</code></pre> <ul> <li>\u53c2\u6570\uff1a</li> <li><code>xTaskToResume</code>\uff1a \u8981\u6062\u590d\u7684\u4efb\u52a1\u7684\u53e5\u67c4\u3002</li> <li>\u8fd4\u56de\u503c\uff1a</li> <li><code>pdTRUE</code>\uff1a \u6062\u590d\u64cd\u4f5c\u5bfc\u81f4\u4e86\u4e00\u4e2a\u66f4\u9ad8\u4f18\u5148\u7ea7\u7684\u4efb\u52a1\u5c31\u7eea\uff0c\u5e76\u4e14\u5f53\u524d\u4e2d\u65ad\u7684\u4f18\u5148\u7ea7\u8db3\u591f\u4f4e\uff0c\u5728\u4e2d\u65ad\u9000\u51fa\u540e\u5e94\u8be5\u8fdb\u884c\u4e00\u6b21\u4e0a\u4e0b\u6587\u5207\u6362\u3002</li> <li><code>pdFALSE</code>\uff1a \u6062\u590d\u64cd\u4f5c\u4e0d\u9700\u8981\u8fdb\u884c\u4e0a\u4e0b\u6587\u5207\u6362\u3002</li> <li>\u793a\u4f8b</li> </ul> <pre><code>    BaseType_t xYieldRequired;\n    xYieldRequired = xTaskResumeFromISR( xMotorTaskHandle );\n    // \u5982\u679c\u5efa\u8bae\u8fdb\u884c\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u5e76\u4e14\u5f53\u524d\u4e2d\u65ad\u4f18\u5148\u7ea7\u5141\u8bb8\uff0c\u5219\u6267\u884c\u7aef\u53e3\u7279\u5b9a\u7684\u5207\u6362\n    if( xYieldRequired == pdTRUE ) {\n        portYIELD_FROM_ISR();\n    }\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#_1","title":"\u91cd\u8981\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u6302\u8d77\u8ba1\u6570\uff1a</li> </ol> <p><code>c    vTaskSuspend( xTask ); // \u6302\u8d77\u8ba1\u6570 = 1    vTaskSuspend( xTask ); // \u6302\u8d77\u8ba1\u6570 = 2    vTaskResume( xTask );  // \u6302\u8d77\u8ba1\u6570 = 1 (\u4efb\u52a1\u4ecd\u7136\u88ab\u6302\u8d77)    vTaskResume( xTask );  // \u6302\u8d77\u8ba1\u6570 = 0 (\u4efb\u52a1\u73b0\u5728\u6062\u590d\u4e86)</code></p> <p>\u52a1\u5fc5\u786e\u4fdd\u6302\u8d77\u548c\u6062\u590d\u7684\u6b21\u6570\u5339\u914d\u3002</p> <ol> <li> <p>c\u4e0d\u8981\u6302\u8d77\u8c03\u5ea6\u5668\u672c\u8eab\uff1a <code>vTaskSuspend</code> \u6302\u8d77\u7684\u662f\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u6574\u4e2a\u8c03\u5ea6\u5668\u3002\u6302\u8d77\u6240\u6709\u4efb\u52a1\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u6b7b\u9501\u3002</p> </li> <li> <p>\u8c28\u614e\u4f7f\u7528\uff1a \u6ee5\u7528\u6302\u8d77/\u6062\u590d\u53ef\u80fd\u4f1a\u5bfc\u81f4\u590d\u6742\u7684\u903b\u8f91\u9519\u8bef\uff0c\u6bd4\u5982\u4efb\u52a1\u4f9d\u8d56\uff08A\u4efb\u52a1\u7b49\u5f85B\u4efb\u52a1\u7684\u6570\u636e\uff0c\u4f46B\u88ab\u6302\u8d77\u4e86\uff09\u5bfc\u81f4\u7684\u6b7b\u9501\u3002\u5728\u8bbe\u8ba1\u65f6\uff0c\u901a\u5e38\u66f4\u63a8\u8350\u4f7f\u7528\u4fe1\u53f7\u91cf\u3001\u961f\u5217\u3001\u4e8b\u4ef6\u7ec4\u7b49\u540c\u6b65\u901a\u4fe1\u673a\u5236\u6765\u534f\u8c03\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7c97\u66b4\u5730\u6302\u8d77\u3002</p> </li> <li> <p><code>vTaskSuspend(NULL)</code> \u662f\u552f\u4e00\u6302\u8d77\u81ea\u8eab\u7684\u65b9\u5f0f\uff1a \u4e00\u4e2a\u4efb\u52a1\u4e0d\u80fd\u901a\u8fc7\u4f20\u9012\u81ea\u5df1\u7684\u53e5\u67c4\u6765\u6302\u8d77\u81ea\u5df1\uff0c\u5fc5\u987b\u4f7f\u7528 <code>NULL</code>\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#_2","title":"\u603b\u7ed3","text":"\u7279\u6027/\u51fd\u6570 <code>vTaskSuspend</code> <code>vTaskResume</code> <code>xTaskResumeFromISR</code> \u64cd\u4f5c\u5bf9\u8c61 \u4efb\u52a1\uff08\u81ea\u5df1\u6216\u4ed6\u4eba\uff09 \u4efb\u52a1\uff08\u4ed6\u4eba\uff09 \u4efb\u52a1\uff08\u4ed6\u4eba\uff09 \u8c03\u7528\u4e0a\u4e0b\u6587 \u4efb\u52a1 \u4efb\u52a1 \u4e2d\u65ad \u8ba1\u6570\u578b \u662f \u662f \u662f \u4e3b\u8981\u7528\u9014 \u6682\u505c\u4e00\u4e2a\u4efb\u52a1\u7684\u6267\u884c \u6062\u590d\u4e00\u4e2a\u88ab\u6302\u8d77\u7684\u4efb\u52a1 \u5728\u4e2d\u65ad\u4e2d\u6062\u590d\u4efb\u52a1"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#cmsisv2-api","title":"CMSISv2 API","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#1-osthreadsuspend","title":"1. \u6302\u8d77\u7ebf\u7a0b <code>osThreadSuspend()</code>","text":"<p>\u51fd\u6570\u539f\u578b\uff1a</p> <pre><code>osStatus_t osThreadSuspend (osThreadId_t thread_id);\n</code></pre> <ul> <li>\u53c2\u6570\uff1a</li> <li><code>thread_id</code>\uff1a \u8981\u6302\u8d77\u7684\u7ebf\u7a0b\u7684\u7ebf\u7a0bID\uff08\u53e5\u67c4\uff09\u3002\u8fd9\u662f\u4e00\u4e2a\u5728\u521b\u5efa\u7ebf\u7a0b\u65f6\u83b7\u53d6\u7684\u6807\u8bc6\u7b26\u3002</li> <li>\u8fd4\u56de\u503c\uff1a</li> <li><code>osOK</code>\uff1a \u7ebf\u7a0b\u5df2\u88ab\u6210\u529f\u6302\u8d77\u3002</li> <li><code>osErrorParameter</code>\uff1a \u53c2\u6570 <code>thread_id</code> \u662f NULL \u6216\u65e0\u6548\u3002</li> <li><code>osErrorResource</code>\uff1a \u6307\u5b9a\u7684\u7ebf\u7a0b\u5904\u4e8e\u65e0\u6548\u72b6\u6001\uff08\u4f8b\u5982\u5df2\u7ecf\u7ec8\u6b62\uff09\u3002</li> <li><code>osErrorISR</code>\uff1a \u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u8c03\u7528\u4e86\u6b64\u51fd\u6570\uff08\u4e0d\u5141\u8bb8\uff09\u3002</li> </ul> <p>\u5173\u952e\u70b9\uff1a</p> <ul> <li>\u6b64\u51fd\u6570\u53ef\u4ee5\u6302\u8d77\u5176\u4ed6\u7ebf\u7a0b\uff0c\u4e5f\u53ef\u4ee5\u6302\u8d77\u81ea\u5df1\uff08\u901a\u8fc7\u4f20\u9012\u81ea\u5df1\u7684 <code>thread_id</code>\uff09\u3002</li> <li>\u88ab\u6302\u8d77\u7684\u7ebf\u7a0b\u4f1a\u7acb\u5373\u505c\u6b62\u6267\u884c\uff0c\u5e76\u653e\u5f03 CPU\u3002</li> <li>\u6302\u8d77\u662f\u8ba1\u6570\u578b\u7684\u3002\u5982\u679c\u540c\u4e00\u4e2a\u7ebf\u7a0b\u88ab\u591a\u6b21\u6302\u8d77\uff0c\u5b83\u4e5f\u9700\u8981\u88ab\u6062\u590d\u76f8\u540c\u7684\u6b21\u6570\u624d\u80fd\u91cd\u65b0\u53d8\u4e3a\u5c31\u7eea\u72b6\u6001\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#2-osthreadresume","title":"2. \u6062\u590d\u7ebf\u7a0b <code>osThreadResume()</code>","text":"<p>\u51fd\u6570\u539f\u578b\uff1a</p> <pre><code>osStatus_t osThreadResume (osThreadId_t thread_id);\n</code></pre> <ul> <li>\u53c2\u6570\uff1a</li> <li><code>thread_id</code>\uff1a \u8981\u6062\u590d\u7684\u7ebf\u7a0b\u7684\u7ebf\u7a0bID\u3002</li> <li>\u8fd4\u56de\u503c\uff1a</li> <li><code>osOK</code>\uff1a \u7ebf\u7a0b\u5df2\u88ab\u6210\u529f\u6062\u590d\u3002</li> <li><code>osErrorParameter</code>\uff1a \u53c2\u6570 <code>thread_id</code> \u662f NULL \u6216\u65e0\u6548\u3002</li> <li><code>osErrorResource</code>\uff1a \u6307\u5b9a\u7684\u7ebf\u7a0b\u4e0d\u5904\u4e8e\u6302\u8d77\u72b6\u6001\u3002</li> <li><code>osErrorISR</code>\uff1a \u5728\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\u4e2d\u8c03\u7528\u4e86\u6b64\u51fd\u6570\uff08\u4e0d\u5141\u8bb8\uff09\u3002</li> </ul> <p>\u5173\u952e\u70b9\uff1a</p> <ul> <li>\u6b64\u51fd\u6570\u7528\u4e8e\u6062\u590d\u4e00\u4e2a\u88ab <code>osThreadSuspend</code> \u6302\u8d77\u7684\u7ebf\u7a0b\u3002</li> <li>\u5b83\u4f1a\u9012\u51cf\u8be5\u7ebf\u7a0b\u7684\u6302\u8d77\u8ba1\u6570\u3002\u5f53\u8ba1\u6570\u51cf\u5230 0 \u65f6\uff0c\u7ebf\u7a0b\u53d8\u4e3a\u5c31\u7eea\u72b6\u6001\uff0c\u5e76\u53ef\u6839\u636e\u5176\u4f18\u5148\u7ea7\u53c2\u4e0e\u8c03\u5ea6\u3002</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaksPend%26Resume/#id","title":"\u5982\u4f55\u83b7\u53d6\u7ebf\u7a0b ID\uff1f","text":"<p>\u8981\u6302\u8d77\u6216\u6062\u590d\u4e00\u4e2a\u7ebf\u7a0b\uff0c\u4f60\u5fc5\u987b\u62e5\u6709\u5b83\u7684 <code>osThreadId_t</code>\u3002</p> <ul> <li>\u521b\u5efa\u7ebf\u7a0b\u65f6\u83b7\u53d6\uff1a<code>osThreadNew</code> \u51fd\u6570\u4f1a\u8fd4\u56de\u7ebf\u7a0b ID\u3002</li> </ul> <p><code>osThreadId_t myThreadId = osThreadNew(thread_func, NULL, &amp;attr);</code></p> <ul> <li>\u83b7\u53d6\u81ea\u5df1\u7684\u7ebf\u7a0b ID\uff1a\u4f7f\u7528 <code>osThreadGetId</code> \u51fd\u6570\u3002</li> </ul> <p><code>osThreadId_t myOwnId = osThreadGetId();</code></p> <ul> <li>\u901a\u8fc7\u7ebf\u7a0b\u540d\u67e5\u627e\uff08\u5982\u679c\u914d\u7f6e\u652f\u6301\uff09\uff1a\u4f7f\u7528 <code>osThreadGetByName</code> \u51fd\u6570\u3002</li> </ul> <p><code>osThreadId_t foundThreadId = osThreadGetByName(\"MyThreadName\");</code></p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/","title":"TaskNote","text":"<p>\u961f\u5217\u3001\u4fe1\u53f7\u91cf\u3001\u4e92\u65a5\u91cf\u90fd\u662f\u591a\u5bf9\u591a\uff0c\u6211\u4eec\u4e0d\u77e5\u9053\u662f\u8c01\u5728\u53c2\u4e0e\u3002\u5982\u679c\u6211\u4eec\u60f3\u5355\u72ec\u901a\u77e5\u4e00\u4e2a\u7279\u5b9a\u7684\u4efb\u52a1\u600e\u4e48\u529e\u5462\uff1f\u4f7f\u7528\u4efb\u52a1\u901a\u77e5</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_1","title":"\u4efb\u52a1\u901a\u77e5\u7684\u57fa\u672c\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_2","title":"\u4ec0\u4e48\u662f\u4efb\u52a1\u901a\u77e5","text":"<p>\u4efb\u52a1\u901a\u77e5\u662fFreeRTOS\u4e2d\u6700\u9ad8\u6548\u7684\u8f7b\u91cf\u7ea7\u901a\u4fe1\u673a\u5236\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u81ea\u5e26\u7684\"\u90ae\u7bb1\"\uff1a</p> <ul> <li>\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u4e00\u4e2a\u5185\u7f6e\u7684\u901a\u77e5\u503c\uff1a\u5c31\u50cf\u6bcf\u4e2a\u4eba\u90fd\u6709\u4e00\u4e2a\u4e13\u5c5e\u6536\u4ef6\u7bb1</li> <li>\u76f4\u63a5\u901a\u77e5\u76ee\u6807\u4efb\u52a1\uff1a\u65e0\u9700\u521b\u5efa\u4e2d\u95f4\u5bf9\u8c61\uff0c\u76f4\u63a5\u5411\u76ee\u6807\u4efb\u52a1\u7684\"\u90ae\u7bb1\"\u6295\u9012\u6d88\u606f</li> <li>\u591a\u79cd\u901a\u4fe1\u6a21\u5f0f\uff1a\u53ef\u4ee5\u6a21\u62df\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u3001\u8ba1\u6570\u4fe1\u53f7\u91cf\u3001\u4e8b\u4ef6\u7ec4\u3001\u8f7b\u91cf\u7ea7\u961f\u5217</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_3","title":"\u4efb\u52a1\u901a\u77e5\u7684\u5f62\u8c61\u6bd4\u55bb","text":"<p>\u90ae\u7bb1\u7cfb\u7edf\uff1a - \u6bcf\u4e2a\u4efb\u52a1\u5c31\u50cf\u6709\u4e00\u4e2a\u4e13\u5c5e\u90ae\u7bb1\uff08\u901a\u77e5\u503c\uff09 - \u5176\u4ed6\u4efb\u52a1\u53ef\u4ee5\u76f4\u63a5\u5411\u8fd9\u4e2a\u90ae\u7bb1\u6295\u9012\u4fe1\u4ef6\uff08\u53d1\u9001\u901a\u77e5\uff09 - \u4efb\u52a1\u53ef\u4ee5\u968f\u65f6\u68c0\u67e5\u81ea\u5df1\u7684\u90ae\u7bb1\uff08\u63a5\u6536\u901a\u77e5\uff09 - \u90ae\u7bb1\u53ef\u4ee5\u8bbe\u7f6e\u4e0d\u540c\u7684\u63a5\u6536\u89c4\u5219\uff08\u901a\u77e5\u52a8\u4f5c\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_4","title":"\u4efb\u52a1\u901a\u77e5\u7684\u6838\u5fc3\u7279\u70b9","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_5","title":"\u6027\u80fd\u4f18\u52bf","text":"<p>\u901f\u5ea6\u6700\u5feb\uff1a - \u6bd4\u961f\u5217\u5feb45% - \u6bd4\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf\u5feb45% - \u6bd4\u8ba1\u6570\u4fe1\u53f7\u91cf\u5feb10%</p> <p>\u5185\u5b58\u5f00\u9500\u6700\u5c0f\uff1a - \u6bcf\u4e2a\u4efb\u52a1\u53ea\u9700\u8981\u989d\u59168\u5b57\u8282\u5b58\u50a8\u901a\u77e5\u503c - \u65e0\u9700\u521b\u5efa\u72ec\u7acb\u7684\u6d88\u606f\u5bf9\u8c61 - \u96f6\u5185\u5b58\u5206\u914d\u5f00\u9500</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_6","title":"\u529f\u80fd\u7279\u6027","text":"<p>\u7075\u6d3b\u6027\u9ad8\uff1a - \u53ef\u4ee5\u6a21\u62df\u591a\u79cd\u901a\u4fe1\u673a\u5236 - \u652f\u6301\u6570\u636e\u4f20\u9012\u548c\u4e8b\u4ef6\u901a\u77e5 - \u63d0\u4f9b\u4e30\u5bcc\u7684\u901a\u77e5\u52a8\u4f5c\u9009\u9879</p> <p>\u76f4\u63a5\u901a\u4fe1\uff1a - \u4e00\u5bf9\u4e00\u901a\u4fe1\u6a21\u5f0f - \u65e0\u9700\u4e2d\u95f4\u901a\u4fe1\u5bf9\u8c61 - \u76f4\u63a5\u64cd\u4f5c\u76ee\u6807\u4efb\u52a1\u72b6\u6001</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_7","title":"\u5c40\u9650\u6027","text":"<ul> <li>\u53ea\u80fd\u4e00\u5bf9\u4e00\u901a\u4fe1\uff1a\u4e00\u4e2a\u901a\u77e5\u53ea\u80fd\u53d1\u7ed9\u4e00\u4e2a\u7279\u5b9a\u4efb\u52a1</li> <li>\u65e0\u5b58\u50a8\u961f\u5217\uff1a\u53ea\u80fd\u4fdd\u5b58\u4e00\u4e2a\u901a\u77e5\u503c\uff0c\u65b0\u901a\u77e5\u53ef\u80fd\u8986\u76d6\u65e7\u503c</li> <li>\u9700\u8981\u77e5\u9053\u76ee\u6807\u4efb\u52a1\u53e5\u67c4\uff1a\u5fc5\u987b\u6301\u6709\u76ee\u6807\u4efb\u52a1\u7684TaskHandle_t</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#freertos","title":"FreeRTOS\u4efb\u52a1\u901a\u77e5\u64cd\u4f5c\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_8","title":"\u53d1\u9001\u901a\u77e5\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotify-","title":"xTaskNotify - \u901a\u7528\u901a\u77e5\u53d1\u9001","text":"<pre><code>BaseType_t xTaskNotify(TaskHandle_t xTaskToNotify, \n                      uint32_t ulValue, \n                      eNotifyAction eAction);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a - <code>xTaskToNotify</code>\uff1a\u76ee\u6807\u4efb\u52a1\u53e5\u67c4 - <code>ulValue</code>\uff1a\u901a\u77e5\u503c - <code>eAction</code>\uff1a\u901a\u77e5\u52a8\u4f5c\u7c7b\u578b</p> <p>\u901a\u77e5\u52a8\u4f5c\u7c7b\u578b\uff1a</p> <pre><code>typedef enum {\n    eNoAction = 0,           // \u4ec5\u66f4\u65b0\u901a\u77e5\u72b6\u6001\uff0c\u4e0d\u4fee\u6539\u901a\u77e5\u503c\n    eSetBits,                // \u8bbe\u7f6e\u901a\u77e5\u503c\u7684\u6307\u5b9a\u4f4d\n    eIncrement,              // \u9012\u589e\u901a\u77e5\u503c\n    eSetValueWithOverwrite,  // \u76f4\u63a5\u8986\u76d6\u901a\u77e5\u503c\n    eSetValueWithoutOverwrite // \u4ec5\u5728\u901a\u77e5\u672a\u5904\u7406\u65f6\u8986\u76d6\n} eNotifyAction;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotifyfromisr-","title":"xTaskNotifyFromISR - \u4e2d\u65ad\u4e2d\u53d1\u9001\u901a\u77e5","text":"<pre><code>BaseType_t xTaskNotifyFromISR(TaskHandle_t xTaskToNotify,\n                             uint32_t ulValue,\n                             eNotifyAction eAction,\n                             BaseType_t *pxHigherPriorityTaskWoken);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotifygive-","title":"xTaskNotifyGive - \u8f7b\u91cf\u7ea7\u4fe1\u53f7\u91cf\u53d1\u9001","text":"<pre><code>void xTaskNotifyGive(TaskHandle_t xTaskToNotify);\n</code></pre> <p>\u76f8\u5f53\u4e8e<code>xTaskNotify(xTaskToNotify, 0, eIncrement)</code></p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotifyandquery-","title":"xTaskNotifyAndQuery - \u5e26\u67e5\u8be2\u7684\u53d1\u9001","text":"<pre><code>BaseType_t xTaskNotifyAndQuery(TaskHandle_t xTaskToNotify,\n                              uint32_t ulValue,\n                              eNotifyAction eAction,\n                              uint32_t *pulPreviousNotifyValue);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_9","title":"\u63a5\u6536\u901a\u77e5\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#ultasknotifytake-","title":"ulTaskNotifyTake - \u8f7b\u91cf\u7ea7\u4fe1\u53f7\u91cf\u63a5\u6536","text":"<pre><code>uint32_t ulTaskNotifyTake(BaseType_t xClearCountOnExit,\n                         TickType_t xTicksToWait);\n</code></pre> <p>\u4e13\u4e3a\u6a21\u62df\u4fe1\u53f7\u91cf\u8bbe\u8ba1\uff0c\u8fd4\u56de\u901a\u77e5\u503c\u7684\u5f53\u524d\u503c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotifywait-","title":"xTaskNotifyWait - \u901a\u7528\u901a\u77e5\u63a5\u6536","text":"<pre><code>BaseType_t xTaskNotifyWait(uint32_t ulBitsToClearOnEntry,\n                          uint32_t ulBitsToClearOnExit,\n                          uint32_t *pulNotificationValue,\n                          TickType_t xTicksToWait);\n</code></pre> <p>\u529f\u80fd\u6700\u5168\u9762\u7684\u901a\u77e5\u63a5\u6536\u51fd\u6570\uff0c\u652f\u6301\u4f4d\u64cd\u4f5c\u548c\u6570\u636e\u83b7\u53d6\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_10","title":"\u67e5\u8be2\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#uxtasknotifygetcounter-","title":"uxTaskNotifyGetCounter - \u83b7\u53d6\u901a\u77e5\u8ba1\u6570\u5668","text":"<pre><code>uint32_t uxTaskNotifyGetCounter(TaskHandle_t xTask);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#xtasknotifystateclear-","title":"xTaskNotifyStateClear - \u6e05\u9664\u901a\u77e5\u72b6\u6001","text":"<pre><code>BaseType_t xTaskNotifyStateClear(TaskHandle_t xTask);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#freertos_1","title":"FreeRTOS\u4efb\u52a1\u901a\u77e5\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#1","title":"\u6a21\u5f0f1\uff1a\u6a21\u62df\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf","text":"<p>\u53d1\u9001\u65b9\uff1a</p> <pre><code>void vSenderTask(void *pvParameters) {\n    TaskHandle_t xReceiverHandle = (TaskHandle_t)pvParameters;\n\n    while(1) {\n        // \u5b8c\u6210\u5de5\u4f5c\u540e\u901a\u77e5\u63a5\u6536\u65b9\n        do_work();\n\n        // \u53d1\u9001\u901a\u77e5\uff08\u76f8\u5f53\u4e8egive\u4fe1\u53f7\u91cf\uff09\n        xTaskNotifyGive(xReceiverHandle);\n\n        vTaskDelay(100 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre> <p>\u63a5\u6536\u65b9\uff1a</p> <pre><code>void vReceiverTask(void *pvParameters) {\n    uint32_t ulNotificationValue;\n\n    while(1) {\n        // \u7b49\u5f85\u901a\u77e5\uff08\u76f8\u5f53\u4e8etake\u4fe1\u53f7\u91cf\uff09\n        ulNotificationValue = ulTaskNotifyTake(pdTRUE, portMAX_DELAY);\n\n        if(ulNotificationValue &gt; 0) {\n            // \u5904\u7406\u5de5\u4f5c\n            process_work();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#2","title":"\u6a21\u5f0f2\uff1a\u6a21\u62df\u4e8b\u4ef6\u7ec4","text":"<p>\u53d1\u9001\u65b9\uff1a</p> <pre><code>void vEventSenderTask(void *pvParameters) {\n    TaskHandle_t xProcessorHandle = (TaskHandle_t)pvParameters;\n\n    while(1) {\n        // \u8bbe\u7f6e\u4e0d\u540c\u7684\u4e8b\u4ef6\u4f4d\n        if(sensor1_ready()) {\n            xTaskNotify(xProcessorHandle, (1UL &lt;&lt; 0), eSetBits);  // \u4e8b\u4ef6\u4f4d0\n        }\n\n        if(sensor2_ready()) {\n            xTaskNotify(xProcessorHandle, (1UL &lt;&lt; 1), eSetBits);  // \u4e8b\u4ef6\u4f4d1\n        }\n\n        vTaskDelay(50 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre> <p>\u63a5\u6536\u65b9\uff1a</p> <pre><code>void vEventProcessorTask(void *pvParameters) {\n    uint32_t ulNotifiedValue;\n    const uint32_t ALL_EVENTS_MASK = (1UL &lt;&lt; 0) | (1UL &lt;&lt; 1);\n\n    while(1) {\n        // \u7b49\u5f85\u7279\u5b9a\u4e8b\u4ef6\u4f4d\n        xTaskNotifyWait(0,                          // \u8fdb\u5165\u65f6\u4e0d\u6e05\u9664\u4f4d\n                       ALL_EVENTS_MASK,            // \u9000\u51fa\u65f6\u6e05\u9664\u6240\u6709\u4e8b\u4ef6\u4f4d\n                       &amp;ulNotifiedValue,           // \u83b7\u53d6\u901a\u77e5\u503c\n                       portMAX_DELAY);\n\n        if((ulNotifiedValue &amp; ALL_EVENTS_MASK) == ALL_EVENTS_MASK) {\n            printf(\"All events received!\\n\");\n            process_all_events();\n        } else if(ulNotifiedValue &amp; (1UL &lt;&lt; 0)) {\n            printf(\"Event 0 received\\n\");\n            process_event_0();\n        } else if(ulNotifiedValue &amp; (1UL &lt;&lt; 1)) {\n            printf(\"Event 1 received\\n\");\n            process_event_1();\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#3","title":"\u6a21\u5f0f3\uff1a\u6570\u636e\u4f20\u9012","text":"<p>\u53d1\u9001\u65b9\uff1a</p> <pre><code>void vDataSenderTask(void *pvParameters) {\n    TaskHandle_t xReceiverHandle = (TaskHandle_t)pvParameters;\n    uint32_t data_counter = 0;\n\n    while(1) {\n        // \u51c6\u5907\u6570\u636e\n        uint32_t data_packet = (data_counter++ &amp; 0xFFFFFF) | (1UL &lt;&lt; 31);\n\n        // \u53d1\u9001\u6570\u636e\uff08\u8986\u76d6\u6a21\u5f0f\uff09\n        xTaskNotify(xReceiverHandle, data_packet, eSetValueWithOverwrite);\n\n        vTaskDelay(200 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre> <p>\u63a5\u6536\u65b9\uff1a</p> <pre><code>void vDataReceiverTask(void *pvParameters) {\n    uint32_t received_data;\n\n    while(1) {\n        // \u63a5\u6536\u6570\u636e\n        if(xTaskNotifyWait(0, 0, &amp;received_data, 1000 / portTICK_PERIOD_MS) == pdTRUE) {\n            if(received_data &amp; (1UL &lt;&lt; 31)) {\n                uint32_t actual_data = received_data &amp; 0xFFFFFF;\n                printf(\"Received data: %lu\\n\", actual_data);\n            }\n        } else {\n            printf(\"No data received within timeout\\n\");\n        }\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#4","title":"\u6a21\u5f0f4\uff1a\u8ba1\u6570\u4fe1\u53f7\u91cf","text":"<p>\u53d1\u9001\u65b9\uff1a</p> <pre><code>void vResourceProducerTask(void *pvParameters) {\n    TaskHandle_t xConsumerHandle = (TaskHandle_t)pvParameters;\n\n    while(1) {\n        // \u751f\u4ea7\u591a\u4e2a\u8d44\u6e90\n        for(int i = 0; i &lt; 3; i++) {\n            xTaskNotifyGive(xConsumerHandle);  // \u589e\u52a0\u8ba1\u6570\n        }\n\n        vTaskDelay(5000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre> <p>\u63a5\u6536\u65b9\uff1a</p> <pre><code>void vResourceConsumerTask(void *pvParameters) {\n    uint32_t available_resources;\n\n    while(1) {\n        // \u83b7\u53d6\u53ef\u7528\u8d44\u6e90\u6570\u91cf\uff08\u4e0d\u6e05\u9664\u8ba1\u6570\uff09\n        available_resources = ulTaskNotifyTake(pdFALSE, 0);\n\n        if(available_resources &gt; 0) {\n            // \u4f7f\u7528\u4e00\u4e2a\u8d44\u6e90\uff08\u51cf\u5c11\u8ba1\u6570\uff09\n            ulTaskNotifyTake(pdTRUE, 0);\n\n            printf(\"Using resource, %lu remaining\\n\", available_resources - 1);\n            use_resource();\n        } else {\n            // \u7b49\u5f85\u8d44\u6e90\u53ef\u7528\n            available_resources = ulTaskNotifyTake(pdTRUE, portMAX_DELAY);\n            printf(\"Resource acquired, now using\\n\");\n            use_resource();\n        }\n\n        vTaskDelay(1000 / portTICK_PERIOD_MS);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_11","title":"\u4efb\u52a1\u901a\u77e5\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_12","title":"\u4f7f\u7528\u573a\u666f\u9009\u62e9","text":"<p>\u9002\u5408\u4f7f\u7528\u4efb\u52a1\u901a\u77e5\u7684\u573a\u666f\uff1a - \u4e00\u5bf9\u4e00\u4efb\u52a1\u901a\u4fe1 - \u8f7b\u91cf\u7ea7\u540c\u6b65\u9700\u6c42 - \u6027\u80fd\u654f\u611f\u7684\u5e94\u7528 - \u5185\u5b58\u53d7\u9650\u7684\u7cfb\u7edf - \u7b80\u5355\u7684\u6570\u636e\u4f20\u9012</p> <p>\u4e0d\u9002\u5408\u4f7f\u7528\u4efb\u52a1\u901a\u77e5\u7684\u573a\u666f\uff1a - \u4e00\u5bf9\u591a\u901a\u4fe1\uff08\u4f7f\u7528\u4e8b\u4ef6\u7ec4\u6216\u961f\u5217\uff09 - \u9700\u8981\u5b58\u50a8\u591a\u4e2a\u6d88\u606f\uff08\u4f7f\u7528\u961f\u5217\uff09 - \u591a\u4e2a\u4efb\u52a1\u7b49\u5f85\u540c\u4e00\u8d44\u6e90\uff08\u4f7f\u7528\u4fe1\u53f7\u91cf/\u4e92\u65a5\u91cf\uff09 - \u590d\u6742\u7684\u6570\u636e\u7ed3\u6784\u4f20\u9012\uff08\u4f7f\u7528\u961f\u5217\uff09</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_13","title":"\u6027\u80fd\u4f18\u5316\u6280\u5de7","text":"<p>\u51cf\u5c11\u901a\u77e5\u4e22\u5931\uff1a</p> <pre><code>// \u4f7f\u7528\u4e0d\u8986\u76d6\u6a21\u5f0f\u4fdd\u62a4\u91cd\u8981\u6570\u636e\nxTaskNotify(xTargetTask, important_data, eSetValueWithoutOverwrite);\n\n// \u4f7f\u7528\u4f4d\u64cd\u4f5c\u7d2f\u52a0\u4e8b\u4ef6\nxTaskNotify(xTargetTask, event_mask, eSetBits);\n</code></pre> <p>\u5408\u7406\u9009\u62e9\u901a\u77e5\u52a8\u4f5c\uff1a</p> <pre><code>// \u540c\u6b65\u573a\u666f - \u4f7f\u7528\u9012\u589e\nxTaskNotifyGive(xTargetTask);\n\n// \u4e8b\u4ef6\u573a\u666f - \u4f7f\u7528\u4f4d\u8bbe\u7f6e  \nxTaskNotify(xTargetTask, EVENT_MASK, eSetBits);\n\n// \u6570\u636e\u4f20\u9012 - \u4f7f\u7528\u8986\u76d6\nxTaskNotify(xTargetTask, data_value, eSetValueWithOverwrite);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_14","title":"\u9519\u8bef\u5904\u7406\u7b56\u7565","text":"<p>\u68c0\u67e5\u53d1\u9001\u7ed3\u679c\uff1a</p> <pre><code>BaseType_t result = xTaskNotify(xTargetTask, value, action);\nif(result != pdPASS) {\n    // \u5904\u7406\u53d1\u9001\u5931\u8d25\n    handle_notification_failure();\n}\n</code></pre> <p>\u5904\u7406\u63a5\u6536\u8d85\u65f6\uff1a</p> <pre><code>if(xTaskNotifyWait(ulBitsToClearOnEntry, ulBitsToClearOnExit, \n                   &amp;ulNotifiedValue, reasonable_timeout) != pdTRUE) {\n    // \u5904\u7406\u8d85\u65f6\u60c5\u51b5\n    handle_notification_timeout();\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#cmsis-rtos-v2","title":"CMSIS-RTOS v2 \u4efb\u52a1\u901a\u77e5\u7b49\u6548\u673a\u5236","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#cmsis-rtos-v2_1","title":"CMSIS-RTOS v2 \u4e2d\u7684\u4efb\u52a1\u901a\u77e5\u66ff\u4ee3\u65b9\u6848","text":"<p>CMSIS-RTOS v2 \u6ca1\u6709\u76f4\u63a5\u7b49\u6548\u7684\u4efb\u52a1\u901a\u77e5\u673a\u5236\uff0c\u4f46\u63d0\u4f9b\u4e86\u7ebf\u7a0b\u6807\u5fd7\uff08Thread Flags\uff09 \u4f5c\u4e3a\u6700\u63a5\u8fd1\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7ebf\u7a0b\u6807\u5fd7\u63d0\u4f9b\u4e86\u7c7b\u4f3c\u4efb\u52a1\u901a\u77e5\u7684\u529f\u80fd\uff0c\u4f46\u5b9e\u73b0\u65b9\u5f0f\u6709\u6240\u4e0d\u540c\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#thread-flags","title":"\u7ebf\u7a0b\u6807\u5fd7\uff08Thread Flags\uff09","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_15","title":"\u7ebf\u7a0b\u6807\u5fd7\u7684\u57fa\u672c\u6982\u5ff5","text":"<p>\u7ebf\u7a0b\u6807\u5fd7\u662fCMSIS-RTOS v2\u4e2d\u6bcf\u4e2a\u7ebf\u7a0b\u81ea\u5e26\u768432\u4f4d\u6807\u5fd7\u5bc4\u5b58\u5668\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</p> <ul> <li>\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u670932\u4e2a\u6807\u5fd7\u4f4d\uff1a\u6bcf\u4e2a\u7ebf\u7a0b\u62e5\u6709\u72ec\u7acb\u768432\u4f4d\u6807\u5fd7\u5bc4\u5b58\u5668</li> <li>\u76f4\u63a5\u7ebf\u7a0b\u95f4\u901a\u4fe1\uff1a\u4efb\u4f55\u7ebf\u7a0b\u90fd\u53ef\u4ee5\u8bbe\u7f6e\u5176\u4ed6\u7ebf\u7a0b\u7684\u6807\u5fd7\u4f4d</li> <li>\u591a\u79cd\u7b49\u5f85\u9009\u9879\uff1a\u652f\u6301\u7b49\u5f85\u4efb\u610f\u6807\u5fd7\u3001\u6240\u6709\u6807\u5fd7\u6216\u7279\u5b9a\u7ec4\u5408</li> <li>\u65e0\u5b58\u50a8\u9650\u5236\uff1a\u4e0d\u4f1a\u50cfFreeRTOS\u4efb\u52a1\u901a\u77e5\u90a3\u6837\u88ab\u65b0\u503c\u8986\u76d6</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#vs-freertos","title":"\u7ebf\u7a0b\u6807\u5fd7 vs FreeRTOS\u4efb\u52a1\u901a\u77e5","text":"\u7279\u6027 FreeRTOS\u4efb\u52a1\u901a\u77e5 CMSIS-RTOS v2\u7ebf\u7a0b\u6807\u5fd7 \u5b58\u50a8\u5927\u5c0f 32\u4f4d\u503c 32\u4e2a\u72ec\u7acb\u6807\u5fd7\u4f4d \u6570\u636e\u4f20\u9012 \u53ef\u4ee5\u4f20\u901232\u4f4d\u6570\u636e \u53ea\u80fd\u4f20\u9012\u6807\u5fd7\u4f4d\u72b6\u6001 \u8986\u76d6\u884c\u4e3a \u53ef\u80fd\u88ab\u65b0\u901a\u77e5\u8986\u76d6 \u6807\u5fd7\u4f4d\u53ef\u4ee5\u72ec\u7acb\u8bbe\u7f6e/\u6e05\u9664 \u901a\u77e5\u52a8\u4f5c \u591a\u79cd\u52a8\u4f5c\u7c7b\u578b \u53ea\u6709\u8bbe\u7f6e\u6807\u5fd7\u4f4d \u6027\u80fd \u975e\u5e38\u9ad8 \u9ad8 \u7075\u6d3b\u6027 \u4e2d\u7b49 \u9ad8\uff0832\u4e2a\u72ec\u7acb\u6807\u5fd7\uff09"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#cmsis-rtos-v2_2","title":"CMSIS-RTOS v2 \u7ebf\u7a0b\u6807\u5fd7\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#osthreadflagsset-","title":"osThreadFlagsSet - \u8bbe\u7f6e\u7ebf\u7a0b\u6807\u5fd7","text":"<pre><code>uint32_t osThreadFlagsSet(osThreadId_t thread_id, uint32_t flags);\n</code></pre> <p>\u53c2\u6570\uff1a - <code>thread_id</code>\uff1a\u76ee\u6807\u7ebf\u7a0bID - <code>flags</code>\uff1a\u8981\u8bbe\u7f6e\u7684\u6807\u5fd7\u4f4d\u63a9\u7801</p> <p>\u8fd4\u56de\u503c\uff1a - \u6210\u529f\uff1a\u8bbe\u7f6e\u540e\u7ebf\u7a0b\u7684\u6807\u5fd7\u4f4d\u72b6\u6001 - \u5931\u8d25\uff1a\u6700\u9ad8\u4f4d\u4e3a1\u7684\u9519\u8bef\u4ee3\u7801</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>// \u8bbe\u7f6e\u7ebf\u7a0b\u7684\u6807\u5fd7\u4f4d0\u548c2\nuint32_t result = osThreadFlagsSet(target_thread, (1UL &lt;&lt; 0) | (1UL &lt;&lt; 2));\nif((result &amp; 0x80000000) == 0) {\n    printf(\"Flags set successfully, current flags: 0x%08lX\\n\", result);\n} else {\n    printf(\"Failed to set flags: error 0x%08lX\\n\", result);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#osthreadflagswait-","title":"osThreadFlagsWait - \u7b49\u5f85\u7ebf\u7a0b\u6807\u5fd7","text":"<pre><code>uint32_t osThreadFlagsWait(uint32_t flags, uint32_t options, uint32_t timeout);\n</code></pre> <p>\u53c2\u6570\uff1a - <code>flags</code>\uff1a\u8981\u7b49\u5f85\u7684\u6807\u5fd7\u4f4d\u63a9\u7801 - <code>options</code>\uff1a\u7b49\u5f85\u9009\u9879 - <code>timeout</code>\uff1a\u8d85\u65f6\u65f6\u95f4\uff08\u6beb\u79d2\uff09</p> <p>\u7b49\u5f85\u9009\u9879\uff1a</p> <pre><code>osFlagsWaitAny    // \u7b49\u5f85\u4efb\u610f\u6307\u5b9a\u6807\u5fd7\u4f4d\nosFlagsWaitAll    // \u7b49\u5f85\u6240\u6709\u6307\u5b9a\u6807\u5fd7\u4f4d\nosFlagsNoClear    // \u7b49\u5f85\u540e\u4e0d\u6e05\u9664\u6807\u5fd7\u4f4d\n</code></pre> <p>\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>// \u7b49\u5f85\u6807\u5fd7\u4f4d0\u62161\uff08\u4efb\u610f\u4e00\u4e2a\uff09\nuint32_t received_flags = osThreadFlagsWait((1UL &lt;&lt; 0) | (1UL &lt;&lt; 1), \n                                           osFlagsWaitAny, \n                                           osWaitForever);\n\n// \u7b49\u5f85\u6807\u5fd7\u4f4d2\u548c3\uff08\u4e24\u4e2a\u90fd\u5fc5\u987b\u8bbe\u7f6e\uff09\nuint32_t received_flags = osThreadFlagsWait((1UL &lt;&lt; 2) | (1UL &lt;&lt; 3),\n                                           osFlagsWaitAll,\n                                           5000);  // 5\u79d2\u8d85\u65f6\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#osthreadflagsclear-","title":"osThreadFlagsClear - \u6e05\u9664\u7ebf\u7a0b\u6807\u5fd7","text":"<pre><code>uint32_t osThreadFlagsClear(uint32_t flags);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#osthreadflagsget-","title":"osThreadFlagsGet - \u83b7\u53d6\u5f53\u524d\u7ebf\u7a0b\u6807\u5fd7\u72b6\u6001","text":"<pre><code>uint32_t osThreadFlagsGet(void);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_16","title":"\u8f85\u52a9\u51fd\u6570","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#osthreadgetid-id","title":"osThreadGetId - \u83b7\u53d6\u5f53\u524d\u7ebf\u7a0bID","text":"<pre><code>osThreadId_t osThreadGetId(void);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_17","title":"\u7ebf\u7a0b\u6807\u5fd7\u4f7f\u7528\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#1_1","title":"\u6a21\u5f0f1\uff1a\u6a21\u62df\u4e8c\u8fdb\u5236\u4fe1\u53f7\u91cf","text":"<p>FreeRTOS\u4efb\u52a1\u901a\u77e5\u65b9\u5f0f\uff1a</p> <pre><code>// \u53d1\u9001\u65b9\nxTaskNotifyGive(xReceiverTask);\n\n// \u63a5\u6536\u65b9\nulTaskNotifyTake(pdTRUE, portMAX_DELAY);\n</code></pre> <p>CMSIS-RTOS v2\u7ebf\u7a0b\u6807\u5fd7\u65b9\u5f0f\uff1a</p> <pre><code>// \u53d1\u9001\u65b9\nosThreadFlagsSet(receiver_thread, 0x01);  // \u4f7f\u7528\u6807\u5fd7\u4f4d0\u4f5c\u4e3a\u4fe1\u53f7\u91cf\n\n// \u63a5\u6536\u65b9\nosThreadFlagsWait(0x01, osFlagsWaitAny, osWaitForever);  // \u7b49\u5f85\u540e\u81ea\u52a8\u6e05\u9664\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#2_1","title":"\u6a21\u5f0f2\uff1a\u6a21\u62df\u4e8b\u4ef6\u7ec4","text":"<p>FreeRTOS\u4efb\u52a1\u901a\u77e5\u65b9\u5f0f\uff1a</p> <pre><code>// \u53d1\u9001\u65b9\nxTaskNotify(xTargetTask, (1UL &lt;&lt; 0) | (1UL &lt;&lt; 2), eSetBits);\n\n// \u63a5\u6536\u65b9\nxTaskNotifyWait(0, (1UL &lt;&lt; 0) | (1UL &lt;&lt; 2), &amp;value, portMAX_DELAY);\n</code></pre> <p>CMSIS-RTOS v2\u7ebf\u7a0b\u6807\u5fd7\u65b9\u5f0f\uff1a</p> <pre><code>// \u53d1\u9001\u65b9\nosThreadFlagsSet(target_thread, (1UL &lt;&lt; 0) | (1UL &lt;&lt; 2));\n\n// \u63a5\u6536\u65b9\nuint32_t flags = osThreadFlagsWait((1UL &lt;&lt; 0) | (1UL &lt;&lt; 2), \n                                  osFlagsWaitAny, \n                                  osWaitForever);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#3_1","title":"\u6a21\u5f0f3\uff1a\u591a\u4e8b\u4ef6\u5206\u79bb\u5904\u7406","text":"<p>CMSIS-RTOS v2\u7279\u6709\u4f18\u52bf\uff1a</p> <pre><code>void event_handler_thread(void *argument) {\n    uint32_t flags;\n\n    while(1) {\n        // \u7b49\u5f85\u591a\u79cd\u7c7b\u578b\u7684\u4e8b\u4ef6\n        flags = osThreadFlagsWait(0x000000FF, osFlagsWaitAny, osWaitForever);\n\n        // \u6839\u636e\u5177\u4f53\u6807\u5fd7\u4f4d\u5904\u7406\u4e0d\u540c\u4e8b\u4ef6\n        if(flags &amp; 0x01) {\n            handle_temperature_event();\n            osThreadFlagsClear(0x01);  // \u660e\u786e\u6e05\u9664\u5df2\u5904\u7406\u7684\u4e8b\u4ef6\n        }\n        if(flags &amp; 0x02) {\n            handle_humidity_event();\n            osThreadFlagsClear(0x02);\n        }\n        if(flags &amp; 0x04) {\n            handle_pressure_event();\n            osThreadFlagsClear(0x04);\n        }\n        if(flags &amp; 0x08) {\n            handle_system_event();\n            osThreadFlagsClear(0x08);\n        }\n        // ... \u53ef\u4ee5\u5904\u7406\u6700\u591a32\u79cd\u72ec\u7acb\u4e8b\u4ef6\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_18","title":"\u7ebf\u7a0b\u6807\u5fd7\u9ad8\u7ea7\u7528\u6cd5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_19","title":"\u6807\u5fd7\u4f4d\u7ba1\u7406\u7b56\u7565","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_20","title":"\u6309\u529f\u80fd\u5206\u7ec4\u6807\u5fd7\u4f4d","text":"<pre><code>// \u4f20\u611f\u5668\u76f8\u5173\u6807\u5fd7\uff08\u4f4e8\u4f4d\uff09\n#define SENSOR_MASK          0x000000FF\n#define TEMPERATURE_EVENT    (1UL &lt;&lt; 0)\n#define HUMIDITY_EVENT       (1UL &lt;&lt; 1)\n#define PRESSURE_EVENT       (1UL &lt;&lt; 2)\n\n// \u7cfb\u7edf\u4e8b\u4ef6\u6807\u5fd7\uff08\u4e2d8\u4f4d\uff09  \n#define SYSTEM_MASK          0x0000FF00\n#define SYSTEM_ALERT         (1UL &lt;&lt; 8)\n#define CONFIG_CHANGE        (1UL &lt;&lt; 9)\n#define DATA_BACKUP          (1UL &lt;&lt; 10)\n\n// \u7528\u6237\u4e8b\u4ef6\u6807\u5fd7\uff08\u9ad88\u4f4d\uff09\n#define USER_MASK           0x00FF0000\n#define USER_INPUT           (1UL &lt;&lt; 16)\n#define DISPLAY_UPDATE       (1UL &lt;&lt; 17)\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_21","title":"\u591a\u6761\u4ef6\u7b49\u5f85","text":"<pre><code>void advanced_wait_example(void) {\n    uint32_t flags;\n\n    // \u65b9\u68481\uff1a\u7b49\u5f85\u4efb\u610f\u4f20\u611f\u5668\u4e8b\u4ef6\u6216\u7cfb\u7edf\u8b66\u62a5\n    flags = osThreadFlagsWait(SENSOR_MASK | SYSTEM_ALERT, \n                             osFlagsWaitAny, \n                             osWaitForever);\n\n    // \u65b9\u68482\uff1a\u7b49\u5f85\u6e29\u5ea6+\u6e7f\u5ea6\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\n    flags = osThreadFlagsWait(TEMPERATURE_EVENT | HUMIDITY_EVENT,\n                             osFlagsWaitAll,\n                             1000);  // 1\u79d2\u8d85\u65f6\n\n    // \u65b9\u68483\uff1a\u7b49\u5f85\u4f46\u4e0d\u6e05\u9664\u6807\u5fd7\uff08\u7528\u4e8e\u76d1\u63a7\uff09\n    flags = osThreadFlagsWait(USER_INPUT, \n                             osFlagsWaitAny | osFlagsNoClear,\n                             500);\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_22","title":"\u9519\u8bef\u5904\u7406\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_23","title":"\u68c0\u67e5\u51fd\u6570\u8fd4\u56de\u503c","text":"<pre><code>uint32_t result = osThreadFlagsSet(target_thread, flags);\nif(result &amp; 0x80000000) {\n    // \u5904\u7406\u9519\u8bef\n    switch(result) {\n        case osErrorParameter:\n            printf(\"Error: Invalid thread ID or flags\\n\");\n            break;\n        case osErrorResource:\n            printf(\"Error: Thread flags resource not available\\n\");\n            break;\n        case osErrorISR:\n            printf(\"Error: Cannot call from ISR context\\n\");\n            break;\n        default:\n            printf(\"Error: Unknown error (0x%08lX)\\n\", result);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_24","title":"\u8d85\u65f6\u5904\u7406\u7b56\u7565","text":"<pre><code>uint32_t flags = osThreadFlagsWait(EVENT_MASK, osFlagsWaitAny, timeout_ms);\nif((flags &amp; 0x80000000) == 0) {\n    // \u6210\u529f\u63a5\u6536\u5230\u6807\u5fd7\n    process_events(flags);\n} else {\n    // \u8d85\u65f6\u6216\u5176\u4ed6\u9519\u8bef\n    if(flags == osErrorTimeout) {\n        handle_timeout();\n    } else {\n        handle_error(flags);\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#freertos_2","title":"\u4e0eFreeRTOS\u4efb\u52a1\u901a\u77e5\u7684\u8fc1\u79fb\u6307\u5357","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_25","title":"\u51fd\u6570\u6620\u5c04\u8868","text":"FreeRTOS\u4efb\u52a1\u901a\u77e5 CMSIS-RTOS v2\u7ebf\u7a0b\u6807\u5fd7 \u8bf4\u660e <code>xTaskNotifyGive()</code> <code>osThreadFlagsSet(thread, 0x01)</code> \u7b80\u5355\u4fe1\u53f7\u91cf\u6a21\u62df <code>ulTaskNotifyTake(pdTRUE, timeout)</code> <code>osThreadFlagsWait(0x01, osFlagsWaitAny, timeout)</code> \u63a5\u6536\u5e76\u6e05\u9664 <code>xTaskNotify(task, value, eSetBits)</code> <code>osThreadFlagsSet(thread, flags)</code> \u8bbe\u7f6e\u6807\u5fd7\u4f4d <code>xTaskNotifyWait(0, mask, &amp;value, timeout)</code> <code>osThreadFlagsWait(mask, osFlagsWaitAny, timeout)</code> \u7b49\u5f85\u6807\u5fd7\u4f4d <code>uxTaskNotifyGetCounter()</code> <code>osThreadFlagsGet()</code> \u83b7\u53d6\u5f53\u524d\u6807\u5fd7\u72b6\u6001"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_26","title":"\u8fc1\u79fb\u6ce8\u610f\u4e8b\u9879","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_27","title":"\u6570\u636e\u4f20\u9012\u5dee\u5f02","text":"<pre><code>// FreeRTOS\uff1a\u53ef\u4ee5\u4f20\u901232\u4f4d\u6570\u636e\nuint32_t sensor_data = 0x12345678;\nxTaskNotify(xProcessor, sensor_data, eSetValueWithOverwrite);\n\n// CMSIS-RTOS v2\uff1a\u53ea\u80fd\u4f20\u9012\u6807\u5fd7\u4f4d\u72b6\u6001\uff0c\u9700\u8981\u5176\u4ed6\u65b9\u5f0f\u4f20\u9012\u6570\u636e\n// \u65b9\u68481\uff1a\u4f7f\u7528\u6d88\u606f\u961f\u5217\u4f20\u9012\u6570\u636e\nosMessageQueuePut(data_queue, &amp;sensor_data, 0, 0);\nosThreadFlagsSet(processor_thread, DATA_READY_FLAG);\n\n// \u65b9\u68482\uff1a\u4f7f\u7528\u5168\u5c40\u53d8\u91cf+\u6807\u5fd7\u4f4d\ng_sensor_data = sensor_data;\nosThreadFlagsSet(processor_thread, DATA_READY_FLAG);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_28","title":"\u8986\u76d6\u884c\u4e3a\u5dee\u5f02","text":"<pre><code>// FreeRTOS\uff1a\u65b0\u901a\u77e5\u53ef\u80fd\u8986\u76d6\u65e7\u503c\nxTaskNotify(xTask, value1, eSetValueWithOverwrite);  // \u53ef\u80fd\u4e22\u5931value1\nxTaskNotify(xTask, value2, eSetValueWithOverwrite);\n\n// CMSIS-RTOS v2\uff1a\u6807\u5fd7\u4f4d\u53ef\u4ee5\u7d2f\u79ef\uff0c\u4e0d\u4f1a\u88ab\u8986\u76d6\nosThreadFlagsSet(thread, FLAG1);  // FLAG1\u4fdd\u6301\u8bbe\u7f6e\nosThreadFlagsSet(thread, FLAG2);  // FLAG1\u548cFLAG2\u90fd\u4fdd\u6301\u8bbe\u7f6e\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/TaskNote/#_29","title":"\u6027\u80fd\u8003\u8651","text":"<p>CMSIS-RTOS v2\u7ebf\u7a0b\u6807\u5fd7\u7684\u4f18\u52bf\uff1a - 32\u4e2a\u72ec\u7acb\u6807\u5fd7\u4f4d\uff0c\u4e0d\u4f1a\u76f8\u4e92\u8986\u76d6 - \u6807\u5fd7\u4f4d\u72b6\u6001\u6301\u4e45\u5316\uff0c\u76f4\u5230\u663e\u5f0f\u6e05\u9664 - \u652f\u6301\u590d\u6742\u7684\u7b49\u5f85\u6761\u4ef6\u7ec4\u5408</p> <p>FreeRTOS\u4efb\u52a1\u901a\u77e5\u7684\u4f18\u52bf\uff1a - \u53ef\u4ee5\u4f20\u901232\u4f4d\u4efb\u610f\u6570\u636e - \u66f4\u7075\u6d3b\u7684\u901a\u77e5\u52a8\u4f5c\u7c7b\u578b - \u7565\u5fae\u66f4\u597d\u7684\u6027\u80fd</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/","title":"Tasks Create&Delete","text":"<ul> <li>1. \u539f\u751fAPI<ul> <li>xTaskCreate()<ul> <li>\u8bf4\u660e</li> <li>\u64cd\u4f5c\u6d41\u7a0b</li> <li>\u89e3\u6790</li> </ul> </li> <li>tskTCB<ul> <li>\u57fa\u672c\u6982\u5ff5</li> <li>\u4e0e\u4efb\u52a1\u53e5\u67c4\u5173\u7cfb</li> </ul> </li> <li>xTaskCreateStatic()<ul> <li>\u64cd\u4f5c\u6d41\u7a0b</li> </ul> </li> <li>vTaskDelete()<ul> <li>\u6d41\u7a0b</li> </ul> </li> <li>vTaskStartScheduler()<ul> <li>\u51fd\u6570\u6267\u884c\u7684\u4e3b\u8981\u6b65\u9aa4\uff1a</li> </ul> </li> </ul> </li> <li>2. CMSIS-RTOS v2 API<ul> <li>\u6838\u5fc3\u521b\u5efa\u51fd\u6570\uff1aosThreadNew()<ul> <li>\u7ebf\u7a0b\u5c5e\u6027\u7ed3\u6784\u4f53\uff1aosThreadAttr_t<ul> <li>\u9ed8\u8ba4\u503c\u89c4\u5219</li> </ul> </li> </ul> </li> <li>\u7ebf\u7a0b\u5220\u9664<ul> <li>1. \u7ec8\u6b62\u7ebf\u7a0b\uff1aosThreadTerminate()</li> <li>2. \u9000\u51fa\u5f53\u524d\u7ebf\u7a0b\uff1aosThreadExit()</li> </ul> </li> <li>\u5f00\u542f\u4efb\u52a1\u8c03\u5ea6</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#1-api","title":"1. \u539f\u751fAPI","text":"<ul> <li><code>xTaskCreate()</code> \u52a8\u6001\u65b9\u5f0f\u521b\u5efa\uff0cFreeRTOS\u7ed9\u6211\u4eec\u5206\u914d</li> <li><code>xTaskCreateStatic()</code> \u9759\u6001\u65b9\u5f0f\u521b\u5efa\uff0c\u6211\u4eec\u81ea\u5df1\u5206\u914d\u5185\u5b58</li> <li><code>xTaskDelete()</code> \u5220\u9664\u4efb\u52a1</li> <li><code>vTaskStartScheduler()</code> \u542f\u52a8\u4efb\u52a1\u8c03\u5ea6</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#xtaskcreate","title":"<code>xTaskCreate()</code>","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_1","title":"\u8bf4\u660e","text":"<p>\u52a8\u6001\u521b\u5efa\u5185\u5b58</p> <pre><code>BaseType_t xTaskCreate( \n    TaskFunction_t                  pxTaskCode,      /*\u6307\u5411\u4efb\u52a1\u51fd\u6570\u7684\u6307\u9488*/\n    const char * const              pcName,          /*\u4efb\u52a1\u540d\u5b57\uff0c\u5927\u5c0f\u914d\u7f6e\u8fc7:configMAX_TASK_NAME_LEN*/\n    const configSTACK_DEPTH_TYPE     usStackDepth,    /*\u4efb\u52a1\u5206\u914d\u7684\u5806\u6808\u5927\u5c0f\uff0c\u5355\u4f4d\u4e3a\u5b57(word)*/\n    void * const                    pvParameters,   /*\u4f20\u9012\u7ed9\u4efb\u52a1\u51fd\u6570\u7684\u53c2\u6570\uff0c\u4e00\u822c\u4e3aNULL*/\n    UBaseType_t                     uxPriority,     /*\u4efb\u52a1\u4f18\u5148\u7ea7\uff0c\u8303\u56f4:0~configMAX_PRIORITIES - 1*/\n    TaskHandle_t * const             pxCreatedTask   /*\u4efb\u52a1\u53e5\u67c4\uff0c\u5c31\u662f\u8fd9\u4e2a\u4efb\u52a1\u63a7\u5236\u5757*/\n                      )\n</code></pre> <pre><code>xReturn = pdPASS;//\u6210\u529f\nxReturn = errCOULD_NOT_ALLOCATE_REQUIRED_MEMORY;//\u5931\u8d25\uff0c\u7533\u8bf7\u5806\u6808\u592a\u5927\nreturn xReturn;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_2","title":"\u64cd\u4f5c\u6d41\u7a0b","text":"<ol> <li>configSUPPORT_DYNAMIC_ALLOCATION = 1\uff08\u9ed8\u8ba4\uff09</li> <li>\u5b9a\u4e49\u51fd\u6570\u5165\u53e3\u53c2\u6570</li> <li> <p>\u7f16\u5199\u4efb\u52a1\u51fd\u6570</p> </li> <li> <p>\u521b\u5efa\u7ed3\u675f\u540e\u8fdb\u5165\u5c31\u7eea\u6001\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_3","title":"\u89e3\u6790","text":"<pre><code>#if( configSUPPORT_DYNAMIC_ALLOCATION == 1 )\n\n    BaseType_t xTaskCreate( TaskFunction_t pxTaskCode,\n                            const char * const pcName,      /*lint !e971 Unqualified char types are allowed for strings and single characters only. */\n                            const configSTACK_DEPTH_TYPE usStackDepth,\n                            void * const pvParameters,\n                            UBaseType_t uxPriority,\n                            TaskHandle_t * const pxCreatedTask )\n    {\n    TCB_t *pxNewTCB;\n    BaseType_t xReturn;\n\n        /* If the stack grows down then allocate the stack then the TCB so the stack\n        does not grow into the TCB.  Likewise if the stack grows up then allocate\n        the TCB then the stack. */\n        //\u5224\u65ad\u5806\u6808\uff0c\u5806\u4e3a\u5411\u4e0a\u751f\u957f\uff0c\u6808\u4e3a\u5411\u4e0b\u751f\u957f\u3002\n        #if( portSTACK_GROWTH &gt; 0 )\n        {\n            /* Allocate space for the TCB.  Where the memory comes from depends on\n            the implementation of the port malloc function and whether or not static\n            allocation is being used. */\n            pxNewTCB = ( TCB_t * ) pvPortMalloc( sizeof( TCB_t ) );\n\n            if( pxNewTCB != NULL )\n            {\n                /* Allocate space for the stack used by the task being created.\n                The base of the stack memory stored in the TCB so the task can\n                be deleted later if required. */\n                pxNewTCB-&gt;pxStack = ( StackType_t * ) pvPortMalloc( ( ( ( size_t ) usStackDepth ) * sizeof( StackType_t ) ) ); /*lint !e961 MISRA exception as the casts are only redundant for some ports. */\n\n                if( pxNewTCB-&gt;pxStack == NULL )\n                {\n                    /* Could not allocate the stack.  Delete the allocated TCB. */\n                    vPortFree( pxNewTCB );\n                    pxNewTCB = NULL;\n                }\n            }\n        }\n        #else /* portSTACK_GROWTH */\n        {\n        //\u5b58\u653e\u4efb\u52a1\u6808\u7684\u9996\u5730\u5740\n        StackType_t *pxStack;\n\n            /* Allocate space for the stack used by the task being created. */\n            //\u7cfb\u7edf\u5e2e\u6211\u4eec\u7533\u8bf7\u5730\u5740\n            pxStack = pvPortMalloc( ( ( ( size_t ) usStackDepth ) * sizeof( StackType_t ) ) ); /*lint !e9079 All values returned by pvPortMalloc() have at least the alignment required by the MCU's stack and this allocation is the stack. */\n\n            if( pxStack != NULL )\n            {\n                /* Allocate space for the TCB. */\n                //\u7533\u8bf7\u6210\u529f\uff0c\u5219\u7533\u8bf7\u4efb\u52a1\u63a7\u5236\u5757\n                pxNewTCB = ( TCB_t * ) pvPortMalloc( sizeof( TCB_t ) ); /*lint !e9087 !e9079 All values returned by pvPortMalloc() have at least the alignment required by the MCU's stack, and the first member of TCB_t is always a pointer to the task's stack. */\n\n                if( pxNewTCB != NULL )\n                {\n                    /* Store the stack location in the TCB. */\n                    pxNewTCB-&gt;pxStack = pxStack;\n                }\n                else\n                {\n                    /* The stack cannot be used as the TCB was not created.  Free\n                    it again. */\n                    vPortFree( pxStack );\n                }\n            }\n            else\n            {\n                pxNewTCB = NULL;\n            }\n        }\n        #endif /* portSTACK_GROWTH */\n\n        if( pxNewTCB != NULL )\n        {\n            #if( tskSTATIC_AND_DYNAMIC_ALLOCATION_POSSIBLE != 0 ) /*lint !e9029 !e731 Macro has been consolidated for readability reasons. */\n            {\n                /* Tasks can be created statically or dynamically, so note this\n                task was created dynamically in case it is later deleted. */\n                //\u6807\u8bb0\u8fd9\u662f\u52a8\u6001\u751f\u6210\u7684\u8fd8\u662f\u9759\u6001\u751f\u6210\u7684\n                pxNewTCB-&gt;ucStaticallyAllocated = tskDYNAMICALLY_ALLOCATED_STACK_AND_TCB;\n            }\n            #endif /* tskSTATIC_AND_DYNAMIC_ALLOCATION_POSSIBLE */\n            //\u521d\u59cb\u5316\u4efb\u52a1\u63a7\u5236\u5757\u4e2d\u7684\u6210\u5458\n            prvInitialiseNewTask( pxTaskCode, pcName, ( uint32_t ) usStackDepth, pvParameters, uxPriority, pxCreatedTask, pxNewTCB, NULL );\n            //\u5c31\u7eea\n            prvAddNewTaskToReadyList( pxNewTCB );\n            xReturn = pdPASS;\n        }\n        else\n        {\n            xReturn = errCOULD_NOT_ALLOCATE_REQUIRED_MEMORY;\n        }\n\n        return xReturn;\n    }\n\n#endif /* configSUPPORT_DYNAMIC_ALLOCATION */\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#tsktcb","title":"<code>tskTCB</code>","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_4","title":"\u57fa\u672c\u6982\u5ff5","text":"<p><code>tskTCB</code> \u662f FreeRTOS \u7528\u4e8e\u7ba1\u7406\u4efb\u52a1\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684 TCB\uff0c\u76f8\u5f53\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u7684\u8eab\u4efd\u8bc1</p> <pre><code>// \u7b80\u5316\u7248\u7684 tskTCB \u7ed3\u6784\uff08\u5b9e\u9645\u7248\u672c\u53ef\u80fd\u56e0\u914d\u7f6e\u800c\u5f02\uff09\ntypedef struct tskTaskControlBlock\n{\n    // \u5806\u6808\u76f8\u5173\n    volatile StackType_t *pxTopOfStack;     // \u5f53\u524d\u5806\u6808\u9876\u6307\u9488\n    StackType_t *pxStack;                   // \u5806\u6808\u8d77\u59cb\u5730\u5740\n\n    // \u4efb\u52a1\u5217\u8868\u76f8\u5173\n    ListItem_t xStateListItem;              // \u72b6\u6001\u5217\u8868\u9879\uff08\u5c31\u7eea\u3001\u963b\u585e\u3001\u6302\u8d77\uff09\n    ListItem_t xEventListItem;               // \u4e8b\u4ef6\u5217\u8868\u9879\n\n    // \u4efb\u52a1\u4f18\u5148\u7ea7\n    UBaseType_t uxPriority;                 // \u4efb\u52a1\u4f18\u5148\u7ea7\n    UBaseType_t uxBasePriority;             // \u57fa\u7840\u4f18\u5148\u7ea7\uff08\u7528\u4e8e\u4f18\u5148\u7ea7\u7ee7\u627f\uff09\n\n    // \u4efb\u52a1\u6807\u8bc6\n    TaskHandle_t xHandle;                   // \u4efb\u52a1\u53e5\u67c4\n    const char *pcTaskName;                 // \u4efb\u52a1\u540d\u79f0\n\n    // \u5806\u6808\u4fe1\u606f\n    configSTACK_DEPTH_TYPE usStackDepth;    // \u5806\u6808\u6df1\u5ea6\n\n    // \u4efb\u52a1\u72b6\u6001\n    eTaskState eCurrentState;               // \u5f53\u524d\u4efb\u52a1\u72b6\u6001\n    UBaseType_t uxCriticalNesting;          // \u4e34\u754c\u533a\u5d4c\u5957\u8ba1\u6570\n\n    // \u8c03\u8bd5\u548c\u7edf\u8ba1\u4fe1\u606f\n    #if ( configUSE_TRACE_FACILITY == 1 )\n        UBaseType_t uxTCBNumber;            // TCB \u7f16\u53f7\uff08\u8c03\u8bd5\u7528\uff09\n    #endif\n\n    #if ( configGENERATE_RUN_TIME_STATS == 1 )\n        uint32_t ulRunTimeCounter;          // \u8fd0\u884c\u65f6\u95f4\u7edf\u8ba1\n    #endif\n\n    // \u5176\u4ed6\u6269\u5c55\u5b57\u6bb5...\n} tskTCB;\n\n// \u5728 FreeRTOS \u4e2d\uff0ctskTCB \u901a\u5e38\u88ab\u91cd\u5b9a\u4e49\u4e3a TCB_t\ntypedef tskTCB TCB_t;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_5","title":"\u4e0e\u4efb\u52a1\u53e5\u67c4\u5173\u7cfb","text":"<pre><code>typedef struct tskTaskControlBlock * TaskHandle_t;\nTaskHandle_t CurrentTask;\nCurrenTask-&gt;pcTaskName;\n</code></pre> <p>\u53ef\u89c1\uff0c\u4efb\u52a1\u53e5\u67c4\u5c31\u662f\u6307\u5411TCB\u7684\u7ed3\u6784\u4f53\u6307\u9488\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#xtaskcreatestatic","title":"<code>xTaskCreateStatic()</code>","text":"<p>\u9759\u6001\u521b\u5efa\u4efb\u52a1</p> <pre><code>TaskHandle_t xTaskCreateStatic(\n    TaskFunction_t pxTaskCode,        //\u4efb\u52a1\u51fd\u6570\u6307\u9488\n    const char * const pcName,        //\u4efb\u52a1\u540d\u79f0\u5b57\u7b26\u4e32\n    uint32_t ulStackDepth,            //\u5806\u6808\u6df1\u5ea6\uff08\u4ee5\u5b57\u4e3a\u5355\u4f4d\uff09\n    void *pvParameters,               //\u4f20\u9012\u7ed9\u4efb\u52a1\u51fd\u6570\u7684\u53c2\u6570\n    UBaseType_t uxPriority,           //\u4efb\u52a1\u4f18\u5148\u7ea7\n    StackType_t *puxStackBuffer,      //\u5806\u6808\u7f13\u51b2\u533a\u6307\u9488\n    StaticTask_t *pxTaskBuffer        //TCB\u7f13\u51b2\u533a\u6307\u9488\n);\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_6","title":"\u64cd\u4f5c\u6d41\u7a0b","text":"<ol> <li>configSUPPORT_STATIC_ALLOCATION = 1</li> <li>\u5b9a\u4e49\u7a7a\u95f2\u4efb\u52a1\uff08\u5fc5\u987b\uff09&amp;\u5b9a\u65f6\u5668\u4efb\u52a1\uff08\u53ef\u9009\uff09\u7684\u4efb\u52a1\u5806\u6808\u53caTCB</li> <li>\u5b9e\u73b0\u4e24\u4e2a\u63a5\u53e3\u51fd\u6570\uff08<code>vApplicationGetIdleTaskMemory</code>\u548c <code>vApplicationGetTimerTaskMemory</code>\uff09</li> <li>\u5b9a\u4e49\u51fd\u6570\u5165\u53e3\u53c2\u6570</li> <li>\u7f16\u5199\u4efb\u52a1\u51fd\u6570</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#vtaskdelete","title":"<code>vTaskDelete()</code>","text":"<pre><code>void vTaskDelete(TaskHandle_t xTaskToDelete);\n</code></pre> <p>[!NOTE]</p> <ol> <li>\u5f53\u4f20\u5165\u7684\u4f4dNULL\u65f6\uff0c\u4f1a\u5220\u9664\u4efb\u52a1\u81ea\u5df1\uff0c\u5373\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u7684\u4efb\u52a1\u3002</li> <li>\u7a7a\u95f2\u4efb\u52a1\u8d1f\u8d23\u91ca\u653e\u88ab\u5220\u9664\u4efb\u52a1\u4e2d\u7531\u7cfb\u7edf\u5206\u914d\u7684\u5185\u5b58\uff0c\u4f46\u90a3\u4e9b\u7531\u4efb\u52a1\u9759\u6001\u521b\u5efa\u81ea\u5df1\u5206\u914d\u7684\u5185\u5b58\u8981\u7528\u6237\u63d0\u524d\u91ca\u653e\uff0c\u5426\u5219\u4f1a\u5bfc\u81f4\u5185\u5b58\u6cc4\u6f0f</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_7","title":"\u6d41\u7a0b","text":"<ol> <li>INCLUDE_vTaskDelete = 1</li> <li>\u4f7f\u7528\u51fd\u6570\uff0c\u4f20\u5165\u53e5\u67c4\u3002</li> </ol>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#vtaskstartscheduler","title":"<code>vTaskStartScheduler()</code>","text":"<p>\u5f00\u542f\u4efb\u52a1\u8c03\u5ea6\u3002\u9700\u8981\u5728\u521b\u5efa\u5b8c\u6240\u6709\u4efb\u52a1\u540e\u542f\u7528\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_8","title":"\u51fd\u6570\u6267\u884c\u7684\u4e3b\u8981\u6b65\u9aa4\uff1a","text":"<pre><code>void vTaskStartScheduler( void )\n{\n    // 1. \u521b\u5efa\u7a7a\u95f2\u4efb\u52a1 (IDLE Task)\n    xIdleTaskHandle = xTaskCreate( \n        prvIdleTask, \n        \"IDLE\", \n        configMINIMAL_STACK_SIZE, \n        NULL, \n        tskIDLE_PRIORITY, \n        &amp;xIdleTaskHandle \n    );\n\n    #if ( configUSE_TIMERS == 1 )\n    // 2. \u5982\u679c\u542f\u7528\u8f6f\u4ef6\u5b9a\u65f6\u5668\uff0c\u521b\u5efa\u5b9a\u65f6\u5668\u670d\u52a1\u4efb\u52a1\n    xTimerTaskHandle = xTaskCreate( \n        prvTimerTask, \n        \"Tmr Svc\", \n        configTIMER_TASK_STACK_DEPTH, \n        NULL, \n        configTIMER_TASK_PRIORITY, \n        &amp;xTimerTaskHandle \n    );\n    #endif\n\n    // 3. \u521d\u59cb\u5316\u7cfb\u7edf\u8282\u62cd\u5b9a\u65f6\u5668 (Systick)\n    if( xPortStartScheduler() != pdFALSE )\n    {\n        // 4. \u5f00\u59cb\u7b2c\u4e00\u4e2a\u4efb\u52a1\u7684\u6267\u884c\n        // \u8fd9\u91cc\u4e0d\u4f1a\u8fd4\u56de\n    }\n    else\n    {\n        // 5. \u8c03\u5ea6\u5668\u542f\u52a8\u5931\u8d25\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#2-cmsis-rtos-v2-api","title":"2. CMSIS-RTOS v2 API","text":"<p>CMSIS-RTOS \u662fARM\u6307\u5b9a\u7684\u7528\u4e8e\u517c\u5bb9\u4e0d\u540cRTOS\u7684API\u89c4\u8303\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#osthreadnew","title":"\u6838\u5fc3\u521b\u5efa\u51fd\u6570\uff1a<code>osThreadNew()</code>","text":"<pre><code>osThreadId_t osThreadNew(osThreadFunc_t func, void *argument, const osThreadAttr_t *attr);\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a</p> <ul> <li><code>func</code>\uff1a\u7ebf\u7a0b\u51fd\u6570\u6307\u9488\uff08\u7ebf\u7a0b\u6267\u884c\u7684\u4ee3\u7801\uff09</li> <li><code>argument</code>\uff1a\u4f20\u9012\u7ed9\u7ebf\u7a0b\u51fd\u6570\u7684\u53c2\u6570</li> <li><code>attr</code>\uff1a\u7ebf\u7a0b\u5c5e\u6027\u7ed3\u6784\u4f53\u6307\u9488\uff08\u53ef\u4e3a NULL \u4f7f\u7528\u9ed8\u8ba4\u503c\uff09</li> </ul> <p>\u8fd4\u56de\u503c\uff1a</p> <ul> <li>\u6210\u529f\uff1a\u8fd4\u56de\u7ebf\u7a0b ID (<code>osThreadId_t</code>)</li> <li>\u5931\u8d25\uff1a\u8fd4\u56de <code>NULL</code></li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#osthreadattr_t","title":"\u7ebf\u7a0b\u5c5e\u6027\u7ed3\u6784\u4f53\uff1a<code>osThreadAttr_t</code>","text":"<pre><code>typedef struct {\n  const char                   *name;        // \u7ebf\u7a0b\u540d\u79f0\n  uint32_t                 attr_bits;        // \u5c5e\u6027\u4f4dosThreadDetached\uff08\u7ebf\u7a0b\u7ec8\u6b62\u65f6\u81ea\u52a8\u6e05\u7406\u8d44\u6e90\uff09\u3001osThreadJoinable\uff08\u7ebf\u7a0b\u53ef\u88ab\u5176\u4ed6\u7ebf\u7a0b\u7b49\u5f85\uff09\u7b49\n  void                      *cb_mem;         // \u63a7\u5236\u5757\u5185\u5b58\n  uint32_t                   cb_size;        // \u63a7\u5236\u5757\u5927\u5c0f\n  void                   *stack_mem;         // \u6808\u5185\u5b58\n  uint32_t                stack_size;        // \u6808\u5927\u5c0f\uff08\u5b57\u8282\uff09\n  osPriority_t              priority;        // \u4f18\u5148\u7ea7\n  TZ_ModuleId_t            tz_module;        // TrustZone \u6a21\u5757 ID\n  uint32_t                  reserved;        // \u4fdd\u7559\n} osThreadAttr_t;\n</code></pre>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_9","title":"\u9ed8\u8ba4\u503c\u89c4\u5219","text":"<p>\u5982\u679c<code>attr</code>\u7684\u67d0\u4e9b\u53c2\u6570\u4e3a <code>NULL</code> \u6216 <code>0</code>\uff0c\u7cfb\u7edf\u4f1a\u4f7f\u7528\u9ed8\u8ba4\u503c\uff1a</p> \u53c2\u6570 \u9ed8\u8ba4\u503c <code>name</code> \u7cfb\u7edf\u751f\u6210\u7684\u540d\u79f0 <code>attr_bits</code> <code>osThreadDetached</code> <code>cb_mem</code> \u52a8\u6001\u5206\u914d\u63a7\u5236\u5757 <code>stack_mem</code> \u52a8\u6001\u5206\u914d\u6808\u5185\u5b58 <code>priority</code> <code>osPriorityNormal</code> <code>tz_module</code> \u975e\u5b89\u5168\u57df <code>reserved</code> \u5fc5\u987b\u4e3a 0"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_10","title":"\u7ebf\u7a0b\u5220\u9664","text":""},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#1-osthreadterminate","title":"1. \u7ec8\u6b62\u7ebf\u7a0b\uff1a<code>osThreadTerminate()</code>","text":"<pre><code>osStatus_t osThreadTerminate(osThreadId_t thread_id);\n</code></pre> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>thread_id</code>\uff1a\u8981\u7ec8\u6b62\u7684\u7ebf\u7a0b ID</li> </ul> <p>\u8fd4\u56de\u503c\uff1a</p> <ul> <li><code>osOK</code>\uff1a\u6210\u529f</li> <li><code>osErrorParameter</code>\uff1a\u53c2\u6570\u9519\u8bef</li> <li><code>osErrorResource</code>\uff1a\u7ebf\u7a0b\u4e0d\u5b58\u5728</li> </ul>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#2-osthreadexit","title":"2. \u9000\u51fa\u5f53\u524d\u7ebf\u7a0b\uff1a<code>osThreadExit()</code>","text":"<pre><code>void osThreadExit(void);\n</code></pre> <p>\u8bf4\u660e\uff1a \u5728\u7ebf\u7a0b\u5185\u90e8\u8c03\u7528\uff0c\u7528\u4e8e\u81ea\u6211\u7ec8\u6b62\u3002</p>"},{"location":"EmbeddedSoft/RTOS/FreeRTOS/Tasks_Create%26Delete/#_11","title":"\u5f00\u542f\u4efb\u52a1\u8c03\u5ea6","text":"<pre><code>osKernelStart();    \n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/","title":"1. ADC\u57fa\u7840\u6982\u5ff5","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#11-adc","title":"1.1 ADC\u5de5\u4f5c\u539f\u7406","text":"<p>\u6a21\u6570\u8f6c\u6362\u5668\uff08ADC\uff09\u5c06\u6a21\u62df\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6570\u5b57\u4fe1\u53f7\uff0cSTM32\u7684ADC\u901a\u5e38\u662f\u9010\u6b21\u903c\u8fd1\u578b\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#12","title":"1.2 \u4e3b\u8981\u53c2\u6570","text":"<ul> <li> <p>\u5206\u8fa8\u7387: 12\u4f4d\uff080-4095\uff09</p> </li> <li> <p>\u91c7\u6837\u65f6\u95f4: \u53ef\u914d\u7f6e\uff0c\u5f71\u54cd\u8f6c\u6362\u7cbe\u5ea6</p> </li> <li> <p>\u53c2\u8003\u7535\u538b: VREF+ \u548c VREF-</p> </li> <li> <p>\u8f6c\u6362\u6a21\u5f0f: \u5355\u6b21\u3001\u8fde\u7eed\u3001\u626b\u63cf\u3001\u95f4\u65ad</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#2","title":"2.\u914d\u7f6e\u8bf4\u660e","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#11","title":"1.1 \u6b65\u9aa4","text":"<ol> <li>\u9009\u62e9\u901a\u9053\u503c</li> <li>\u9009\u62e9ADC1\u6216ADC2</li> <li>\u8fdb\u5165\u5bf9\u5e94ADC\u914d\u7f6e\u754c\u9762\u8fdb\u884c\u914d\u7f6e</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#12_1","title":"1.2 \u53c2\u6570\u8bf4\u660e","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_1","title":"\u5de5\u4f5c\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#vs-adc","title":"\u72ec\u7acb\u6a21\u5f0f vs \u591aADC\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#independent-mode","title":"\u72ec\u7acb\u6a21\u5f0f (Independent mode)","text":"<ul> <li>\u63cf\u8ff0: \u5355\u4e2aADC\u72ec\u7acb\u5de5\u4f5c\uff0c\u4e0d\u4e0e\u5176\u4ed6ADC\u4ea4\u4e92</li> <li>\u5e94\u7528\u573a\u666f: \u7b80\u5355\u7684\u5355\u901a\u9053\u6216\u591a\u901a\u9053\u91c7\u6837\u9700\u6c42</li> <li>\u914d\u7f6e\u65b9\u6cd5:</li> </ul> <pre><code>hadc1.Init.ScanConvMode = DISABLE;  // \u5355\u901a\u9053\n// \u6216\nhadc1.Init.ScanConvMode = ENABLE;   // \u591a\u901a\u9053\u626b\u63cf\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#adc-dual-adc-modes","title":"\u53ccADC\u7ec4\u5408\u6a21\u5f0f (Dual ADC modes)","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#1-combined-regular-simultaneous-mode","title":"1. \u7ec4\u5408\u89c4\u5219\u540c\u6b65\u6a21\u5f0f (Combined Regular Simultaneous Mode)","text":"<ul> <li>\u5de5\u4f5c\u539f\u7406: \u4e24\u4e2aADC\u540c\u65f6\u91c7\u6837\u4e0d\u540c\u7684\u901a\u9053\uff0c\u8f6c\u6362\u540c\u6b65\u8fdb\u884c</li> <li>\u4f18\u52bf: \u63d0\u9ad8\u91c7\u6837\u541e\u5410\u91cf\uff0c\u540c\u65f6\u83b7\u53d6\u591a\u4e2a\u4fe1\u53f7</li> <li>\u5178\u578b\u5e94\u7528: \u9700\u8981\u540c\u6b65\u91c7\u6837\u7684\u591a\u8def\u4fe1\u53f7</li> </ul> <pre><code>// ADC1\u548cADC2\u540c\u65f6\u91c7\u6837\u4e0d\u540c\u901a\u9053\n// \u8f6c\u6362\u7ed3\u679c\u5206\u522b\u5b58\u50a8\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#2-interleaved-mode","title":"2. \u4ea4\u66ff\u6a21\u5f0f (Interleaved Mode)","text":"<ul> <li>\u5de5\u4f5c\u539f\u7406: \u4e24\u4e2aADC\u4ea4\u66ff\u5bf9\u540c\u4e00\u901a\u9053\u8fdb\u884c\u91c7\u6837</li> <li>\u4f18\u52bf: \u6709\u6548\u63d0\u9ad8\u91c7\u6837\u7387\uff08\u51e0\u4e4e\u7ffb\u500d\uff09</li> <li>\u65f6\u5e8f: ADC1\u91c7\u6837 \u2192 ADC1\u8f6c\u6362 \u2192 ADC2\u91c7\u6837 \u2192 ADC2\u8f6c\u6362</li> </ul> <pre><code>// \u9002\u7528\u4e8e\u9ad8\u9891\u4fe1\u53f7\u91c7\u96c6\n// \u91c7\u6837\u7387 = 2 \u00d7 \u5355\u4e2aADC\u91c7\u6837\u7387\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#3-alternate-trigger-mode","title":"3. \u4ea4\u66ff\u89e6\u53d1\u6a21\u5f0f (Alternate Trigger Mode)","text":"<ul> <li>\u5de5\u4f5c\u539f\u7406: \u4f7f\u7528\u5916\u90e8\u89e6\u53d1\u4fe1\u53f7\u4ea4\u66ff\u89e6\u53d1\u4e24\u4e2aADC</li> <li>\u4f18\u52bf: \u7cbe\u786e\u63a7\u5236\u91c7\u6837\u65f6\u5e8f</li> <li>\u5e94\u7528: \u9700\u8981\u4e25\u683c\u65f6\u5e8f\u63a7\u5236\u7684\u5e94\u7528</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#4-combined-injected-simultaneous-mode","title":"4. \u7ec4\u5408\u6ce8\u5165\u540c\u6b65\u6a21\u5f0f (Combined Injected Simultaneous Mode)","text":"<ul> <li>\u5de5\u4f5c\u539f\u7406: \u4e24\u4e2aADC\u540c\u6b65\u6267\u884c\u6ce8\u5165\u7ec4\u8f6c\u6362</li> <li>\u4f18\u52bf: \u9ad8\u4f18\u5148\u7ea7\u4fe1\u53f7\u7684\u540c\u6b65\u91c7\u96c6</li> <li>\u5e94\u7528: \u7d27\u6025\u4e8b\u4ef6\u5904\u7406\u3001\u5173\u952e\u53c2\u6570\u76d1\u6d4b</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#5-watchdog-mode","title":"5. \u770b\u95e8\u72d7\u6a21\u5f0f (Watchdog Mode)","text":"<ul> <li>\u5de5\u4f5c\u539f\u7406: \u76d1\u63a7\u7279\u5b9aADC\u901a\u9053\uff0c\u5f53\u6570\u503c\u8d85\u51fa\u8bbe\u5b9a\u8303\u56f4\u65f6\u89e6\u53d1\u4e2d\u65ad</li> <li>\u914d\u7f6e\u793a\u4f8b:</li> </ul> <pre><code>ADC_AnalogWDGConfTypeDef AnalogWDGConfig;\nAnalogWDGConfig.WatchdogNumber = ADC_ANALOGWATCHDOG_1;\nAnalogWDGConfig.Channel = ADC_CHANNEL_0;\nAnalogWDGConfig.ITMode = ENABLE;\nAnalogWDGConfig.HighThreshold = 3000;\nAnalogWDGConfig.LowThreshold = 1000;\nHAL_ADC_AnalogWDGConfig(&amp;hadc1, &amp;AnalogWDGConfig);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#scan-mode","title":"\u626b\u63cf\u6a21\u5f0f (Scan Mode)","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_2","title":"\u5355\u6b21\u626b\u63cf","text":"<ul> <li>\u884c\u4e3a: \u914d\u7f6e\u7684\u6240\u6709\u901a\u9053\u6309\u987a\u5e8f\u8f6c\u6362\u4e00\u6b21\u540e\u505c\u6b62</li> <li>\u914d\u7f6e:</li> </ul> <pre><code>hadc1.Init.ScanConvMode = ENABLE;\nhadc1.Init.ContinuousConvMode = DISABLE;\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_3","title":"\u8fde\u7eed\u626b\u63cf","text":"<ul> <li>\u884c\u4e3a: \u914d\u7f6e\u7684\u6240\u6709\u901a\u9053\u5faa\u73af\u8fde\u7eed\u8f6c\u6362</li> <li>\u914d\u7f6e:</li> </ul> <pre><code>hadc1.Init.ScanConvMode = ENABLE;\nhadc1.Init.ContinuousConvMode = ENABLE;\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_4","title":"\u8f6c\u6362\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#single-conversion","title":"\u5355\u6b21\u8f6c\u6362\u6a21\u5f0f (Single Conversion)","text":"<ul> <li>\u5de5\u4f5c\u6d41\u7a0b: \u542f\u52a8 \u2192 \u8f6c\u6362\u6307\u5b9a\u901a\u9053 \u2192 \u505c\u6b62</li> <li>\u9002\u7528\u573a\u666f: \u4f4e\u901f\u3001\u4f4e\u529f\u8017\u5e94\u7528</li> </ul> <pre><code>hadc1.Init.ContinuousConvMode = DISABLE;\nHAL_ADC_Start(&amp;hadc1);  // \u6bcf\u6b21\u90fd\u9700\u8981\u91cd\u65b0\u542f\u52a8\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#continuous-conversion","title":"\u8fde\u7eed\u8f6c\u6362\u6a21\u5f0f (Continuous Conversion)","text":"<ul> <li>\u5de5\u4f5c\u6d41\u7a0b: \u542f\u52a8\u540e\u81ea\u52a8\u8fde\u7eed\u8f6c\u6362\uff0c\u65e0\u9700\u91cd\u590d\u89e6\u53d1</li> <li>\u9002\u7528\u573a\u666f: \u9ad8\u901f\u6570\u636e\u91c7\u96c6</li> </ul> <pre><code>hadc1.Init.ContinuousConvMode = ENABLE;\nHAL_ADC_Start(&amp;hadc1);  // \u542f\u52a8\u540e\u6301\u7eed\u8fd0\u884c\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_5","title":"\u89e6\u53d1\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_6","title":"\u8f6f\u4ef6\u89e6\u53d1","text":"<ul> <li>\u63a7\u5236\u65b9\u5f0f: \u901a\u8fc7\u4ee3\u7801\u63a7\u5236\u8f6c\u6362\u5f00\u59cb</li> <li>\u7075\u6d3b\u6027: \u9ad8\uff0c\u53ef\u7cbe\u786e\u63a7\u5236\u91c7\u6837\u65f6\u523b</li> </ul> <pre><code>hadc1.Init.ExternalTrigConv = ADC_SOFTWARE_START;\nHAL_ADC_Start(&amp;hadc1);  // \u8f6f\u4ef6\u542f\u52a8\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_7","title":"\u786c\u4ef6\u89e6\u53d1","text":"<ul> <li>\u89e6\u53d1\u6e90: \u5b9a\u65f6\u5668\u3001\u5916\u90e8\u5f15\u811a\u7b49</li> <li>\u4f18\u52bf: \u7cbe\u786e\u7684\u5b9a\u65f6\u91c7\u6837\uff0c\u4e0d\u4f9d\u8d56CPU</li> </ul> <pre><code>// \u4f7f\u7528TIM1\u89e6\u53d1ADC\u8f6c\u6362\nhadc1.Init.ExternalTrigConv = ADC_EXTERNALTRIGCONV_T1_TRGO;\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_8","title":"\u6570\u636e\u7ba1\u7406\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#polling","title":"\u8f6e\u8be2\u6a21\u5f0f (Polling)","text":"<pre><code>HAL_ADC_Start(&amp;hadc1);\nHAL_ADC_PollForConversion(&amp;hadc1, timeout);\nvalue = HAL_ADC_GetValue(&amp;hadc1);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#interrupt","title":"\u4e2d\u65ad\u6a21\u5f0f (Interrupt)","text":"<pre><code>HAL_ADC_Start_IT(&amp;hadc1);\n// \u5728\u56de\u8c03\u51fd\u6570\u4e2d\u5904\u7406\u6570\u636e\nvoid HAL_ADC_ConvCpltCallback(ADC_HandleTypeDef* hadc)\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#dma-direct-memory-access","title":"DMA\u6a21\u5f0f (Direct Memory Access)","text":"<ul> <li>\u4f18\u52bf: \u4e0d\u5360\u7528CPU\uff0c\u9ad8\u6548\u5904\u7406\u5927\u91cf\u6570\u636e</li> <li>\u914d\u7f6e:</li> </ul> <pre><code>// \u5355\u6b21DMA\u4f20\u8f93\nHAL_ADC_Start_DMA(&amp;hadc1, (uint32_t*)buffer, length);\n\n// \u5faa\u73afDMA\u4f20\u8f93\uff08\u9700\u914d\u7f6eDMA\u4e3a\u5faa\u73af\u6a21\u5f0f\uff09\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#vs","title":"\u6ce8\u5165\u7ec4 vs \u89c4\u5219\u7ec4","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#regular-group","title":"\u89c4\u5219\u7ec4 (Regular Group)","text":"<ul> <li>\u4f18\u5148\u7ea7: \u666e\u901a</li> <li>\u901a\u9053\u6570\u91cf: \u6700\u591a16\u4e2a\u901a\u9053</li> <li>\u89e6\u53d1\u65b9\u5f0f: \u8f6f\u4ef6\u6216\u786c\u4ef6\u89e6\u53d1</li> <li>\u5178\u578b\u5e94\u7528: \u5e38\u89c4\u6570\u636e\u91c7\u96c6</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#injected-group","title":"\u6ce8\u5165\u7ec4 (Injected Group)","text":"<ul> <li>\u4f18\u5148\u7ea7: \u9ad8\uff08\u53ef\u4e2d\u65ad\u89c4\u5219\u7ec4\u8f6c\u6362\uff09</li> <li>\u901a\u9053\u6570\u91cf: \u6700\u591a4\u4e2a\u901a\u9053  </li> <li>\u89e6\u53d1\u65b9\u5f0f: \u81ea\u52a8\u6216\u5916\u90e8\u89e6\u53d1</li> <li>\u5178\u578b\u5e94\u7528: \u7d27\u6025\u4e8b\u4ef6\u5904\u7406\u3001\u5173\u952e\u53c2\u6570</li> </ul> <pre><code>// \u914d\u7f6e\u6ce8\u5165\u7ec4\nADC_InjectionConfTypeDef sConfigInjected;\nsConfigInjected.InjectedNbrOfConversion = 2;\nsConfigInjected.InjectedChannel = ADC_CHANNEL_1;\nsConfigInjected.InjectedRank = 1;\nHAL_ADCEx_InjectedConfigChannel(&amp;hadc1, &amp;sConfigInjected);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_9","title":"\u4f4e\u529f\u8017\u6a21\u5f0f","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#auto-delay","title":"\u81ea\u52a8\u5ef6\u8fdf\u6a21\u5f0f (Auto-delay)","text":"<ul> <li>\u529f\u80fd: \u5728\u8f6c\u6362\u95f4\u63d2\u5165\u5ef6\u8fdf\u4ee5\u964d\u4f4e\u529f\u8017</li> <li>\u9002\u7528: \u5bf9\u91c7\u6837\u7387\u8981\u6c42\u4e0d\u9ad8\u7684\u5e94\u7528</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#discontinuous-mode","title":"\u95f4\u65ad\u6a21\u5f0f (Discontinuous Mode)","text":"<ul> <li>\u5de5\u4f5c\u65b9\u5f0f: \u6bcf\u6b21\u89e6\u53d1\u53ea\u8f6c\u6362\u6307\u5b9a\u6570\u91cf\u7684\u901a\u9053</li> <li>\u8282\u80fd\u6548\u679c: \u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8f6c\u6362\u64cd\u4f5c</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_10","title":"\u6a21\u5f0f\u9009\u62e9\u6307\u5357","text":"\u5e94\u7528\u573a\u666f \u63a8\u8350\u6a21\u5f0f \u7406\u7531 \u7b80\u5355\u5355\u901a\u9053\u91c7\u6837 \u72ec\u7acb\u6a21\u5f0f + \u8f6e\u8be2 \u5b9e\u73b0\u7b80\u5355\uff0c\u8d44\u6e90\u5360\u7528\u5c11 \u9ad8\u901f\u6570\u636e\u91c7\u96c6 \u72ec\u7acb\u6a21\u5f0f + DMA + \u8fde\u7eed\u8f6c\u6362 \u9ad8\u541e\u5410\u91cf\uff0cCPU\u5360\u7528\u4f4e \u540c\u6b65\u591a\u8def\u4fe1\u53f7 \u7ec4\u5408\u89c4\u5219\u540c\u6b65\u6a21\u5f0f \u540c\u65f6\u91c7\u96c6\uff0c\u6570\u636e\u76f8\u5173\u6027\u597d \u8d85\u9ad8\u9891\u4fe1\u53f7 \u4ea4\u66ff\u6a21\u5f0f \u6709\u6548\u63d0\u5347\u91c7\u6837\u7387 \u5173\u952e\u53c2\u6570\u76d1\u63a7 \u770b\u95e8\u72d7\u6a21\u5f0f + \u6ce8\u5165\u7ec4 \u5b9e\u65f6\u76d1\u63a7\uff0c\u5feb\u901f\u54cd\u5e94 \u5b9a\u65f6\u7cbe\u786e\u91c7\u6837 \u786c\u4ef6\u89e6\u53d1\u6a21\u5f0f \u65f6\u5e8f\u7cbe\u786e\uff0c\u4e0d\u4f9d\u8d56\u8f6f\u4ef6"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_11","title":"\u914d\u7f6e\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li>\u65f6\u949f\u914d\u7f6e: \u786e\u4fddADC\u65f6\u949f\u4e0d\u8d85\u8fc7\u5668\u4ef6\u6700\u5927\u9891\u7387</li> <li>\u91c7\u6837\u65f6\u95f4: \u6839\u636e\u4fe1\u53f7\u6e90\u963b\u6297\u8c03\u6574\u91c7\u6837\u65f6\u95f4</li> <li>DMA\u914d\u7f6e: \u591aADC\u6a21\u5f0f\u9700\u8981\u5408\u7406\u914d\u7f6eDMA\u6570\u636e\u6d41</li> <li>\u4e2d\u65ad\u4f18\u5148\u7ea7: \u6ce8\u5165\u7ec4\u4e2d\u65ad\u5e94\u8bbe\u7f6e\u4e3a\u8f83\u9ad8\u4f18\u5148\u7ea7</li> <li>\u7535\u6e90\u566a\u58f0: \u6a21\u62df\u90e8\u5206\u4f9b\u7535\u9700\u8981\u826f\u597d\u7684\u53bb\u8026\u8bbe\u8ba1</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_12","title":"\u57fa\u672c\u8bbe\u7f6e","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_13","title":"\u6570\u636e\u5bf9\u9f50","text":"<p>\u5de6\u5bf9\u9f50\u6216\u53f3\u5bf9\u9f50</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_14","title":"\u626b\u63cf\u6a21\u5f0f","text":"<p>\u662f\u5426\u5bf9\u591a\u901a\u9053\u626b\u63cf</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_15","title":"\u8fde\u7eed\u8f6c\u6362\u6a21\u5f0f","text":"<p>\u662f\u5426\u542f\u7528\u8fde\u7eed\u8f6c\u6362</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_16","title":"\u89c4\u5219\u7ec4\u8bbe\u7f6e","text":"<p> - Number Of Conversion: \u6bcf\u6b21\u8f6c\u6362\u591a\u5c11\u901a\u9053\u3002\u82e5\u6b64\u503c\u5927\u4e8e1\uff0c\u6700\u597d\u914d\u5408DMA\uff0c\u56e0\u4e3a\u89c4\u5219\u7ec4\u8f6c\u6362\u5b8c\u6210\u5b58\u50a8\u6570\u636e\u7684\u5bc4\u5b58\u5668\u53ea\u6709\u4e00\u4e2a\uff0c\u540e\u6765\u7684\u4f1a\u8986\u76d6\u524d\u9762\u7684\u6570\u636e\u3002 - External Trigger Conversion Source: \u9009\u62e9\u8f6c\u6362\u89e6\u53d1\u6e90\uff1a\u8f6f\u4ef6 or \u786c\u4ef6\uff08\u5b9a\u65f6\u5668\uff09 - Rank&amp;Channel&amp;SampleTIme: \u914d\u7f6e\u5bf9\u5e94\u901a\u9053\u8f6c\u6362\u7684\u5e8f\u53f7\u4ee5\u53ca\u91c7\u6837\u7387[^1]</p> <p>[^1]: SampleTime 1.5 Cycles\u00a0\u6307\u7684\u662fADC\u5bf9\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u91c7\u6837\u7684\u65f6\u95f4\u957f\u5ea6\u4e3a1.5\u4e2aADC\u65f6\u949f\u5468\u671f\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_17","title":"\u6ce8\u5165\u7ec4\u8bbe\u7f6e&amp;\u5f00\u95e8\u72d7\u542f\u7528","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#nvic","title":"\u4e2d\u65ad\u8bbe\u7f6e\uff08NVIC\uff09","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_18","title":"\u4e2d\u65ad\u7684\u57fa\u672c\u6982\u5ff5","text":"<p>ADC\u5168\u5c40\u4e2d\u65ad\u7528\u4e8e\u5728\u7279\u5b9aADC\u4e8b\u4ef6\u53d1\u751f\u65f6\u901a\u77e5CPU\uff0c\u5b9e\u73b0\u5f02\u6b65\u4e8b\u4ef6\u5904\u7406\uff0c\u907f\u514d\u8f6e\u8be2\u7b49\u5f85\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_19","title":"\u4e3b\u8981\u4e2d\u65ad\u4e8b\u4ef6\u7c7b\u578b","text":"<pre><code>// ADC\u5e38\u89c1\u4e2d\u65ad\u4e8b\u4ef6\n#define ADC_IT_EOC      ((uint32_t)ADC_IER_EOCIE)      // \u8f6c\u6362\u5b8c\u6210\u4e2d\u65ad\n#define ADC_IT_JEOC     ((uint32_t)ADC_IER_JEOCIE)     // \u6ce8\u5165\u7ec4\u8f6c\u6362\u5b8c\u6210\n#define ADC_IT_AWD      ((uint32_t)ADC_IER_AWDIE)      // \u6a21\u62df\u770b\u95e8\u72d7\u4e2d\u65ad\n#define ADC_IT_OVR      ((uint32_t)ADC_IER_OVRIE)      // \u6ea2\u51fa\u4e2d\u65ad\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_20","title":"\u7528\u6237\u5b9e\u73b0","text":""},{"location":"EmbeddedSoft/STM32_HAL/ADC/#_21","title":"\u81ea\u6821\u51c6","text":"<p>\u9700\u8981\u5728cubemx\u751f\u6210\u7684adc.c\u4e2dADC\u521d\u59cb\u5316\u51fd\u6570\u8865\u5145(\u4f8badc1)</p> <pre><code>/* USER CODE BEGIN ADC1_Init 2 */  \n//\u81ea\u6821\u51c6  \nif (HAL_ADCEx_Calibration_Start(&amp;hadc1)!= HAL_OK)  \n{  \n  Error_Handler();  \n}  \n/* USER CODE END ADC1_Init 2 */\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/ADC/#adc2","title":"\u8bfb\u53d6\u7279\u5b9a\u901a\u9053\u503c\uff08\u72ec\u7acb\u5355\u6b21\u975e\u626b\u63cf,adc2\uff09","text":"<pre><code>ADC_ChannelConfTypeDef sConfig = {0};\nsConfig.Channel = ADC_CHANNEL_2;//\u914d\u7f6e\u5bf9\u5e94\u901a\u9053\u5230\u9996\u4f4d\nsConfig.Rank = ADC_REGULAR_RANK_1;  \nsConfig.SamplingTime = ADC_SAMPLETIME_1CYCLE_5;  \nif (HAL_ADC_ConfigChannel(&amp;hadc2, &amp;sConfig) != HAL_OK)  \n{  \n  Error_Handler();  \n}  \n// \u542f\u52a8ADC\u8f6c\u6362  \nif (HAL_ADC_Start(&amp;hadc2) == HAL_OK)  \n{  \n  // \u7b49\u5f85\u8f6c\u6362\u5b8c\u6210  \n  if (HAL_ADC_PollForConversion(&amp;hadc2, 100) == HAL_OK)  \n  {  \n    return HAL_ADC_GetValue(&amp;hadc2);  \n  }  \n}  \nError_Handler();\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/","title":"1. RTC (Real-Time Clock) \u5b9e\u65f6\u65f6\u949f","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#rtc","title":"RTC \u7ed3\u6784\u4e0e\u5de5\u4f5c\u539f\u7406","text":"<p>RTC \u6a21\u5757\u662f STM32 \u82af\u7247\u4e2d\u4e00\u4e2a\u72ec\u7acb\u4e14\u9ad8\u5ea6\u4e13\u4e1a\u7684\u8ba1\u6570\u5668\uff0c\u5176\u8bbe\u8ba1\u6838\u5fc3\u662f\u56f4\u7ed5\u4f4e\u529f\u8017\u548c\u6389\u7535\u4fdd\u6301\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_1","title":"\u6838\u5fc3\u7ed3\u6784\u7ec4\u6210","text":"<ol> <li>\u65f6\u949f\u6e90 (RTC Clock Source)\uff1a</li> <li>\u901a\u5e38\u9009\u62e9 LSE (Low Speed External) 32.768 kHz \u6676\u632f\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u529f\u8017\u7279\u6027\u3002</li> <li>\u4e5f\u53ef\u4ee5\u9009\u62e9 LSI\uff08\u5185\u90e8\u4f4e\u901f RC \u632f\u8361\u5668\uff09\u6216 HSE/128\uff08\u901a\u5e38\u4e3a8MHz/128\uff09\u3002</li> <li>\u8ba1\u6570\u5668</li> <li>32\u4f4d\u7684\u53ef\u7f16\u7a0b\u8ba1\u6570\u5668\uff0c\u53ef\u5bf9\u5e94Unix\u65f6\u95f4\u6233\u7684\u79d2\u8ba1\u6570\u5668\u3002\uff081s\u589e\u52a01\u6b21\uff0c\u7406\u8bba\u4e0a\u5f88\u4e45\u5f88\u4e45\u624d\u4f1a\u6ea2\u51fa\uff09</li> <li>\u9884\u5206\u9891\u5668 (RTC Prescalers)\uff1a</li> <li>\u5305\u542b \u5f02\u6b65\u9884\u5206\u9891\u5668 (Asynchronous Prescaler) \u548c \u540c\u6b65\u9884\u5206\u9891\u5668 (Synchronous Prescaler)\u3002</li> <li>20\u4f4d\u7684\u53ef\u7f16\u7a0b\u9884\u5206\u9891\u5668\u3002</li> <li>\u5b83\u4eec\u7684\u4efb\u52a1\u662f\u5c06 32.768 kHz \u8f93\u5165\u65f6\u949f\u7cbe\u786e\u5206\u9891\uff0c\u6700\u7ec8\u5f97\u5230 1 Hz \u7684\u65f6\u57fa\uff08\u6bcf\u79d2\u4ea7\u751f\u4e00\u6b21\u8109\u51b2\uff09\u3002</li> <li>$$f_{\\text{out}} = \\frac{f_{\\text{in}}}{(\\text{Asynch} + 1) \\times (\\text{Synch} + 1)} = 1 \\text{ Hz}$$</li> <li>\u65f6\u95f4/\u65e5\u5386\u5bc4\u5b58\u5668\uff1a</li> <li>\u76f4\u63a5\u5b58\u50a8\u79d2\u3001\u5206\u3001\u65f6\u3001\u65e5\u3001\u6708\u3001\u5e74\u3001\u661f\u671f\u7b49 BCD (Binary Coded Decimal) \u683c\u5f0f\u7684\u6570\u636e\u3002</li> <li>\u62a5\u8b66/\u5524\u9192\u903b\u8f91\uff1a</li> <li>\u5305\u542b\u62a5\u8b66\u5bc4\u5b58\u5668\uff08Alarm A/B\uff09\u548c\u5524\u9192\u5b9a\u65f6\u5668\uff0c\u7528\u4e8e\u5728\u7279\u5b9a\u65f6\u95f4\u6216\u5468\u671f\u6027\u5730\u4ea7\u751f\u4e8b\u4ef6\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemx-rtc","title":"CubeMX RTC \u914d\u7f6e\u4e0e\u5b9e\u73b0","text":"\u6982\u5ff5 \u63cf\u8ff0 \u72ec\u7acb\u7535\u6e90 RTC \u6a21\u5757\u7531\u4e13\u7528\u7684 VBAT \u5f15\u811a\u4f9b\u7535\uff08\u901a\u5e38\u662f\u7ebd\u6263\u7535\u6c60\uff09\u3002\u5373\u4f7f\u4e3b\u7535\u6e90 $V_{DD}$ \u65ad\u7535\uff0cRTC \u4e5f\u80fd\u4fdd\u6301\u8fd0\u884c\u548c\u8ba1\u65f6\u3002 \u65f6\u949f\u6e90 RTC \u53ef\u4ee5\u9009\u62e9\u591a\u79cd\u65f6\u949f\u6e90\uff0c\u6700\u5e38\u7528\u7684\u662f LSE (Low Speed External)\uff0c\u5373 32.768 kHz \u5916\u90e8\u6676\u632f\uff0c\u7cbe\u5ea6\u6700\u9ad8\u3002 \u65e5\u5386\u529f\u80fd RTC \u6a21\u5757\u5185\u90e8\u53ef\u4ee5\u5b58\u50a8\u79d2\u3001\u5206\u3001\u65f6\u3001\u65e5\u3001\u6708\u3001\u5e74\u3001\u661f\u671f\u7b49\u4fe1\u606f\uff0c\u5e76\u652f\u6301\u95f0\u5e74\u81ea\u52a8\u8865\u507f\u3002 \u62a5\u8b66\u529f\u80fd \u53ef\u4ee5\u8bbe\u7f6e\u4e24\u4e2a\u72ec\u7acb\u7684\u62a5\u8b66\u65f6\u95f4\uff08Alarm A \u548c Alarm B\uff09\uff0c\u5f53\u65f6\u95f4\u5339\u914d\u65f6\u4ea7\u751f\u4e2d\u65ad\u3002 \u5524\u9192\u529f\u80fd \u5468\u671f\u6027\u5730\u5524\u9192\u7cfb\u7edf\u6216\u4ea7\u751f\u4e2d\u65ad\u3002 <p>\u914d\u7f6e RTC \u5f80\u5f80\u662f\u914d\u7f6e\u6574\u4e2a\u5907\u4efd\u57df\u7684\u7b2c\u4e00\u6b65\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#1-rcc","title":"1. \u65f6\u949f\u6e90\u914d\u7f6e (RCC)","text":"<ol> <li>Pinout &amp; Configuration $\\rightarrow$ System Core $\\rightarrow$ RCC\u3002</li> <li>\u5728 LSE \u9009\u9879\u4e2d\uff0c\u9009\u62e9 <code>Crystal/Ceramic Resonator</code> (\u4f7f\u7528\u5916\u90e8 32.768 kHz \u6676\u632f)\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2-rtc","title":"2. RTC \u6a21\u5f0f\u914d\u7f6e","text":"<ol> <li>Pinout &amp; Configuration $\\rightarrow$ Timers $\\rightarrow$ RTC\u3002</li> <li>Activate Clock Source\uff1a\u5fc5\u987b\u52fe\u9009\u3002</li> <li>Parameter Settings (\u5173\u952e)\uff1a</li> <li>Asynchronous Predivider \u548c Synchronous Predivider\uff1a\u8fd9\u4e24\u4e2a\u9884\u5206\u9891\u5668\u5c06 32.768 kHz \u65f6\u949f\u6e90\u5206\u9891\uff0c\u6700\u7ec8\u5f97\u5230 1 Hz\uff081\u79d2\u949f\u8ba1\u6570\u4e00\u6b21\uff09\u3002<ul> <li>\u516c\u5f0f: $\\text{LSE} / ((\\text{Asynch} + 1) \\times (\\text{Synch} + 1)) = 1 \\text{ Hz}$</li> <li>CubeMX \u4f1a\u81ea\u52a8\u8ba1\u7b97\uff0c\u786e\u4fdd\u7ed3\u679c\u4e3a 1 Hz\u3002</li> </ul> </li> <li>Hour Format\uff1a\u9009\u62e9 12 \u5c0f\u65f6\u5236 (<code>12 Hour Format</code>) \u6216 24 \u5c0f\u65f6\u5236 (<code>24 Hour Format</code>)\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#3-nvic","title":"3. NVIC \u914d\u7f6e\uff08\u5982\u679c\u9700\u8981\u4e2d\u65ad\uff09","text":"<ol> <li>\u5728 RTC \u914d\u7f6e\u9875\u9762\u7684 NVIC Settings \u4e2d\u3002</li> <li>\u52fe\u9009 <code>RTC global interrupt</code>\uff08RTC \u62a5\u8b66\u6216\u5524\u9192\u4e2d\u65ad\uff09\u3002</li> <li>\u8bbe\u7f6e\u4f18\u5148\u7ea7\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#hal-rtc-api","title":"HAL \u5e93 RTC \u6838\u5fc3 API \u4e0e\u5b9e\u73b0","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#1-rtc","title":"1. RTC \u521d\u59cb\u5316\u4e0e\u8bbe\u7f6e\u65f6\u95f4","text":"<p>\u5728 <code>main()</code> \u51fd\u6570\u4e2d\uff0c\u901a\u5e38\u5728 <code>MX_RTC_Init()</code> \u4e4b\u540e\u8c03\u7528\u4ee5\u4e0b\u51fd\u6570\u6765\u8bbe\u7f6e\u521d\u59cb\u65f6\u95f4\u548c\u65e5\u671f\u3002</p> <pre><code>// \u5b9a\u4e49\u65f6\u95f4\u7ed3\u6784\u4f53\u548c\u65e5\u671f\u7ed3\u6784\u4f53\nRTC_TimeTypeDef sTime = {0};\nRTC_DateTypeDef sDate = {0};\n\n// \u8bbe\u7f6e\u65f6\u95f4\uff08\u4f8b\u5982 14:30:00\uff09\nsTime.Hours = 14;\nsTime.Minutes = 30;\nsTime.Seconds = 0;\n// sTime.DayLightSaving = RTC_DAYLIGHTSAVING_NONE; // \u590f\u4ee4\u65f6\uff0c\u901a\u5e38\u4e0d\u7528\n// sTime.StoreOperation = RTC_STOREOPERATION_RESET;\n\n// \u8bbe\u7f6e\u65e5\u671f\uff08\u4f8b\u5982 2025\u5e7412\u67087\u65e5\uff0c\u661f\u671f\u65e5\uff09\nsDate.WeekDay = RTC_WEEKDAY_SUNDAY;\nsDate.Month = RTC_MONTH_DECEMBER;\nsDate.Date = 7;\nsDate.Year = 25; // HAL \u5e93\u4e2d\u53ea\u7528\u540e\u4e24\u4f4d\n\n// \u5c06\u65f6\u95f4\u4fe1\u606f\u5199\u5165 RTC\nHAL_RTC_SetTime(&amp;hrtc, &amp;sTime, RTC_FORMAT_BIN);\nHAL_RTC_SetDate(&amp;hrtc, &amp;sDate, RTC_FORMAT_BIN);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2","title":"2. \u8bfb\u53d6\u65f6\u95f4\u548c\u65e5\u671f","text":"<p>\u8bfb\u53d6\u65f6\u5fc5\u987b\u5148\u8bfb\u53d6\u65f6\u95f4\uff0c\u518d\u8bfb\u53d6\u65e5\u671f\uff0c\u56e0\u4e3a RTC \u5728\u8bfb\u53d6\u65f6\u95f4\u65f6\u4f1a\u9501\u5b58\u65e5\u671f\u5bc4\u5b58\u5668\uff0c\u9632\u6b62\u8bfb\u6570\u8fc7\u7a0b\u4e2d\u7684\u66f4\u65b0\u3002</p> <pre><code>// \u4e34\u65f6\u53d8\u91cf\u7528\u4e8e\u5b58\u50a8\u8bfb\u51fa\u7684\u7ed3\u679c\nRTC_TimeTypeDef gTime = {0};\nRTC_DateTypeDef gDate = {0};\n\n// \u5148\u8bfb\u53d6\u65f6\u95f4\uff08\u4f1a\u9501\u5b58\u65e5\u671f\uff09\nif (HAL_RTC_GetTime(&amp;hrtc, &amp;gTime, RTC_FORMAT_BIN) == HAL_OK) {\n    // \u540e\u8bfb\u53d6\u65e5\u671f\n    HAL_RTC_GetDate(&amp;hrtc, &amp;gDate, RTC_FORMAT_BIN);\n\n    // \u6253\u5370\u6216\u4f7f\u7528 gTime \u548c gDate\n    printf(\"Current Time: %02d:%02d:%02d\\n\", gTime.Hours, gTime.Minutes, gTime.Seconds);\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#3","title":"3. \u62a5\u8b66\u529f\u80fd\u8bbe\u7f6e (\u4e2d\u65ad)","text":"<p>\u4f7f\u7528 <code>HAL_RTC_SetAlarm_IT</code> \u8bbe\u7f6e\u62a5\u8b66\u65f6\u95f4\u5e76\u542f\u7528\u4e2d\u65ad\u3002</p> <pre><code>RTC_AlarmTypeDef sAlarm = {0};\n\n// \u5047\u8bbe\u6211\u4eec\u8bbe\u7f6e Alarm A \u5728\u6bcf\u5929\u7684 15:00:00 \u62a5\u8b66\nsAlarm.AlarmTime.Hours = 15;\nsAlarm.AlarmTime.Minutes = 0;\nsAlarm.AlarmTime.Seconds = 0;\n\n// \u914d\u7f6e\u62a5\u8b66\u5339\u914d\u65b9\u5f0f\uff1a\u53ea\u5339\u914d\u65f6/\u5206/\u79d2\uff0c\u65e5\u671f\u548c\u661f\u671f\u4e0d\u5339\u914d\nsAlarm.AlarmMask = RTC_ALARMMASK_DATEWEEKDAY; \nsAlarm.Alarm = RTC_ALARM_A;\n\n// \u8bbe\u7f6e\u62a5\u8b66\u5e76\u542f\u52a8\u4e2d\u65ad\nHAL_RTC_SetAlarm_IT(&amp;hrtc, &amp;sAlarm, RTC_FORMAT_BIN);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#4","title":"4. \u62a5\u8b66\u4e2d\u65ad\u56de\u8c03\u51fd\u6570","text":"<pre><code>void HAL_RTC_AlarmAEventCallback(RTC_HandleTypeDef *hrtc)\n{\n    // \u62a5\u8b66 A \u4e8b\u4ef6\u53d1\u751f\uff0c\u6267\u884c\u4efb\u52a1\n    printf(\"Alarm A Triggered!\\n\");\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2-bkp-backup-registers","title":"2. BKP (Backup Registers) \u5907\u4efd\u5bc4\u5b58\u5668","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#bkp","title":"BKP \u7ed3\u6784\u4e0e\u8bbf\u95ee","text":"<p>BKP \u5bc4\u5b58\u5668\u662f\u4f4d\u4e8e RTC \u57df \u7684\u4e00\u7ec4 SRAM \u5355\u5143\uff0c\u7528\u4e8e\u5b58\u50a8\u7528\u6237\u6570\u636e\uff0c\u5176\u6700\u5927\u7684\u7279\u70b9\u662f\u72ec\u7acb\u4e8e\u4e3b\u7535\u6e90\u4f9b\u7535\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#bkp_1","title":"BKP \u6838\u5fc3\u7279\u6027","text":"<ul> <li>\u975e\u6613\u5931\u6027\u5b58\u50a8\uff1aBKP \u5bc4\u5b58\u5668\u540c\u6837\u7531 VBAT \u4f9b\u7535\uff0c\u56e0\u6b64\u5373\u4f7f\u4e3b\u7535\u6e90\u65ad\u7535\uff0c\u5b58\u50a8\u5728\u5176\u4e2d\u7684\u6570\u636e\u4e5f\u4e0d\u4f1a\u4e22\u5931\u3002</li> <li>\u5bb9\u91cf\uff1a\u4e0d\u540c\u7cfb\u5217\u7684 STM32 \u82af\u7247\u5bb9\u91cf\u4e0d\u540c\uff0c\u4f8b\u5982 F1 \u7cfb\u5217\u6709 42 \u4e2a 16 \u4f4d\u5bc4\u5b58\u5668\uff0c\u800c F4/F7/H7 \u7cfb\u5217\u6709\u6570\u767e\u4e2a 32 \u4f4d\u5bc4\u5b58\u5668\uff08\u79f0\u4e3a RTC-BKP \u5bc4\u5b58\u5668\uff09\u3002</li> <li>\u7528\u9014\uff1a\u5b58\u50a8\u7cfb\u7edf\u72b6\u6001\u3001\u6821\u51c6\u53c2\u6570\u3001\u4e0a\u7535\u8ba1\u6570\u5668\u3001\u6216\u7528\u4e8e\u5224\u65ad\u7cfb\u7edf\u662f\u6b63\u5e38\u91cd\u542f\u8fd8\u662f\u9996\u6b21\u4e0a\u7535\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_2","title":"\u6838\u5fc3\u7ed3\u6784\u7ec4\u6210","text":"<ol> <li>\u4f9b\u7535\uff1a\u4e0e RTC \u5171\u4eab VBAT \u7535\u6e90\u3002</li> <li>\u5b58\u50a8\u5355\u5143\uff1a\u7531\u4e00\u7cfb\u5217 32 \u4f4d\uff08\u6216\u8001\u7cfb\u5217\u4e2d\u7684 16 \u4f4d\uff09\u901a\u7528\u5bc4\u5b58\u5668 (<code>RTC_BKP_DRx</code>) \u7ec4\u6210\u3002</li> <li>\u5199\u4fdd\u62a4\uff1a\u7531 DBP (Disable Backup Protection) \u4f4d\u63a7\u5236\u3002\u5728\u4efb\u4f55\u5199\u5165 BKP \u5bc4\u5b58\u5668\u6216 RTC \u76f8\u5173\u7684\u914d\u7f6e\u65f6\uff0c\u5fc5\u987b\u5148\u6e05\u9664\u5199\u4fdd\u62a4\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#bkp_2","title":"\u8bbf\u95ee BKP \u533a\u57df\u7684\u7279\u6b8a\u8981\u6c42","text":"<p>\u7531\u4e8e BKP \u533a\u57df\u4e0e RTC \u5171\u4eab\u7535\u6e90\u548c\u65f6\u949f\uff0c\u8bbf\u95ee BKP \u5bc4\u5b58\u5668\u9700\u8981\u7279\u6b8a\u7684\u6b65\u9aa4\uff1a</p> <ol> <li>\u4f7f\u80fd\u65f6\u949f\uff1a(\u8bbe\u7f6eRCC_APB1ENR\u7684PWREN\u548cBKPEN,)\u4f7f\u80fd PWR\uff08\u7535\u6e90\uff09 \u548c BKP\uff08\u6216 DBP\uff09 \u57df\u7684\u65f6\u949f\u3002</li> <li>\u5931\u80fd\u5199\u5165\u4fdd\u62a4\uff1aSTM32 \u9ed8\u8ba4\u5bf9\u5907\u4efd\u57df\u5f00\u542f\u4e86\u5199\u4fdd\u62a4\uff0c\u5fc5\u987b\u7981\u7528\u8be5\u4fdd\u62a4\u624d\u80fd\u5199\u5165\u6570\u636e\u3002(\u8bbe\u7f6ePWR_CR\u7684DBP)</li> <li>\u82e5\u5728\u8bfb\u53d6RTC\u5bc4\u5b58\u5668\u65f6\uff0cRTC\u7684APB1\u63a5\u53e3\u66fe\u7ecf\u5904\u4e8e\u7981\u6b62\u72b6\u6001\uff0c\u5219\u8f6f\u4ef6\u9996\u5148\u5fc5\u987b\u7b49\u5f85RTC_CRL\u5bc4\u5b58\u5668\u4e2d\u7684RSF\u4f4d\uff08\u5bc4\u5b58\u5668\u540c\u6b65\u6807\u5fd7\uff09\u88ab\u786c\u4ef6\u7f6e1</li> <li>\u5fc5\u987b\u8bbe\u7f6eRTC_CRL\u5bc4\u5b58\u5668\u4e2d\u7684CNF\u4f4d\uff0c\u4f7fRTC\u8fdb\u5165\u914d\u7f6e\u6a21\u5f0f\u540e\uff0c\u624d\u80fd\u5199\u5165RTC_PRL\u3001RTC_CNT\u3001RTC_ALR\u5bc4\u5b58\u5668</li> <li>\u5bf9RTC\u4efb\u4f55\u5bc4\u5b58\u5668\u7684\u5199\u64cd\u4f5c\uff0c\u90fd\u5fc5\u987b\u5728\u524d\u4e00\u6b21\u5199\u64cd\u4f5c\u7ed3\u675f\u540e\u8fdb\u884c\u3002\u53ef\u4ee5\u901a\u8fc7\u67e5\u8be2RTC_CR\u5bc4\u5b58\u5668\u4e2d\u7684RTOFF\u72b6\u6001\u4f4d\uff0c\u5224\u65adRTC\u5bc4\u5b58\u5668\u662f\u5426\u5904\u4e8e\u66f4\u65b0\u4e2d\u3002\u4ec5\u5f53RTOFF\u72b6\u6001\u4f4d\u662f1\u65f6\uff0c\u624d\u53ef\u4ee5\u5199\u5165RTC\u5bc4\u5b58\u5668</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#hal-bkp-api","title":"HAL \u5e93 BKP \u6838\u5fc3 API \u4e0e\u5b9e\u73b0","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#1-bkp-hal","title":"1. \u8bbf\u95ee BKP \u533a\u57df\u7684\u901a\u7528\u6b65\u9aa4 (HAL \u5e93\u81ea\u52a8\u5c01\u88c5)","text":"<p>\u5728 HAL \u5e93\u4e2d\uff0c\u64cd\u4f5c RTC \u548c BKP \u5bc4\u5b58\u5668\u65f6\uff0c\u901a\u5e38\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <pre><code>// 1. \u542f\u52a8\u7535\u6e90\u63a5\u53e3\u65f6\u949f\n__HAL_RCC_PWR_CLK_ENABLE(); \n// 2. \u4f7f\u80fd\u5bf9\u5907\u4efd\u57df\u7684\u8bbf\u95ee\uff08\u7981\u7528\u5199\u4fdd\u62a4\uff09\nHAL_PWR_EnableBkUpAccess(); \n\n// ... \u6267\u884c BKP \u6216 RTC \u8bfb\u5199\u64cd\u4f5c ...\n\n// 3. \u7981\u7528\u5bf9\u5907\u4efd\u57df\u7684\u8bbf\u95ee\uff08\u91cd\u65b0\u542f\u7528\u5199\u4fdd\u62a4\uff0c\u53ef\u9009\uff09\nHAL_PWR_DisableBkUpAccess();\n</code></pre> <p>\u6ce8\u610f: \u5728 CubeMX \u751f\u6210\u7684\u4ee3\u7801\u4e2d\uff0c<code>HAL_PWR_EnableBkUpAccess()</code> \u5df2\u7ecf\u88ab\u5c01\u88c5\u5728 <code>HAL_RTC_Init()</code> \u7684\u5f00\u5934\uff0c\u6240\u4ee5\u901a\u5e38\u4e0d\u9700\u8981\u624b\u52a8\u8c03\u7528\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2-bkp","title":"2. \u5199\u5165 BKP \u5bc4\u5b58\u5668","text":"<p>BKP \u5bc4\u5b58\u5668\u5728 HAL \u5e93\u4e2d\u88ab\u62bd\u8c61\u4e3a <code>RTC_BKP_DRx</code>\uff08x \u4e3a\u5bc4\u5b58\u5668\u7f16\u53f7\uff09\u3002</p> <pre><code>// \u5047\u8bbe\u6211\u4eec\u8981\u5411 BKP \u5bc4\u5b58\u5668 DR1 \u5199\u5165\u4e00\u4e2a\u9b54\u6570\uff08Magic Number\uff09\n#define BKP_MAGIC_NUMBER 0xABCDABCD\n#define BKP_INDEX RTC_BKP_DR1\n\n// \u786e\u4fdd\u5907\u4efd\u57df\u5199\u4fdd\u62a4\u5df2\u7981\u7528\uff08\u901a\u8fc7 HAL_PWR_EnableBkUpAccess()\uff09\n\n// \u5199\u5165\u6570\u636e\nHAL_RTCEx_BKUPWrite(&amp;hrtc, BKP_INDEX, BKP_MAGIC_NUMBER);\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#3-bkp","title":"3. \u8bfb\u53d6 BKP \u5bc4\u5b58\u5668","text":"<p>\u8bfb\u53d6\u64cd\u4f5c\u4e0d\u9700\u8981\u7981\u7528\u5199\u4fdd\u62a4\u3002</p> <pre><code>uint32_t read_value;\n\n// \u8bfb\u53d6 BKP \u5bc4\u5b58\u5668 DR1 \u7684\u503c\nread_value = HAL_RTCEx_BKUPRead(&amp;hrtc, BKP_INDEX);\n\nif (read_value == BKP_MAGIC_NUMBER) {\n    // BKP \u6570\u636e\u6709\u6548\uff0c\u7cfb\u7edf\u662f\u6b63\u5e38\u91cd\u542f\n    printf(\"System rebooted normally.\\n\");\n} else {\n    // BKP \u6570\u636e\u65e0\u6548\uff08\u4f8b\u5982\u7b2c\u4e00\u6b21\u4e0a\u7535\u6216 VBAT \u65ad\u7535\uff09\uff0c\u9700\u8981\u91cd\u65b0\u521d\u59cb\u5316\n    printf(\"First power on or VBAT lost. Initializing BKP.\\n\");\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#4_1","title":"4. \u7efc\u5408\u5e94\u7528\uff1a\u5224\u65ad\u9996\u6b21\u4e0a\u7535","text":"<p>\u8fd9\u662f BKP \u5bc4\u5b58\u5668\u6700\u7ecf\u5178\u7684\u5e94\u7528\u573a\u666f\u3002</p> <ol> <li> <p>\u521d\u59cb\u5316\u6d41\u7a0b:</p> </li> <li> <p>\u8bfb\u53d6 BKP \u5bc4\u5b58\u5668\uff08\u5982 DR1\uff09\u3002</p> </li> <li>\u5982\u679c\u8bfb\u51fa\u7684\u503c\u4e0d\u7b49\u4e8e\u9884\u8bbe\u7684\u9b54\u6570\uff08\u5373\u968f\u673a\u8bbe\u5b9a\u7684\u6570\uff0c\u4f8b\u5982 0x1234\uff09\uff0c\u8bf4\u660e\u8fd9\u662f\u9996\u6b21\u4e0a\u7535\u6216 VBAT \u65ad\u7535\u3002<ul> <li>\u6b64\u65f6\uff0c\u8c03\u7528 <code>HAL_RTC_SetTime/SetDate</code> \u8bbe\u7f6e\u521d\u59cb\u65f6\u95f4\u3002</li> <li>\u5411 BKP \u5199\u5165\u9b54\u6570 0x1234\u3002</li> </ul> </li> <li> <p>\u5982\u679c\u8bfb\u51fa\u7684\u503c\u7b49\u4e8e\u9b54\u6570\uff0c\u8bf4\u660e RTC \u65f6\u95f4\u662f\u6709\u6548\u7684\uff0c\u65e0\u9700\u91cd\u65b0\u8bbe\u7f6e\u3002</p> </li> <li> <p>\u4ee3\u7801\u9aa8\u67b6:</p> </li> </ol> <p>```c    #define VALID_FLAG 0x1234</p> <p>// \u5728 main() \u51fd\u6570\u4e2d\uff0c\u521d\u59cb\u5316 RTC \u53e5\u67c4\u540e\uff1a    if (HAL_RTCEx_BKUPRead(&amp;hrtc, RTC_BKP_DR1) != VALID_FLAG)    {        // \u7b2c\u4e00\u6b21\u542f\u52a8\uff0c\u8bbe\u7f6eRTC\u65f6\u95f4\u548cBKP        Set_Initial_RTC_Time();         HAL_RTCEx_BKUPWrite(&amp;hrtc, RTC_BKP_DR1, VALID_FLAG);    }    // \u5426\u5219\uff0cRTC\u4f1a\u7ee7\u7eed\u8ba1\u65f6    ```</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#3-pwr-power-control","title":"3. PWR (Power Control) \u7535\u6e90\u63a7\u5236\u6a21\u5757\u8be6\u89e3","text":"<p>PWR \u6a21\u5757\u662f\u914d\u7f6e\u548c\u63a7\u5236\u6574\u4e2a\u82af\u7247\u7535\u6e90\u57df\uff08\u5305\u62ec\u5907\u4efd\u57df\uff09\u7684\u5173\u952e\u3002RTC \u548c BKP \u7684\u6240\u6709\u64cd\u4f5c\u90fd\u4f9d\u8d56\u4e8e PWR \u6a21\u5757\u7684\u6388\u6743\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#pwr","title":"PWR \u6838\u5fc3\u4f5c\u7528","text":"<ol> <li>\u5907\u4efd\u57df\u63a7\u5236\uff1a\u63a7\u5236\u5bf9 RTC \u548c BKP \u5bc4\u5b58\u5668\u533a\u57df\u7684\u5199\u8bbf\u95ee\u4fdd\u62a4\u3002</li> <li>\u4f4e\u529f\u8017\u6a21\u5f0f\uff1aPWR\u8d1f\u8d23\u7ba1\u7406STM32\u5185\u90e8\u7684\u7535\u6e90\u4f9b\u7535\u90e8\u5206\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u7f16\u7a0b\u7535\u538b\u76d1\u6d4b\u5668\u548c\u4f4e\u529f\u8017\u6a21\u5f0f\u7684\u529f\u80fd\u3002\u4f4e\u529f\u8017\u6a21\u5f0f\u5305\u62ec\u7761\u7720\u6a21\u5f0f\uff08Sleep\uff09\u3001\u505c\u673a\u6a21\u5f0f\uff08Stop\uff09\u548c\u5f85\u673a\u6a21\u5f0f\uff08Standby\uff09\uff0c\u53ef\u5728\u7cfb\u7edf\u7a7a\u95f2\u65f6\uff0c\u964d\u4f4eSTM32\u7684\u529f\u8017\uff0c\u5ef6\u957f\u8bbe\u5907\u4f7f\u7528\u65f6\u95f4</li> <li>\u7535\u538b\u76d1\u6d4b\uff1a\u7ba1\u7406 PVD (Programmable Voltage Detector) \u529f\u80fd\u3002\u53ef\u7f16\u7a0b\u7535\u538b\u76d1\u6d4b\u5668\uff08PVD\uff09\u53ef\u4ee5\u76d1\u63a7VDD\u7535\u6e90\u7535\u538b\uff0c\u5f53VDD\u4e0b\u964d\u5230PVD\u9600\u503c\u4ee5\u4e0b\u6216\u4e0a\u5347\u5230PVD\u9600\u503c\u4e4b\u4e0a\u65f6\uff0cPVD\u4f1a\u89e6\u53d1\u4e2d\u65ad\uff0c\u7528\u4e8e\u6267\u884c\u7d27\u6025\u5173\u95ed\u4efb\u52a1</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemx-pwr","title":"CubeMX PWR \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li>Pinout &amp; Configuration $\\rightarrow$ System Core $\\rightarrow$ PWR\u3002</li> <li>Parameter Settings:</li> <li>Voltage Regulator Output: \u901a\u5e38\u8bbe\u7f6e\u4e3a <code>Reset</code>\uff08\u9ed8\u8ba4\u64cd\u4f5c\uff09\u3002</li> <li>PVD (Programmable Voltage Detector)\uff1a\u53ef\u9009\u914d\u7f6e\uff0c\u7528\u4e8e\u5728\u7535\u6e90\u7535\u538b\u4f4e\u4e8e\u8bbe\u5b9a\u9608\u503c\u65f6\u4ea7\u751f\u4e2d\u65ad\u6216\u590d\u4f4d\u3002</li> <li>Low Power Settings: \u7528\u4e8e\u914d\u7f6e\u8fdb\u5165 Stop/Standby \u6a21\u5f0f\u7684\u9009\u9879\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#hal-pwr-api-rtc-bkp","title":"HAL \u5e93 PWR \u5173\u952e API (RTC &amp; BKP \u4f9d\u8d56)","text":"<p>\u5728 HAL \u5e93\u4e2d\uff0c\u5bf9 RTC \u548c BKP \u7684\u64cd\u4f5c\uff08\u5199\u5165\uff09\u5fc5\u987b\u6267\u884c\u4ee5\u4e0b\u4e24\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff0c\u901a\u5e38\u7531 HAL \u5e93\u81ea\u52a8\u5c01\u88c5\u6216\u5728 <code>MX_RTC_Init()</code> \u4e2d\u5b8c\u6210\uff1a</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#1-pwr-rcc","title":"1. \u542f\u52a8 PWR \u65f6\u949f (RCC)","text":"<p>\u5728\u4efb\u4f55\u64cd\u4f5c PWR \u6216\u5907\u4efd\u57df\u4e4b\u524d\uff0c\u5fc5\u987b\u786e\u4fdd PWR \u6a21\u5757\u7684\u65f6\u949f\u5df2\u5f00\u542f\u3002</p> <p>C</p> <pre><code>// \u786e\u4fdd PWR \u6a21\u5757\u7684\u65f6\u949f\u5df2\u5f00\u542f\n__HAL_RCC_PWR_CLK_ENABLE(); \n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2-dbp","title":"2. \u542f\u7528\u5907\u4efd\u57df\u8bbf\u95ee (DBP \u4f4d\u63a7\u5236)","text":"<p>\u8fd9\u662f\u64cd\u4f5c RTC \u548c BKP \u5bc4\u5b58\u5668\u7684\u5fc5\u5907\u6b65\u9aa4\u3002</p> <ul> <li>\u529f\u80fd\uff1a\u6e05\u9664 DBP (Disable Backup Protection) \u4f4d\uff0c\u6253\u5f00 RTC \u548c BKP \u57df\u7684\u5199\u5165\u901a\u9053\u3002</li> </ul> <p>C</p> <pre><code>// \u542f\u7528\u5bf9\u5907\u4efd\u57df\u7684\u5199\u8bbf\u95ee\uff08\u7981\u7528\u5199\u4fdd\u62a4\uff09\nHAL_PWR_EnableBkUpAccess(); \n\n// ... \u6267\u884c\u5199\u5165 RTC \u6216 BKP \u7684\u64cd\u4f5c ...\n\n// \u7981\u7528\u5bf9\u5907\u4efd\u57df\u7684\u5199\u8bbf\u95ee\uff08\u91cd\u65b0\u542f\u7528\u5199\u4fdd\u62a4\uff0c\u53ef\u9009\uff09\nHAL_PWR_DisableBkUpAccess();\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p>\u51e0\u4e4e\u6240\u6709 HAL_RTC_SetTime\u3001HAL_RTC_SetDate\u3001HAL_RTCEx_BKUPWrite \u7b49\u6d89\u53ca\u5199\u5165 RTC/BKP \u7684 HAL \u51fd\u6570\uff0c\u5728\u5176\u5185\u90e8\u90fd\u4f1a\u4e34\u65f6\u8c03\u7528 HAL_PWR_EnableBkUpAccess() \u548c HAL_PWR_DisableBkUpAccess()\uff0c\u56e0\u6b64\u591a\u6570\u60c5\u51b5\u4e0b\u4f60\u4e0d\u9700\u8981\u624b\u52a8\u64cd\u4f5c\u3002\u53ea\u6709\u5f53\u4f60\u5728 HAL \u51fd\u6570\u5916\u90e8\u76f4\u63a5\u64cd\u4f5c\u5bc4\u5b58\u5668\u65f6\u624d\u9700\u8981\u6ce8\u610f\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#rtcbkppwr","title":"\u603b\u7ed3\uff1aRTC/BKP/PWR \u7684\u8054\u52a8\u5173\u7cfb","text":"\u6a21\u5757 \u89d2\u8272 \u5173\u952e\u914d\u7f6e\u70b9 RTC \u6838\u5fc3\u529f\u80fd\uff1a\u8ba1\u65f6\u3001\u65e5\u5386\u3001\u62a5\u8b66\u3002 LSE \u65f6\u949f\u3001\u9884\u5206\u9891\u5668\uff08\u786e\u4fdd 1 Hz\uff09\u3002 BKP \u6570\u636e\u5b58\u50a8\uff1a\u5b58\u50a8\u975e\u6613\u5931\u6027\u6570\u636e\u3002 \u8bfb\u5199 <code>RTC_BKP_DRx</code> \u5bc4\u5b58\u5668\u3002 PWR \u6743\u9650\u7ba1\u7406\uff1a\u63a7\u5236\u5bf9 RTC/BKP \u57df\u7684\u5199\u5165\u6743\u9650\u3002 \u8c03\u7528 <code>HAL_PWR_EnableBkUpAccess()</code>\u3002"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#4_2","title":"4. \u4f4e\u529f\u8017\u6a21\u5f0f\u8be6\u89e3","text":"<p>\u597d\u7684\uff0c\u6211\u4e3a\u4f60\u6574\u7406\u4e00\u4efd\u5173\u4e8e STM32 \u4f4e\u529f\u8017\u6a21\u5f0f \u7684\u8be6\u7ec6\u7b14\u8bb0\u3002\u8fd9\u4efd\u7b14\u8bb0\u5c06\u7531\u6d45\u5165\u6df1\u5730\u4ecb\u7ecd STM32 \u7684\u4f4e\u529f\u8017\u539f\u7406\u3001\u5404\u4e2a\u6a21\u5f0f\u7684\u7279\u6027\u3001\u4ee5\u53ca\u5728 STM32CubeMX \u548c HAL \u5e93 \u4e2d\u5982\u4f55\u914d\u7f6e\u548c\u4f7f\u7528\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#1","title":"1. \u4f4e\u529f\u8017\u57fa\u7840\u6982\u5ff5\u4e0e\u76ee\u7684","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_3","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4f4e\u529f\u8017\u6a21\u5f0f\uff1f","text":"<p>\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u7535\u6c60\u4f9b\u7535\uff08\u5982\u7269\u8054\u7f51\u8bbe\u5907\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u8fdc\u7a0b\u4f20\u611f\u5668\uff09\u7684\u5e94\u7528\u4e2d\uff0c\u5927\u90e8\u5206\u65f6\u95f4\u82af\u7247\u5904\u4e8e\u7b49\u5f85\u72b6\u6001\u3002\u4f4e\u529f\u8017\u6a21\u5f0f\u7684\u76ee\u7684\u662f\uff1a</p> <ol> <li>\u5ef6\u957f\u7535\u6c60\u5bff\u547d\uff1a\u5728\u4e0d\u6267\u884c\u5173\u952e\u4efb\u52a1\u65f6\uff0c\u5173\u95ed\u6216\u964d\u4f4e\u4e0d\u5fc5\u8981\u7684\u65f6\u949f\u548c\u7535\u6e90\uff0c\u5c06\u5e73\u5747\u7535\u6d41\u964d\u81f3\u5fae\u5b89\uff08$\\mu\\text{A}$\uff09\u751a\u81f3\u7eb3\u5b89\uff08$\\text{nA}$\uff09\u7ea7\u522b\u3002</li> <li>\u4fdd\u6301\u5173\u952e\u72b6\u6001\uff1a\u5728\u6df1\u5ea6\u7761\u7720\u65f6\uff0c\u4fdd\u7559 RAM \u4e2d\u7684\u6570\u636e\u6216 RTC\uff08\u5b9e\u65f6\u65f6\u949f\uff09\u7684\u8fd0\u884c\u3002</li> <li>\u5feb\u901f\u5524\u9192\uff1a\u786e\u4fdd\u7cfb\u7edf\u80fd\u591f\u5728\u9700\u8981\u65f6\uff08\u4f8b\u5982\uff0c\u5916\u90e8\u4e8b\u4ef6\u3001\u5b9a\u65f6\u5668\u4e2d\u65ad\uff09\u5feb\u901f\u8fd4\u56de\u5230\u8fd0\u884c\u72b6\u6001\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_4","title":"\u529f\u8017\u7684\u4e3b\u8981\u6765\u6e90","text":"<ul> <li>\u52a8\u6001\u529f\u8017\uff1a\u7531\u6676\u4f53\u7ba1\u5f00\u5173\u5f15\u8d77\u7684\u529f\u8017\uff0c\u4e0e\u65f6\u949f\u9891\u7387 ($f$) \u548c \u7535\u538b ($V$) \u7684\u5e73\u65b9\u6210\u6b63\u6bd4\u3002$\\text{P}_{\\text{dynamic}} \\propto f \\cdot V^2$\u3002\u8fd9\u662f\u4e3b\u52a8\u964d\u4f4e\u65f6\u949f\u9891\u7387\u548c\u5de5\u4f5c\u7535\u538b\u7684\u539f\u56e0\u3002</li> <li>\u9759\u6001\u529f\u8017\uff1a\u7531\u6f0f\u7535\u6d41\u5f15\u8d77\u7684\u529f\u8017\u3002\u8fd9\u662f\u901a\u8fc7\u5173\u95ed\u7535\u6e90\u57df\uff08\u5982\u8fdb\u5165 Standby \u6a21\u5f0f\uff09\u6765\u89e3\u51b3\u7684\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#2-stm32","title":"2. STM32 \u4e3b\u8981\u4f4e\u529f\u8017\u6a21\u5f0f\u5bf9\u6bd4","text":"<p>STM32 \u7684\u4f4e\u529f\u8017\u6a21\u5f0f\u53ef\u4ee5\u6839\u636e\u5176\u529f\u8017\u6df1\u5ea6\u3001\u5524\u9192\u65f6\u95f4\u548c\u4fdd\u7559\u7684\u6570\u636e\u5206\u4e3a\u4e09\u5927\u7c7b\uff1a</p> \u6a21\u5f0f\u540d\u79f0 \u529f\u8017\u6df1\u5ea6 CPU \u72b6\u6001 \u65f6\u949f\u6e90\u72b6\u6001 \u5524\u9192\u65f6\u95f4 \u5178\u578b\u7535\u6d41 (\u03bcA) \u4fdd\u7559\u6570\u636e Run Mode \u6700\u9ad8 \u6267\u884c\u4ee3\u7801 \u5168\u901f\u8fd0\u884c N/A 1000 - 5000 \u6240\u6709 Sleep Mode (C-Sleep) \u4e2d\u7b49 \u505c\u6b62 CPU \u65f6\u949f\u505c\u6b62 \u6781\u5feb (0) 1000 - 500 \u6240\u6709 Stop Mode (P-Stop) \u4f4e \u505c\u6b62 PLL/HSE/HSI \u505c\u6b62 \u5feb ($\\sim 5 \\mu\\text{s}$) 5 - 50 RAM, \u5bc4\u5b58\u5668 Standby Mode \u6700\u4f4e \u505c\u6b62 \u6240\u6709\u65f6\u949f\u505c\u6b62 \u6162 ($\\sim 60 \\mu\\text{s}$) &lt; 1 RTC, BKP \u5bc4\u5b58\u5668 <p>\u6ce8\uff1aC-Sleep (Core Sleep)\uff0cP-Stop (Peripheral Stop)\u3002\u7535\u6d41\u503c\u4ec5\u4e3a\u793a\u4f8b\uff0c\u5b9e\u9645\u503c\u53d6\u51b3\u4e8e\u82af\u7247\u7cfb\u5217\u548c\u5916\u8bbe\u914d\u7f6e\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#3_1","title":"3. \u5404\u4f4e\u529f\u8017\u6a21\u5f0f\u8be6\u89e3\u4e0e\u914d\u7f6e","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#a-sleep-mode","title":"A. Sleep Mode (\u7761\u7720\u6a21\u5f0f)","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_5","title":"\u539f\u7406","text":"<p>\u4ec5\u5173\u95ed Cortex-M \u5185\u6838\u65f6\u949f\uff0c\u6240\u6709\u5916\u8bbe\u548c SRAM \u4f9d\u7136\u5e26\u7535\u8fd0\u884c\uff0c\u65f6\u949f\u4fdd\u6301\u5f00\u542f\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_6","title":"\u5524\u9192\u6e90","text":"<p>\u4efb\u4f55\u542f\u7528\u7684\u4e2d\u65ad\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemxhal","title":"CubeMX/HAL \u914d\u7f6e","text":"<ol> <li> <p>\u914d\u7f6e\uff1a\u5728\u5916\u8bbe\uff08\u5982 UART\u3001TIM\u3001GPIO EXTI\uff09\u4e2d\u914d\u7f6e\u5e76\u542f\u7528\u4e2d\u65ad\u3002</p> </li> <li> <p>HAL API\uff1a</p> </li> </ol> <p><code>c    // \u8fdb\u5165 Sleep Mode (WFI: Wait For Interrupt)    HAL_PWR_EnterSLEEPMode(PWR_MAINREGULATOR_ON, PWR_SLEEPENTRY_WFI);     // PWR_MAINREGULATOR_ON: \u4f7f\u7528\u4e3b\u8c03\u538b\u5668\uff0c\u5feb\u901f\u5524\u9192\u3002    // PWR_SLEEPENTRY_WFI: \u7b49\u5f85\u4e2d\u65ad\u3002</code></p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#b-stop-mode","title":"B. Stop Mode (\u505c\u6b62\u6a21\u5f0f)","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_7","title":"\u539f\u7406","text":"<p>\u5728 Sleep \u6a21\u5f0f\u7684\u57fa\u7840\u4e0a\uff0c\u5173\u95ed\u4e86\u9ad8\u6027\u80fd\u65f6\u949f\uff08PLL\u3001HSE\u3001HSI\uff09\u3002\u5927\u90e8\u5206 SRAM \u548c\u6240\u6709\u5bc4\u5b58\u5668\u5185\u5bb9\u5f97\u4ee5\u4fdd\u7559\u3002\u529f\u8017\u4e3b\u8981\u7531 SRAM \u7684\u4fdd\u6301\u7535\u6d41\u51b3\u5b9a\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_8","title":"\u5524\u9192\u6e90","text":"<ul> <li>EXTI (\u5916\u90e8\u4e2d\u65ad)\uff1a\u4efb\u4f55\u8fde\u63a5\u5230 EXTI \u7ebf\u7684 GPIO \u4fe1\u53f7\u3002</li> <li>RTC \u62a5\u8b66/\u5524\u9192\u3002</li> <li>LSE (\u4f4e\u901f\u5916\u90e8\u65f6\u949f) \u4f9d\u7136\u53ef\u4ee5\u8fd0\u884c\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemxhal_1","title":"CubeMX/HAL \u914d\u7f6e","text":"<ol> <li> <p>\u914d\u7f6e\uff1a\u914d\u7f6e\u5524\u9192\u6e90\uff08\u4f8b\u5982\uff0c\u914d\u7f6e GPIO \u4e3a EXTI \u6a21\u5f0f\u5e76\u4f7f\u80fd NVIC\uff09\u3002</p> </li> <li> <p>HAL API\uff1a</p> </li> </ol> <p>```c    // 1. \u914d\u7f6e\u5524\u9192\u5f15\u811a\uff08\u5982\u679c\u4f7f\u7528 GPIO/EXTI \u5524\u9192\uff09    // 2. \u8fdb\u5165 Stop Mode    HAL_PWR_EnterSTOPMode(PWR_MAINREGULATOR_ON, PWR_STOPENTRY_WFI); </p> <p>// 3. \u5524\u9192\u540e\uff0c\u9700\u8981\u91cd\u65b0\u914d\u7f6e\u7cfb\u7edf\u65f6\u949f\uff08PLL/HSE/HSI \u5df2\u88ab\u5173\u95ed\uff09    SystemClock_Config();     ```</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#c-standby-mode","title":"C. Standby Mode (\u5f85\u673a\u6a21\u5f0f)","text":""},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_9","title":"\u539f\u7406","text":"<p>\u6700\u6df1\u7684\u4f4e\u529f\u8017\u6a21\u5f0f\u3002\u6240\u6709\u7535\u6e90\u57df\u90fd\u88ab\u5173\u95ed\uff0c\u53ea\u6709 <code>VBAT</code> \u4f9b\u7535\u7684 RTC \u57df\uff08\u5305\u62ec RTC \u548c BKP \u5bc4\u5b58\u5668\uff09\u4fdd\u6301\u8fd0\u884c\u3002\u6240\u6709 SRAM \u5185\u5bb9\u548c CPU/\u5916\u8bbe\u5bc4\u5b58\u5668\u5185\u5bb9\u5168\u90e8\u4e22\u5931\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#_10","title":"\u5524\u9192\u6e90","text":"<p>\u5524\u9192\u540e\u76f8\u5f53\u4e8e \u4e00\u6b21\u590d\u4f4d (Reset)\u3002</p> <ul> <li>WKUP Pin\uff1a\u4e13\u7528\u7684\u5524\u9192\u5f15\u811a\uff08\u5982 PA0\uff09\u3002</li> <li>RTC \u4e8b\u4ef6\uff1aRTC \u62a5\u8b66\u6216\u5524\u9192\u5b9a\u65f6\u5668\u4e8b\u4ef6\u3002</li> <li>NRST Pin\uff1a\u5916\u90e8\u590d\u4f4d\u3002</li> <li>IWDG\uff1a\u72ec\u7acb\u770b\u95e8\u72d7\u590d\u4f4d\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemxhal_2","title":"CubeMX/HAL \u914d\u7f6e","text":"<ol> <li> <p>CubeMX \u914d\u7f6e\u5524\u9192\u6e90\uff1a</p> </li> <li> <p>Pinout &amp; Configuration $\\rightarrow$ System Core $\\rightarrow$ PWR\u3002</p> </li> <li> <p>Low Power Settings\uff1a\u52fe\u9009 <code>WKUP Pin</code> \u6216\u914d\u7f6e RTC \u5524\u9192\u3002</p> </li> <li> <p>HAL API\uff1a</p> </li> </ol> <p>```c    // 1. \u4f7f\u80fd WKUP \u5f15\u811a\uff08\u5982\u679c\u4f7f\u7528\uff09    HAL_PWR_EnableWakeUpPin(PWR_WAKEUP_PIN1_HIGH); </p> <p>// 2. \u8fdb\u5165 Standby Mode    HAL_PWR_EnterSTANDBYMode(); </p> <p>// \u6ce8\u610f\uff1a\u7a0b\u5e8f\u6267\u884c\u5230\u8fd9\u91cc\u540e\uff0c\u82af\u7247\u4f1a\u7acb\u5373\u505c\u6b62\uff0c\u76f4\u5230\u5524\u9192\u5e76\u590d\u4f4d\u3002    ```</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#4-stm32cubemx","title":"4. STM32CubeMX \u4f4e\u529f\u8017\u914d\u7f6e\u6d41\u7a0b\u603b\u7ed3","text":"<p>\u914d\u7f6e\u4f4e\u529f\u8017\u6a21\u5f0f\u7684\u5173\u952e\u5728\u4e8e\u7ba1\u7406\u65f6\u949f\u548c\u5524\u9192\u6e90\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#step-1-rcc","title":"Step 1: \u65f6\u949f\u6e90\u9009\u62e9 (RCC)","text":"<ul> <li>\u5982\u679c\u9700\u8981\u8fdb\u5165 Stop \u6216 Standby \u6a21\u5f0f\uff0c\u5efa\u8bae\u4f7f\u7528 LSE (32.768 kHz) \u4f5c\u4e3a RTC \u65f6\u949f\u6e90\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u6df1\u5ea6\u7761\u7720\u4e2d\u4fdd\u6301\u8fd0\u884c\uff0c\u5e76\u4f5c\u4e3a\u53ef\u9760\u7684\u5524\u9192\u5b9a\u65f6\u5668\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#step-2-wakeup-sources","title":"Step 2: \u5916\u8bbe\u914d\u7f6e (Wakeup Sources)","text":"<ul> <li>EXTI/GPIO \u5524\u9192\uff1a\u914d\u7f6e\u4e00\u4e2a GPIO \u5f15\u811a\u4e3a EXTI Mode\uff0c\u5e76\u542f\u7528\u5176 NVIC \u4e2d\u65ad\u3002</li> <li>RTC \u5524\u9192\uff1a\u914d\u7f6e RTC \u6a21\u5757\uff0c\u5e76\u542f\u7528 Wakeup Timer\uff0c\u8bbe\u7f6e\u5468\u671f\u6027\u5524\u9192\u65f6\u95f4\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#step-3-pwr","title":"Step 3: \u4f4e\u529f\u8017\u6a21\u5f0f\u9009\u62e9 (PWR)","text":"<ul> <li>\u8def\u5f84\uff1aPinout &amp; Configuration $\\rightarrow$ System Core $\\rightarrow$ PWR\u3002</li> <li>\u6839\u636e\u76ee\u6807\u6a21\u5f0f\uff08Stop \u6216 Standby\uff09\u914d\u7f6e\u76f8\u5173\u9009\u9879\uff08\u4f8b\u5982\uff0c\u542f\u7528 WKUP \u5f15\u811a\uff09\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#step-4-hal-api","title":"Step 4: \u8f6f\u4ef6\u5b9e\u73b0 (HAL API)","text":"<p>\u5728\u4e3b\u7a0b\u5e8f\u903b\u8f91\u4e2d\uff0c\u5c06\u7cfb\u7edf\u914d\u7f6e\u597d\u540e\uff0c\u8c03\u7528\u5bf9\u5e94\u7684 <code>HAL_PWR_EnterXXXXMode()</code> API \u8fdb\u5165\u4f11\u7720\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#step-5","title":"Step 5: \u5524\u9192\u540e\u7684\u5904\u7406","text":"<ul> <li>Sleep/Stop Mode\uff1a\u5524\u9192\u540e\u4ee3\u7801\u4ece\u4f11\u7720 API \u7684\u4e0b\u4e00\u884c\u7ee7\u7eed\u6267\u884c\u3002\u5728 Stop Mode \u5524\u9192\u540e\uff0c\u5fc5\u987b\u91cd\u65b0\u8c03\u7528 <code>SystemClock_Config()</code> \u6062\u590d\u7cfb\u7edf\u4e3b\u65f6\u949f\u3002</li> <li>Standby Mode\uff1a\u5524\u9192\u540e\u6267\u884c \u51b7\u542f\u52a8 (Cold Start)\uff0c\u76f8\u5f53\u4e8e\u4e0a\u7535\u590d\u4f4d\uff0c\u4ee3\u7801\u4ece <code>main()</code> \u51fd\u6570\u7684\u5f00\u5934\u6267\u884c\u3002\u901a\u5e38\u9700\u8981\u68c0\u67e5 BKP \u5bc4\u5b58\u5668 \u6216 RTC \u6807\u5fd7\u4f4d \u6765\u5224\u65ad\u662f\u5426\u4e3a Standby \u5524\u9192\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#5-rtc-standby","title":"5. \u6df1\u5ea6\u5e94\u7528\uff1a\u5229\u7528 RTC \u5524\u9192 Standby","text":"<p>\u8fd9\u662f\u6700\u5e38\u89c1\u7684\u4f4e\u529f\u8017\u5e94\u7528\u573a\u666f\uff1a\u5728\u6781\u4f4e\u529f\u8017\u4e0b\u5468\u671f\u6027\u5524\u9192\uff0c\u6267\u884c\u4efb\u52a1\uff0c\u7136\u540e\u518d\u6b21\u4f11\u7720\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/BKP%E3%80%81RTC%26PWR/#cubemxhal_3","title":"CubeMX/HAL \u6b65\u9aa4","text":"<ol> <li>RTC \u914d\u7f6e\uff1a\u914d\u7f6e LSE\uff0c\u5e76\u8ba1\u7b97\u597d 1 Hz \u65f6\u57fa\u3002</li> <li>RTC Wakeup \u914d\u7f6e\uff1a</li> <li>\u542f\u7528 <code>RTC global interrupt</code> (NVIC)\u3002</li> <li>\u5728 RTC \u53c2\u6570\u4e2d\u8bbe\u7f6e Wakeup Clock (\u5982 <code>RTCCLK/16</code>)\u3002</li> <li>Standby \u6d41\u7a0b\u4ee3\u7801\uff1a</li> </ol> <pre><code>// Step 1: \u68c0\u67e5\u662f\u5426\u662f RTC \u5524\u9192\nif (__HAL_PWR_GET_FLAG(PWR_FLAG_SB) != RESET) {\n    // 1. \u6e05\u9664 Standby \u6807\u5fd7\n    __HAL_PWR_CLEAR_FLAG(PWR_FLAG_SB); \n    // 2. \u6e05\u9664 RTC \u5524\u9192\u6807\u5fd7\n    __HAL_RTC_WAKEUPTIMER_CLEAR_FLAG(&amp;hrtc, RTC_FLAG_WUTF); \n    // 3. \u6267\u884c\u5524\u9192\u4efb\u52a1\n    printf(\"Woke up from Standby via RTC!\\n\");\n} else {\n    // \u9996\u6b21\u4e0a\u7535\u6216\u5176\u4ed6\u590d\u4f4d\n    printf(\"System Cold Start.\\n\");\n}\n\n// Step 2: \u542f\u52a8 RTC \u5524\u9192\u5b9a\u65f6\u5668\uff08\u8bbe\u7f6e 10 \u79d2\u540e\u5524\u9192\uff09\nHAL_RTCEx_SetWakeUpTimer_IT(&amp;hrtc, 10, RTC_WAKEUPCLOCK_RTCCLK_DIV16);\n\n// Step 3: \u914d\u7f6e\u5e76\u8fdb\u5165 Standby Mode\nHAL_PWR_EnterSTANDBYMode(); \n\n// **\u6ce8\u610f\uff1a\u7a0b\u5e8f\u6267\u884c\u5230\u8fd9\u91cc\u5c31\u4f1a\u505c\u6b62\uff0c\u76f4\u5230 10 \u79d2\u540e\u590d\u4f4d\uff0c\u4ece main() \u91cd\u65b0\u5f00\u59cb\u6267\u884c**\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/","title":"\u5b58\u50a8\u5668\u6620\u50cf","text":"\u7c7b\u578b \u8d77\u59cb\u5730\u5740 \u5b58\u50a8\u5668 \u7528\u9014 ROM 0x0800 0000 \u7a0b\u5e8f\u5b58\u50a8\u5668Flash \u5b58\u50a8C\u8bed\u8a00\u7f16\u8bd1\u540e\u7684\u7a0b\u5e8f\u4ee3\u7801 0x1FFF F000 \u7cfb\u7edf\u5b58\u50a8\u5668 \u5b58\u50a8BootLoader\uff0c\u7528\u4e8e\u4e32\u53e3\u4e0b\u8f7d 0x1FFF F800 \u9009\u9879\u5b57\u8282 \u5b58\u50a8\u4e00\u4e9b\u72ec\u7acb\u4e8e\u7a0b\u5e8f\u4ee3\u7801\u7684\u914d\u7f6e\u53c2\u6570 RAM 0x2000 0000 \u8fd0\u884c\u5185\u5b58SRAM \u5b58\u50a8\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e34\u65f6\u53d8\u91cf 0x4000 0000 \u5916\u8bbe\u5bc4\u5b58\u5668 \u5b58\u50a8\u5404\u4e2a\u5916\u8bbe\u7684\u914d\u7f6e\u53c2\u6570 0xE000 0000 \u5185\u6838\u5916\u8bbe\u5bc4\u5b58\u5668 \u5b58\u50a8\u5185\u6838\u5404\u4e2a\u5916\u8bbe\u7684\u914d\u7f6e\u53c2\u6570"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#dma","title":"DMA \u57fa\u7840\u6982\u5ff5\u4e0e\u5de5\u4f5c\u539f\u7406","text":""},{"location":"EmbeddedSoft/STM32_HAL/DMA/#1-dma","title":"1. DMA \u662f\u4ec0\u4e48\uff1f","text":"<p>DMA\uff08Direct Memory Access\uff0c\u76f4\u63a5\u5185\u5b58\u5b58\u53d6\uff09\u662f\u4e00\u79cd\u786c\u4ef6\u673a\u5236\uff0c\u5141\u8bb8\u5916\u8bbe\uff08\u5982 UART\u3001SPI\u3001ADC\u3001TIM \u7b49\uff09\u4e0e\u5185\u5b58\uff08SRAM\u3001Flash\uff09\u4e4b\u95f4\u76f4\u63a5\u8fdb\u884c\u6570\u636e\u4f20\u8f93\uff0c\u800c\u65e0\u9700 CPU \u7684\u5e72\u9884\u3002</p> <ul> <li>\u6838\u5fc3\u76ee\u7684\uff1a\u5c06\u6570\u636e\u4f20\u8f93\u4efb\u52a1\u4ece CPU \u4e0a\u89e3\u653e\u51fa\u6765\uff0c\u8ba9 CPU \u53ef\u4ee5\u4e13\u6ce8\u4e8e\u6267\u884c\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\uff0c\u5927\u5e45\u63d0\u9ad8\u7cfb\u7edf\u7684\u6548\u7387\u548c\u5b9e\u65f6\u6027\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#2","title":"2. \u5de5\u4f5c\u6d41\u7a0b","text":"<ol> <li> <p>CPU \u914d\u7f6e\uff1aCPU \u8d1f\u8d23\u521d\u59cb\u5316 DMA \u63a7\u5236\u5668\uff0c\u8bbe\u7f6e\u6570\u636e\u6e90\u5730\u5740\u3001\u76ee\u6807\u5730\u5740\u3001\u6570\u636e\u4f20\u8f93\u91cf\u548c\u4f20\u8f93\u65b9\u5411\u3002</p> </li> <li> <p>DMA \u8bf7\u6c42\uff1a\u5f53\u5916\u8bbe\u51c6\u5907\u597d\u6570\u636e\uff08\u4f8b\u5982 UART \u63a5\u6536\u5230\u4e00\u4e2a\u5b57\u8282\uff09\u6216\u9700\u8981\u53d1\u9001\u6570\u636e\u65f6\uff0c\u5b83\u4f1a\u5411 DMA \u63a7\u5236\u5668\u53d1\u51fa\u8bf7\u6c42\uff08Trigger Event Selection\uff09\u3002</p> </li> <li> <p>\u603b\u7ebf\u4ef2\u88c1\uff1aDMA \u63a7\u5236\u5668\u5411 Cortex-M \u5185\u6838\u7533\u8bf7\u603b\u7ebf\u63a7\u5236\u6743\u3002</p> </li> <li> <p>\u6570\u636e\u4f20\u8f93\uff1aDMA \u63a7\u5236\u5668\u76f4\u63a5\u63a7\u5236\u603b\u7ebf\uff0c\u5b8c\u6210\u6570\u636e\u4f20\u8f93\u3002</p> </li> <li> <p>\u4f20\u8f93\u5b8c\u6210\uff1a\u5f53\u8bbe\u5b9a\u7684\u6570\u636e\u91cf\u4f20\u8f93\u5b8c\u6bd5\u540e\uff0cDMA \u63a7\u5236\u5668\u4f1a\u5411 NVIC \u53d1\u9001\u4e2d\u65ad\u4fe1\u53f7\uff08\u53ef\u9009\uff09\uff0c\u901a\u77e5 CPU \u4efb\u52a1\u5b8c\u6210\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#3","title":"3. \u6846\u56fe","text":""},{"location":"EmbeddedSoft/STM32_HAL/DMA/#stm32-dma-hal","title":"STM32 DMA \u6838\u5fc3\u7279\u6027\u4e0e HAL \u5e93\u62bd\u8c61","text":""},{"location":"EmbeddedSoft/STM32_HAL/DMA/#0","title":"0. \u57fa\u672c\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/DMA/#1-dma-channel-stream","title":"1. DMA \u901a\u9053\u548c\u6d41 (Channel &amp; Stream)","text":"<ul> <li> <p>STM32 \u82af\u7247\u901a\u5e38\u6709\u591a\u4e2a DMA \u63a7\u5236\u5668\uff08DMA1 \u548c DMA2\uff09\u3002</p> </li> <li> <p>\u6bcf\u4e2a\u63a7\u5236\u5668\u5305\u542b\u591a\u4e2a\u6d41 (Stream)\uff08\u901a\u5e38 4 \u5230 8 \u4e2a\uff09\uff0c\u6bcf\u4e2a\u6d41\u53ef\u4ee5\u72ec\u7acb\u914d\u7f6e\u3002</p> </li> <li> <p>\u6bcf\u4e2a\u6d41\u53ef\u4ee5\u901a\u8fc7\u901a\u9053 (Channel) \u6620\u5c04\u5230\u7279\u5b9a\u7684\u5916\u8bbe\u8bf7\u6c42\u3002</p> </li> </ul> <p></p>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#2_1","title":"2. \u4f20\u8f93\u65b9\u5411\u4e0e\u6a21\u5f0f","text":"\u7279\u6027 \u6a21\u5f0f (CubeMX/HAL) \u63cf\u8ff0 \u5178\u578b\u5e94\u7528 \u4f20\u8f93\u65b9\u5411 <code>Memory to Peripheral</code> \u4ece\u5185\u5b58\u5230\u5916\u8bbe\u3002 UART \u53d1\u9001\u3001SPI \u53d1\u9001\u3002 <code>Peripheral to Memory</code> \u4ece\u5916\u8bbe\u5230\u5185\u5b58\u3002 UART \u63a5\u6536\u3001ADC \u91c7\u6837\u3002 <code>Memory to Memory</code> \u5185\u5b58\u5757\u590d\u5236\uff08\u8f6f\u4ef6\u89e6\u53d1\uff09\u3002 \u6570\u636e\u5757\u5feb\u901f\u521d\u59cb\u5316/\u590d\u5236\u3002 \u589e\u91cf\u6a21\u5f0f <code>Memory Increment</code> \u4f20\u8f93\u6570\u636e\u540e\uff0c\u5185\u5b58\u5730\u5740\u9012\u589e\u3002 \u4f20\u8f93\u6570\u7ec4\u3002 <code>Peripheral Increment</code> \u4f20\u8f93\u6570\u636e\u540e\uff0c\u5916\u8bbe\u5730\u5740\u9012\u589e\uff08\u901a\u5e38\u7981\u7528\uff09\u3002 \u7528\u4e8e F4/F7 \u7684 FIFO \u6a21\u5f0f\u3002 \u5faa\u73af\u6a21\u5f0f <code>Circular</code> \u4f20\u8f93\u5b8c\u6210\u540e\uff0cDMA \u8ba1\u6570\u5668\u81ea\u52a8\u91cd\u88c5\u8f7d\uff0c\u6301\u7eed\u8fdb\u884c\u4f20\u8f93\u3002 \u8fde\u7eed ADC \u91c7\u6837\u3001\u6301\u7eed UART \u63a5\u6536\u3002 <code>Normal</code> \u4f20\u8f93\u5b8c\u6210\u540e\u505c\u6b62\u3002 \u4e00\u6b21\u6027\u6570\u636e\u5757\u4f20\u8f93\u3002"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#3-data-width","title":"3. \u6570\u636e\u5bbd\u5ea6 (Data Width)","text":"<p>HAL \u5e93\u5141\u8bb8\u914d\u7f6e\u6570\u636e\u4f20\u8f93\u7684\u5bbd\u5ea6\uff0c\u901a\u5e38\u4e0e\u5916\u8bbe\u7684\u6570\u636e\u4f4d\u5bbd\u4fdd\u6301\u4e00\u81f4\uff1a</p> <ul> <li> <p>Byte (8-bit): \u9002\u7528\u4e8e UART\u3001SPI\u3002</p> </li> <li> <p>Half Word (16-bit): \u9002\u7528\u4e8e 12 \u4f4d ADC \u7ed3\u679c\u3002</p> </li> <li> <p>Word (32-bit): \u9002\u7528\u4e8e\u5185\u5b58\u5230\u5185\u5b58\u4f20\u8f93\u3001ADC \u53cc\u6a21\u5f0f\u7ed3\u679c\u3002</p> </li> </ul> <p></p>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#stm32cubemx-dma","title":"STM32CubeMX DMA \u914d\u7f6e\u6d41\u7a0b","text":""},{"location":"EmbeddedSoft/STM32_HAL/DMA/#1uart-peripheral-to-memory-normal-mode","title":"\u914d\u7f6e\u793a\u4f8b1\uff1aUART \u63a5\u6536\uff08Peripheral to Memory, Normal Mode\uff09","text":"<p>\u8fd9\u662f\u6700\u57fa\u7840\u7684 DMA \u5e94\u7528\uff0c\u7528\u4e8e\u5728\u63a5\u6536\u56fa\u5b9a\u957f\u5ea6\u6570\u636e\u65f6\u89e3\u653e CPU\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#cubemx","title":"CubeMX \u6b65\u9aa4","text":"<ol> <li> <p>\u542f\u7528\u5916\u8bbe\uff1a\u5728 Pinout &amp; Configuration \u4e2d\u914d\u7f6e USART1\uff08\u5f02\u6b65\u6a21\u5f0f\uff0c\u8bbe\u7f6e\u6ce2\u7279\u7387\u7b49\uff09\u3002</p> </li> <li> <p>DMA \u8bf7\u6c42\u6dfb\u52a0\uff1a</p> <ul> <li> <p>\u5728 USART1 \u7684\u914d\u7f6e\u9875\u9762\u4e2d\uff0c\u5207\u6362\u5230 DMA Settings \u9009\u9879\u5361\u3002</p> </li> <li> <p>\u70b9\u51fb Add $\\rightarrow$ \u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9 <code>USART1_RX</code>\u3002</p> </li> <li> <p>Direction\uff1a<code>Peripheral to Memory</code>\u3002</p> </li> </ul> </li> <li> <p>DMA \u914d\u7f6e\u7ec6\u8282\uff1a</p> <ul> <li> <p>Mode\uff1a<code>Normal</code>\u3002</p> </li> <li> <p>Priority\uff1a<code>Low</code> (\u9ed8\u8ba4)\u3002</p> </li> <li> <p>Data Width (Memory)\uff1a<code>Byte</code> (8 bit)\u3002</p> </li> <li> <p>Data Width (Peripheral)\uff1a<code>Byte</code> (8 bit)\u3002</p> </li> <li> <p>Increment Address (Memory)\uff1a<code>Enable</code> (\u63a5\u6536\u6570\u636e\u8981\u5199\u5165\u6570\u7ec4\u7684\u4e0d\u540c\u5730\u5740)\u3002</p> </li> <li> <p>Increment Address (Peripheral)\uff1a<code>Disable</code> (USART \u7684\u6570\u636e\u5bc4\u5b58\u5668\u5730\u5740\u56fa\u5b9a)\u3002</p> </li> </ul> </li> <li> <p>NVIC\uff1a\u901a\u5e38\u4e0d\u542f\u7528 DMA \u4e2d\u65ad\uff0c\u800c\u662f\u4f7f\u7528\u5916\u8bbe\uff08\u5982 UART \u7a7a\u95f2\u4e2d\u65ad\uff09\u6216 HAL \u5e93\u63d0\u4f9b\u7684\u56de\u8c03\u673a\u5236\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#hal","title":"HAL \u5e93\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>#define RX_BUFFER_SIZE 100\nuint8_t rx_data_buffer[RX_BUFFER_SIZE];\n\n// \u5728 main() \u51fd\u6570\u4e2d\u542f\u52a8 DMA \u63a5\u6536\n// \u542f\u52a8 DMA \u63a5\u6536\uff1a\u63a5\u6536\u6765\u81ea &amp;huart1 \u7684\u6570\u636e\uff0c\u5b58\u5165 rx_data_buffer\uff0c\u957f\u5ea6\u4e3a RX_BUFFER_SIZE\nHAL_UART_Receive_DMA(&amp;huart1, rx_data_buffer, RX_BUFFER_SIZE); \n\n// **\u63a5\u6536\u5b8c\u6210\u56de\u8c03\u51fd\u6570\uff08HAL \u5e93\u62bd\u8c61\uff09**\n// \u5f53 100 \u5b57\u8282\u6570\u636e\u5168\u90e8\u63a5\u6536\u5b8c\u6bd5\u540e\uff0c\u4f1a\u89e6\u53d1\u6b64\u56de\u8c03\nvoid HAL_UART_RxCpltCallback(UART_HandleTypeDef *huart)\n{\n    if (huart-&gt;Instance == USART1) {\n        // 100 \u5b57\u8282\u6570\u636e\u5df2\u5b8c\u6574\u63a5\u6536\uff0c\u5728\u6b64\u5904\u7406\u6570\u636e\n        // \u6ce8\u610f\uff1aDMA \u5df2\u505c\u6b62\uff0c\u9700\u8981\u518d\u6b21\u542f\u52a8\u4ee5\u63a5\u6536\u4e0b\u4e00\u6279\u6570\u636e\n        HAL_UART_Receive_DMA(&amp;huart1, rx_data_buffer, RX_BUFFER_SIZE); \n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#2adc-peripheral-to-memory-circular-mode","title":"\u914d\u7f6e\u793a\u4f8b2\uff1aADC \u8fde\u7eed\u91c7\u6837 (Peripheral to Memory, Circular Mode)","text":"<p>\u7528\u4e8e\u8fde\u7eed\u3001\u4e0d\u95f4\u65ad\u5730\u91c7\u96c6\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u5c06\u7ed3\u679c\u5b58\u5165\u5185\u5b58\u7f13\u51b2\u533a\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#cubemx_1","title":"CubeMX \u6b65\u9aa4","text":"<ol> <li> <p>ADC \u914d\u7f6e\uff1a\u914d\u7f6e ADC\uff08\u901a\u9053\u3001\u91c7\u6837\u65f6\u95f4\u7b49\uff09\u3002</p> </li> <li> <p>DMA \u8bf7\u6c42\u6dfb\u52a0\uff1a</p> <ul> <li> <p>\u5728 ADC1 \u7684\u914d\u7f6e\u9875\u9762\u4e2d\uff0c\u5207\u6362\u5230 DMA Settings \u9009\u9879\u5361\u3002</p> </li> <li> <p>\u70b9\u51fb Add $\\rightarrow$ \u9009\u62e9 <code>ADC1</code>\u3002</p> </li> </ul> </li> <li> <p>DMA \u914d\u7f6e\u7ec6\u8282\uff1a</p> <ul> <li> <p>Mode\uff1a<code>Circular</code> (\u5faa\u73af\u6a21\u5f0f)\u3002</p> </li> <li> <p>Data Width (Memory)\uff1a<code>Half Word</code> (16 bit\uff0c\u56e0\u4e3a ADC \u662f 12 \u4f4d\u8f93\u51fa)\u3002</p> </li> <li> <p>Data Width (Peripheral)\uff1a<code>Half Word</code> (16 bit)\u3002</p> </li> <li> <p>Increment Address (Memory)\uff1a<code>Enable</code>\u3002</p> </li> <li> <p>Increment Address (Peripheral)\uff1a<code>Disable</code>\u3002</p> </li> </ul> </li> <li> <p>NVIC\uff1a\u53ef\u4ee5\u542f\u7528 DMA \u4e2d\u65ad\uff08\u5982 <code>DMA1 stream 0 global interrupt</code>\uff09\uff0c\u7528\u4e8e\u5728\u534a\u4f20\u8f93\u548c\u5168\u4f20\u8f93\u5b8c\u6210\u65f6\u89e6\u53d1\u56de\u8c03\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#hal_1","title":"HAL \u5e93\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>#define ADC_BUF_SIZE 50\nuint16_t adc_values[ADC_BUF_SIZE];\n\n// \u542f\u52a8 DMA \u8fde\u7eed\u91c7\u6837\n// \u5c06 ADC \u7ed3\u679c\u8fde\u7eed\u5b58\u5165 adc_values \u6570\u7ec4\uff0c\u957f\u5ea6\u4e3a ADC_BUF_SIZE\nHAL_ADC_Start_DMA(&amp;hadc1, (uint32_t*)adc_values, ADC_BUF_SIZE); \n\n// **\u5168\u4f20\u8f93\u5b8c\u6210\u56de\u8c03\uff08Full Transfer Complete Callback\uff09**\n// \u5f53\u6570\u7ec4\u586b\u6ee1\u65f6\uff0850\u4e2a\u6570\u636e\uff09\nvoid HAL_ADC_ConvCpltCallback(ADC_HandleTypeDef *hadc)\n{\n    if (hadc-&gt;Instance == ADC1) {\n        // \u5728\u5faa\u73af\u6a21\u5f0f\u4e0b\uff0cDMA \u81ea\u52a8\u91cd\u88c5\u8f7d\u5e76\u7ee7\u7eed\uff0c\u4ece\u5934\u8986\u76d6\u5199\u5165\uff0c\u65e0\u9700\u518d\u6b21\u542f\u52a8\n        // \u5904\u7406 adc_values \u6570\u7ec4\u4e2d\u540e\u534a\u90e8\u5206\u6570\u636e\uff08\u5982\u679c\u4f7f\u7528\u4e86\u53cc\u7f13\u51b2\u6216\u4ec5\u5904\u7406\u6240\u6709\u6570\u636e\uff09\n    }\n}\n\n// **\u534a\u4f20\u8f93\u5b8c\u6210\u56de\u8c03\uff08Half Transfer Complete Callback\uff09**\n// \u5f53\u6570\u7ec4\u586b\u5145\u5230\u4e00\u534a\u65f6\uff0825\u4e2a\u6570\u636e\uff09\nvoid HAL_ADC_ConvHalfCpltCallback(ADC_HandleTypeDef *hadc)\n{\n    if (hadc-&gt;Instance == ADC1) {\n        // DMA \u4ecd\u5728\u4f20\u8f93\uff0c\u5904\u7406 adc_values \u6570\u7ec4\u4e2d\u7684\u524d\u534a\u90e8\u5206\u6570\u636e\uff08\u5b9e\u73b0\u53cc\u7f13\u51b2\uff09\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#3-memory-to-memory","title":"\u914d\u7f6e\u793a\u4f8b3\uff1a\u5185\u5b58\u5230\u5185\u5b58 (Memory to Memory)","text":"<p>\u7528\u4e8e\u5728\u4e0d\u9700\u8981\u5916\u8bbe\u8bf7\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5feb\u901f\u5728\u5185\u5b58\u533a\u57df\u4e4b\u95f4\u590d\u5236\u6570\u636e\u3002\u5373\u8f6f\u4ef6\u89e6\u53d1\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#cubemx_2","title":"CubeMX \u6b65\u9aa4","text":"<ol> <li> <p>DMA \u914d\u7f6e\uff1a\u5728\u5de6\u4fa7\u914d\u7f6e\u680f\u7684 DMA1/DMA2 \u9009\u9879\u4e0b\uff0c\u70b9\u51fb Add\u3002</p> </li> <li> <p>\u8bf7\u6c42\u6e90\uff1a\u9009\u62e9 <code>MemToMem</code>\u3002</p> </li> <li> <p>\u914d\u7f6e\u7ec6\u8282\uff1a</p> <ul> <li> <p>Direction\uff1a<code>Memory to Memory</code>\u3002</p> </li> <li> <p>Increment Address (Memory)\uff1a<code>Enable</code> (\u6e90\u548c\u76ee\u6807\u90fd\u9700\u8981\u9012\u589e)\u3002</p> </li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#hal_2","title":"HAL \u5e93\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>uint8_t source_data[20] = {1, 2, ..., 20};\nuint8_t dest_data[20];\n\n// \u5728 main() \u51fd\u6570\u4e2d\u542f\u52a8 MemToMem \u4f20\u8f93\nHAL_DMA_Start_IT(hdma_mem2mem, \n                 (uint32_t)source_data, \n                 (uint32_t)dest_data, \n                 20); // \u4f20\u8f93 20 \u4e2a\u5b57\u8282\n\n// \u4f20\u8f93\u5b8c\u6210\u56de\u8c03\nvoid HAL_DMA_TxCpltCallback(DMA_HandleTypeDef *hdma)\n{\n    if (hdma-&gt;Instance == DMA1_Stream1) {\n        // \u6570\u636e\u590d\u5236\u5b8c\u6210\n    }\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#_2","title":"\u4e0d\u7528\u5173\u5fc3","text":"<p>DMA \u548c HAL \u5e93\u4e3a\u4f60\u62bd\u8c61\u4e86\u5927\u91cf\u7684\u5e95\u5c42\u7ec6\u8282\uff0c\u56e0\u6b64\u4f60\u4e0d\u7528\u5173\u5fc3\uff1a</p> <ol> <li> <p>DMA \u5bc4\u5b58\u5668\u5730\u5740\uff1a\u4f60\u4e0d\u7528\u76f4\u63a5\u64cd\u4f5c <code>DMA_SxCR</code>\uff08\u63a7\u5236\u5bc4\u5b58\u5668\uff09\u3001<code>DMA_SxNDTR</code>\uff08\u6570\u636e\u957f\u5ea6\u5bc4\u5b58\u5668\uff09\u7b49\u6765\u914d\u7f6e\u6d41\u3002HAL \u5e93\u901a\u8fc7\u914d\u7f6e\u53e5\u67c4\uff08<code>hdma_...</code>\uff09\u81ea\u52a8\u5904\u7406\u3002</p> </li> <li> <p>\u5916\u8bbe\u89e6\u53d1\u4fe1\u53f7\uff1a\u4f60\u4e0d\u7528\u5173\u5fc3\u5916\u8bbe\uff08\u5982 UART\uff09\u5728\u786c\u4ef6\u4e0a\u5982\u4f55\u5411 DMA \u63a7\u5236\u5668\u53d1\u9001\u8bf7\u6c42\uff08Request ID\uff09\u3002CubeMX \u81ea\u52a8\u5b8c\u6210\u4e86 Trigger Event Selection \u5230 DMA \u901a\u9053/\u6d41\u7684\u6620\u5c04\u3002</p> </li> <li> <p>\u603b\u7ebf\u4ef2\u88c1\uff1a\u4f60\u4e0d\u9700\u8981\u7f16\u5199\u4ee3\u7801\u6765\u5904\u7406 DMA \u548c CPU \u4e4b\u95f4\u7684\u603b\u7ebf\u7ade\u4e89\u3002\u8fd9\u662f\u7531 DMA \u786c\u4ef6\u903b\u8f91\u81ea\u52a8\u7ba1\u7406\u7684\u3002</p> </li> <li> <p>\u4e2d\u65ad\u6807\u5fd7\u4f4d\u6e05\u9664\uff1aHAL \u5e93\u7684\u901a\u7528 DMA \u4e2d\u65ad\u5904\u7406\u51fd\u6570\uff08<code>HAL_DMA_IRQHandler</code>\uff09\u4f1a\u81ea\u52a8\u6e05\u9664 DMA \u76f8\u5173\u7684\u4e2d\u65ad\u6807\u5fd7\u4f4d\uff08\u5982 TCIF, HTIF\uff09\uff0c\u7136\u540e\u624d\u8c03\u7528\u4f60\u7684\u56de\u8c03\u51fd\u6570\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/DMA/#dma_1","title":"\u5ef6\u4f38\uff1aDMA \u4e2d\u65ad\u4e0e\u9519\u8bef\u5904\u7406","text":"<p>\u867d\u7136\u5728 <code>Normal</code> \u6a21\u5f0f\u4e0b\u53ef\u4ee5\u4e0d\u4f7f\u7528 DMA \u4e2d\u65ad\uff0c\u4f46\u5728 <code>Circular</code> \u6a21\u5f0f\u6216\u9700\u8981\u9ad8\u53ef\u9760\u6027\u65f6\uff0cDMA \u4e2d\u65ad\u975e\u5e38\u91cd\u8981\u3002</p> \u4e2d\u65ad\u7c7b\u578b \u63cf\u8ff0 HAL \u56de\u8c03\u51fd\u6570 \u5e94\u7528\u573a\u666f \u5168\u4f20\u8f93\u5b8c\u6210 (TC) \u4f20\u8f93\u6240\u6709\u6570\u636e\u5b8c\u6210\u3002 <code>HAL_DMA_TxCpltCallback</code> \u6570\u636e\u5904\u7406\u3001\u4efb\u52a1\u5207\u6362\u3002 \u534a\u4f20\u8f93\u5b8c\u6210 (HT) \u4f20\u8f93\u4e00\u534a\u6570\u636e\u5b8c\u6210\u3002 <code>HAL_DMA_TxHalfCpltCallback</code> \u53cc\u7f13\u51b2/Ping-Pong \u7f13\u51b2\u3002 \u4f20\u8f93\u9519\u8bef (TE) \u4f20\u8f93\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u9519\u8bef\uff08\u5982\u603b\u7ebf\u9519\u8bef\uff09\u3002 <code>HAL_DMA_AbortCallback</code> \u7cfb\u7edf\u544a\u8b66\u3001\u9519\u8bef\u6062\u590d\u3002 <p>\u5728 CubeMX \u4e2d\uff0c\u4f60\u53ea\u9700\u542f\u7528 DMA \u5bf9\u5e94\u7684 Stream \u7684 NVIC\uff0cHAL \u5e93\u4f1a\u81ea\u52a8\u5c06\u8fd9\u4e9b\u786c\u4ef6\u4e2d\u65ad\u6620\u5c04\u5230\u4e0a\u8ff0\u56de\u8c03\u51fd\u6570\u4e2d\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/","title":"GPIO","text":"<ul> <li>GPIO \u57fa\u672c\u6982\u5ff5\u4e0e CubeMX \u914d\u7f6e\u6d41\u7a0b</li> <li>GPIO\u7ed3\u6784<ul> <li>\u7247\u4e0a\u7ed3\u6784</li> <li>\u4f4d\u7ed3\u6784</li> <li>CubeMX \u914d\u7f6e\u6d41\u7a0b</li> <li>GPIO \u5de5\u4f5c\u6a21\u5f0f</li> </ul> </li> <li>HAL \u5e93 GPIO \u6838\u5fc3 API<ul> <li>\u521d\u59cb\u5316</li> <li>\u8f93\u51fa\u64cd\u4f5c\uff08\u5199\u64cd\u4f5c\uff09</li> <li>\u8f93\u5165\u64cd\u4f5c\uff08\u8bfb\u64cd\u4f5c\uff09</li> </ul> </li> <li>\u5916\u90e8\u4e2d\u65ad\uff08EXTI\uff09\u4e0e\u56de\u8c03\u51fd\u6570<ul> <li>EXTI\u57fa\u672c\u7ed3\u6784</li> <li>AFIO\uff08\u590d\u7528IO\u53e3\uff09\u8bf4\u660e</li> <li>EXTI\u6846\u56fe</li> <li>EXTI \u914d\u7f6e\u8981\u70b9</li> <li>\u4e2d\u65ad\u56de\u8c03\u51fd\u6570<ul> <li>\u8bf4\u660e<ul> <li>HAL \u5e93\u7684\u62bd\u8c61\u673a\u5236</li> <li>\u5982\u4f55\u533a\u5206\u4e0d\u540c\u7684\u4e2d\u65ad\u6e90\uff1f</li> </ul> </li> </ul> </li> <li>\u6ce8\u610f\u4e8b\u9879</li> </ul> </li> <li>\u4f7f\u7528\u4e86 CubeMX \u540e\u4e0d\u7528\u5173\u5fc3\u4ec0\u4e48\uff1f</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#gpio-cubemx","title":"GPIO \u57fa\u672c\u6982\u5ff5\u4e0e CubeMX \u914d\u7f6e\u6d41\u7a0b","text":"<p>GPIO (General Purpose Input/Output) \u662f\u5fae\u63a7\u5236\u5668\u4e0e\u5916\u90e8\u786c\u4ef6\u8fdb\u884c\u4ea4\u4e92\u7684\u57fa\u7840\u63a5\u53e3\u3002\u5728 STM32 \u4e2d\uff0c\u6bcf\u4e2a GPIO \u5f15\u811a\u90fd\u53ef\u4ee5\u72ec\u7acb\u914d\u7f6e\u4e3a\u4e0d\u540c\u7684\u6a21\u5f0f\u548c\u529f\u80fd\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#gpio","title":"GPIO\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_1","title":"\u7247\u4e0a\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_2","title":"\u4f4d\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#cubemx","title":"CubeMX \u914d\u7f6e\u6d41\u7a0b","text":"<p>STM32CubeMX \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u56fe\u5f62\u5316\u914d\u7f6e\u5de5\u5177\uff0c\u7528\u4e8e\u751f\u6210\u521d\u59cb\u5316\u4ee3\u7801\uff0c\u6781\u5927\u5730\u7b80\u5316\u4e86 GPIO \u7684\u914d\u7f6e\u5de5\u4f5c\u3002</p> <ol> <li>\u5f15\u811a\u6a21\u5f0f\u9009\u62e9\uff1a\u5728\u82af\u7247\u5f15\u811a\u56fe\u4e0a\uff0c\u70b9\u51fb\u6216\u53f3\u952e\u70b9\u51fb\u8981\u914d\u7f6e\u7684\u5f15\u811a\u3002</li> <li>\u6a21\u5f0f\u914d\u7f6e\uff1a\u9009\u62e9\u6240\u9700\u7684 GPIO \u6a21\u5f0f\uff0c\u4f8b\u5982\uff1a</li> <li>GPIO_Output\uff1a\u901a\u7528\u63a8\u633d\u8f93\u51fa\u6216\u5f00\u6f0f\u8f93\u51fa\u3002</li> <li>GPIO_Input\uff1a\u6d6e\u7a7a\u8f93\u5165\u3001\u4e0a\u62c9\u8f93\u5165\u6216\u4e0b\u62c9\u8f93\u5165\u3002</li> <li>Alternate Function (AF)\uff1a\u7528\u4e8e\u8fde\u63a5\u7247\u4e0a\u5916\u8bbe\uff08\u5982 USART\u3001SPI\u3001I2C \u7b49\uff09\u3002</li> <li>Analog\uff1a\u7528\u4e8e ADC/DAC \u7b49\u6a21\u62df\u529f\u80fd\u3002</li> <li>\u53c2\u6570\u8bbe\u7f6e (Parameters Settings)\uff1a\u5728\u5de6\u4fa7\u7684 Pinout &amp; Configuration \u680f\u76ee\u4e2d\uff0c\u9009\u62e9 GPIO \u9009\u9879\u5361\u3002\u5bf9\u6bcf\u4e2a\u542f\u7528\u7684 GPIO \u7aef\u53e3\uff08\u5982 PA, PB, PC...\uff09\u8fdb\u884c\u8be6\u7ec6\u914d\u7f6e\uff1a</li> <li>GPIO Mode\uff1a\u786e\u8ba4\u5f15\u811a\u7684\u8f93\u5165/\u8f93\u51fa/\u590d\u7528\u529f\u80fd\u3002</li> <li>Pull-up/Pull-down\uff1a\u8bbe\u7f6e\u4e0a\u62c9/\u4e0b\u62c9\u7535\u963b\uff08\u9002\u7528\u4e8e\u8f93\u5165\u6a21\u5f0f\uff09\u3002</li> <li>Maximum Output Speed\uff1a\u8bbe\u7f6e\u8f93\u51fa\u901f\u5ea6\uff08Low, Medium, High, Very High\uff09\u3002</li> <li>User Label\uff1a\u5f3a\u70c8\u5efa\u8bae\u4e3a\u6bcf\u4e2a\u5f15\u811a\u8bbe\u7f6e\u4e00\u4e2a\u6e05\u6670\u7684\u522b\u540d\uff08\u4f8b\u5982 <code>LED_R_Pin</code>, <code>KEY_WKUP_Pin</code>\uff09\uff0c\u8fd9\u5c06\u76f4\u63a5\u53cd\u6620\u5728\u751f\u6210\u7684 HAL \u5e93\u4ee3\u7801\u4e2d\uff0c\u63d0\u9ad8\u4ee3\u7801\u53ef\u8bfb\u6027\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#gpio_1","title":"GPIO \u5de5\u4f5c\u6a21\u5f0f","text":"<p>GPIO \u6709\u56db\u5927\u5de5\u4f5c\u6a21\u5f0f\uff0cCubeMX \u4e2d\u914d\u7f6e\u7684\u662f\u8fd9\u4e9b\u6a21\u5f0f\u7684\u5b50\u96c6\uff1a</p> <ol> <li>\u8f93\u5165\u6a21\u5f0f (Input Mode)\uff1a\u7528\u4e8e\u4ece\u5916\u90e8\u8bfb\u53d6\u7535\u5e73\u3002\u53ef\u914d\u7f6e\u4e3a\u6d6e\u7a7a\u3001\u4e0a\u62c9\u6216\u4e0b\u62c9\u3002</li> <li>\u8f93\u51fa\u6a21\u5f0f (Output Mode)\uff1a\u7528\u4e8e\u5411\u5916\u90e8\u8f93\u51fa\u7535\u5e73\u3002\u53ef\u914d\u7f6e\u4e3a\u63a8\u633d\uff08\u80fd\u8f93\u51fa\u9ad8\u4f4e\u7535\u5e73\uff09\u6216\u5f00\u6f0f\uff08\u53ea\u80fd\u8f93\u51fa\u4f4e\u7535\u5e73\u6216\u9ad8\u963b\u6001\uff09\u3002</li> <li>\u590d\u7528\u529f\u80fd\u6a21\u5f0f (Alternate Function Mode)\uff1a\u7528\u4e8e\u5c06\u5f15\u811a\u8fde\u63a5\u5230\u82af\u7247\u5185\u90e8\u5916\u8bbe\uff08\u5982\u5b9a\u65f6\u5668\u3001\u4e32\u53e3\u7b49\uff09\u3002</li> <li>\u6a21\u62df\u6a21\u5f0f (Analog Mode)\uff1a\u7528\u4e8e ADC/DAC \u7b49\u6a21\u62df\u4fe1\u53f7\u5904\u7406\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#hal-gpio-api","title":"HAL \u5e93 GPIO \u6838\u5fc3 API","text":"<p>HAL \u5e93\u63d0\u4f9b\u4e86\u4e00\u5957\u7b80\u6d01\u3001\u7edf\u4e00\u7684 API \u6765\u64cd\u4f5c GPIO\uff0c\u5176\u51fd\u6570\u547d\u540d\u901a\u5e38\u9075\u5faa <code>HAL_GPIO_...</code> \u683c\u5f0f\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_3","title":"\u521d\u59cb\u5316","text":"<ul> <li><code>HAL_GPIO_Init(GPIO_TypeDef \\*GPIOx, GPIO_InitTypeDef \\*GPIO_Init)</code></li> <li>\u529f\u80fd\uff1a\u521d\u59cb\u5316\u6307\u5b9a\u7684 GPIO \u7aef\u53e3\u548c\u5f15\u811a\u3002</li> <li>\u4f7f\u7528\u65b9\u5f0f\uff1a\u8fd9\u4e2a\u51fd\u6570\u7531 CubeMX \u6839\u636e\u4f60\u7684\u914d\u7f6e\u81ea\u52a8\u751f\u6210\uff0c\u5e76\u5728 <code>main.c</code> \u4e2d\u7684 <code>MX_GPIO_Init()</code> \u51fd\u6570\u91cc\u88ab\u8c03\u7528\u3002\u4f60\u65e0\u9700\u624b\u52a8\u8c03\u7528\u6216\u4fee\u6539\u5b83\uff0c\u9664\u975e\u9700\u8981\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_4","title":"\u8f93\u51fa\u64cd\u4f5c\uff08\u5199\u64cd\u4f5c\uff09","text":"<ul> <li> <p><code>HAL_GPIO_WritePin(GPIO_TypeDef \\*GPIOx, uint16_t GPIO_Pin, GPIO_PinState PinState)</code></p> </li> <li> <p>\u529f\u80fd\uff1a\u8bbe\u7f6e\u6307\u5b9a\u5f15\u811a\u7684\u8f93\u51fa\u7535\u5e73\u3002</p> </li> <li> <p>\u53c2\u6570\uff1a</p> <ul> <li><code>GPIOx</code>\uff1aGPIO \u7aef\u53e3\uff08\u5982 <code>GPIOA</code>, <code>GPIOB</code> \u7b49\uff09\u3002</li> <li><code>GPIO_Pin</code>\uff1a\u5f15\u811a\u53f7\uff08\u4f7f\u7528 CubeMX \u751f\u6210\u7684\u522b\u540d\uff0c\u5982 <code>LED_R_Pin</code>\uff09\u3002</li> <li><code>PinState</code>\uff1a\u7535\u5e73\u72b6\u6001\uff08<code>GPIO_PIN_SET</code> \u4e3a\u9ad8\u7535\u5e73\uff0c<code>GPIO_PIN_RESET</code> \u4e3a\u4f4e\u7535\u5e73\uff09\u3002</li> </ul> </li> <li> <p>\u793a\u4f8b\uff08\u70b9\u4eae LED\uff09\uff1a</p> <p><code>c HAL_GPIO_WritePin(GPIOB, LED_R_Pin, GPIO_PIN_RESET); // \u5047\u8bbe\u4f4e\u7535\u5e73\u70b9\u4eae</code></p> </li> <li> <p><code>HAL_GPIO_TogglePin(GPIO_TypeDef \\*GPIOx, uint16_t GPIO_Pin)</code></p> </li> <li> <p>\u529f\u80fd\uff1a\u7ffb\u8f6c\u6307\u5b9a\u5f15\u811a\u7684\u8f93\u51fa\u7535\u5e73\u3002</p> </li> <li> <p>\u793a\u4f8b\uff08LED \u95ea\u70c1\uff09\uff1a</p> <p><code>c HAL_GPIO_TogglePin(GPIOB, LED_R_Pin);</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_5","title":"\u8f93\u5165\u64cd\u4f5c\uff08\u8bfb\u64cd\u4f5c\uff09","text":"<ul> <li> <p><code>GPIO_PinState HAL_GPIO_ReadPin(GPIO_TypeDef \\*GPIOx, uint16_t GPIO_Pin)</code></p> </li> <li> <p>\u529f\u80fd\uff1a\u8bfb\u53d6\u6307\u5b9a\u5f15\u811a\u7684\u8f93\u5165\u7535\u5e73\u72b6\u6001\u3002</p> </li> <li> <p>\u8fd4\u56de\u503c\uff1a<code>GPIO_PinState</code> \u7c7b\u578b\uff08<code>GPIO_PIN_SET</code> \u4e3a\u9ad8\u7535\u5e73\uff0c<code>GPIO_PIN_RESET</code> \u4e3a\u4f4e\u7535\u5e73\uff09\u3002</p> </li> <li> <p>\u793a\u4f8b\uff08\u8bfb\u53d6\u6309\u952e\u72b6\u6001\uff09\uff1a</p> <p><code>c if (HAL_GPIO_ReadPin(GPIOA, KEY_WKUP_Pin) == GPIO_PIN_RESET) {     // \u6309\u952e\u88ab\u6309\u4e0b\uff08\u5047\u8bbe\u6309\u952e\u662f\u4e0b\u62c9\u8f93\u5165\uff0c\u6309\u4e0b\u65f6\u8fde\u63a5\u5230 GND\uff0c\u8bfb\u53d6\u4f4e\u7535\u5e73\uff09 }</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#exti","title":"\u5916\u90e8\u4e2d\u65ad\uff08EXTI\uff09\u4e0e\u56de\u8c03\u51fd\u6570","text":"<p>\u5bf9\u4e8e\u6309\u952e\u7b49\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u8f93\u5165\uff0c\u901a\u5e38\u914d\u7f6e\u4e3a \u5916\u90e8\u4e2d\u65ad\uff08EXTI\uff09 \u6a21\u5f0f\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#exti_1","title":"EXTI\u57fa\u672c\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#afioio","title":"AFIO\uff08\u590d\u7528IO\u53e3\uff09\u8bf4\u660e","text":"<ul> <li> <p>AFIO\u4e3b\u8981\u7528\u4e8e\u5f15\u811a\u590d\u7528\u529f\u80fd\u7684\u9009\u62e9\u548c\u91cd\u5b9a\u4e49</p> </li> <li> <p>\u5728STM32\u4e2d\uff0cAFIO\u4e3b\u8981\u5b8c\u6210\u4e24\u4e2a\u4efb\u52a1\uff1a\u590d\u7528\u529f\u80fd\u5f15\u811a\u91cd\u6620\u5c04\u3001\u4e2d\u65ad\u5f15\u811a\u9009\u62e9</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#exti_2","title":"EXTI\u6846\u56fe","text":""},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#exti_3","title":"EXTI \u914d\u7f6e\u8981\u70b9","text":"<ol> <li>\u5f15\u811a\u6a21\u5f0f\uff1a\u5728 CubeMX \u4e2d\u5c06\u5f15\u811a\u914d\u7f6e\u4e3a GPIO_EXTIxx\uff08\u4f8b\u5982 <code>GPIO_EXTI0</code>\uff09\u3002</li> <li>\u8fb9\u6cbf\u68c0\u6d4b\uff1a\u914d\u7f6e\u4e2d\u65ad\u89e6\u53d1\u7684\u8fb9\u6cbf\uff08\u4e0a\u5347\u6cbf\u3001\u4e0b\u964d\u6cbf\u6216\u53cc\u8fb9\u6cbf\uff09\u3002</li> <li>NVIC \u4f7f\u80fd\uff1a\u5728 NVIC Settings \u4e2d\u4f7f\u80fd\u5bf9\u5e94\u7684 EXTI \u4e2d\u65ad\u7ebf\u5e76\u8bbe\u7f6e\u4f18\u5148\u7ea7\u3002</li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_6","title":"\u4e2d\u65ad\u56de\u8c03\u51fd\u6570","text":"<ul> <li>\u4f60\u9700\u8981\u5b9e\u73b0\u7684\u51fd\u6570\uff1a</li> </ul> <p><code>c   void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)   {       if (GPIO_Pin == KEY_WKUP_Pin) {           // \u5728\u8fd9\u91cc\u7f16\u5199\u4e2d\u65ad\u89e6\u53d1\u540e\u7684\u5904\u7406\u4ee3\u7801           // \u63a8\u8350\uff1a\u53ea\u8bbe\u7f6e\u6807\u5fd7\u4f4d\u6216\u53d1\u9001\u6d88\u606f\uff0c\u5c06\u8017\u65f6\u64cd\u4f5c\u7559\u7ed9\u4e3b\u5faa\u73af       }   }</code></p> <ul> <li>\u8bf4\u660e\uff1a\u4e2d\u65ad\u89e6\u53d1\u65f6\uff0cHAL \u5e93\u4f1a\u81ea\u52a8\u6267\u884c\u4e2d\u65ad\u5411\u91cf\u8868\u4e2d\u7684 ISR\uff08\u5728 <code>stm32fxxx_it.c</code> \u4e2d\uff09\uff0c\u8be5 ISR \u4f1a\u8c03\u7528\u4e00\u4e2a\u901a\u7528\u7684\u5f31\uff08<code>__weak</code>\uff09\u56de\u8c03\u51fd\u6570\u3002\u4f60\u53ea\u9700\u8981\u5728\u4f60\u7684\u4ee3\u7801\u4e2d\u91cd\u5199\uff08\u5b9e\u73b0\uff09\u8fd9\u4e2a <code>HAL_GPIO_EXTI_Callback</code> \u51fd\u6570\u5373\u53ef\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_7","title":"\u8bf4\u660e","text":"<p>\u7b80\u800c\u8a00\u4e4b\uff0c<code>HAL_GPIO_EXTI_Callback</code> \u5bf9\u6240\u6709\u901a\u8fc7 GPIO \u5f15\u811a\u89e6\u53d1\u7684\u5916\u90e8\u4e2d\u65ad (EXTI) \u90fd\u6709\u6548\u3002</p> <p>\u5b83\u662f\u4e00\u4e2a \u901a\u7528\u7684\u3001\u7edf\u4e00\u7684\u56de\u8c03\u51fd\u6570\uff0c\u7528\u4e8e\u5904\u7406\u6240\u6709\u8fde\u63a5\u5230 EXTI \u7ebf\u7684 GPIO \u4e2d\u65ad\u4e8b\u4ef6\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#hal","title":"HAL \u5e93\u7684\u62bd\u8c61\u673a\u5236","text":"<p>HAL \u5e93\u901a\u8fc7\u4e00\u79cd\u591a\u5c42\u5d4c\u5957\u7684\u51fd\u6570\u8c03\u7528\u7ed3\u6784\uff0c\u5c06\u5e95\u5c42\u4e0d\u540c\u7684\u4e2d\u65ad\u5411\u91cf\u62bd\u8c61\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u56de\u8c03\u51fd\u6570\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u4f60\u53ea\u9700\u8981\u5b9e\u73b0\u4e00\u4e2a <code>HAL_GPIO_EXTI_Callback</code>\uff1a</p> <ol> <li>\u5e95\u5c42\uff1a\u786c\u4ef6\u4e2d\u65ad\u5411\u91cf (ISR)\uff1a</li> </ol> <p>\u4f60\u63d0\u5230\u4e0d\u540c\u7684\u5916\u90e8\u4e2d\u65ad\u6709\u4e0d\u540c\u7684\u4e2d\u65ad\u5411\u91cf\uff0c\u8fd9\u662f\u6b63\u786e\u7684\u3002\u4f8b\u5982\uff0cSTM32 F4 \u7cfb\u5217\u82af\u7247\u4e2d\uff1a</p> <ul> <li> <p>EXTI \u7ebf 0 \u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf\uff08\u4f8b\u5982 <code>EXTI0_IRQHandler</code>\uff09\u3002</p> </li> <li> <p>EXTI \u7ebf 1 \u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf\uff08\u4f8b\u5982 <code>EXTI1_IRQHandler</code>\uff09\u3002</p> </li> <li> <p>EXTI \u7ebf 10 \u5230 15 \u5171\u7528\u4e00\u4e2a\u5411\u91cf\uff08\u4f8b\u5982 EXTI15_10_IRQHandler\uff09\u3002</p> <p>\u8fd9\u4e9b\u51fd\u6570\u4f4d\u4e8e CubeMX \u751f\u6210\u7684 stm32fxx_it.c \u6587\u4ef6\u4e2d\u3002</p> </li> <li> <p>\u4e2d\u95f4\u5c42\uff1aHAL \u5e93\u4e2d\u65ad\u5904\u7406\u51fd\u6570\uff1a</p> </li> </ul> <p>\u5728\u6bcf\u4e2a\u5e95\u5c42\u7684\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f (ISR) \u4e2d\uff0c\u4f1a\u8c03\u7528 HAL \u5e93\u63d0\u4f9b\u7684\u5177\u4f53\u5904\u7406\u51fd\u6570\uff0c\u4f8b\u5982\uff1a</p> <p><code>c    // \u4f4d\u4e8e stm32fxx_it.c    void EXTI0_IRQHandler(void)    {        HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_0); // \u8c03\u7528HAL\u5e93\u5904\u7406\u51fd\u6570    }</code></p> <p>HAL \u5e93\u7684\u8fd9\u4e2a\u5904\u7406\u51fd\u6570\u4f1a\u8d1f\u8d23\uff1a</p> <ul> <li>\u6e05\u9664\u5bf9\u5e94\u7684 EXTI \u6302\u8d77\u5bc4\u5b58\u5668 (PR) \u6807\u5fd7\u3002</li> <li> <p>\u68c0\u67e5\u4e2d\u65ad\u6e90\u662f\u5426\u771f\u7684\u88ab\u4f7f\u80fd\u548c\u89e6\u53d1\u3002</p> </li> <li> <p>\u9876\u5c42\uff1a\u901a\u7528\u56de\u8c03\u51fd\u6570\uff1a</p> </li> </ul> <p>HAL \u5e93\u5904\u7406\u51fd\u6570\u7684\u6700\u540e\u4e00\u6b65\uff0c\u5c31\u662f\u8c03\u7528\u8fd9\u4e2a \u5f31\uff08__weak\uff09 \u58f0\u660e\u7684 HAL_GPIO_EXTI_Callback \u51fd\u6570\uff0c\u5e76\u5c06\u89e6\u53d1\u4e2d\u65ad\u7684\u5f15\u811a\u53f7\u4f5c\u4e3a\u53c2\u6570\u4f20\u8fdb\u53bb\u3002</p> <ul> <li><code>__weak</code> \u5173\u952e\u5b57\u5141\u8bb8\u4f60\u5728\u4f60\u7684\u5e94\u7528\u4ee3\u7801\uff08\u5982 <code>main.c</code> \u6216\u9a71\u52a8\u6587\u4ef6\uff09\u4e2d\u91cd\u5199\u8fd9\u4e2a\u51fd\u6570\u3002</li> <li>\u53c2\u6570 <code>GPIO_Pin</code>\uff1a\u901a\u8fc7\u68c0\u67e5\u8fd9\u4e2a\u53c2\u6570\uff0c\u4f60\u53ef\u4ee5\u5728\u4e00\u4e2a\u56de\u8c03\u51fd\u6570\u4e2d\u5224\u65ad\u662f\u54ea\u4e2a\u5f15\u811a\u89e6\u53d1\u4e86\u4e2d\u65ad\uff0c\u4ece\u800c\u6267\u884c\u4e0d\u540c\u7684\u903b\u8f91\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_8","title":"\u5982\u4f55\u533a\u5206\u4e0d\u540c\u7684\u4e2d\u65ad\u6e90\uff1f","text":"<p>\u867d\u7136\u56de\u8c03\u51fd\u6570\u662f\u7edf\u4e00\u7684\uff0c\u4f46\u4f60\u9700\u8981\u6839\u636e\u4f20\u5165\u7684 <code>GPIO_Pin</code> \u53c2\u6570\u6765\u533a\u5206\u548c\u5904\u7406\u4e0d\u540c\u7684\u5916\u90e8\u4e2d\u65ad\u4e8b\u4ef6\uff1a</p> <pre><code>void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)\n{\n    if (GPIO_Pin == KEY_START_Pin) {\n        // \u5904\u7406\u5f00\u59cb\u6309\u952e\u7684\u903b\u8f91\n    } else if (GPIO_Pin == KEY_STOP_Pin) {\n        // \u5904\u7406\u505c\u6b62\u6309\u952e\u7684\u903b\u8f91\n    } else if (GPIO_Pin == RFID_DET_Pin) {\n        // \u5904\u7406 RFID \u68c0\u6d4b\u7684\u903b\u8f91\n    }\n    // \u6ce8\u610f\uff1a\u65e0\u9700\u624b\u52a8\u6e05\u9664\u6302\u8d77\u6807\u5fd7\uff0cHAL \u5e93\u4e2d\u95f4\u5c42\u5df2\u7ecf\u5904\u7406\u4e86\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#_9","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u6d88\u6296 (Debouncing)\uff1a\u5bf9\u4e8e\u673a\u68b0\u6309\u952e\uff0c\u52a1\u5fc5\u5728\u56de\u8c03\u51fd\u6570\u4e2d\u6216\u5728\u540e\u7eed\u5904\u7406\u4e2d\u8003\u8651\u8f6f\u4ef6\u6d88\u6296\u903b\u8f91\uff0c\u9632\u6b62\u5f15\u811a\u6296\u52a8\u5bfc\u81f4\u591a\u6b21\u8bef\u89e6\u53d1\u3002</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/GPIO/#cubemx_1","title":"\u4f7f\u7528\u4e86 CubeMX \u540e\u4e0d\u7528\u5173\u5fc3\u4ec0\u4e48\uff1f","text":"<p>\u5f53\u4f60\u4f7f\u7528 STM32CubeMX \u548c HAL \u5e93\u8fdb\u884c\u5f00\u53d1\u65f6\uff0cCubeMX \u5df2\u7ecf\u4e3a\u4f60\u5904\u7406\u4e86\u5927\u91cf\u7684\u5e95\u5c42\u914d\u7f6e\u548c\u521d\u59cb\u5316\u5de5\u4f5c\u3002\u56e0\u6b64\uff0c\u4f5c\u4e3a\u5e94\u7528\u5f00\u53d1\u8005\uff0c\u4f60\u51e0\u4e4e\u4e0d\u7528\u5173\u5fc3\u4ee5\u4e0b\u8fd9\u4e9b\u7ec6\u8282\uff1a</p> \u7ec6\u8282\u7c7b\u522b \u63cf\u8ff0 GPIO \u65f6\u949f\u4f7f\u80fd CubeMX \u81ea\u52a8\u5904\u7406\u3002 \u5728 <code>HAL_MspInit()</code> \u51fd\u6570\u4e2d\uff0cCubeMX \u5df2\u7ecf\u81ea\u52a8\u751f\u6210\u4e86 <code>__HAL_RCC_GPIOx_CLK_ENABLE()</code> \u8c03\u7528\uff0c\u786e\u4fdd\u5728\u4f7f\u7528\u7aef\u53e3\u524d\u65f6\u949f\u5df2\u6253\u5f00\u3002 \u5bc4\u5b58\u5668\u5730\u5740\u4e0e\u4f4d\u64cd\u4f5c HAL \u5e93\u5c01\u88c5\u4e86\u3002 \u4f60\u4e0d\u9700\u8981\u76f4\u63a5\u64cd\u4f5c MODER\u3001OTYPER\u3001PUPDR \u7b49\u5bc4\u5b58\u5668\u6765\u8bbe\u7f6e\u6a21\u5f0f\u3001\u7c7b\u578b\u6216\u4e0a\u4e0b\u62c9\uff0c\u53ea\u9700\u4f7f\u7528 <code>HAL_GPIO_WritePin()</code> \u7b49 API \u5373\u53ef\u3002 \u5e95\u5c42\u521d\u59cb\u5316\u51fd\u6570\u8c03\u7528 CubeMX \u81ea\u52a8\u751f\u6210\u5e76\u8c03\u7528\u3002 <code>MX_GPIO_Init()</code> \u51fd\u6570\u4f1a\u5728 <code>main()</code> \u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002\u4f60\u65e0\u9700\u5173\u5fc3\u5b83\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u88ab\u8c03\u7528\u3002 NVIC \u5411\u91cf\u8868\u914d\u7f6e CubeMX \u81ea\u52a8\u914d\u7f6e\u3002 \u5bf9\u4e8e EXTI\uff0cCubeMX \u4f1a\u81ea\u52a8\u914d\u7f6e\u5e76\u4f7f\u80fd\u5bf9\u5e94\u7684 NVIC \u4e2d\u65ad\u901a\u9053\uff0c\u4f60\u53ea\u9700\u8981\u5b9e\u73b0\u4f60\u7684 \u56de\u8c03\u51fd\u6570 \u5373\u53ef\u3002 \u590d\u4f4d\u503c (Reset Value) HAL \u5e93\u8d1f\u8d23\u521d\u59cb\u5316\u3002 \u542f\u52a8\u65f6 GPIO \u9ed8\u8ba4\u5904\u4e8e\u6a21\u62df\u6216\u6d6e\u7a7a\u72b6\u6001\uff0c\u4f46 <code>HAL_GPIO_Init()</code> \u4f1a\u5c06\u5176\u8bbe\u7f6e\u4e3a\u4f60\u5728 CubeMX \u4e2d\u914d\u7f6e\u7684\u72b6\u6001\u3002\u4f60\u4e0d\u7528\u5173\u5fc3\u542f\u52a8\u65f6\u7684\u9ed8\u8ba4\u72b6\u6001\u3002 <p>\u603b\u7ed3\uff1a CubeMX \u7684\u76ee\u6807\u5c31\u662f\u8ba9\u4f60\u4e13\u6ce8\u4e8e\u4f7f\u7528 HAL API\uff08\u5982 <code>HAL_GPIO_WritePin</code>\u3001<code>HAL_GPIO_ReadPin</code>\uff09\u548c\u5b9e\u73b0 \u56de\u8c03\u51fd\u6570\uff08\u5982 <code>HAL_GPIO_EXTI_Callback</code>\uff09\uff0c\u800c\u4e0d\u7528\u82b1\u8d39\u65f6\u95f4\u5904\u7406\u5e95\u5c42\u5bc4\u5b58\u5668\u7684\u914d\u7f6e\u7ec6\u8282\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/","title":"\u57fa\u672c\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#nvic","title":"NVIC \u6838\u5fc3\u6982\u5ff5\u4e0e\u5de5\u4f5c\u539f\u7406","text":"<p>NVIC (Nested Vectored Interrupt Controller) \u662f ARM Cortex-M \u5185\u6838\uff08STM32 \u91c7\u7528\uff09\u4e2d\u4e00\u4e2a\u9ad8\u5ea6\u96c6\u6210\u7684\u6a21\u5757\uff0c\u4e13\u95e8\u8d1f\u8d23\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u4e2d\u65ad\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#1-vector-table","title":"1. \u4e2d\u65ad\u5411\u91cf\u8868 (Vector Table)","text":"<ul> <li> <p>\u5b9a\u4e49: \u8fd9\u662f\u4e00\u4e2a\u5185\u5b58\u5730\u5740\u8868\u683c\uff0c\u5b58\u653e\u4e86\u6240\u6709\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f\uff08ISR\uff09\u7684\u5165\u53e3\u5730\u5740\u3002</p> </li> <li> <p>\u4f5c\u7528: \u5f53\u4e00\u4e2a\u4e2d\u65ad\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0cNVIC \u4f1a\u67e5\u8be2\u5411\u91cf\u8868\uff0c\u83b7\u53d6\u5bf9\u5e94\u7684 ISR \u5730\u5740\uff0c\u5e76\u8df3\u8f6c\u6267\u884c\u3002</p> </li> <li> <p>STM32 \u7279\u6027: \u5411\u91cf\u8868\u901a\u5e38\u4f4d\u4e8e Flash \u6216 SRAM \u7684\u8d77\u59cb\u5730\u5740\uff0c\u5176\u57fa\u5730\u5740\u662f\u53ef\u7f16\u7a0b\u7684\uff08\u901a\u8fc7 SCB \u5bc4\u5b58\u5668\uff09\u3002CubeMX \u751f\u6210\u7684\u9879\u76ee\u4e2d\uff0c\u5411\u91cf\u8868\u9ed8\u8ba4\u4f4d\u4e8e Flash \u7684\u8d77\u59cb\u5730\u5740\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#2","title":"2. \u4e2d\u65ad\u6e90\u5206\u7c7b","text":"<p>STM32 \u7684\u4e2d\u65ad\u6e90\u5927\u81f4\u5206\u4e3a\u4e24\u7c7b\uff1a</p> \u7c7b\u578b \u63cf\u8ff0 \u793a\u4f8b \u4f18\u5148\u7ea7\u8bbe\u5b9a \u5185\u6838\u5f02\u5e38 (Core Exceptions) \u7531 CPU \u5185\u6838\u5185\u90e8\u4ea7\u751f\uff0c\u7528\u4e8e\u5904\u7406\u7cfb\u7edf\u9519\u8bef\u6216\u8c03\u8bd5\u3002 Reset, NMI, Hard Fault, SysTick \u62e5\u6709\u56fa\u5b9a\u7684\u4f18\u5148\u7ea7\u548c\u53ef\u7f16\u7a0b\u7684\u5b50\u4f18\u5148\u7ea7 \u5916\u90e8\u4e2d\u65ad (External Interrupts) \u7531\u7247\u4e0a\u5916\u8bbe\uff08Peripherals\uff09\u6216\u5916\u90e8\u5f15\u811a\u4ea7\u751f\u3002 EXTI, USART, SPI, TIM, ADC \u4f18\u5148\u7ea7\u5b8c\u5168\u53ef\u7f16\u7a0b"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#3-priority-management","title":"3. \u4f18\u5148\u7ea7\u7ba1\u7406 (Priority Management)","text":"<p>NVIC \u7684\u6838\u5fc3\u804c\u80fd\u662f\u5904\u7406\u591a\u91cd\u4e2d\u65ad\u7684\u62a2\u5360\u548c\u4ef2\u88c1\u3002</p> <ul> <li> <p>\u62a2\u5360\u4f18\u5148\u7ea7 (Preemption Priority)\uff1a\u8fd9\u662f\u4e2d\u65ad\u7684\u9996\u8981\u51b3\u5b9a\u56e0\u7d20\u3002\u9ad8\u62a2\u5360\u4f18\u5148\u7ea7\u7684\u4e2d\u65ad\u53ef\u4ee5\u6253\u65ad\uff08\u62a2\u5360\uff09\u4f4e\u62a2\u5360\u4f18\u5148\u7ea7\u7684\u4e2d\u65ad\u3002</p> </li> <li> <p>\u5b50\u4f18\u5148\u7ea7 (Sub-Priority / Sub-Group Priority)\uff1a\u5f53\u4e24\u4e2a\u4e2d\u65ad\u7684\u62a2\u5360\u4f18\u5148\u7ea7\u76f8\u540c\u65f6\uff0c\u5b50\u4f18\u5148\u7ea7\u624d\u53d1\u6325\u4f5c\u7528\u3002\u5b83\u51b3\u5b9a\u4e86\u5728\u76f8\u540c\u62a2\u5360\u7ea7\u522b\u4e0b\uff0c\u54ea\u4e2a\u4e2d\u65ad\u5148\u88ab\u670d\u52a1\uff08\u4f46\u4e0d\u80fd\u4e92\u76f8\u62a2\u5360\uff09\u3002</p> </li> <li> <p>\u4f18\u5148\u7ea7\u5206\u7ec4 (Priority Grouping)\uff1aARM Cortex-M \u5141\u8bb8\u5f00\u53d1\u8005\u5c06 8 \u4f4d\uff08\u6216\u66f4\u5c11\uff09\u7684\u4f18\u5148\u7ea7\u5b57\u6bb5\u5206\u6210\u62a2\u5360\u4f18\u5148\u7ea7\u548c\u5b50\u4f18\u5148\u7ea7\u4e24\u90e8\u5206\u3002</p> <ul> <li> <p>\u4f8b\u5982\uff0c\u5982\u679c\u914d\u7f6e\u4e3a Group 2\uff08CubeMX \u9ed8\u8ba4\uff09\uff0c\u5219\u4f18\u5148\u7ea7\u5b57\u6bb5\u7684\u6700\u9ad8 4 \u4f4d\u7528\u4e8e\u62a2\u5360\u4f18\u5148\u7ea7\uff0c\u6700\u4f4e 0 \u4f4d\u7528\u4e8e\u5b50\u4f18\u5148\u7ea7\uff08\u5373\u6ca1\u6709\u5b50\u4f18\u5148\u7ea7\uff09\u3002</p> </li> <li> <p>CubeMX \u9ed8\u8ba4\u5206\u7ec4\uff1aSTM32 HAL \u5e93\u901a\u5e38\u9ed8\u8ba4\u4f7f\u7528 Group 4\uff08\u5373 4 \u4f4d\u7528\u4e8e\u62a2\u5360\u4f18\u5148\u7ea7\uff0c0 \u4f4d\u7528\u4e8e\u5b50\u4f18\u5148\u7ea7\uff09\uff0c\u6216 Group 3\uff083 \u4f4d\u62a2\u5360\uff0c1 \u4f4d\u5b50\u4f18\u5148\u7ea7\uff09\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u82af\u7247\u7cfb\u5217\u548c HAL \u5e93\u7248\u672c\u3002</p> </li> </ul> </li> </ul> <p>\u4f18\u5148\u7ea7\u6570\u503c\u89c4\u5219\uff1a</p> <ul> <li> <p>NVIC \u4e2d\uff0c\u6570\u503c\u8d8a\u5c0f\uff0c\u4f18\u5148\u7ea7\u8d8a\u9ad8\uff080 \u662f\u6700\u9ad8\u4f18\u5148\u7ea7\uff09\u3002</p> </li> <li> <p>\u62a2\u5360\u4f18\u5148\u7ea7\u9ad8\u7684\u4e2d\u65ad\u624d\u80fd\u62a2\u5360\u6b63\u5728\u6267\u884c\u7684\u4f4e\u4f18\u5148\u7ea7\u4e2d\u65ad\u3002</p> </li> </ul> \u5206\u7ec4\u65b9\u5f0f \u62a2\u5360\u4f18\u5148\u7ea7 \u54cd\u5e94\u4f18\u5148\u7ea7 \u5206\u7ec40 0\u4f4d\uff0c\u53d6\u503c\u4e3a0 4\u4f4d\uff0c\u53d6\u503c\u4e3a0~15 \u5206\u7ec41 1\u4f4d\uff0c\u53d6\u503c\u4e3a0~1 3\u4f4d\uff0c\u53d6\u503c\u4e3a0~7 \u5206\u7ec42 2\u4f4d\uff0c\u53d6\u503c\u4e3a0~3 2\u4f4d\uff0c\u53d6\u503c\u4e3a0~3 \u5206\u7ec43 3\u4f4d\uff0c\u53d6\u503c\u4e3a0~7 1\u4f4d\uff0c\u53d6\u503c\u4e3a0~1 \u5206\u7ec44 4\u4f4d\uff0c\u53d6\u503c\u4e3a0~15 0\u4f4d\uff0c\u53d6\u503c\u4e3a0 # STM32CubeMX \u4e2d\u7684 NVIC \u914d\u7f6e <p>CubeMX \u6781\u5927\u5730\u7b80\u5316\u4e86 NVIC \u7684\u914d\u7f6e\u5de5\u4f5c\uff0c\u4f7f\u5176\u56fe\u5f62\u5316\u3001\u53ef\u89c6\u5316\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#1","title":"1. \u4f18\u5148\u7ea7\u5206\u7ec4\u914d\u7f6e","text":"<ul> <li> <p>\u8def\u5f84: Pinout &amp; Configuration $\\rightarrow$ System Core $\\rightarrow$ NVIC\u3002</p> </li> <li> <p>\u914d\u7f6e\u9879: Priority Group\u3002</p> <ul> <li> <p>HAL \u5e93\u9ed8\u8ba4\u503c: \u5e38\u89c1\u4e3a <code>NVIC Priority Group 4</code> (\u6216 <code>__NVIC_PRIO_BITS</code> \u51b3\u5b9a\u7684\u5206\u7ec4)\u3002</p> </li> <li> <p>\u610f\u4e49: Group 4 \u610f\u5473\u7740\u6240\u6709\u53ef\u7528\u4f18\u5148\u7ea7\u4f4d\uff08\u901a\u5e38 4 \u4f4d\uff09\u90fd\u5206\u914d\u7ed9\u62a2\u5360\u4f18\u5148\u7ea7\uff0c\u6ca1\u6709\u5b50\u4f18\u5148\u7ea7\u3002\u8fd9\u4fdd\u8bc1\u4e86\u4f18\u5148\u7ea7\u8bbe\u5b9a\u7684\u76f4\u89c2\u6027\u548c\u62a2\u5360\u7684\u786e\u5b9a\u6027\u3002</p> </li> <li> <p>\u5efa\u8bae: \u5728\u9879\u76ee\u5f00\u53d1\u4e2d\uff0c\u4e0d\u8981\u8f7b\u6613\u4fee\u6539 CubeMX \u9ed8\u8ba4\u7684\u4f18\u5148\u7ea7\u5206\u7ec4\uff0c\u9664\u975e\u4f60\u5bf9\u4e2d\u65ad\u62a2\u5360\u673a\u5236\u6709\u975e\u5e38\u6df1\u5165\u7684\u7406\u89e3\u548c\u7279\u6b8a\u8981\u6c42\u3002</p> </li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#2_1","title":"2. \u5916\u90e8\u4e2d\u65ad\u914d\u7f6e","text":"<ul> <li> <p>\u8def\u5f84: Pinout &amp; Configuration $\\rightarrow$ NVIC Settings (\u5728\u6bcf\u4e2a\u5916\u8bbe\u6216 EXTI \u7684\u914d\u7f6e\u9875\u9762\u4e2d)\u3002</p> </li> <li> <p>\u64cd\u4f5c:</p> <ol> <li> <p>\u627e\u5230\u76ee\u6807\u5916\u8bbe\uff08\u5982 TIM3\u3001USART1 \u6216 EXTI Line 0\uff09\u3002</p> </li> <li> <p>\u52fe\u9009\u5bf9\u5e94\u7684\u4e2d\u65ad\u7ebf\uff08\u5982 <code>TIM3 global interrupt</code>\uff09\u3002</p> </li> <li> <p>\u8bbe\u7f6e Preemption Priority\uff08\u62a2\u5360\u4f18\u5148\u7ea7\uff09\u548c Subpriority\uff08\u5b50\u4f18\u5148\u7ea7\uff09\u3002</p> </li> </ol> </li> </ul> <p>CubeMX \u7ec6\u8282\uff1a\u5982\u679c\u4f60\u9009\u62e9\u4e86 Group 4\uff080 \u4f4d\u5b50\u4f18\u5148\u7ea7\uff09\uff0c\u90a3\u4e48 Subpriority \u5b57\u6bb5\u5c06\u65e0\u6cd5\u7f16\u8f91\uff0c\u4f60\u53ea\u9700\u5173\u6ce8 Preemption Priority\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#hal-nvic-api","title":"**HAL \u5e93\u4e0e NVIC \u76f8\u5173\u7684 API","text":"<p>HAL \u5e93\u63d0\u4f9b\u4e86\u4e00\u5957\u7b80\u6d01\u7684 API \u6765\u7ba1\u7406 NVIC\u3002\u8fd9\u4e9b\u51fd\u6570\u901a\u5e38\u7531 CubeMX \u81ea\u52a8\u751f\u6210\uff0c\u4f46\u4f60\u4e5f\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u7528\u5b83\u4eec\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#1_1","title":"1. \u4f18\u5148\u7ea7\u5206\u7ec4\u914d\u7f6e (\u81ea\u52a8)","text":"<ul> <li> <p>\u51fd\u6570: <code>HAL_NVIC_SetPriorityGrouping(uint32_t PriorityGroup)</code></p> </li> <li> <p>\u8bf4\u660e: \u6b64\u51fd\u6570\u8bbe\u7f6e\u4f18\u5148\u7ea7\u5206\u7ec4\u3002CubeMX \u4f1a\u5728 <code>HAL_Init()</code> \u6216 <code>MX_NVIC_Init()</code> \u4e2d\u81ea\u52a8\u8c03\u7528\u5b83\uff0c\u4f7f\u7528\u4f60\u5728\u914d\u7f6e\u754c\u9762\u9009\u62e9\u7684\u5206\u7ec4\uff08\u901a\u5e38\u662f <code>NVIC_PRIORITYGROUP_4</code>\uff09\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#2_2","title":"2. \u914d\u7f6e\u548c\u8bbe\u7f6e\u4f18\u5148\u7ea7 (\u81ea\u52a8/\u624b\u52a8)","text":"<ul> <li> <p>\u51fd\u6570: <code>HAL_NVIC_SetPriority(IRQn_Type IRQn, uint32_t PreemptPriority, uint32_t SubPriority)</code></p> </li> <li> <p>\u8bf4\u660e: \u7528\u4e8e\u8bbe\u7f6e\u7279\u5b9a\u4e2d\u65ad\u6e90\u7684\u62a2\u5360\u4f18\u5148\u7ea7\u548c\u5b50\u4f18\u5148\u7ea7\u3002</p> <ul> <li> <p><code>IRQn</code>: \u4e2d\u65ad\u8bf7\u6c42\u53f7\uff08\u4f8b\u5982 <code>TIM3_IRQn</code>, <code>USART1_IRQn</code>\uff09\u3002</p> </li> <li> <p>CubeMX \u4f1a\u5728 <code>MX_NVIC_Init()</code> \u4e2d\u4e3a\u6240\u6709\u542f\u7528\u7684\u4e2d\u65ad\u6e90\u81ea\u52a8\u751f\u6210\u8c03\u7528\u3002</p> </li> </ul> <p><code>c // \u793a\u4f8b\uff1aCubeMX \u81ea\u52a8\u751f\u6210\u7684\u4ee3\u7801\u7247\u6bb5 HAL_NVIC_SetPriority(TIM3_IRQn, 0, 0); // \u8bbe\u7f6e TIM3 \u4e2d\u65ad\uff0c\u62a2\u5360\u4f18\u5148\u7ea7\u4e3a 0 HAL_NVIC_EnableIRQ(TIM3_IRQn);</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#3","title":"3. \u4e2d\u65ad\u7684\u4f7f\u80fd\u4e0e\u7981\u7528","text":"<ul> <li> <p>\u51fd\u6570: <code>HAL_NVIC_EnableIRQ(IRQn_Type IRQn)</code></p> </li> <li> <p>\u51fd\u6570: <code>HAL_NVIC_DisableIRQ(IRQn_Type IRQn)</code></p> </li> <li> <p>\u8bf4\u660e: \u7528\u4e8e\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u5f00\u542f\u6216\u5173\u95ed\u7279\u5b9a\u7684\u4e2d\u65ad\u8bf7\u6c42\u3002</p> <ul> <li>\u6ce8\u610f: \u5373\u4f7f\u5728 CubeMX \u4e2d\u542f\u7528\u4e86\u4e2d\u65ad\uff0c\u5728\u8fdb\u5165 <code>main()</code> \u51fd\u6570\u4e4b\u524d\uff0c\u8fd9\u4e9b\u4e2d\u65ad\u901a\u5e38\u4ecd\u5904\u4e8e\u7981\u7528\u72b6\u6001\u3002<code>MX_NVIC_Init()</code> \u4f1a\u5728\u521d\u59cb\u5316\u9636\u6bb5\u8c03\u7528 <code>HAL_NVIC_EnableIRQ</code> \u5b8f\u6765\u4f7f\u80fd\u5b83\u4eec\u3002</li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#4","title":"4. \u4e2d\u65ad\u6302\u8d77\u4e0e\u6e05\u9664","text":"<ul> <li> <p>\u51fd\u6570: <code>HAL_NVIC_SetPendingIRQ(IRQn_Type IRQn)</code></p> </li> <li> <p>\u51fd\u6570: <code>HAL_NVIC_ClearPendingIRQ(IRQn_Type IRQn)</code></p> </li> <li> <p>\u8bf4\u660e: \u7528\u4e8e\u8f6f\u4ef6\u6a21\u62df\u4e2d\u65ad\u89e6\u53d1\uff08\u8bbe\u7f6e\u6302\u8d77\u6807\u5fd7\uff09\u6216\u6e05\u9664\u6302\u8d77\u6807\u5fd7\u3002\u4e00\u822c\u4e0d\u5e38\u7528\uff0c\u4f46\u5728\u6d4b\u8bd5\u548c\u7279\u6b8a\u540c\u6b65\u673a\u5236\u4e2d\u53ef\u80fd\u7528\u5230\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#isr","title":"\u4e2d\u65ad\u670d\u52a1\u7a0b\u5e8f (ISR) \u4e0e\u56de\u8c03\u51fd\u6570\u673a\u5236","text":"<p>\u7406\u89e3 HAL \u5e93\u5982\u4f55\u5904\u7406 ISR \u662f\u4f7f\u7528\u4e2d\u65ad\u7684\u5173\u952e\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#1-isr-cubemx","title":"1. \u4e2d\u65ad\u5411\u91cf\u8868\u4e0e ISR (\u7531 CubeMX \u751f\u6210)","text":"<ul> <li> <p>\u6587\u4ef6: <code>stm32fxxx_it.c</code></p> </li> <li> <p>\u5185\u5bb9: \u8fd9\u4e2a\u6587\u4ef6\u5305\u542b\u4e86\u6240\u6709\u4e2d\u65ad\u5411\u91cf\u8868\u6307\u5411\u7684\u5165\u53e3\u51fd\u6570\uff0c\u4f8b\u5982 <code>TIM3_IRQHandler</code>\u3002</p> </li> <li> <p>HAL \u5e93\u7684\u4f5c\u7528: \u8fd9\u4e9b ISR \u51fd\u6570\u5185\u90e8\u901a\u5e38\u53ea\u505a\u4e24\u4ef6\u4e8b\uff1a</p> <ol> <li> <p>\u8c03\u7528 HAL \u5e93\u63d0\u4f9b\u7684\u901a\u7528\u5904\u7406\u51fd\u6570\uff08\u4f8b\u5982 <code>HAL_TIM_IRQHandler(&amp;htim3)</code>\uff09\u3002</p> </li> <li> <p>\u5728\u8fd9\u4e2a\u901a\u7528\u5904\u7406\u51fd\u6570\u5185\u90e8\uff0cHAL \u5e93\u4f1a\u6e05\u9664\u4e2d\u65ad\u6807\u5fd7\u4f4d\uff0c\u5e76\u6700\u7ec8\u8c03\u7528\u56de\u8c03\u51fd\u6570\u3002</p> </li> </ol> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/NVIC/#2_3","title":"2. \u4e2d\u65ad\u56de\u8c03\u51fd\u6570 (\u4f60\u9700\u8981\u5b9e\u73b0)","text":"<ul> <li> <p>\u673a\u5236: HAL \u5e93\u5c06\u5e95\u5c42\u7684 ISR \u62bd\u8c61\u6210\u7528\u6237\u53cb\u597d\u7684 \u5f31\uff08<code>__weak</code>\uff09\u56de\u8c03\u51fd\u6570\u3002</p> </li> <li> <p>\u793a\u4f8b:</p> <ul> <li> <p>\u5b9a\u65f6\u5668\u6ea2\u51fa\uff1a\u4f60\u9700\u8981\u5b9e\u73b0 <code>HAL_TIM_PeriodElapsedCallback(TIM_HandleTypeDef *htim)</code></p> </li> <li> <p>GPIO \u5916\u90e8\u4e2d\u65ad\uff1a\u4f60\u9700\u8981\u5b9e\u73b0 <code>HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)</code>\u3002</p> </li> </ul> </li> <li> <p>\u4f60\u7684\u4efb\u52a1: \u4f60\u53ea\u9700\u8981\u5728\u4f60\u7684\u5e94\u7528\u4ee3\u7801\uff08\u5982 <code>main.c</code> \u6216\u4f60\u7684\u9a71\u52a8\u6587\u4ef6\uff09\u4e2d\u91cd\u5199\u8fd9\u4e9b\u56de\u8c03\u51fd\u6570\uff0c\u5b9e\u73b0\u4f60\u7684\u4e1a\u52a1\u903b\u8f91\u3002</p> </li> </ul> <p>\u6838\u5fc3\u539f\u5219: HAL \u5e93\u5e2e\u4f60\u5904\u7406\u4e86\u6240\u6709\u5bc4\u5b58\u5668\u7ea7\u522b\u7684\u64cd\u4f5c\uff08\u6e05\u9664\u6807\u5fd7\u4f4d\u3001\u8df3\u8f6c\uff09\uff0c\u4f60\u53ea\u9700\u8981\u4e13\u6ce8\u4e8e\u5b9e\u73b0\u56de\u8c03\u51fd\u6570\u4e2d\u7684\u4e1a\u52a1\u903b\u8f91\u5373\u53ef\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/STM32_HAL/","title":"\u5de5\u7a0b\u67b6\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/STM32_HAL/#_2","title":"\u76ee\u5f55","text":"<ul> <li>\u57fa\u7840\uff1aARM_Arch</li> <li>GPIO</li> <li>TIM</li> <li>connective</li> <li>NVIC</li> <li>DMA</li> <li>ADC</li> <li>BKP &amp; RTC</li> <li>WDG</li> <li>FLASH</li> <li>Porting</li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/","title":"0. \u5b9a\u65f6\u5668\u7684\u5206\u7c7b\u548c\u7ed3\u6784","text":""},{"location":"EmbeddedSoft/STM32_HAL/TIM/#1-base-timebase","title":"1. \u5b9a\u65f6\u5668\u57fa\u7840\u53c2\u6570\u4e0e\u65f6\u57fa\u914d\u7f6e (Base &amp; Timebase)","text":"<p>\u8fd9\u662f\u6240\u6709\u5b9a\u65f6\u5668\u529f\u80fd\u7684\u5730\u57fa\uff0c\u65e0\u8bba\u4f7f\u7528 PWM \u8fd8\u662f\u7f16\u7801\u5668\uff0c\u9996\u5148\u90fd\u8981\u914d\u7f6e\u65f6\u57fa\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_1","title":"\u57fa\u672c\u6982\u5ff5","text":"<ul> <li> <p>CK_INT: \u5185\u90e8\u65f6\u949f\u6e90\u9891\u7387\uff08\u901a\u5e38\u7531 APB1 \u6216 APB2 \u603b\u7ebf\u63d0\u4f9b\uff0c\u9700\u67e5\u9605 Clock Tree\uff09\u3002</p> </li> <li> <p>PSC (Prescaler): \u9884\u5206\u9891\u7cfb\u6570\uff0816\u4f4d\uff0c0~65535\uff09\u3002</p> </li> <li> <p>ARR (AutoReload Register): \u81ea\u52a8\u91cd\u88c5\u8f7d\u503c\uff08\u5468\u671f\u503c\uff09\u3002</p> </li> <li> <p>CNT (Counter): \u5f53\u524d\u8ba1\u6570\u503c\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Pinout &amp; Configuration: \u9009\u62e9 TIMx\u3002</p> </li> <li> <p>Clock Source: \u9009\u62e9 <code>Internal Clock</code> (\u901a\u5e38\u60c5\u51b5)\u3002</p> </li> <li> <p>Parameter Settings:</p> <ul> <li> <p>Prescaler (PSC): \u8f93\u5165 $N-1$\u3002\u4f8b\u5982\u5206\u9891 72\uff0c\u8f93\u5165 <code>71</code>\u3002</p> </li> <li> <p>Counter Mode: <code>Up</code> (\u5411\u4e0a\u8ba1\u6570) \u6700\u5e38\u7528\u3002</p> </li> <li> <p>Counter Period (ARR): \u8f93\u5165 $M-1$\u3002\u51b3\u5b9a\u4e86\u6ea2\u51fa/\u66f4\u65b0\u7684\u65f6\u95f4\u70b9\u3002</p> </li> <li> <p>Internal Clock Division (CKD): \u6570\u5b57\u6ee4\u6ce2\u5668\u4f7f\u7528\u7684\u91c7\u6837\u65f6\u949f\u5206\u9891\uff0c\u901a\u5e38\u9009 <code>No Division</code>\u3002</p> </li> <li> <p>Auto-reload preload: <code>Enable</code> (\u5efa\u8bae\u5f00\u542f\uff0c\u4f7f\u4fee\u6539 ARR \u5728\u4e0b\u4e2a\u66f4\u65b0\u4e8b\u4ef6\u751f\u6548\uff0c\u9632\u6b62\u6ce2\u5f62\u9519\u4e71)\u3002</p> </li> </ul> </li> </ol> <p>\u6838\u5fc3\u516c\u5f0f:</p> <p>$$\\text{\u6ea2\u51fa\u9891\u7387} = \\frac{\\text{CK_INT}}{(PSC+1) \\times (ARR+1)}$$</p> <p>$$\\text{\u6ea2\u51fa\u65f6\u95f4} = \\frac{(PSC+1) \\times (ARR+1)}{\\text{CK_INT}}$$</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#2-timer-interrupt","title":"2. \u5b9a\u65f6\u4e2d\u65ad (Timer Interrupt)","text":"<p>\u6700\u57fa\u7840\u7684\u529f\u80fd\uff0c\u7528\u4e8e\u5468\u671f\u6027\u6267\u884c\u4efb\u52a1\u3002 </p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_1","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>\u914d\u7f6e\u65f6\u57fa: \u5982\u4e0a\u6240\u8ff0\uff0c\u8ba1\u7b97\u597d PSC \u548c ARR \u4ee5\u83b7\u5f97\u76ee\u6807\u4e2d\u65ad\u9891\u7387\uff08\u4f8b\u5982 1ms \u6216 1s\uff09\u3002</p> </li> <li> <p>NVIC Settings:</p> <ul> <li> <p>Enabled: \u5fc5\u987b\u52fe\u9009 <code>TIMx global interrupt</code>\u3002</p> </li> <li> <p>Preemption Priority: \u6839\u636e\u7cfb\u7edf\u5b9e\u65f6\u6027\u8981\u6c42\u8bbe\u7f6e\u4f18\u5148\u7ea7\u3002</p> </li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_2","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> <p>\u542f\u52a8:</p> <p><code>c // \u5728 main.c \u7684 user code begin 2 HAL_TIM_Base_Start_IT(&amp;htim2); // \u542f\u52a8\u5b9a\u65f6\u5668\u5e76\u5f00\u542f\u66f4\u65b0\u4e2d\u65ad</code></p> </li> <li> <p>\u56de\u8c03\u51fd\u6570:</p> <p><code>c // \u5728 main.c \u6216\u4efb\u610f\u6e90\u6587\u4ef6\u4e2d\u91cd\u5199 void HAL_TIM_PeriodElapsedCallback(TIM_HandleTypeDef *htim) {     if (htim-&gt;Instance == TIM2) {         // \u6267\u884c\u5468\u671f\u6027\u4efb\u52a1\uff0c\u4f8b\u5982 1ms \u4e00\u6b21         // \u6ce8\u610f\uff1a\u4e0d\u8981\u5728\u6b64\u6267\u884c\u8017\u65f6\u64cd\u4f5c\uff08\u5982 printf \u5ef6\u8fdf\uff09     } }</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#3-pwm-pulse-width-modulation","title":"3. PWM \u8f93\u51fa (Pulse Width Modulation)","text":"<p>\u57fa\u4e8e 4.\u8f93\u51fa\u6bd4\u8f83 \u5b9e\u73b0\uff0c \u7528\u4e8e\u7535\u673a\u8c03\u901f\u3001LED \u547c\u5438\u706f\u3001\u8702\u9e23\u5668\u9a71\u52a8\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_2","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Mode: \u5728 Pinout \u4e2d\u5c06 Channel x \u9009\u4e3a <code>PWM Generation CHx</code>\u3002</p> </li> <li> <p>Configuration -&gt; Parameter Settings:</p> <ul> <li> <p>Counter Settings: \u914d\u7f6e PSC \u548c ARR \u51b3\u5b9a PWM \u9891\u7387\u3002</p> </li> <li> <p>PWM Generation Channel x:</p> <ul> <li> <p>Mode:</p> <ul> <li> <p><code>PWM mode 1</code> (\u5e38\u7528): CNT &lt; CCR \u65f6\u6709\u6548\u3002</p> </li> <li> <p><code>PWM mode 2</code>: CNT &gt; CCR \u65f6\u6709\u6548\u3002</p> </li> </ul> </li> <li> <p>Pulse (CCR): \u521d\u59cb\u5360\u7a7a\u6bd4\u6570\u503c\u3002$\\text{Duty} = \\frac{CCR}{ARR+1}$\u3002</p> </li> <li> <p>Output compare preload: <code>Enable</code> (\u91cd\u8981\uff0c\u9632\u6b62\u4fee\u6539\u5360\u7a7a\u6bd4\u65f6\u4ea7\u751f\u6bdb\u523a)\u3002</p> </li> <li> <p>CH Polarity: <code>High</code> (\u6709\u6548\u7535\u5e73\u4e3a\u9ad8) \u6216 <code>Low</code>\u3002</p> </li> </ul> </li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_3","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> <p>\u542f\u52a8:</p> <p><code>c HAL_TIM_PWM_Start(&amp;htim3, TIM_CHANNEL_1); // \u5f00\u542f PWM \u8f93\u51fa</code></p> </li> <li> <p>\u8fd0\u884c\u65f6\u4fee\u6539\u5360\u7a7a\u6bd4:</p> <p><code>c // \u4fee\u6539 CCR \u5bc4\u5b58\u5668\uff0c\u8303\u56f4 0 ~ ARR __HAL_TIM_SET_COMPARE(&amp;htim3, TIM_CHANNEL_1, 500);</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#4-output-compare-oc","title":"4. \u8f93\u51fa\u6bd4\u8f83 (Output Compare - OC)","text":"<p>\u8f93\u51fa\u6bd4\u8f83\u53ef\u4ee5\u901a\u8fc7\u6bd4\u8f83CNT\u4e0eCCR\u5bc4\u5b58\u5668\u503c\u7684\u5173\u7cfb\uff0c\u6765\u5bf9\u8f93\u51fa\u7535\u5e73\u8fdb\u884c\u7f6e1\u3001\u7f6e0\u6216\u7ffb\u8f6c\u7684\u64cd\u4f5c\uff0c\u7528\u4e8e\u8f93\u51fa\u4e00\u5b9a\u9891\u7387\u548c\u5360\u7a7a\u6bd4\u7684PWM\u6ce2\u5f62</p> <p>\u4e0e PWM \u7c7b\u4f3c\u4f46\u66f4\u7075\u6d3b\uff0c\u7528\u4e8e\u751f\u6210\u7cbe\u786e\u7684\u8109\u51b2\u3001\u76f8\u4f4d\u63a7\u5236\u6216\u5728\u7279\u5b9a\u65f6\u95f4\u7ffb\u8f6c\u7535\u5e73\uff08Toggle\uff09\u3002\u5b83\u4e0d\u5f3a\u5236\u8981\u6c42\u8fde\u7eed\u6ce2\u5f62\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_3","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Mode: \u5c06 Channel x \u9009\u4e3a <code>Output Compare CHx</code>\u3002</p> </li> <li> <p>Configuration -&gt; Parameter Settings:</p> <ul> <li> <p>Output Compare Channel x:</p> <ul> <li> <p>Mode:</p> <ul> <li> <p><code>Toggle on match</code>: CNT == CCR \u65f6\u7ffb\u8f6c\u5f15\u811a\u7535\u5e73\uff08\u5e38\u7528\uff0c\u751f\u6210\u56fa\u5b9a\u9891\u7387\u65b9\u6ce2\uff0c\u9891\u7387\u4e3a\u6ea2\u51fa\u9891\u7387\u7684\u4e00\u534a\uff09\u3002</p> </li> <li> <p><code>Set active on match</code>: \u5339\u914d\u65f6\u7f6e\u9ad8\u3002</p> </li> <li> <p><code>PWM mode</code>: \u5176\u5b9e OC \u4e5f\u53ef\u4ee5\u914d\u6210 PWM\uff0c\u4f46\u529f\u80fd\u5c11\u4e8e\u4e13\u7528 PWM \u6a21\u5f0f\u3002</p> </li> </ul> </li> <li> <p>Pulse: \u521d\u59cb\u6bd4\u8f83\u503c\u3002</p> </li> </ul> </li> </ul> </li> <li> <p>NVIC Settings: \u5982\u679c\u9700\u8981\u5728\u5339\u914d\u65f6\u89e6\u53d1\u4e2d\u65ad\u5904\u7406\uff08\u4f8b\u5982\u76f8\u4f4d\u63a7\u5236\uff09\uff0c\u9700\u8981\u5f00\u542f NVIC\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_4","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> <p>\u542f\u52a8:</p> <p>```c // \u65b9\u5f0f1: \u4ec5\u8f93\u51fa\u7535\u5e73\uff0c\u4e0d\u8fdb\u4e2d\u65ad HAL_TIM_OC_Start(&amp;htim3, TIM_CHANNEL_1);</p> <p>// \u65b9\u5f0f2: \u5f00\u542f\u8f93\u51fa\u5e76\u5f00\u542f\u5339\u914d\u4e2d\u65ad HAL_TIM_OC_Start_IT(&amp;htim3, TIM_CHANNEL_1); ```</p> </li> <li> <p>\u4e2d\u65ad\u56de\u8c03 (\u5982\u679c\u5f00\u542f\u4e86 IT):</p> <p><code>c void HAL_TIM_OC_DelayElapsedCallback(TIM_HandleTypeDef *htim) {     if(htim-&gt;Channel == HAL_TIM_ACTIVE_CHANNEL_1) {         // \u5339\u914d\u4e8b\u4ef6\u53d1\u751f\uff0c\u53ef\u4ee5\u5728\u6b64\u52a8\u6001\u4fee\u6539\u4e0b\u4e00\u6b21\u7684 CCR \u503c\u4ee5\u6539\u53d8\u76f8\u4f4d         uint32_t current_ccr = __HAL_TIM_GET_COMPARE(htim, TIM_CHANNEL_1);         __HAL_TIM_SET_COMPARE(htim, TIM_CHANNEL_1, current_ccr + offset);     } }</code></p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#5-input-capture-ic","title":"5. \u8f93\u5165\u6355\u83b7 (Input Capture - IC)","text":"<p>\u8f93\u5165\u6355\u83b7\u6a21\u5f0f\u4e0b\uff0c\u5f53\u901a\u9053\u8f93\u5165\u5f15\u811a\u51fa\u73b0\u6307\u5b9a\u7535\u5e73\u8df3\u53d8\u65f6\uff0c\u5f53\u524dCNT\u7684\u503c\u5c06\u88ab\u9501\u5b58\u5230CCR\u4e2d\uff0c\u53ef\u7528\u4e8e\u6d4b\u91cfPWM\u6ce2\u5f62\u7684\u9891\u7387\u3001\u5360\u7a7a\u6bd4\u3001\u8109\u51b2\u95f4\u9694\u3001\u7535\u5e73\u6301\u7eed\u65f6\u95f4\u7b49\u53c2\u6570</p> <p>\u7528\u4e8e\u6d4b\u91cf\u5916\u90e8\u4fe1\u53f7\u7684\u8109\u5bbd\u3001\u5468\u671f\u3001\u9891\u7387\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_4","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Mode: Channel x \u9009\u4e3a <code>Input Capture direct mode</code>\u3002</p> </li> <li> <p>Configuration -&gt; Parameter Settings:</p> <ul> <li> <p>Counter Settings: \u914d\u7f6e PSC \u4f7f\u5f97\u5b9a\u65f6\u5668\u4e0d\u90a3\u4e48\u5feb\u6ea2\u51fa\uff0c\u4f46\u7cbe\u5ea6\u53c8\u8981\u8db3\u591f\u3002</p> </li> <li> <p>Input Capture Channel x:</p> <ul> <li> <p>Polarity: <code>Rising Edge</code> (\u4e0a\u5347\u6cbf) \u6216 <code>Falling Edge</code> (\u4e0b\u964d\u6cbf)\u3002</p> </li> <li> <p>Selection: <code>Direct</code> (TI1\u6620\u5c04\u5230IC1)\u3002</p> </li> <li> <p>Prescaler: \u8fd9\u91cc\u7684\u9884\u5206\u9891\u662f\u5bf9\u8f93\u5165\u4fe1\u53f7\u7684\u5206\u9891\uff08\u4f8b\u5982\u6bcf8\u4e2a\u6cbf\u6355\u83b7\u4e00\u6b21\uff09\uff0c\u6d4b\u9ad8\u9891\u65f6\u6709\u7528\uff0c\u901a\u5e38\u9009 <code>No prescaler</code>\u3002</p> </li> <li> <p>Input Filter: <code>0</code>~<code>15</code>\uff0c\u6570\u5b57\u6ee4\u6ce2\uff0c\u6ee4\u9664\u8f93\u5165\u4fe1\u53f7\u6bdb\u523a\u3002</p> </li> </ul> </li> </ul> </li> <li> <p>NVIC Settings: \u5fc5\u987b\u5f00\u542f <code>TIMx global interrupt</code>\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_5","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> <p>\u542f\u52a8:</p> <p><code>c HAL_TIM_IC_Start_IT(&amp;htim2, TIM_CHANNEL_1);</code></p> </li> <li> <p>\u6d4b\u91cf\u903b\u8f91 (\u56de\u8c03\u51fd\u6570):</p> <p>```c uint32_t val1 = 0, val2 = 0; uint8_t capture_idx = 0;</p> <p>void HAL_TIM_IC_CaptureCallback(TIM_HandleTypeDef *htim) {     if (htim-&gt;Instance == TIM2 &amp;&amp; htim-&gt;Channel == HAL_TIM_ACTIVE_CHANNEL_1)     {         if (capture_idx == 0) {             val1 = HAL_TIM_ReadCapturedValue(htim, TIM_CHANNEL_1);             capture_idx = 1;         } else {             val2 = HAL_TIM_ReadCapturedValue(htim, TIM_CHANNEL_1);             // \u5904\u7406\u6ea2\u51fa\u903b\u8f91\uff08\u5982\u679c val2 &lt; val1\uff0c\u8bf4\u660e\u4e2d\u95f4\u53d1\u751f\u8fc7 ARR \u6ea2\u51fa\uff09             // \u8ba1\u7b97\u5468\u671f = val2 - val1 (\u9700\u8003\u8651\u6ea2\u51fa\u8865\u507f)             capture_idx = 0;          }     } } ```</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#6-combined-channels-pwm","title":"6. \u7ec4\u5408\u901a\u9053 (Combined Channels) \u4e0e PWM \u8f93\u5165\u6a21\u5f0f","text":"<p>CubeMX \u4e2d\u7684 <code>Combined Channels</code> \u9009\u9879\u901a\u5e38\u7528\u4e8e\u7279\u6b8a\u7684\u786c\u4ef6\u7ed1\u5b9a\u529f\u80fd\uff0c\u6700\u5178\u578b\u7684\u662f PWM Input Mode\uff08\u540c\u65f6\u6d4b\u91cf\u9891\u7387\u548c\u5360\u7a7a\u6bd4\uff09\u6216 Encoder Mode\u3001Hall Sensor Mode\u3002</p> <p>\u4ee5 PWM Input Mode \u4e3a\u4f8b\uff08\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u529f\u80fd\uff0c\u7528\u4e00\u4e2a\u5b9a\u65f6\u5668\u5f15\u811a\u540c\u65f6\u6355\u83b7\u5468\u671f\u548c\u8109\u5bbd\uff09\uff1a</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_5","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Mode:</p> <ul> <li> <p>Slave Mode: <code>Reset Mode</code> (\u590d\u4f4d\u6a21\u5f0f)\u3002</p> </li> <li> <p>Trigger Source: <code>TI1FP1</code> (\u5047\u8bbe\u4fe1\u53f7\u4ece CH1 \u8fdb)\u3002</p> </li> <li> <p>Combined Channels: \u52fe\u9009 <code>PWM Input on CH1</code> (CubeMX \u4f1a\u81ea\u52a8\u628a CH2 \u53d8\u6210 Indirect mode)\u3002</p> </li> </ul> </li> <li> <p>Configuration:</p> <ul> <li> <p>Channel 1 (IC1): Polarity <code>Rising Edge</code> (\u6d4b\u5468\u671f)\uff0cSelection <code>Direct</code>\u3002</p> </li> <li> <p>Channel 2 (IC2): Polarity <code>Falling Edge</code> (\u6d4b\u5360\u7a7a\u6bd4)\uff0cSelection <code>Indirect</code>\u3002</p> </li> <li> <p>\u539f\u7406: \u4e0a\u5347\u6cbf\u89e6\u53d1 Reset\uff08CNT\u6e05\u96f6\uff09\u5e76\u7531 IC1 \u6355\u83b7\uff08\u6b64\u65f6\u662f0\uff0c\u4f46\u4e0b\u4e2a\u5468\u671f\u5c31\u662f\u5468\u671f\u503c\uff09\uff0c\u4e0b\u964d\u6cbf\u7531 IC2 \u6355\u83b7\uff08\u6b64\u65f6\u662f\u8109\u5bbd\u503c\uff09\u3002</p> </li> </ul> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_6","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<pre><code>// \u542f\u52a8 PWM \u8f93\u5165\u6355\u83b7\uff08\u9700\u8981\u540c\u65f6\u542f\u52a8\u4e24\u4e2a\u901a\u9053\uff09\nHAL_TIM_IC_Start_IT(&amp;htim2, TIM_CHANNEL_1);\nHAL_TIM_IC_Start_IT(&amp;htim2, TIM_CHANNEL_2);\n\n// \u5728\u56de\u8c03\u4e2d\u8bfb\u53d6\nuint32_t IC1_Value = HAL_TIM_ReadCapturedValue(&amp;htim2, TIM_CHANNEL_1); // \u5468\u671f (Frequency)\nuint32_t IC2_Value = HAL_TIM_ReadCapturedValue(&amp;htim2, TIM_CHANNEL_2); // \u8109\u5bbd (Duty)\nif (IC1_Value != 0) {\n    float duty = (float)IC2_Value / IC1_Value * 100;\n}\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#7-encoder-mode","title":"7. \u7f16\u7801\u5668\u6a21\u5f0f (Encoder Mode)","text":"<p>\u7528\u4e8e\u8bfb\u53d6\u65cb\u8f6c\u7f16\u7801\u5668\uff08AB\u76f8\uff09\u7684\u4fe1\u53f7\uff0c\u81ea\u52a8\u5904\u7406\u6b63\u53cd\u8f6c\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#cubemx_6","title":"CubeMX \u914d\u7f6e\u7ec6\u8282","text":"<ol> <li> <p>Mode:</p> <ul> <li>Combined Channels: <code>Encoder Mode</code>\u3002</li> </ul> </li> <li> <p>Configuration -&gt; Parameter Settings:</p> <ul> <li> <p>Encoder Mode:</p> <ul> <li> <p><code>Encoder Mode TI1 and TI2</code>: 4\u500d\u9891\u6a21\u5f0f (\u7cbe\u5ea6\u6700\u9ad8\uff0c\u4e0a\u4e0b\u6cbf\u90fd\u8ba1\u6570)\u3002</p> </li> <li> <p><code>Encoder Mode TI1</code>: 2\u500d\u9891\u3002</p> </li> </ul> </li> <li> <p>Counter Period (ARR): \u901a\u5e38\u8bbe\u4e3a\u6700\u5927 <code>65535</code> (16bit) \u6216 <code>4294967295</code> (32bit)\uff0c\u9632\u6b62\u8fc7\u5feb\u6ea2\u51fa\u3002</p> </li> <li> <p>Input Filter: \u5efa\u8bae\u8bbe\u4e3a <code>10</code> \u6216\u66f4\u9ad8\uff0c\u9632\u6b62\u673a\u68b0\u6296\u52a8\u8bef\u5224\u3002</p> </li> </ul> </li> <li> <p>NVIC: \u901a\u5e38\u4e0d\u9700\u8981\u5f00\u542f\u4e2d\u65ad\uff0c\u9664\u975e\u4f60\u8981\u8ba1\u7b97\u591a\u5708\u7edd\u5bf9\u4f4d\u7f6e\uff08\u5904\u7406\u6ea2\u51fa\uff09\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_7","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<ul> <li> <p>\u542f\u52a8:</p> <p><code>c HAL_TIM_Encoder_Start(&amp;htim3, TIM_CHANNEL_ALL);</code></p> </li> <li> <p>\u8bfb\u53d6\u6570\u636e (\u8f6e\u8be2):</p> <p>```c // int16_t \u5f3a\u8f6c\u662f\u4e3a\u4e86\u5904\u7406\u53cd\u8f6c\u65f6\u7684\u8d1f\u6570\uff08\u8865\u7801\uff09 int16_t speed = (int16_t)__HAL_TIM_GET_COUNTER(&amp;htim3); </p> <p>// \u8bfb\u53d6\u540e\u901a\u5e38\u9700\u8981\u6e05\u96f6 CNT \u4ee5\u6d4b\u91cf\u901f\u5ea6\uff08\u5355\u4f4d\u65f6\u95f4\u5185\u7684\u8109\u51b2\u6570\uff09 __HAL_TIM_SET_COUNTER(&amp;htim3, 0);</p> <p>// \u5982\u679c\u662f\u6d4b\u7edd\u5bf9\u4f4d\u7f6e\uff0c\u5219\u4e0d\u6e05\u96f6\uff0c\u9700\u5904\u7406\u6ea2\u51fa\u903b\u8f91 ```</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#8-trigger-source","title":"8. \u89e6\u53d1\u6e90 (Trigger Source) \u4e0e\u4e3b\u4ece\u6a21\u5f0f","text":"<p>\u8fd9\u662f STM32 \u5b9a\u65f6\u5668\u7684\u7075\u9b42\u529f\u80fd\uff0c\u5141\u8bb8\u786c\u4ef6\u81ea\u52a8\u540c\u6b65\uff0c\u65e0\u9700 CPU \u5e72\u9884\u3002</p>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_8","title":"\u57fa\u672c\u6982\u5ff5","text":"<ul> <li> <p>ITR (Internal Trigger): \u5185\u90e8\u89e6\u53d1\uff0c\u5b9a\u65f6\u5668\u7ea7\u8054\uff08\u5982 TIM1 \u6ea2\u51fa\u89e6\u53d1 TIM2 \u542f\u52a8\uff09\u3002</p> </li> <li> <p>ETR (External Trigger): \u5916\u90e8\u5f15\u811a\u89e6\u53d1\u3002</p> </li> <li> <p>TIxFPx: \u8f93\u5165\u6355\u83b7\u5f15\u811a\u4fe1\u53f7\u4f5c\u4e3a\u89e6\u53d1\u6e90\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#tim1-tim2","title":"\u5e38\u89c1\u5e94\u7528\u573a\u666f\uff1aTIM1 \u4e3b\u63a7 TIM2 (\u7ea7\u8054)","text":"<ul> <li> <p>Master (TIM1) \u914d\u7f6e:</p> <ul> <li>Trigger Output (TRGO): <code>Update Event</code> (\u5f53 TIM1 \u53d1\u751f\u66f4\u65b0/\u6ea2\u51fa\u65f6\u53d1\u9001\u4fe1\u53f7)\u3002</li> </ul> </li> <li> <p>Slave (TIM2) \u914d\u7f6e:</p> <ul> <li> <p>Slave Mode: <code>External Clock Mode 1</code> (\u5916\u90e8\u65f6\u949f\u6a21\u5f0f\uff0c\u5373 TIM2 \u8ba1\u6570\u5668\u7684\u9a71\u52a8\u4e0d\u662f\u5185\u90e8\u6676\u632f\uff0c\u800c\u662f TIM1 \u7684\u6ea2\u51fa\u8109\u51b2)\u3002</p> </li> <li> <p>Trigger Source: <code>ITR0</code> (\u67e5\u9605\u624b\u518c\uff0cITR0 \u5bf9\u5e94 TIM1)\u3002</p> </li> </ul> </li> <li> <p>NVIC: \u90fd\u4e0d\u9700\u8981\u5f00\u4e2d\u65ad\uff0c\u786c\u4ef6\u5168\u81ea\u52a8\u3002</p> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#gated-mode","title":"\u5e38\u89c1\u5e94\u7528\u573a\u666f\uff1a\u5916\u90e8\u5f15\u811a\u63a7\u5236\u542f\u505c (Gated Mode)","text":"<ul> <li> <p>Slave (TIMx) \u914d\u7f6e:</p> <ul> <li> <p>Slave Mode: <code>Gated Mode</code>\u3002</p> </li> <li> <p>Trigger Source: <code>TI1FP1</code> (CH1 \u5f15\u811a)\u3002</p> </li> <li> <p>\u6548\u679c: \u5f53 CH1 \u4e3a\u9ad8\u7535\u5e73\u65f6\uff0c\u5b9a\u65f6\u5668\u8ba1\u6570\uff1b\u4f4e\u7535\u5e73\u65f6\uff0c\u5b9a\u65f6\u5668\u6682\u505c\u3002</p> </li> </ul> </li> </ul>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#_9","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u5927\u90e8\u5206\u914d\u7f6e\u5728 MX_TIMx_Init \u4e2d\u81ea\u52a8\u751f\u6210 (HAL_TIM_SlaveConfigSynchro)\u3002</p> <p>\u7528\u6237\u53ea\u9700\u542f\u52a8\u5b9a\u65f6\u5668\uff1a</p> <pre><code>HAL_TIM_Base_Start(&amp;htim1); // \u542f\u52a8\u4e3b\u5b9a\u65f6\u5668\nHAL_TIM_Base_Start(&amp;htim2); // \u542f\u52a8\u4ece\u5b9a\u65f6\u5668\n</code></pre>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#9-cubemx","title":"9. \u4f7f\u7528\u4e86 CubeMX \u540e\u4f60\u4e0d\u7528\u5173\u5fc3\u7684\u7ec6\u8282","text":"<ol> <li> <p>\u65f6\u949f\u4f7f\u80fd (__HAL_RCC_TIMx_CLK_ENABLE):</p> <p>CubeMX \u81ea\u52a8\u5728 stm32fxxx_hal_msp.c \u7684 HAL_TIM_Base_MspInit \u4e2d\u751f\u6210\u3002</p> </li> <li> <p>GPIO \u590d\u7528\u6620\u5c04 (AF):</p> <p>CubeMX \u81ea\u52a8\u914d\u7f6e GPIO \u7684 Alternate Function \u5bc4\u5b58\u5668\uff08\u5982\u5c06 PA6 \u590d\u7528\u4e3a TIM3_CH1\uff09\u3002</p> </li> <li> <p>\u4e2d\u65ad\u6e05\u9664:</p> <p>HAL \u5e93\u7684\u901a\u7528\u4e2d\u65ad\u5904\u7406\u51fd\u6570 HAL_TIM_IRQHandler \u4f1a\u81ea\u52a8\u8bfb\u53d6\u5e76\u6e05\u9664 SR \u5bc4\u5b58\u5668\u7684\u6807\u5fd7\u4f4d\uff0c\u7136\u540e\u624d\u8c03\u7528\u4f60\u7684 Callback\u3002\u4e0d\u8981\u5728 Callback \u91cc\u624b\u52a8\u6e05\u9664\u6807\u5fd7\u4f4d\uff0c\u5426\u5219\u53ef\u80fd\u5bfc\u81f4\u903b\u8f91\u9519\u8bef\u3002</p> </li> <li> <p>\u7ed3\u6784\u4f53\u521d\u59cb\u5316:</p> <p>TIM_HandleTypeDef\u3001TIM_OC_InitTypeDef \u7b49\u7e41\u7410\u7684\u7ed3\u6784\u4f53\u8d4b\u503c\u5168\u90e8\u81ea\u52a8\u5316\u3002</p> </li> </ol>"},{"location":"EmbeddedSoft/STM32_HAL/TIM/#10-api","title":"10. \u5e38\u7528 API \u901f\u67e5\u8868","text":"\u529f\u80fd API \u5907\u6ce8 \u542f\u52a8/\u505c\u6b62 (\u65e0\u4e2d\u65ad) <code>HAL_TIM_Base_Start</code>, <code>HAL_TIM_Base_Stop</code> \u57fa\u672c\u8ba1\u6570 \u542f\u52a8/\u505c\u6b62 (\u5e26\u4e2d\u65ad) <code>HAL_TIM_Base_Start_IT</code>, <code>HAL_TIM_Base_Stop_IT</code> \u5468\u671f\u4efb\u52a1 PWM \u64cd\u4f5c <code>HAL_TIM_PWM_Start</code>, <code>__HAL_TIM_SET_COMPARE</code> \u8c03\u901f/\u8c03\u5149 \u8f93\u5165\u6355\u83b7 <code>HAL_TIM_IC_Start_IT</code>, <code>HAL_TIM_ReadCapturedValue</code> \u6d4b\u9891/\u6d4b\u5bbd \u7f16\u7801\u5668 <code>HAL_TIM_Encoder_Start</code>, <code>__HAL_TIM_GET_COUNTER</code> \u6d4b\u901f/\u4f4d\u7f6e \u8bbe\u7f6e CNT <code>__HAL_TIM_SET_COUNTER(&amp;htimx, value)</code> \u4fee\u6b63\u8ba1\u6570\u503c \u8bbe\u7f6e ARR <code>__HAL_TIM_SET_AUTORELOAD(&amp;htimx, value)</code> \u52a8\u6001\u6539\u5468\u671f # 11. RCC\u65f6\u949f\u6811"},{"location":"EmbeddedSoft/STM32_HAL/connective/","title":"Connective","text":"<p>\u534f\u8bae\u8be6\u60c5\uff1aConnective</p>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/","title":"\u4eba\u5de5\u667a\u80fd\u5b66\u4e60\u968f\u8bb0","text":"<p>\u6211\u662f\u8ddf\u7740\u5434\u6069\u8fbe\u673a\u5668\u5b66\u4e60\u6765\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\u7684\uff0c\u4ee5\u4e0b\u662f\u6211\u7684\u7406\u89e3\u548c\u7ebf\u8def\u3002</p>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#1-python","title":"1. Python\u57fa\u7840","text":"<p>\u8fd9\u4e2a\u4e0d\u7528\u5e9f\u8bdd\uff0c\u80af\u5b9a\u662f\u6700\u5148\u5b66\u7684\u3002</p>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#2","title":"2. \u73af\u5883\u642d\u5efa","text":"<ul> <li> <p>\u5b66\u4f1a\u4f7f\u7528pip\u547d\u4ee4</p> </li> <li> <p>\u90e8\u7f72Python\u73af\u5883</p> </li> <li> <p>\u90e8\u7f72conda\u73af\u5883</p> </li> <li> <p>Python\u6216conda\u4e2d\u73af\u5883(env)\u4e0e\u9694\u79bb\u7684\u6982\u5ff5\uff0c\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u505a\uff0c\u5b66\u4f1a\u73af\u5883\u7ba1\u7406\u3002</p> </li> <li> <p>PyCharm\u548cjupyter notebook\u7684\u57fa\u672c\u4f7f\u7528\u3002</p> </li> </ul>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#3","title":"3. \u673a\u5668\u5b66\u4e60\u57fa\u7840","text":"<p>\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u539f\u7406\uff0c\u5305\u62ec\uff1a</p> <ul> <li> <p>\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u6982\u5ff5</p> </li> <li> <p>\u7ebf\u6027\u56de\u5f52\u6a21\u578b</p> </li> <li> <p>\u6210\u672c\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570</p> </li> <li> <p>\u68af\u5ea6\u3001\u68af\u5ea6\u4e0b\u964d</p> </li> <li> <p>\u4ee3\u7801\u4e0a\u5982\u4f55\u5b9e\u73b0\u68af\u5ea6\u4e0b\u964d</p> </li> <li> <p>\u5b66\u4e60\u7387</p> </li> <li> <p>\u7279\u5f81\uff08features\uff09</p> </li> <li> <p>\u5411\u91cf\u5316\uff0c\u5b66\u4f1anumpy</p> </li> <li> <p>\u7279\u5f81\u7f29\u653e</p> </li> <li> <p>\u591a\u5143\u7ebf\u6027\u56de\u5f52</p> </li> <li> <p>\u591a\u9879\u5f0f\u56de\u5f52</p> </li> <li> <p>\u903b\u8f91\u56de\u5f52(Logistic)</p> </li> <li> <p>\u51b3\u7b56\u8fb9\u754c</p> </li> <li> <p>\u4e8c\u5143\u7c7b\u522b\u2014\u2014Sigmoid\u51fd\u6570\uff08\u6216\u4f7f\u7528\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\uff09</p> </li> <li> <p>\u8fc7\u62df\u5408\u53ca\u89e3\u51b3\u65b9\u6848</p> </li> <li> <p>\u6b63\u5219\u5316\u7684\u5b9e\u73b0</p> </li> </ul>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#4","title":"4. \u6df1\u5ea6\u5b66\u4e60\u90e8\u5206","text":""},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#_1","title":"\u57fa\u672c","text":"<ul> <li> <p>\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\uff08\u6216\u795e\u7ecf\u7f51\u7edc\uff09</p> </li> <li> <p>\u795e\u7ecf\u5143\u7684\u6982\u5ff5</p> </li> <li> <p>\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u3001\u795e\u7ecf\u5143\u6570\u4ee5\u53ca\u5404\u5c42\u4e4b\u95f4\u7684\u8fde\u63a5</p> </li> <li> <p>\u524d\u5411\u4f20\u64ad\u7684\u6982\u5ff5\u4ee5\u53ca\u5b9e\u73b0\u4ee3\u7801</p> </li> <li> <p>\u4f7f\u7528TensorFlow</p> </li> <li> <p>\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u5b9e\u73b0\u65b9\u5f0f\u2014\u2014\u77e9\u9635\u5e76\u884c\u8fd0\u7b97\uff08\u5f20\u91cf\uff09</p> </li> <li> <p>\u6fc0\u6d3b\u51fd\u6570\u7684\u6982\u5ff5\u4ee5\u53ca\u9009\u62e9\u3002\u4e3a\u4ec0\u4e48\u8981\u6fc0\u6d3b\u51fd\u6570\uff1f</p> </li> <li> <p>ReLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u7279\u70b9\u3002\uff08\u63d0\u4f9b\u8f6c\u6298\u70b9\uff09</p> </li> <li> <p>\u5206\u7c7b\u95ee\u9898\u4e4b\u591a\u7c7b\u522b\u7684\u5b9e\u73b0\u2014\u2014Softmax\u6fc0\u6d3b\u51fd\u6570\uff08\u6216\u4f7f\u7528\u591a\u5206\u7c7b\u4ea4\u53c9\u71b5\uff09</p> </li> <li> <p>\u5377\u79ef\u5c42</p> </li> <li> <p>\u8ba1\u7b97\u56fe\u2014\u2014\u53cd\u5411\u4f20\u64ad</p> </li> <li> <p>\u6289\u62e9\uff1b\u6a21\u578b\u8bc4\u4f30\uff1b\u6a21\u578b\u9009\u62e9\u3001\u8bad\u7ec3\u4e0e\u9a8c\u8bc1</p> </li> <li> <p>\u8bca\u65ad\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5982\u4f55\u89e3\u51b3\uff1f</p> </li> <li> <p>\u9519\u8bef\u5206\u6790\u65b9\u6cd5</p> </li> <li> <p>\u6570\u636e\u589e\u52a0\u6280\u5de7\uff0c\u5982\u4f55\u83b7\u5f97\u66f4\u591a\u6570\u636e\uff1f</p> </li> <li> <p>\u8fc1\u79fb\u5b66\u4e60\uff1a\u9884\u8bad\u7ec3&amp;\u5fae\u8c03</p> </li> <li> <p>\u673a\u5668\u5b66\u4e60\u9879\u76ee\u6d41\u7a0b</p> </li> <li> <p>\u6a21\u578b\u8bc4\u4f30\uff1a\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u6df7\u6dc6\u77e9\u9635\u3002</p> </li> <li> <p>\u6743\u8861Precision/recall numbers\uff1a$F_1\\;score$ (\u8c03\u548c\u5e73\u5747\uff0c\u5f3a\u8c03\u8f83\u5c0f\u503c)</p> </li> </ul>"},{"location":"MachineLearning/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/#_2","title":"\u51b3\u7b56\u6811\u6a21\u578b","text":""},{"location":"MachineLearning/Pytorch_Learning/Test/","title":"Test","text":"In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch In\u00a0[2]: Copied! <pre>torch.cuda.is_available()\n</pre> torch.cuda.is_available() Out[2]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/data/","title":"Data","text":"<p>\u5bf9\u6570\u636e\u8fdb\u884c\u5904\u7406\u662f\u8bad\u7ec3\u4e00\u4e2a\u826f\u597d\u6a21\u578b\u7684\u524d\u63d0\u3002</p> <p>\u8be6\u89c1 Pytorch\u5b98\u65b9\u6570\u636e\u96c6\u4e0e\u6570\u636e\u52a0\u8f7d\u5668</p> In\u00a0[4]: Copied! <pre>import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n</pre> import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt   training_data = datasets.FashionMNIST(     root=\"data\",     train=True,     download=True,     transform=ToTensor() )  test_data = datasets.FashionMNIST(     root=\"data\",     train=False,     download=True,     transform=ToTensor() ) <pre>100.0%\n100.0%\n100.0%\n100.0%\n</pre> In\u00a0[5]: Copied! <pre>print(training_data)\n</pre> print(training_data) <pre>Dataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\n</pre> In\u00a0[19]: Copied! <pre>import random\nsample_idx = random.randint(0, len(training_data))\nprint(sample_idx)\n</pre> import random sample_idx = random.randint(0, len(training_data)) print(sample_idx) <pre>26389\n</pre> In\u00a0[\u00a0]: Copied! <pre># this is the same\nsample_idx = torch.randint(len(training_data), size=(1,)).item()\n</pre> # this is the same sample_idx = torch.randint(len(training_data), size=(1,)).item() In\u00a0[20]: Copied! <pre>labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    # sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    sample_idx = random.randint(0, len(training_data))\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n</pre> labels_map = {     0: \"T-Shirt\",     1: \"Trouser\",     2: \"Pullover\",     3: \"Dress\",     4: \"Coat\",     5: \"Sandal\",     6: \"Shirt\",     7: \"Sneaker\",     8: \"Bag\",     9: \"Ankle Boot\", } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1):     # sample_idx = torch.randint(len(training_data), size=(1,)).item()     sample_idx = random.randint(0, len(training_data))     img, label = training_data[sample_idx]     figure.add_subplot(rows, cols, i)     plt.title(labels_map[label])     plt.axis(\"off\")     plt.imshow(img.squeeze(), cmap=\"gray\") plt.show() In\u00a0[21]: Copied! <pre>from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n</pre> from torch.utils.data import DataLoader  train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True) In\u00a0[24]: Copied! <pre># Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")\n</pre> # Display image and label. train_features, train_labels = next(iter(train_dataloader)) print(f\"Feature batch shape: {train_features.size()}\") print(f\"Labels batch shape: {train_labels.size()}\") img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=\"gray\") plt.show() print(f\"Label: {label}\") <pre>Feature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\n</pre> <pre>Label: 6\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/data/","title":"\ud83d\udca1 \u610f\u4e49\u4e00\uff1a\u5de5\u7a0b\u5316\u4e0e\u6548\u7387\uff08\u4e3a\u8bad\u7ec3\u63d0\u901f\uff09\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/data/#1-batching","title":"1. \u6279\u91cf\u5904\u7406 (Batching)\u00b6","text":"<ul> <li>\u95ee\u9898\uff1a \u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u91c7\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d (SGD) \u53ca\u5176\u53d8\u79cd\uff08\u5982 Adam\uff09\u3002\u8fd9\u4e9b\u4f18\u5316\u7b97\u6cd5\u4e0d\u662f\u5bf9\u5355\u4e2a\u6837\u672c\u6c42\u68af\u5ea6\uff0c\u800c\u662f\u5bf9\u4e00\u6279\uff08Batch\uff09\u6837\u672c\u7684\u5e73\u5747\u68af\u5ea6\u8fdb\u884c\u66f4\u65b0\u3002</li> <li><code>DataLoader</code> \u7684\u610f\u4e49\uff1a <code>DataLoader</code> \u8d1f\u8d23\u5c06\u6570\u767e\u4e07\u4e2a\u6837\u672c\u7ec4\u7ec7\u6210\u4e00\u4e2a\u4e2a\u9884\u5b9a\u5927\u5c0f\u7684 Batch\u3002\u8fd9\u4f7f\u5f97 GPU \u6216 CPU \u80fd\u591f\u8fdb\u884c\u9ad8\u6548\u7684\u5e76\u884c\u8ba1\u7b97\uff0c\u5145\u5206\u5229\u7528\u786c\u4ef6\u8d44\u6e90\uff0c\u5927\u5927\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/#2","title":"2. \u5185\u5b58\u7ba1\u7406\u4e0e\u52a0\u8f7d\u4f18\u5316\u00b6","text":"<ul> <li>\u95ee\u9898\uff1a \u8bb8\u591a\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u56fe\u50cf\u548c\u89c6\u9891\uff09\u975e\u5e38\u5e9e\u5927\uff0c\u65e0\u6cd5\u4e00\u6b21\u6027\u5168\u90e8\u52a0\u8f7d\u5230\u5185\u5b58\u6216 GPU \u663e\u5b58\u4e2d\u3002</li> <li><code>Dataset</code> &amp; <code>DataLoader</code> \u7684\u610f\u4e49\uff1a<ul> <li><code>Dataset</code> \u8d1f\u8d23\u5b9a\u4e49\u5982\u4f55\u83b7\u53d6\u5355\u4e2a\u6837\u672c\u53ca\u5176\u6807\u7b7e\u3002</li> <li><code>DataLoader</code> \u5219\u91c7\u7528 \u6309\u9700\u52a0\u8f7d\uff08On-demand Loading\uff09 \u673a\u5236\u3002\u5b83\u53ea\u5728\u9700\u8981\u65f6\uff08\u5373\u5f53\u524d Batch \u8fed\u4ee3\u65f6\uff09\u624d\u4ece\u78c1\u76d8\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8bbe\u7f6e <code>num_workers</code> \u53c2\u6570\u542f\u7528 \u591a\u8fdb\u7a0b (Multiprocessing) \u6765\u63d0\u524d\u5e76\u884c\u52a0\u8f7d\u4e0b\u4e00\u4e2a Batch \u7684\u6570\u636e\uff0c\u8fd9\u88ab\u79f0\u4e3a \u201c\u9884\u53d6 (Prefetching)\u201d\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684 \u201c\u6570\u636e\u74f6\u9888\u201d\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/#3-shuffling","title":"3. \u6570\u636e\u6df7\u6d17 (Shuffling)\u00b6","text":"<ul> <li>\u610f\u4e49\uff1a \u5728\u6bcf\u4e2a Epoch \u5f00\u59cb\u524d\uff0c<code>DataLoader</code> \u901a\u5e38\u4f1a\u5bf9\u6570\u636e\u987a\u5e8f\u8fdb\u884c\u968f\u673a\u6253\u4e71 (Shuffling)\u3002\u8fd9\u5bf9\u4e8e SGD \u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u80fd\u786e\u4fdd\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u88ab\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u987a\u5e8f\u6a21\u5f0f\u6240\u8bef\u5bfc\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/","title":"\ud83d\udca1 \u610f\u4e49\u4e8c\uff1a\u6570\u636e\u62bd\u8c61\u4e0e\u7075\u6d3b\u6027\uff08\u5b9a\u5236\u5316\u6570\u636e\u6d41\uff09\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/data/#4-decoupling","title":"4. \u62bd\u8c61\u5316\u548c\u89e3\u8026 (Decoupling)\u00b6","text":"<ul> <li><code>Dataset</code> \u7684\u610f\u4e49\uff1a \u5b83\u5c06 \u201c\u6570\u636e\u8bfb\u53d6\u548c\u9884\u5904\u7406\u201d \u7684\u903b\u8f91\u4e0e \u201c\u6a21\u578b\u8bad\u7ec3\u201d \u7684\u903b\u8f91\u5f7b\u5e95\u5206\u79bb\u3002<ul> <li>\u65e0\u8bba\u662f\u4ece\u6587\u4ef6\u7cfb\u7edf\u8bfb\u53d6\u56fe\u50cf\u3001\u4ece CSV \u6587\u4ef6\u8bfb\u53d6\u6587\u672c\uff0c\u8fd8\u662f\u4ece\u6570\u636e\u5e93\u83b7\u53d6\u6570\u636e\uff0c\u8fd9\u4e9b\u5e95\u5c42\u7ec6\u8282\u90fd\u88ab\u5c01\u88c5\u5728 <code>Dataset</code> \u7684 <code>__getitem__</code> \u65b9\u6cd5\u4e2d\u3002</li> <li>\u8fd9\u6837\uff0c\u5f53\u60a8\u66f4\u6362\u6570\u636e\u96c6\u65f6\uff0c\u53ea\u9700\u8981\u4fee\u6539 <code>Dataset</code> \u7684\u5b9e\u73b0\uff0c\u800c\u6838\u5fc3\u7684\u8bad\u7ec3\u5faa\u73af\u4ee3\u7801\uff08\u6a21\u578b\u5b9a\u4e49\u3001\u4f18\u5316\u5668\u3001\u8bad\u7ec3\u6b65\u9aa4\uff09\u51e0\u4e4e\u4e0d\u9700\u8981\u53d8\u52a8\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u590d\u7528\u6027\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/#5-transforms","title":"5. \u6570\u636e\u8f6c\u6362\u4e0e\u589e\u5f3a (Transforms)\u00b6","text":"<ul> <li>\u610f\u4e49\uff1a <code>Dataset</code> \u662f\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\uff08\u5982\u56fe\u50cf\u7684\u968f\u673a\u88c1\u526a\u3001\u7ffb\u8f6c\u3001\u6807\u51c6\u5316\u7b49\uff09\u7684\u7406\u60f3\u573a\u6240\u3002<ul> <li>\u8fd9\u4e9b\u64cd\u4f5c\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6bcf\u4e2a\u6837\u672c\uff0c\u786e\u4fdd\u6240\u6709\u8fdb\u5165\u6a21\u578b\u7684 Batch \u90fd\u7ecf\u8fc7\u4e86\u6b63\u786e\u7684\u8f6c\u6362\u3002</li> <li>\u6570\u636e\u589e\u5f3a\u662f\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3001\u9632\u6b62\u8fc7\u62df\u5408\u7684\u5173\u952e\u624b\u6bb5\u4e4b\u4e00\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/#6-collate","title":"6. \u81ea\u52a8\u6574\u7406 (Collate)\u00b6","text":"<ul> <li><code>DataLoader</code> \u7684\u610f\u4e49\uff1a \u5373\u4f7f\u6240\u6709\u6837\u672c\u90fd\u662f\u4ece <code>Dataset</code> \u83b7\u53d6\u7684\uff0c\u5b83\u4eec\u4e5f\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u6574\u7406\u624d\u80fd\u5806\u53e0\u6210\u4e00\u4e2a\u5f20\u91cf (Tensor Batch)\u3002<ul> <li><code>collate_fn</code> \u53c2\u6570\u5141\u8bb8\u60a8\u81ea\u5b9a\u4e49\u5982\u4f55\u5c06\u5355\u4e2a\u6837\u672c\uff08\u4f8b\u5982\u4e0d\u540c\u957f\u5ea6\u7684\u6587\u672c\u5e8f\u5217\u6216\u5d4c\u5957\u7ed3\u6784\uff09\u6574\u7406\u6210\u4e00\u4e2a\u7edf\u4e00\u7684 Batch \u5f20\u91cf\uff0c\u6ee1\u8db3 PyTorch \u6a21\u578b\u7684\u8f93\u5165\u8981\u6c42\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/data/#dataset-dataloader","title":"\u603b\u7ed3\uff1a<code>Dataset</code> &amp; <code>DataLoader</code> \u7684\u6838\u5fc3\u4ef7\u503c\u00b6","text":"\u7ec4\u4ef6 \u6838\u5fc3\u804c\u80fd \u5bf9\u8bad\u7ec3\u7684\u610f\u4e49 <code>Dataset</code> \u6570\u636e\u62bd\u8c61\u4e0e\u5355\u6837\u672c\u5904\u7406 \u5c01\u88c5\u6570\u636e\u6765\u6e90\u548c\u9884\u5904\u7406\u903b\u8f91\uff0c\u5b9e\u73b0\u4ee3\u7801\u89e3\u8026\u548c\u6570\u636e\u589e\u5f3a\u3002 <code>DataLoader</code> \u9ad8\u6548\u6570\u636e\u6d41\u4e0e\u6279\u91cf\u5904\u7406 \u5b9e\u73b0\u6279\u91cf\u52a0\u8f7d\u3001\u6570\u636e\u6df7\u6d17\u3001\u591a\u8fdb\u7a0b\u9884\u53d6\uff0c\u4fdd\u8bc1 GPU/CPU \u7684\u9ad8\u6548\u5229\u7528\u3002 <p>\u7b80\u5355\u6765\u8bf4\uff0c<code>Dataset</code> \u89e3\u51b3\u4e86\u201c\u5982\u4f55\u5904\u7406\u5355\u4e2a\u6570\u636e\u201d\u7684\u95ee\u9898\uff0c\u800c <code>DataLoader</code> \u89e3\u51b3\u4e86\u201c\u5982\u4f55\u9ad8\u6548\u3001\u7a33\u5b9a\u5730\u5c06\u6279\u6b21\u6570\u636e\u5582\u7ed9\u6a21\u578b\u201d\u7684\u95ee\u9898\u3002 \u5b83\u4eec\u662f\u8fde\u63a5\u539f\u59cb\u6570\u636e\u548c\u9ad8\u6027\u80fd\u8bad\u7ec3\u6d41\u7a0b\u7684\u6865\u6881\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/data/","title":"Data","text":"In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n</pre> from torch.utils.data import Dataset from PIL import Image import os In\u00a0[\u00a0]: Copied! <pre>class MyDataset(Dataset):\n    def __init__(self, root_dir,label_dir):\n        \"\"\"\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u7f6e\u6570\u636e\u96c6\u53c2\u6570\u548c\u52a0\u8f7d\u6570\u636e\"\"\"\n        super().__init__()\n        # \u521d\u59cb\u5316\u4ee3\u7801\n        self.root_dir = root_dir\n        self.label_dir = label_dir\n        self.path = os.path.join(self.root_dir,self.label_dir)\n        self.img_path = os.listdir(self.path)\n\n    def __len__(self):\n        \"\"\"\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\"\"\"\n        return len(self.img_path)\n\n    def __getitem__(self, idx):\n        \"\"\"\u6839\u636e\u7d22\u5f15\u8fd4\u56de\u4e00\u4e2a\u6837\u672c\"\"\"\n        img_name = self.img_path[idx]\n        img_item_path = os.path.join(self.root_dir,self.label_dir, img_name)\n        img = Image.open(img_item_path)\n        label = self.label_dir\n        return img, label\n</pre> class MyDataset(Dataset):     def __init__(self, root_dir,label_dir):         \"\"\"\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u7f6e\u6570\u636e\u96c6\u53c2\u6570\u548c\u52a0\u8f7d\u6570\u636e\"\"\"         super().__init__()         # \u521d\u59cb\u5316\u4ee3\u7801         self.root_dir = root_dir         self.label_dir = label_dir         self.path = os.path.join(self.root_dir,self.label_dir)         self.img_path = os.listdir(self.path)      def __len__(self):         \"\"\"\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\"\"\"         return len(self.img_path)      def __getitem__(self, idx):         \"\"\"\u6839\u636e\u7d22\u5f15\u8fd4\u56de\u4e00\u4e2a\u6837\u672c\"\"\"         img_name = self.img_path[idx]         img_item_path = os.path.join(self.root_dir,self.label_dir, img_name)         img = Image.open(img_item_path)         label = self.label_dir         return img, label In\u00a0[\u00a0]: Copied! <pre># \u53d6\u6570\u636e\u96c6\nants_dataset = MyDataset('dataset/train',\"ants\")\nbees_dataset = MyDataset('dataset/train',\"bees\")\n</pre> # \u53d6\u6570\u636e\u96c6 ants_dataset = MyDataset('dataset/train',\"ants\") bees_dataset = MyDataset('dataset/train',\"bees\") In\u00a0[\u00a0]: Copied! <pre>train_dataset = ants_dataset + bees_dataset\n</pre> train_dataset = ants_dataset + bees_dataset"},{"location":"MachineLearning/Pytorch_Learning/nn/","title":"Nn","text":"In\u00a0[1]: Copied! <pre>import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms In\u00a0[2]: Copied! <pre>device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</pre> device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\" print(f\"Using {device} device\") <pre>Using cuda device\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/tensor/","title":"Tensor","text":"<p>\u8be6\u89c1 Pytorch\u5b98\u65b9Tensor\u6559\u7a0b</p> <p>import torch import numpy as np</p> In\u00a0[8]: Copied! <pre>one = torch.ones((3,4))\nprint(one)\n</pre> one = torch.ones((3,4)) print(one) <pre>tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n</pre> In\u00a0[15]: Copied! <pre>one = torch.ones(3,4) #no difference\nprint(one)\n</pre> one = torch.ones(3,4) #no difference print(one) <pre>tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n</pre> In\u00a0[16]: Copied! <pre># CPU \u4e0a\u7684\u5f20\u91cf\u548c NumPy \u6570\u7ec4\u53ef\u4ee5\u5171\u4eab\u5b83\u4eec\u7684\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u5176\u4e2d\u4e00\u4e2a\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a\u3002\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n</pre> # CPU \u4e0a\u7684\u5f20\u91cf\u548c NumPy \u6570\u7ec4\u53ef\u4ee5\u5171\u4eab\u5b83\u4eec\u7684\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u5176\u4e2d\u4e00\u4e2a\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a\u3002 t = torch.ones(5) print(f\"t: {t}\") n = t.numpy() print(f\"n: {n}\") <pre>t: tensor([1., 1., 1., 1., 1.])\nn: [1. 1. 1. 1. 1.]\n</pre> In\u00a0[17]: Copied! <pre>t.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n</pre> t.add_(1) print(f\"t: {t}\") print(f\"n: {n}\") <pre>t: tensor([2., 2., 2., 2., 2.])\nn: [2. 2. 2. 2. 2.]\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#tensor","title":"Tensor\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/tensor/#tensor","title":"\ud83d\udcd0 \u5f20\u91cf (Tensor) \u8bb2\u89e3\u00b6","text":"<p>\u5f20\u91cf (Tensor) \u662f PyTorch\u3001TensorFlow \u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\uff0c\u5b83\u5728\u6570\u5b66\u4e0a\u662f\u6807\u91cf\u3001\u5411\u91cf\u548c\u77e9\u9635\u6982\u5ff5\u7684\u6cdb\u5316\u3002</p> <p>\u7b80\u5355\u6765\u8bf4\uff0c\u5f20\u91cf\u5c31\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5bb9\u5668\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#1-rank","title":"1. \u5f20\u91cf\u7684\u6570\u5b66\u6982\u5ff5\uff1a\u7ef4\u5ea6 (Rank)\u00b6","text":"<p>\u5f20\u91cf\u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u4e00\u4e2a\u591a\u7ef4\u6570\u7ec4\uff0c\u5b83\u7684\u201c\u7ef4\u5ea6\u201d\u6216\u201c\u9636\u201d\uff08\u5728\u6570\u5b66\u548c\u7269\u7406\u4e2d\u79f0\u4e3a Rank\uff09\u51b3\u5b9a\u4e86\u5b83\u80fd\u8868\u793a\u7684\u6570\u636e\u590d\u6742\u7a0b\u5ea6\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#-rank-shape","title":"- \u7ef4\u6570 (Rank) \u548c\u5f62\u72b6 (Shape)\u00b6","text":"<ul> <li>\u7ef4\u6570 (Rank)\uff1a \u6307\u7684\u662f\u5f20\u91cf\u4e2d \u8f74 (Axes) \u7684\u6570\u91cf\uff0c\u4e5f\u5c31\u662f\u5f20\u91cf\u5f62\u72b6 (Shape) \u5143\u7ec4\u4e2d\u5143\u7d20\u7684\u4e2a\u6570\u3002<ul> <li>\u4f8b\u5982\uff0c\u5f62\u72b6\u4e3a <code>(1, 3, 7)</code> \u7684\u5f20\u91cf\uff0c\u5176\u7ef4\u6570\u662f 3\u3002</li> </ul> </li> <li>\u5f62\u72b6 (Shape)\uff1a \u4e00\u4e2a\u5143\u7ec4\uff0c\u63cf\u8ff0\u4e86\u5f20\u91cf\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5927\u5c0f\u3002<ul> <li>\u5f62\u72b6 <code>(D0, D1, D2, ...)</code> \u4e2d\u7684 $D_i$ \u5c31\u662f\u7b2c $i$ \u7ef4\u7684\u5927\u5c0f\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#-","title":"- \u9ed8\u8ba4\u7ef4\u5ea6\u987a\u5e8f\u00b6","text":"<p>PyTorch \u5f20\u91cf\u9ed8\u8ba4\u7684\u7ef4\u5ea6\u987a\u5e8f\u91c7\u7528 \u4ece\u5de6\u5230\u53f3\u3001\u4ece\u5916\u5230\u5185 \u7684\u7d22\u5f15\u89c4\u5219\uff1a</p> \u7ef4\u5ea6\u540d\u79f0 \u7d22\u5f15\u503c (\u5728 <code>dim</code> \u53c2\u6570\u4e2d) \u9ed8\u8ba4\u542b\u4e49 (\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d) \u5f62\u72b6\u5143\u7ec4\u4e2d\u7684\u4f4d\u7f6e \u7b2c 0 \u7ef4 <code>dim=0</code> Batch Size\uff08\u6279\u91cf\u5927\u5c0f\uff09 \u6700\u5de6\u8fb9\u7b2c\u4e00\u4e2a \u7b2c 1 \u7ef4 <code>dim=1</code> Channels\uff08\u901a\u9053\u6570\uff0c\u5982\u56fe\u50cf $R, G, B$\uff09 \u7b2c\u4e8c\u4e2a \u7b2c 2 \u7ef4 <code>dim=2</code> Height\uff08\u9ad8\u5ea6\uff09 \u7b2c\u4e09\u4e2a \u7b2c 3 \u7ef4 `dim=3$ Width\uff08\u5bbd\u5ea6\uff09 \u7b2c\u56db\u4e2a ... $dim=k$ $k$ \u7ef4 $k+1$ \u4e2a"},{"location":"MachineLearning/Pytorch_Learning/tensor/#-dim","title":"- \u7ef4\u5ea6 (Dim) \u7684\u7406\u89e3\u548c\u5e94\u7528\u00b6","text":"<p>\u5f53\u60a8\u5728 PyTorch \u4e2d\u4f7f\u7528\u6d89\u53ca <code>dim</code> \u53c2\u6570\u7684\u51fd\u6570\u65f6\uff0c\u8fd9\u4e2a\u53c2\u6570\u544a\u8bc9\u51fd\u6570\u60a8\u5e0c\u671b\u6cbf\u7740\u54ea\u4e2a\u8f74\u8fdb\u884c\u64cd\u4f5c\u3002</p> PyTorch \u51fd\u6570 <code>dim</code> \u7684\u610f\u4e49 \u793a\u4f8b <code>torch.sum(tensor, dim=1)</code> \u6cbf\u7740 <code>dim=1</code> \u7684\u65b9\u5411 \u5bf9\u6570\u636e\u8fdb\u884c\u6c42\u548c\uff0c\u7136\u540e\u8fd9\u4e2a\u7ef4\u5ea6\u4f1a\u88ab\u538b\u7f29\u6216\u79fb\u9664\u3002 \u5982\u679c\u5f20\u91cf\u5f62\u72b6\u662f <code>(1, 3, 7)</code>\uff0c\u6cbf\u7740 <code>dim=1</code> \u6c42\u548c\u540e\uff0c\u65b0\u5f20\u91cf\u5f62\u72b6\u4f1a\u53d8\u4e3a <code>(1, 7)</code>\u3002 <code>torch.cat((t1, t2), dim=0)</code> \u6cbf\u7740 <code>dim=0</code> \u7684\u65b9\u5411 \u5c06\u5f20\u91cf\u8fdb\u884c\u62fc\u63a5\u3002 \u5982\u679c $t1$ \u662f <code>(1, 3, 7)</code>\uff0c$t2$ \u4e5f\u662f <code>(1, 3, 7)</code>\uff0c\u6cbf\u7740 <code>dim=0</code> \u62fc\u63a5\u540e\uff0c\u65b0\u5f20\u91cf\u5f62\u72b6\u53d8\u4e3a <code>(2, 3, 7)</code>\u3002 <code>torch.unsqueeze(tensor, dim=1)</code> \u5728 <code>dim=1</code> \u7684\u4f4d\u7f6e \u63d2\u5165\u4e00\u4e2a\u65b0\u7684\u3001\u5927\u5c0f\u4e3a 1 \u7684\u7ef4\u5ea6\u3002 \u5982\u679c\u5f20\u91cf\u5f62\u72b6\u662f <code>(1, 7)</code>\uff0c\u5728 <code>dim=1</code> \u63d2\u5165\u540e\uff0c\u65b0\u5f20\u91cf\u5f62\u72b6\u53d8\u4e3a <code>(1, 1, 7)</code>\u3002 <p>\u7b80\u8bb0\u6cd5\u5219\uff1a <code>dim=k</code> \u59cb\u7ec8\u6307\u5411\u5f62\u72b6\u5143\u7ec4\u4e2d\u7b2c $k+1$ \u4e2a\u6570\u5b57\u6240\u4ee3\u8868\u7684\u8f74\u3002</p> \u9636/\u7ef4\u5ea6 (Rank) \u5f62\u72b6 (Shape) \u63cf\u8ff0 \u793a\u4f8b\u6570\u636e 0 \u9636 <code>()</code> \u6807\u91cf (Scalar)\uff1a\u4e00\u4e2a\u7eaf\u91cf\u3002 <code>5</code>, $\\pi$, \u635f\u5931\u51fd\u6570\u503c <code>loss</code> 1 \u9636 <code>(n,)</code> \u5411\u91cf (Vector)\uff1a\u4e00\u7ef4\u6570\u7ec4\u3002 \u8bcd\u5d4c\u5165 (Word Embedding)\uff0c$\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ 2 \u9636 <code>(n, m)</code> \u77e9\u9635 (Matrix)\uff1a\u4e8c\u7ef4\u6570\u7ec4\u3002 \u56fe\u50cf\u7684\u7070\u5ea6\u503c\uff0c\u7ebf\u6027\u5c42\u6743\u91cd $\\mathbf{W}$ 3 \u9636 <code>(n, m, k)</code> \u4e09\u9636\u5f20\u91cf\uff1a\u4e09\u7ef4\u6570\u7ec4\u3002 \u5f69\u8272\u56fe\u50cf (\u9ad8 x \u5bbd x \u989c\u8272\u901a\u9053)\uff0c\u89c6\u9891\u5e27 4 \u9636 <code>(n, m, k, l)</code> \u56db\u9636\u5f20\u91cf\uff1a\u56db\u7ef4\u6570\u7ec4\u3002 \u6279\u91cf (Batch) \u7684\u5f69\u8272\u56fe\u50cf (\u6279\u91cf\u5927\u5c0f x \u989c\u8272\u901a\u9053 x \u9ad8 x \u5bbd)"},{"location":"MachineLearning/Pytorch_Learning/tensor/#2","title":"2. \u5f20\u91cf\u7684\u4e3b\u8981\u5c5e\u6027\u00b6","text":"<p>\u5728 PyTorch \u7b49\u6846\u67b6\u4e2d\uff0c\u4e00\u4e2a\u5f20\u91cf\u5bf9\u8c61\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u51e0\u4e2a\u5173\u952e\u5c5e\u6027\uff1a</p> \u5c5e\u6027 \u63cf\u8ff0 \u793a\u4f8b Data (\u6570\u636e) \u5b58\u50a8\u5728\u5f20\u91cf\u4e2d\u7684\u5b9e\u9645\u6570\u503c\u3002 <code>[[1, 2], [3, 4]]</code> Shape (\u5f62\u72b6) \u5b9a\u4e49\u4e86\u5f20\u91cf\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5927\u5c0f\u3002 2x2 \u77e9\u9635\u7684\u5f62\u72b6\u662f <code>(2, 2)</code> Dtype (\u6570\u636e\u7c7b\u578b) \u5b58\u50a8\u7684\u5143\u7d20\u7684\u6570\u636e\u7c7b\u578b\u3002 <code>torch.float32</code>, <code>torch.int64</code> \u7b49\u3002 Device (\u8bbe\u5907) \u5f20\u91cf\u5b58\u50a8\u7684\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u662f CPU \u6216 GPU\u3002 <code>cpu</code> \u6216 <code>cuda:0</code> <code>requires_grad</code> \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u9700\u8981\u8ba1\u7b97\u8be5\u5f20\u91cf\u7684\u68af\u5ea6\uff08\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\uff09\u3002 <code>True</code> \u6216 <code>False</code>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#3","title":"3. \u5f20\u91cf\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u00b6","text":"<p>\u5f20\u91cf\u662f\u8868\u793a\u6240\u6709\u6570\u636e\u7684\u901a\u7528\u8bed\u8a00\uff1a</p> \u6570\u636e\u7c7b\u578b \u5bf9\u5e94\u7684\u5f20\u91cf\u5f62\u72b6 (\u5e38\u89c1) \u63cf\u8ff0 \u56fe\u50cf <code>(C, H, W)</code> \u6216 <code>(H, W, C)</code> \u901a\u9053\u6570 (C)\u3001\u9ad8 (H)\u3001\u5bbd (W)\u3002 \u6587\u672c (\u6279\u91cf) <code>(Batch_size, Seq_len, Embedding_dim)</code> \u6279\u91cf\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u3002 \u8bad\u7ec3\u6279\u91cf (Batch) <code>(Batch_size, C, H, W)</code> \u6df1\u5ea6\u5b66\u4e60\u4e2d\u5904\u7406\u6570\u636e\u7684\u4e3b\u6d41\u5f62\u5f0f\uff0c\u4e00\u6b21\u5904\u7406\u591a\u5f20\u56fe\u7247\u3002 \u6a21\u578b\u53c2\u6570 \u5404\u79cd\u5f62\u72b6 \u6a21\u578b\u7684\u6743\u91cd $\\mathbf{W}$ \u548c\u504f\u7f6e $\\mathbf{b}$ \u90fd\u662f\u5f20\u91cf\u3002"},{"location":"MachineLearning/Pytorch_Learning/tensor/#4-pytorch","title":"4. PyTorch \u5f20\u91cf\u7684\u7279\u70b9\uff1a\u81ea\u52a8\u5fae\u5206\u00b6","text":"<p>PyTorch \u7684\u5f20\u91cf (<code>torch.Tensor</code>) \u6700\u5f3a\u5927\u7684\u7279\u6027\u662f\u5b83\u96c6\u6210\u4e86\u81ea\u52a8\u5fae\u5206 (Autograd) \u7cfb\u7edf\u3002</p> <ul> <li>\u5f53\u4f60\u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u5e76\u8bbe\u7f6e <code>requires_grad=True</code> \u65f6\uff0cPyTorch \u4f1a\u5f00\u59cb\u8ddf\u8e2a\u6240\u6709\u4f5c\u7528\u5728\u8fd9\u4e2a\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002</li> <li>\u8fd9\u4f7f\u5f97\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u53ef\u4ee5\u8f7b\u677e\u5730\u901a\u8fc7<code>.backward()</code> \u65b9\u6cd5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u8fd9\u662f\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u548c\u4f18\u5316\u6a21\u578b\u7684\u57fa\u7840\u3002</li> </ul> <pre>import torch\n\n# \u521b\u5efa\u4e00\u4e2a\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684 2 \u9636\u5f20\u91cf (\u77e9\u9635)\nx = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\nprint(f\"\u5f20\u91cf\u7684\u5f62\u72b6: {x.shape}\")\nprint(f\"\u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6: {x.requires_grad}\")\n\n# \u8fdb\u884c\u4e00\u4e9b\u64cd\u4f5c\ny = x + 2\nz = y * y * 3\nout = z.mean()\n\n# out.backward()  # \u8c03\u7528 backward \u4f1a\u81ea\u52a8\u8ba1\u7b97\u6240\u6709 require_grad=True \u7684\u5f20\u91cf\u7684\u68af\u5ea6\n</pre> <p>\u603b\u7ed3: \u5f20\u91cf\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6570\u636e\u7684\u8f7d\u4f53\uff0c\u5b83\u4e0d\u4ec5\u80fd\u5b58\u50a8\u591a\u7ef4\u6570\u636e\uff0c\u8fd8\u96c6\u6210\u4e86\u81ea\u52a8\u5fae\u5206\u80fd\u529b\uff0c\u662f\u6784\u5efa\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u77f3\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/tensor/#pytorch","title":"PyTorch \u5f20\u91cf\u5e38\u89c1\u529f\u80fd\u6620\u5c04\u8868\u00b6","text":"\u529f\u80fd\u7c7b\u522b \u6570\u5b66/NumPy \u6982\u5ff5 PyTorch \u5bf9\u5e94\u51fd\u6570/\u65b9\u6cd5 \u5e38\u89c1\u7528\u9014\u548c\u8bf4\u660e I. \u521b\u5efa\u4e0e\u521d\u59cb\u5316 \u96f6\u77e9\u9635 <code>torch.zeros(shape)</code> \u521b\u5efa\u4e00\u4e2a\u6240\u6709\u5143\u7d20\u90fd\u4e3a 0 \u7684\u5f20\u91cf\u3002 '1'\u77e9\u9635 <code>torch.ones(n, m)</code> \u521b\u5efa\u4e00\u4e2a $n \\times m$ \u7684\u5168'1'\u77e9\u9635\u3002 \u5355\u4f4d\u77e9\u9635 <code>torch.eye(n, m)</code> \u521b\u5efa\u4e00\u4e2a $n \\times m$ \u7684\u5355\u4f4d\u77e9\u9635\u3002 \u968f\u673a\u6570 <code>torch.rand(shape)</code> \u521b\u5efa\u670d\u4ece $[0, 1)$ \u5747\u5300\u5206\u5e03\u7684\u5f20\u91cf\u3002 \u6b63\u6001\u5206\u5e03\u968f\u673a\u6570 <code>torch.randn(shape)</code> \u521b\u5efa\u670d\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03 ($\\mu=0, \\sigma=1$) \u7684\u5f20\u91cf\u3002 \u57fa\u4e8e\u73b0\u6709\u5f20\u91cf\u521b\u5efa <code>torch.ones_like(tensor)</code> \u6839\u636e\u53e6\u4e00\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u521b\u5efa\u65b0\u5f20\u91cf\u3002 \u4ece Python/NumPy \u521b\u5efa <code>torch.tensor(data)</code>, <code>torch.from_numpy(ndarray)</code> \u5c06 Python \u5217\u8868\u3001\u6570\u7ec4\u6216 NumPy \u8f6c\u6362\u6210\u5f20\u91cf\u3002 II. \u5f62\u72b6\u548c\u7ef4\u5ea6\u64cd\u4f5c \u5f62\u72b6\u67e5\u8be2 <code>tensor.shape</code> \u6216 <code>tensor.size()</code> \u8fd4\u56de\u5f20\u91cf\u7684\u5f62\u72b6\uff08\u5143\u7ec4\uff09\u3002 \u7ef4\u5ea6\u91cd\u5851 (NumPy: <code>reshape</code>) <code>tensor.view(new_shape)</code> \u6216 <code>tensor.reshape(new_shape)</code> \u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6\u3002<code>view</code> \u8981\u6c42\u5185\u5b58\u8fde\u7eed\uff0c<code>reshape</code> \u66f4\u7075\u6d3b\u3002 \u589e\u52a0/\u51cf\u5c11\u7ef4\u5ea6 <code>torch.squeeze()</code>, <code>torch.unsqueeze(dim)</code> \u79fb\u9664\uff08\u7ef4\u5ea6\u4e3a 1\uff09\u6216\u589e\u52a0\u7ef4\u5ea6\uff08\u5982\u5c06\u5411\u91cf\u8f6c\u4e3a Batch \uff09\u3002 \u7ef4\u5ea6\u8f6c\u7f6e (NumPy: <code>T</code>) <code>tensor.permute(dims)</code>, <code>tensor.transpose(dim0, dim1)</code> \u4ea4\u6362\u5f20\u91cf\u7684\u7ef4\u5ea6\u4f4d\u7f6e\uff08\u5e38\u7528\u4e8e $HWC \\to CWH$\uff09\u3002 \u5f20\u91cf\u62fc\u63a5 (NumPy: <code>concatenate</code>) <code>torch.cat((t1, t2), dim=0)</code> \u6cbf\u6307\u5b9a\u7ef4\u5ea6\u5c06\u591a\u4e2a\u5f20\u91cf\u62fc\u63a5\u8d77\u6765\uff08\u7ef4\u5ea6\u589e\u52a0\uff09\u3002 \u5f20\u91cf\u5806\u53e0 (NumPy: <code>stack</code>) <code>torch.stack((t1, t2), dim=0)</code> \u6cbf\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u5c06\u591a\u4e2a\u5f20\u91cf\u5806\u53e0\u8d77\u6765\uff08\u7ef4\u5ea6\u6570\u91cf\u589e\u52a0\uff09\u3002 \u57fa\u4e8e\u7d22\u5f15\u7684\u5143\u7d20\u5206\u6563\u64cd\u4f5c <code>target.scatter_(dim, index, src,value (optional))</code> \u5c06\u4e00\u4e2a src \u5f20\u91cf\uff08\u6e90\u6570\u636e\uff09\u4e2d\u7684\u503c\uff0c\u6839\u636e\u4e00\u4e2a index \u5f20\u91cf\uff08\u7d22\u5f15\uff09\u7684\u6307\u793a\uff0c\u5206\u6563\u5199\u5165\uff08\u8986\u76d6\u6216\u7d2f\u52a0\uff09\u5230\u8c03\u7528\u8be5\u65b9\u6cd5\u7684 \u76ee\u6807\u5f20\u91cf\uff08self\uff0c\u5373 target\uff09\u7684\u6307\u5b9a\u4f4d\u7f6e;\u5982\u679c\u4e0d\u4f7f\u7528 src \u5f20\u91cf\uff0c\u53ef\u4ee5\u4f20\u5165\u4e00\u4e2a\u6807\u91cf\u503c\uff0c\u6240\u6709\u5199\u5165\u64cd\u4f5c\u90fd\u4f7f\u7528\u8fd9\u4e2a\u56fa\u5b9a\u503c\u3002 III. \u6570\u5b66\u8fd0\u7b97 \u5143\u7d20\u7ea7\u52a0\u51cf\u4e58\u9664 <code>t1 + t2</code>, <code>t1 * t2</code>, <code>torch.add(t1, t2, out=t3)</code> \u5bf9\u5e94\u5143\u7d20\u8fdb\u884c\u8fd0\u7b97\u3002 \u77e9\u9635\u4e58\u6cd5 (NumPy: <code>@</code> \u6216 <code>dot</code>) <code>torch.matmul(t1, t2)</code> \u6216 <code>t1 @ t2</code> \u9002\u7528\u4e8e\u77e9\u9635\u3001\u5411\u91cf\u4e58\u6cd5\uff0c\u4e5f\u652f\u6301 Batch \u77e9\u9635\u4e58\u6cd5\u3002 \u5411\u91cf\u5185\u79ef (NumPy: <code>dot</code>) <code>torch.dot(vec1, vec2)</code> \u4ec5\u7528\u4e8e\u4e00\u7ef4\u5f20\u91cf\uff08\u5411\u91cf\uff09\u3002 \u6c42\u548c (NumPy: <code>sum</code>) <code>torch.sum(tensor, dim=None)</code> \u5bf9\u6240\u6709\u5143\u7d20\u6216\u6307\u5b9a\u7ef4\u5ea6\u6c42\u548c\u3002 \u5e73\u5747\u503c <code>torch.mean(tensor, dim=None)</code> \u5bf9\u6240\u6709\u5143\u7d20\u6216\u6307\u5b9a\u7ef4\u5ea6\u6c42\u5e73\u5747\u503c\u3002 IV. \u68af\u5ea6\u63a7\u5236\u4e0e\u8bbe\u5907\u7ba1\u7406 \u8bbe\u5907\u67e5\u8be2 <code>tensor.device</code> \u8fd4\u56de\u5f20\u91cf\u6240\u5728\u7684\u8bbe\u5907\uff08\u5982 <code>cpu</code> \u6216 <code>cuda:0</code>\uff09\u3002 \u8bbe\u5907\u79fb\u52a8 <code>tensor.to('cuda')</code>, <code>tensor.cpu()</code> \u5c06\u5f20\u91cf\u5728 CPU \u548c GPU \u4e4b\u95f4\u79fb\u52a8\u3002 \u68af\u5ea6\u8ffd\u8e2a <code>tensor.requires_grad</code> \u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\uff08\u9ed8\u8ba4\u4e3a <code>False</code>\uff09\u3002 \u505c\u6b62\u68af\u5ea6\u8ffd\u8e2a <code>with torch.no_grad():</code> \u6216 <code>tensor.detach()</code> \u5728\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u65f6\uff08\u5982\u8bc4\u4f30\u9636\u6bb5\uff09\u4f7f\u7528\uff0c\u8282\u7701\u8ba1\u7b97\u548c\u5185\u5b58\u3002 V. \u7d22\u5f15\u548c\u5207\u7247 \u6807\u91cf\u503c <code>tensor.item()</code> \u4ece\u53ea\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u7684\u5f20\u91cf\u4e2d\u83b7\u53d6 Python \u6807\u91cf\u503c\u3002 \u6807\u51c6\u7d22\u5f15/\u5207\u7247 <code>tensor[0]</code>, <code>tensor[:, 2:5]</code> \u4e0e NumPy \u7d22\u5f15\u89c4\u5219\u76f8\u540c\u3002 \u63a9\u7801\u7d22\u5f15 <code>tensor[tensor &gt; 5]</code> \u4f7f\u7528\u5e03\u5c14\u5f20\u91cf\u4f5c\u4e3a\u7d22\u5f15\uff08\u83b7\u53d6\u6ee1\u8db3\u6761\u4ef6\u7684\u5143\u7d20\uff09\u3002"},{"location":"MachineLearning/Pytorch_Learning/tensor/#pytorch-numpy","title":"PyTorch \u4e0e NumPy \u7684\u5173\u952e\u533a\u522b\u00b6","text":"<ol> <li>\u8bbe\u5907\u652f\u6301\uff1a PyTorch Tensors \u9ed8\u8ba4\u5728 CPU \u4e0a\u521b\u5efa\uff0c\u4f46\u53ef\u4ee5\u4f7f\u7528 <code>.cuda()</code> \u6216 <code>.to('cuda')</code> \u8f7b\u677e\u79fb\u52a8\u5230 GPU \u4e0a\u8fdb\u884c\u9ad8\u6027\u80fd\u8ba1\u7b97\u3002NumPy $ndarray$ \u53ea\u80fd\u5728 CPU \u4e0a\u8fd0\u884c\u3002</li> <li>\u68af\u5ea6\u8ffd\u8e2a\uff1a PyTorch Tensors \u53ef\u4ee5\u4f7f\u7528 <code>requires_grad=True</code> \u542f\u7528 \u81ea\u52a8\u6c42\u5bfc (Autograd)\uff0c\u8fd9\u662f\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u800c NumPy \u4e0d\u5177\u5907\u6b64\u529f\u80fd\u3002</li> <li>\u539f\u5730\u64cd\u4f5c (In-place Operations)\uff1a PyTorch \u63d0\u4f9b\u4e86\u8bb8\u591a\u4ee5 <code>_</code> \u7ed3\u5c3e\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 <code>add_()</code>\uff09\uff0c\u8fd9\u4e9b\u662f\u539f\u5730\u64cd\u4f5c\uff0c\u4f1a\u76f4\u63a5\u4fee\u6539\u5f20\u91cf\u672c\u8eab\u7684\u503c\u3002\u5728\u6d89\u53ca Autograd \u65f6\u4f7f\u7528\u539f\u5730\u64cd\u4f5c\u9700\u8981\u5c0f\u5fc3\uff0c\u53ef\u80fd\u4f1a\u7834\u574f\u8ba1\u7b97\u56fe\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/","title":"Tensorboard","text":"<p>\ud83d\udca1 TensorBoard \u7b80\u4ecb</p> <p>TensorBoard \u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u548c\u5206\u6790\u6570\u636e\u4e0e\u6a21\u578b\u7684\u5f00\u6e90\u5de5\u5177\u5305\u3002\u5b83\u6700\u521d\u662f\u4e3a TensorFlow \u5f00\u53d1\u7684\uff0c\u4f46\u73b0\u5728\u4e5f\u652f\u6301 PyTorch\u3001Hugging Face Transformers \u7b49\u591a\u79cd AI \u6846\u67b6\u3002</p> In\u00a0[1]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n</pre> from torch.utils.tensorboard import SummaryWriter In\u00a0[2]: Copied! <pre>help(SummaryWriter)\n</pre> help(SummaryWriter) <pre>Help on class SummaryWriter in module torch.utils.tensorboard.writer:\n\nclass SummaryWriter(builtins.object)\n |  SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')\n |  \n |  Writes entries directly to event files in the log_dir to be consumed by TensorBoard.\n |  \n |  The `SummaryWriter` class provides a high-level API to create an event file\n |  in a given directory and add summaries and events to it. The class updates the\n |  file contents asynchronously. This allows a training program to call methods\n |  to add data to the file directly from the training loop, without slowing down\n |  training.\n |  \n |  Methods defined here:\n |  \n |  __enter__(self)\n |  \n |  __exit__(self, exc_type, exc_val, exc_tb)\n |  \n |  __init__(self, log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')\n |      Create a `SummaryWriter` that will write out events and summaries to the event file.\n |      \n |      Args:\n |          log_dir (str): Save directory location. Default is\n |            runs/**CURRENT_DATETIME_HOSTNAME**, which changes after each run.\n |            Use hierarchical folder structure to compare\n |            between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc.\n |            for each new experiment to compare across them.\n |          comment (str): Comment log_dir suffix appended to the default\n |            ``log_dir``. If ``log_dir`` is assigned, this argument has no effect.\n |          purge_step (int):\n |            When logging crashes at step :math:`T+X` and restarts at step :math:`T`,\n |            any events whose global_step larger or equal to :math:`T` will be\n |            purged and hidden from TensorBoard.\n |            Note that crashed and resumed experiments should have the same ``log_dir``.\n |          max_queue (int): Size of the queue for pending events and\n |            summaries before one of the 'add' calls forces a flush to disk.\n |            Default is ten items.\n |          flush_secs (int): How often, in seconds, to flush the\n |            pending events and summaries to disk. Default is every two minutes.\n |          filename_suffix (str): Suffix added to all event filenames in\n |            the log_dir directory. More details on filename construction in\n |            tensorboard.summary.writer.event_file_writer.EventFileWriter.\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |      \n |          # create a summary writer with automatically generated folder name.\n |          writer = SummaryWriter()\n |          # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n |      \n |          # create a summary writer using the specified folder name.\n |          writer = SummaryWriter(\"my_experiment\")\n |          # folder location: my_experiment\n |      \n |          # create a summary writer with comment appended.\n |          writer = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n |          # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n |  \n |  add_audio(self, tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None)\n |      Add audio data to summary.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          snd_tensor (torch.Tensor): Sound data\n |          global_step (int): Global step value to record\n |          sample_rate (int): sample rate in Hz\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      Shape:\n |          snd_tensor: :math:`(1, L)`. The values should lie between [-1, 1].\n |  \n |  add_custom_scalars(self, layout)\n |      Create special chart by collecting charts tags in 'scalars'.\n |      \n |      NOTE: This function can only be called once for each SummaryWriter() object.\n |      \n |      Because it only provides metadata to tensorboard, the function can be called before or after the training loop.\n |      \n |      Args:\n |          layout (dict): {categoryName: *charts*}, where *charts* is also a dictionary\n |            {chartName: *ListOfProperties*}. The first element in *ListOfProperties* is the chart's type\n |            (one of **Multiline** or **Margin**) and the second element should be a list containing the tags\n |            you have used in add_scalar function, which will be collected into the new chart.\n |      \n |      Examples::\n |      \n |          layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n |                       'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n |                            'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n |      \n |          writer.add_custom_scalars(layout)\n |  \n |  add_custom_scalars_marginchart(self, tags, category='default', title='untitled')\n |      Shorthand for creating marginchart.\n |      \n |      Similar to ``add_custom_scalars()``, but the only necessary argument is *tags*,\n |      which should have exactly 3 elements.\n |      \n |      Args:\n |          tags (list): list of tags that have been used in ``add_scalar()``\n |      \n |      Examples::\n |      \n |          writer.add_custom_scalars_marginchart(['twse/0050', 'twse/2330', 'twse/2006'])\n |  \n |  add_custom_scalars_multilinechart(self, tags, category='default', title='untitled')\n |      Shorthand for creating multilinechart. Similar to ``add_custom_scalars()``, but the only necessary argument is *tags*.\n |      \n |      Args:\n |          tags (list): list of tags that have been used in ``add_scalar()``\n |      \n |      Examples::\n |      \n |          writer.add_custom_scalars_multilinechart(['twse/0050', 'twse/2330'])\n |  \n |  add_embedding(self, mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)\n |      Add embedding projector data to summary.\n |      \n |      Args:\n |          mat (torch.Tensor or numpy.ndarray): A matrix which each row is the feature vector of the data point\n |          metadata (list): A list of labels, each element will be converted to string\n |          label_img (torch.Tensor): Images correspond to each data point\n |          global_step (int): Global step value to record\n |          tag (str): Name for the embedding\n |          metadata_header (list): A list of headers for multi-column metadata. If given, each metadata must be\n |              a list with values corresponding to headers.\n |      Shape:\n |          mat: :math:`(N, D)`, where N is number of data and D is feature dimension\n |      \n |          label_img: :math:`(N, C, H, W)`\n |      \n |      Examples::\n |      \n |          import keyword\n |          import torch\n |          meta = []\n |          while len(meta)&lt;100:\n |              meta = meta+keyword.kwlist # get some strings\n |          meta = meta[:100]\n |      \n |          for i, v in enumerate(meta):\n |              meta[i] = v+str(i)\n |      \n |          label_img = torch.rand(100, 3, 10, 32)\n |          for i in range(100):\n |              label_img[i]*=i/100.0\n |      \n |          writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\n |          writer.add_embedding(torch.randn(100, 5), label_img=label_img)\n |          writer.add_embedding(torch.randn(100, 5), metadata=meta)\n |      \n |      .. note::\n |          Categorical (i.e. non-numeric) metadata cannot have more than 50 unique values if they are to be used for\n |          coloring in the embedding projector.\n |  \n |  add_figure(self, tag: str, figure: Union[ForwardRef('Figure'), list['Figure']], global_step: Optional[int] = None, close: bool = True, walltime: Optional[float] = None) -&gt; None\n |      Render matplotlib figure into an image and add it to summary.\n |      \n |      Note that this requires the ``matplotlib`` package.\n |      \n |      Args:\n |          tag: Data identifier\n |          figure: Figure or a list of figures\n |          global_step: Global step value to record\n |          close: Flag to automatically close the figure\n |          walltime: Optional override default walltime (time.time())\n |            seconds after epoch of event\n |  \n |  add_graph(self, model, input_to_model=None, verbose=False, use_strict_trace=True)\n |      Add graph data to summary.\n |      \n |      Args:\n |          model (torch.nn.Module): Model to draw.\n |          input_to_model (torch.Tensor or list of torch.Tensor): A variable or a tuple of\n |              variables to be fed.\n |          verbose (bool): Whether to print graph structure in console.\n |          use_strict_trace (bool): Whether to pass keyword argument `strict` to\n |              `torch.jit.trace`. Pass False when you want the tracer to\n |              record your mutable container types (list, dict)\n |  \n |  add_histogram(self, tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)\n |      Add histogram to summary.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          values (torch.Tensor, numpy.ndarray, or string/blobname): Values to build histogram\n |          global_step (int): Global step value to record\n |          bins (str): One of {'tensorflow','auto', 'fd', ...}. This determines how the bins are made. You can find\n |            other options in: https://numpy.org/doc/stable/reference/generated/numpy.histogram.html\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          import numpy as np\n |          writer = SummaryWriter()\n |          for i in range(10):\n |              x = np.random.random(1000)\n |              writer.add_histogram('distribution centers', x + i, i)\n |          writer.close()\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_histogram.png\n |         :scale: 50 %\n |  \n |  add_histogram_raw(self, tag, min, max, num, sum, sum_squares, bucket_limits, bucket_counts, global_step=None, walltime=None)\n |      Add histogram with raw data.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          min (float or int): Min value\n |          max (float or int): Max value\n |          num (int): Number of values\n |          sum (float or int): Sum of all values\n |          sum_squares (float or int): Sum of squares for all values\n |          bucket_limits (torch.Tensor, numpy.ndarray): Upper value per bucket.\n |            The number of elements of it should be the same as `bucket_counts`.\n |          bucket_counts (torch.Tensor, numpy.ndarray): Number of values per bucket\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |          see: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/histogram/README.md\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          import numpy as np\n |          writer = SummaryWriter()\n |          dummy_data = []\n |          for idx, value in enumerate(range(50)):\n |              dummy_data += [idx + 0.001] * value\n |      \n |          bins = list(range(50+2))\n |          bins = np.array(bins)\n |          values = np.array(dummy_data).astype(float).reshape(-1)\n |          counts, limits = np.histogram(values, bins=bins)\n |          sum_sq = values.dot(values)\n |          writer.add_histogram_raw(\n |              tag='histogram_with_raw_data',\n |              min=values.min(),\n |              max=values.max(),\n |              num=len(values),\n |              sum=values.sum(),\n |              sum_squares=sum_sq,\n |              bucket_limits=limits[1:].tolist(),\n |              bucket_counts=counts.tolist(),\n |              global_step=0)\n |          writer.close()\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_histogram_raw.png\n |         :scale: 50 %\n |  \n |  add_hparams(self, hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None, global_step=None)\n |      Add a set of hyperparameters to be compared in TensorBoard.\n |      \n |      Args:\n |          hparam_dict (dict): Each key-value pair in the dictionary is the\n |            name of the hyper parameter and it's corresponding value.\n |            The type of the value can be one of `bool`, `string`, `float`,\n |            `int`, or `None`.\n |          metric_dict (dict): Each key-value pair in the dictionary is the\n |            name of the metric and it's corresponding value. Note that the key used\n |            here should be unique in the tensorboard record. Otherwise the value\n |            you added by ``add_scalar`` will be displayed in hparam plugin. In most\n |            cases, this is unwanted.\n |          hparam_domain_discrete: (Optional[Dict[str, List[Any]]]) A dictionary that\n |            contains names of the hyperparameters and all discrete values they can hold\n |          run_name (str): Name of the run, to be included as part of the logdir.\n |            If unspecified, will use current timestamp.\n |          global_step (int): Global step value to record\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          with SummaryWriter() as w:\n |              for i in range(5):\n |                  w.add_hparams({'lr': 0.1*i, 'bsize': i},\n |                                {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_hparam.png\n |         :scale: 50 %\n |  \n |  add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n |      Add image data to summary.\n |      \n |      Note that this requires the ``pillow`` package.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |          dataformats (str): Image data format specification of the form\n |            CHW, HWC, HW, WH, etc.\n |      Shape:\n |          img_tensor: Default is :math:`(3, H, W)`. You can use ``torchvision.utils.make_grid()`` to\n |          convert a batch of tensor into 3xHxW format or call ``add_images`` and let us do the job.\n |          Tensor with :math:`(1, H, W)`, :math:`(H, W)`, :math:`(H, W, 3)` is also suitable as long as\n |          corresponding ``dataformats`` argument is passed, e.g. ``CHW``, ``HWC``, ``HW``.\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          import numpy as np\n |          img = np.zeros((3, 100, 100))\n |          img[0] = np.arange(0, 10000).reshape(100, 100) / 10000\n |          img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n |      \n |          img_HWC = np.zeros((100, 100, 3))\n |          img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\n |          img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n |      \n |          writer = SummaryWriter()\n |          writer.add_image('my_image', img, 0)\n |      \n |          # If you have non-default dimension setting, set the dataformats argument.\n |          writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\n |          writer.close()\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_image.png\n |         :scale: 50 %\n |  \n |  add_image_with_boxes(self, tag, img_tensor, box_tensor, global_step=None, walltime=None, rescale=1, dataformats='CHW', labels=None)\n |      Add image and draw bounding boxes on the image.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data\n |          box_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Box data (for detected objects)\n |            box should be represented as [x1, y1, x2, y2].\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |          rescale (float): Optional scale override\n |          dataformats (str): Image data format specification of the form\n |            NCHW, NHWC, CHW, HWC, HW, WH, etc.\n |          labels (list of string): The label to be shown for each bounding box.\n |      Shape:\n |          img_tensor: Default is :math:`(3, H, W)`. It can be specified with ``dataformats`` argument.\n |          e.g. CHW or HWC\n |      \n |          box_tensor: (torch.Tensor, numpy.ndarray, or string/blobname): NX4,  where N is the number of\n |          boxes and each 4 elements in a row represents (xmin, ymin, xmax, ymax).\n |  \n |  add_images(self, tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')\n |      Add batched image data to summary.\n |      \n |      Note that this requires the ``pillow`` package.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |          dataformats (str): Image data format specification of the form\n |            NCHW, NHWC, CHW, HWC, HW, WH, etc.\n |      Shape:\n |          img_tensor: Default is :math:`(N, 3, H, W)`. If ``dataformats`` is specified, other shape will be\n |          accepted. e.g. NCHW or NHWC.\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          import numpy as np\n |      \n |          img_batch = np.zeros((16, 3, 100, 100))\n |          for i in range(16):\n |              img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n |              img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n |      \n |          writer = SummaryWriter()\n |          writer.add_images('my_image_batch', img_batch, 0)\n |          writer.close()\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_images.png\n |         :scale: 30 %\n |  \n |  add_mesh(self, tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None)\n |      Add meshes or 3D point clouds to TensorBoard.\n |      \n |      The visualization is based on Three.js,\n |      so it allows users to interact with the rendered object. Besides the basic definitions\n |      such as vertices, faces, users can further provide camera parameter, lighting condition, etc.\n |      Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for\n |      advanced usage.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          vertices (torch.Tensor): List of the 3D coordinates of vertices.\n |          colors (torch.Tensor): Colors for each vertex\n |          faces (torch.Tensor): Indices of vertices within each triangle. (Optional)\n |          config_dict: Dictionary with ThreeJS classes names and configuration.\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      \n |      Shape:\n |          vertices: :math:`(B, N, 3)`. (batch, number_of_vertices, channels)\n |      \n |          colors: :math:`(B, N, 3)`. The values should lie in [0, 255] for type `uint8` or [0, 1] for type `float`.\n |      \n |          faces: :math:`(B, N, 3)`. The values should lie in [0, number_of_vertices] for type `uint8`.\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          vertices_tensor = torch.as_tensor([\n |              [1, 1, 1],\n |              [-1, -1, 1],\n |              [1, -1, -1],\n |              [-1, 1, -1],\n |          ], dtype=torch.float).unsqueeze(0)\n |          colors_tensor = torch.as_tensor([\n |              [255, 0, 0],\n |              [0, 255, 0],\n |              [0, 0, 255],\n |              [255, 0, 255],\n |          ], dtype=torch.int).unsqueeze(0)\n |          faces_tensor = torch.as_tensor([\n |              [0, 2, 3],\n |              [0, 3, 1],\n |              [0, 1, 2],\n |              [1, 3, 2],\n |          ], dtype=torch.int).unsqueeze(0)\n |      \n |          writer = SummaryWriter()\n |          writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n |      \n |          writer.close()\n |  \n |  add_onnx_graph(self, prototxt)\n |  \n |  add_pr_curve(self, tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)\n |      Add precision recall curve.\n |      \n |      Plotting a precision-recall curve lets you understand your model's\n |      performance under different threshold settings. With this function,\n |      you provide the ground truth labeling (T/F) and prediction confidence\n |      (usually the output of your model) for each target. The TensorBoard UI\n |      will let you choose the threshold interactively.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          labels (torch.Tensor, numpy.ndarray, or string/blobname):\n |            Ground truth data. Binary label for each element.\n |          predictions (torch.Tensor, numpy.ndarray, or string/blobname):\n |            The probability that an element be classified as true.\n |            Value should be in [0, 1]\n |          global_step (int): Global step value to record\n |          num_thresholds (int): Number of thresholds used to draw the curve.\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          import numpy as np\n |          labels = np.random.randint(2, size=100)  # binary label\n |          predictions = np.random.rand(100)\n |          writer = SummaryWriter()\n |          writer.add_pr_curve('pr_curve', labels, predictions, 0)\n |          writer.close()\n |  \n |  add_pr_curve_raw(self, tag, true_positive_counts, false_positive_counts, true_negative_counts, false_negative_counts, precision, recall, global_step=None, num_thresholds=127, weights=None, walltime=None)\n |      Add precision recall curve with raw data.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          true_positive_counts (torch.Tensor, numpy.ndarray, or string/blobname): true positive counts\n |          false_positive_counts (torch.Tensor, numpy.ndarray, or string/blobname): false positive counts\n |          true_negative_counts (torch.Tensor, numpy.ndarray, or string/blobname): true negative counts\n |          false_negative_counts (torch.Tensor, numpy.ndarray, or string/blobname): false negative counts\n |          precision (torch.Tensor, numpy.ndarray, or string/blobname): precision\n |          recall (torch.Tensor, numpy.ndarray, or string/blobname): recall\n |          global_step (int): Global step value to record\n |          num_thresholds (int): Number of thresholds used to draw the curve.\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |          see: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/README.md\n |  \n |  add_scalar(self, tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n |      Add scalar data to summary.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          scalar_value (float or string/blobname): Value to save\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            with seconds after epoch of event\n |          new_style (boolean): Whether to use new style (tensor field) or old\n |            style (simple_value field). New style could lead to faster data loading.\n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          writer = SummaryWriter()\n |          x = range(100)\n |          for i in x:\n |              writer.add_scalar('y=2x', i * 2, i)\n |          writer.close()\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_scalar.png\n |         :scale: 50 %\n |  \n |  add_scalars(self, main_tag, tag_scalar_dict, global_step=None, walltime=None)\n |      Add many scalar data to summary.\n |      \n |      Args:\n |          main_tag (str): The parent name for the tags\n |          tag_scalar_dict (dict): Key-value pair storing the tag and corresponding values\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      \n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          writer = SummaryWriter()\n |          r = 5\n |          for i in range(100):\n |              writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n |                                              'xcosx':i*np.cos(i/r),\n |                                              'tanx': np.tan(i/r)}, i)\n |          writer.close()\n |          # This call adds three values to the same scalar plot with the tag\n |          # 'run_14h' in TensorBoard's scalar section.\n |      \n |      Expected result:\n |      \n |      .. image:: _static/img/tensorboard/add_scalars.png\n |         :scale: 50 %\n |  \n |  add_tensor(self, tag, tensor, global_step=None, walltime=None)\n |      Add tensor data to summary.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          tensor (torch.Tensor): tensor to save\n |          global_step (int): Global step value to record\n |      Examples::\n |      \n |          from torch.utils.tensorboard import SummaryWriter\n |          writer = SummaryWriter()\n |          x = torch.tensor([1,2,3])\n |          writer.add_scalar('x', x)\n |          writer.close()\n |      \n |      Expected result:\n |          Summary::tensor::float_val [1,2,3]\n |                 ::tensor::shape [3]\n |                 ::tag 'x'\n |  \n |  add_text(self, tag, text_string, global_step=None, walltime=None)\n |      Add text data to summary.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          text_string (str): String to save\n |          global_step (int): Global step value to record\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      Examples::\n |      \n |          writer.add_text('lstm', 'This is an lstm', 0)\n |          writer.add_text('rnn', 'This is an rnn', 10)\n |  \n |  add_video(self, tag, vid_tensor, global_step=None, fps=4, walltime=None)\n |      Add video data to summary.\n |      \n |      Note that this requires the ``moviepy`` package.\n |      \n |      Args:\n |          tag (str): Data identifier\n |          vid_tensor (torch.Tensor): Video data\n |          global_step (int): Global step value to record\n |          fps (float or int): Frames per second\n |          walltime (float): Optional override default walltime (time.time())\n |            seconds after epoch of event\n |      Shape:\n |          vid_tensor: :math:`(N, T, C, H, W)`. The values should lie in [0, 255] for type `uint8` or [0, 1] for type `float`.\n |  \n |  close(self)\n |  \n |  flush(self)\n |      Flushes the event file to disk.\n |      \n |      Call this method to make sure that all pending events have been written to\n |      disk.\n |  \n |  get_logdir(self)\n |      Return the directory where event files will be written.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n</pre> In\u00a0[3]: Copied! <pre># \u5b9e\u4f8b\u5316 \u6307\u5b9a\u8def\u5f84\nwriter = SummaryWriter(\"logs\") \n</pre> # \u5b9e\u4f8b\u5316 \u6307\u5b9a\u8def\u5f84 writer = SummaryWriter(\"logs\")  In\u00a0[4]: Copied! <pre>help(SummaryWriter.add_scalar)\n</pre> help(SummaryWriter.add_scalar) <pre>Help on function add_scalar in module torch.utils.tensorboard.writer:\n\nadd_scalar(self, tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n    Add scalar data to summary.\n    \n    Args:\n        tag (str): Data identifier\n        scalar_value (float or string/blobname): Value to save\n        global_step (int): Global step value to record\n        walltime (float): Optional override default walltime (time.time())\n          with seconds after epoch of event\n        new_style (boolean): Whether to use new style (tensor field) or old\n          style (simple_value field). New style could lead to faster data loading.\n    Examples::\n    \n        from torch.utils.tensorboard import SummaryWriter\n        writer = SummaryWriter()\n        x = range(100)\n        for i in x:\n            writer.add_scalar('y=2x', i * 2, i)\n        writer.close()\n    \n    Expected result:\n    \n    .. image:: _static/img/tensorboard/add_scalar.png\n       :scale: 50 %\n\n</pre> <p>\u8bf4\u660e\uff1a <code>scalar_value</code> y\u8f74\u70b9 <code>global_step</code> x\u8f74\u70b9</p> In\u00a0[6]: Copied! <pre>help(SummaryWriter.add_image)\n</pre> help(SummaryWriter.add_image) <pre>Help on function add_image in module torch.utils.tensorboard.writer:\n\nadd_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n    Add image data to summary.\n    \n    Note that this requires the ``pillow`` package.\n    \n    Args:\n        tag (str): Data identifier\n        img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data\n        global_step (int): Global step value to record\n        walltime (float): Optional override default walltime (time.time())\n          seconds after epoch of event\n        dataformats (str): Image data format specification of the form\n          CHW, HWC, HW, WH, etc.\n    Shape:\n        img_tensor: Default is :math:`(3, H, W)`. You can use ``torchvision.utils.make_grid()`` to\n        convert a batch of tensor into 3xHxW format or call ``add_images`` and let us do the job.\n        Tensor with :math:`(1, H, W)`, :math:`(H, W)`, :math:`(H, W, 3)` is also suitable as long as\n        corresponding ``dataformats`` argument is passed, e.g. ``CHW``, ``HWC``, ``HW``.\n    \n    Examples::\n    \n        from torch.utils.tensorboard import SummaryWriter\n        import numpy as np\n        img = np.zeros((3, 100, 100))\n        img[0] = np.arange(0, 10000).reshape(100, 100) / 10000\n        img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n    \n        img_HWC = np.zeros((100, 100, 3))\n        img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\n        img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n    \n        writer = SummaryWriter()\n        writer.add_image('my_image', img, 0)\n    \n        # If you have non-default dimension setting, set the dataformats argument.\n        writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\n        writer.close()\n    \n    Expected result:\n    \n    .. image:: _static/img/tensorboard/add_image.png\n       :scale: 50 %\n\n</pre> In\u00a0[5]: Copied! <pre># \u5173\u95ed\nwriter.close()\n</pre> # \u5173\u95ed writer.close()"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/","title":"\u4e3b\u8981\u529f\u80fd\u548c\u7528\u9014\u00b6","text":"<p>TensorBoard \u63d0\u4f9b\u4e86\u4e00\u5957\u4e30\u5bcc\u7684\u529f\u80fd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u3001\u8c03\u8bd5\u548c\u4f18\u5316\u4ed6\u4eec\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1a</p> <ul> <li><p>\ud83d\udcc8 \u8ddf\u8e2a\u548c\u53ef\u89c6\u5316\u6307\u6807\uff08Metrics\uff09\uff1a</p> <ul> <li>\u5b9e\u65f6\u663e\u793a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u51fd\u6570 (Loss)\u3001\u51c6\u786e\u7387 (Accuracy)\u3001\u5b66\u4e60\u7387\u7b49\u5173\u952e\u6307\u6807\u968f\u65f6\u95f4\u53d8\u5316\u7684\u66f2\u7ebf\u3002</li> <li>\u53ef\u4ee5\u6bd4\u8f83\u4e0d\u540c\u5b9e\u9a8c\uff08\u4f8b\u5982\uff0c\u4e0d\u540c\u8d85\u53c2\u6570\u8bbe\u7f6e\uff09\u4e4b\u95f4\u7684\u7ed3\u679c\u3002</li> </ul> </li> <li><p>\ud83d\udd78\ufe0f \u53ef\u89c6\u5316\u6a21\u578b\u8ba1\u7b97\u56fe\uff08Graph\uff09\uff1a</p> <ul> <li>\u76f4\u89c2\u5730\u5c55\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u3001\u5c42\u4e4b\u95f4\u7684\u8fde\u63a5\u4ee5\u53ca\u6570\u636e\u7684\u6d41\u5411\u3002</li> <li>\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u7684\u6784\u5efa\u548c\u8c03\u8bd5\u7ed3\u6784\u95ee\u9898\u3002</li> </ul> </li> <li><p>\ud83d\udcca \u67e5\u770b\u5f20\u91cf\uff08Tensor\uff09\u7684\u76f4\u65b9\u56fe\uff1a</p> <ul> <li>\u663e\u793a\u6a21\u578b\u4e2d\u7684\u6743\u91cd\uff08Weights\uff09\u3001\u504f\u5dee\uff08Biases\uff09\u6216\u6fc0\u6d3b\u503c\uff08Activations\uff09\u7b49\u5f20\u91cf\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5206\u5e03\u60c5\u51b5\u3002</li> <li>\u6709\u52a9\u4e8e\u68c0\u6d4b\u6f5c\u5728\u7684\u95ee\u9898\uff0c\u5982\u68af\u5ea6\u6d88\u5931\u6216\u68af\u5ea6\u7206\u70b8\u3002</li> </ul> </li> <li><p>\ud83d\uddbc\ufe0f \u663e\u793a\u56fe\u50cf\u3001\u6587\u672c\u548c\u97f3\u9891\u6570\u636e\uff1a</p> <ul> <li>\u53ef\u7528\u4e8e\u53ef\u89c6\u5316\u8f93\u5165\u6570\u636e\u3001\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\uff0c\u6216\u5377\u79ef\u7f51\u7edc\u5c42\u8f93\u51fa\u7684\u7279\u5f81\u56fe\u3002</li> </ul> </li> <li><p>\ud83c\udf0c \u5d4c\u5165\u6295\u5c04\uff08Embedding Projector\uff09\uff1a</p> <ul> <li>\u5c06\u9ad8\u7ef4\u7684\u5d4c\u5165\u5411\u91cf\uff08Embeddings\uff09\u6295\u5f71\u5230\u8f83\u4f4e\u7ef4\u5ea6\u7684\u7a7a\u95f4\uff08\u5982 2D \u6216 3D\uff09\uff0c\u4ee5\u4fbf\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a2\u7d22\u548c\u5206\u6790\u6570\u636e\u7ed3\u6784\u53ca\u6837\u672c\u76f8\u4f3c\u6027\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/","title":"\u603b\u7ed3\u00b6","text":"<p>\u7b80\u800c\u8a00\u4e4b\uff0cTensorBoard \u5c31\u662f\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u201c\u4eea\u8868\u76d8\u201d\u548c\u201c\u4fa6\u67e5\u5de5\u5177\u201d\uff0c\u5b83\u5c06\u590d\u6742\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u6a21\u578b\u5185\u90e8\u72b6\u6001\u4ee5\u53ef\u89c6\u5316\u7684\u65b9\u5f0f\u5448\u73b0\u51fa\u6765\uff0c\u8ba9\u5de5\u7a0b\u5e08\u80fd\u591f\uff1a</p> <ol> <li>\u76d1\u63a7\u8bad\u7ec3\u8fdb\u5ea6\u3002</li> <li>\u7406\u89e3\u6a21\u578b\u7ed3\u6784\u548c\u884c\u4e3a\u3002</li> <li>\u8c03\u8bd5\u548c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/#summarywriter","title":"SummaryWriter\u7c7b\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/tensorboard/#tensorboard-summarywriter","title":"\u270d\ufe0f TensorBoard \u4e2d\u7684 SummaryWriter \u4ecb\u7ecd\u00b6","text":"<p><code>SummaryWriter</code> \u662f PyTorch (\u4ee5\u53ca\u5176\u4ed6\u4e00\u4e9b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5982 TensorFlow \u7684 Keras API) \u4e2d\u7528\u4e8e\u5c06\u6570\u636e\u5199\u5165 TensorBoard \u53ef\u89c6\u5316\u5de5\u5177\u7684\u6838\u5fc3\u7c7b\u3002</p> <p>\u4f60\u53ef\u4ee5\u628a TensorBoard \u770b\u4f5c\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u201c\u6570\u636e\u8bb0\u5f55\u4eea\u201d\u548c\u201c\u5c55\u793a\u677f\u201d\uff0c\u800c <code>SummaryWriter</code> \u5c31\u662f\u4f60\u7528\u6765\u8bb0\u5f55\u548c\u7ec4\u7ec7\u8fd9\u4e9b\u6570\u636e\u7684\u7b14\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/","title":"\u6838\u5fc3\u529f\u80fd\u4e0e\u4f5c\u7528\u00b6","text":"<ul> <li>\u521b\u5efa\u4e8b\u4ef6\u6587\u4ef6 (Event Files): <code>SummaryWriter</code> \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u6307\u5b9a\u7684\u65e5\u5fd7\u76ee\u5f55 (<code>log_dir</code>) \u4e2d\u521b\u5efa TensorBoard \u9700\u8981\u8bfb\u53d6\u7684\u4e8b\u4ef6\u6587\u4ef6\u3002</li> <li>\u8bb0\u5f55\u6570\u636e: \u5b83\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217 <code>add_XXX()</code> \u65b9\u6cd5\uff0c\u5141\u8bb8\u4f60\u5728\u6a21\u578b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u7b49\u8fc7\u7a0b\u4e2d\uff0c\u65b9\u4fbf\u5730\u8bb0\u5f55\u5404\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u4f8b\u5982\uff1a<ul> <li>\u6807\u91cf (Scalars): \u8bb0\u5f55\u635f\u5931\u51fd\u6570 (<code>loss</code>)\u3001\u51c6\u786e\u7387 (<code>accuracy</code>)\u3001\u5b66\u4e60\u7387 (<code>learning rate</code>) \u7b49\u968f\u8bad\u7ec3\u6b65\u6570\u53d8\u5316\u7684\u6570\u503c\u3002</li> <li>\u76f4\u65b9\u56fe (Histograms): \u8bb0\u5f55\u6a21\u578b\u53c2\u6570\uff08\u5982\u6743\u91cd\u3001\u504f\u7f6e\uff09\u6216\u68af\u5ea6\u7b49\u5f20\u91cf\u7684\u5206\u5e03\u60c5\u51b5\u3002</li> <li>\u56fe\u50cf (Images): \u8bb0\u5f55\u8f93\u5165\u56fe\u50cf\u3001\u7279\u5f81\u56fe (feature maps) \u6216\u751f\u6210\u56fe\u50cf\u7b49\u3002</li> <li>\u56fe (Graph): \u8bb0\u5f55\u6a21\u578b\u7ed3\u6784\u3002</li> <li>\u6587\u672c (Text): \u8bb0\u5f55\u914d\u7f6e\u4fe1\u606f\u3001\u91cd\u8981\u7684\u65e5\u5fd7\u7b49\u3002</li> </ul> </li> <li>\u53ef\u89c6\u5316: TensorBoard \u4f1a\u8bfb\u53d6\u8fd9\u4e9b\u4e8b\u4ef6\u6587\u4ef6\uff0c\u5e76\u5c06\u8bb0\u5f55\u7684\u6570\u636e\u4ee5\u56fe\u8868\u3001\u56fe\u50cf\u7b49\u5f62\u5f0f\u5728\u7f51\u9875\u754c\u9762\u4e0a\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5e2e\u52a9\u4f60\u66f4\u597d\u5730\u7406\u89e3\u548c\u8c03\u8bd5\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/#pytorch","title":"\u5e38\u89c1\u7528\u6cd5 (\u4ee5 PyTorch \u4e3a\u4f8b)\u00b6","text":"<ol> <li><p>\u5bfc\u5165\u4e0e\u521d\u59cb\u5316:</p> <p>\u5728 PyTorch \u4e2d\uff0c<code>SummaryWriter</code> \u901a\u5e38\u4ece <code>torch.utils.tensorboard</code> \u4e2d\u5bfc\u5165\uff08\u6216\u65e7\u7248\u4e2d\u4f7f\u7528 <code>tensorboardX</code> \u5e93\uff09\u3002</p> <pre>from torch.utils.tensorboard import SummaryWriter\n\n# \u521d\u59cb\u5316 SummaryWriter\uff0c\u6307\u5b9a\u65e5\u5fd7\u6587\u4ef6\u5b58\u653e\u7684\u76ee\u5f55\nwriter = SummaryWriter(log_dir='runs/my_experiment_name') \n# 'runs/my_experiment_name' \u662f\u9ed8\u8ba4\u6216\u5e38\u7528\u7684\u65e5\u5fd7\u76ee\u5f55\n</pre> <ul> <li><code>log_dir</code>: TensorBoard \u4e8b\u4ef6\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84\u3002</li> </ul> </li> <li><p>\u8bb0\u5f55\u6807\u91cf\u6570\u636e (\u6700\u5e38\u7528):</p> <p>\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u8bb0\u5f55\u635f\u5931\u6216\u51c6\u786e\u7387\u7b49\uff1a</p> <pre># \u5047\u8bbe loss \u548c step \u662f\u5f53\u524d\u8bad\u7ec3\u6b65\u7684\u635f\u5931\u503c\u548c\u6b65\u6570\nwriter.add_scalar('Training/Loss', loss, global_step=step)\nwriter.add_scalar('Training/Accuracy', accuracy, global_step=step)\n</pre> <ul> <li><code>'Training/Loss'</code>\uff1a\u6807\u7b7e (Tag)\uff0c\u7528\u4e8e\u5728 TensorBoard \u754c\u9762\u4e0a\u5206\u7ec4\u663e\u793a\u3002</li> <li><code>loss</code>\uff1a\u6807\u91cf\u503c (Scalar value)\u3002</li> <li><code>global_step</code>\uff1a\u6b65\u6570\uff0c\u5bf9\u5e94 TensorBoard \u56fe\u8868\u4e2d\u7684 X \u8f74\u3002</li> </ul> </li> <li><p>\u8bb0\u5f55\u56fe\u50cf\u6570\u636e:</p> <pre># \u5047\u8bbe img_tensor \u662f\u4e00\u4e2a\u56fe\u50cf\u5f20\u91cf\uff0cstep \u662f\u5f53\u524d\u6b65\u6570\nwriter.add_image('Input Image', img_tensor, global_step=step)\n</pre> </li> <li><p>\u5173\u95ed Writer:</p> <p>\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u8bb0\u5f97\u5173\u95ed Writer \u4ee5\u786e\u4fdd\u6240\u6709\u6570\u636e\u90fd\u88ab\u5199\u5165\u6587\u4ef6\uff1a</p> <pre>writer.close()\n</pre> </li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/tensorboard/#tensorboard","title":"\u5982\u4f55\u67e5\u770b TensorBoard\u00b6","text":"<p>\u5728\u4f60\u7684\u7ec8\u7aef\u6216\u547d\u4ee4\u884c\u4e2d\uff0c\u5bfc\u822a\u5230\u4f60\u7684\u9879\u76ee\u6839\u76ee\u5f55\u6216\u5b58\u653e <code>log_dir</code> (\u4f8b\u5982 <code>runs</code>) \u7684\u76ee\u5f55\uff0c\u7136\u540e\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8 TensorBoard \u670d\u52a1\u5668\uff1a</p> <pre>tensorboard --logdir=runs\n</pre> <p>TensorBoard \u542f\u52a8\u540e\uff0c\u5b83\u4f1a\u63d0\u4f9b\u4e00\u4e2a\u672c\u5730 URL\uff08\u901a\u5e38\u662f <code>http://localhost:6006</code>\uff09\uff0c\u4f60\u901a\u8fc7\u6d4f\u89c8\u5668\u8bbf\u95ee\u8be5 URL \u5373\u53ef\u770b\u5230\u53ef\u89c6\u5316\u7684\u7ed3\u679c\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/","title":"Tensor","text":"In\u00a0[\u00a0]: Copied!"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#tensor","title":"Tensor\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#tensor","title":"\ud83d\udcd0 \u5f20\u91cf (Tensor) \u8bb2\u89e3\u00b6","text":"<p>\u5f20\u91cf (Tensor) \u662f PyTorch\u3001TensorFlow \u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\uff0c\u5b83\u5728\u6570\u5b66\u4e0a\u662f\u6807\u91cf\u3001\u5411\u91cf\u548c\u77e9\u9635\u6982\u5ff5\u7684\u6cdb\u5316\u3002</p> <p>\u7b80\u5355\u6765\u8bf4\uff0c\u5f20\u91cf\u5c31\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5bb9\u5668\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#1-rank","title":"1. \u5f20\u91cf\u7684\u6570\u5b66\u6982\u5ff5\uff1a\u7ef4\u5ea6 (Rank)\u00b6","text":"<p>\u5f20\u91cf\u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u4e00\u4e2a\u591a\u7ef4\u6570\u7ec4\uff0c\u5b83\u7684\u201c\u7ef4\u5ea6\u201d\u6216\u201c\u9636\u201d\uff08\u5728\u6570\u5b66\u548c\u7269\u7406\u4e2d\u79f0\u4e3a Rank\uff09\u51b3\u5b9a\u4e86\u5b83\u80fd\u8868\u793a\u7684\u6570\u636e\u590d\u6742\u7a0b\u5ea6\u3002</p> \u9636/\u7ef4\u5ea6 (Rank) \u5f62\u72b6 (Shape) \u63cf\u8ff0 \u793a\u4f8b\u6570\u636e 0 \u9636 <code>()</code> \u6807\u91cf (Scalar)\uff1a\u4e00\u4e2a\u7eaf\u91cf\u3002 <code>5</code>, $\\pi$, \u635f\u5931\u51fd\u6570\u503c <code>loss</code> 1 \u9636 <code>(n,)</code> \u5411\u91cf (Vector)\uff1a\u4e00\u7ef4\u6570\u7ec4\u3002 \u8bcd\u5d4c\u5165 (Word Embedding)\uff0c$\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ 2 \u9636 <code>(n, m)</code> \u77e9\u9635 (Matrix)\uff1a\u4e8c\u7ef4\u6570\u7ec4\u3002 \u56fe\u50cf\u7684\u7070\u5ea6\u503c\uff0c\u7ebf\u6027\u5c42\u6743\u91cd $\\mathbf{W}$ 3 \u9636 <code>(n, m, k)</code> \u4e09\u9636\u5f20\u91cf\uff1a\u4e09\u7ef4\u6570\u7ec4\u3002 \u5f69\u8272\u56fe\u50cf (\u9ad8 x \u5bbd x \u989c\u8272\u901a\u9053)\uff0c\u89c6\u9891\u5e27 4 \u9636 <code>(n, m, k, l)</code> \u56db\u9636\u5f20\u91cf\uff1a\u56db\u7ef4\u6570\u7ec4\u3002 \u6279\u91cf (Batch) \u7684\u5f69\u8272\u56fe\u50cf (\u6279\u91cf\u5927\u5c0f x \u989c\u8272\u901a\u9053 x \u9ad8 x \u5bbd)"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#2","title":"2. \u5f20\u91cf\u7684\u4e3b\u8981\u5c5e\u6027\u00b6","text":"<p>\u5728 PyTorch \u7b49\u6846\u67b6\u4e2d\uff0c\u4e00\u4e2a\u5f20\u91cf\u5bf9\u8c61\u4e3b\u8981\u5305\u542b\u4ee5\u4e0b\u51e0\u4e2a\u5173\u952e\u5c5e\u6027\uff1a</p> \u5c5e\u6027 \u63cf\u8ff0 \u793a\u4f8b Data (\u6570\u636e) \u5b58\u50a8\u5728\u5f20\u91cf\u4e2d\u7684\u5b9e\u9645\u6570\u503c\u3002 <code>[[1, 2], [3, 4]]</code> Shape (\u5f62\u72b6) \u5b9a\u4e49\u4e86\u5f20\u91cf\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5927\u5c0f\u3002 2x2 \u77e9\u9635\u7684\u5f62\u72b6\u662f <code>(2, 2)</code> Dtype (\u6570\u636e\u7c7b\u578b) \u5b58\u50a8\u7684\u5143\u7d20\u7684\u6570\u636e\u7c7b\u578b\u3002 <code>torch.float32</code>, <code>torch.int64</code> \u7b49\u3002 Device (\u8bbe\u5907) \u5f20\u91cf\u5b58\u50a8\u7684\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u662f CPU \u6216 GPU\u3002 <code>cpu</code> \u6216 <code>cuda:0</code> <code>requires_grad</code> \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u9700\u8981\u8ba1\u7b97\u8be5\u5f20\u91cf\u7684\u68af\u5ea6\uff08\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\uff09\u3002 <code>True</code> \u6216 <code>False</code>"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#3","title":"3. \u5f20\u91cf\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u00b6","text":"<p>\u5f20\u91cf\u662f\u8868\u793a\u6240\u6709\u6570\u636e\u7684\u901a\u7528\u8bed\u8a00\uff1a</p> \u6570\u636e\u7c7b\u578b \u5bf9\u5e94\u7684\u5f20\u91cf\u5f62\u72b6 (\u5e38\u89c1) \u63cf\u8ff0 \u56fe\u50cf <code>(C, H, W)</code> \u6216 <code>(H, W, C)</code> \u901a\u9053\u6570 (C)\u3001\u9ad8 (H)\u3001\u5bbd (W)\u3002 \u6587\u672c (\u6279\u91cf) <code>(Batch_size, Seq_len, Embedding_dim)</code> \u6279\u91cf\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u3002 \u8bad\u7ec3\u6279\u91cf (Batch) <code>(Batch_size, C, H, W)</code> \u6df1\u5ea6\u5b66\u4e60\u4e2d\u5904\u7406\u6570\u636e\u7684\u4e3b\u6d41\u5f62\u5f0f\uff0c\u4e00\u6b21\u5904\u7406\u591a\u5f20\u56fe\u7247\u3002 \u6a21\u578b\u53c2\u6570 \u5404\u79cd\u5f62\u72b6 \u6a21\u578b\u7684\u6743\u91cd $\\mathbf{W}$ \u548c\u504f\u7f6e $\\mathbf{b}$ \u90fd\u662f\u5f20\u91cf\u3002"},{"location":"MachineLearning/Pytorch_Learning/test_tensor/#4-pytorch","title":"4. PyTorch \u5f20\u91cf\u7684\u7279\u70b9\uff1a\u81ea\u52a8\u5fae\u5206\u00b6","text":"<p>PyTorch \u7684\u5f20\u91cf (<code>torch.Tensor</code>) \u6700\u5f3a\u5927\u7684\u7279\u6027\u662f\u5b83\u96c6\u6210\u4e86\u81ea\u52a8\u5fae\u5206 (Autograd) \u7cfb\u7edf\u3002</p> <ul> <li>\u5f53\u4f60\u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u5e76\u8bbe\u7f6e <code>requires_grad=True</code> \u65f6\uff0cPyTorch \u4f1a\u5f00\u59cb\u8ddf\u8e2a\u6240\u6709\u4f5c\u7528\u5728\u8fd9\u4e2a\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002</li> <li>\u8fd9\u4f7f\u5f97\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u53ef\u4ee5\u8f7b\u677e\u5730\u901a\u8fc7<code>.backward()</code> \u65b9\u6cd5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u8fd9\u662f\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u548c\u4f18\u5316\u6a21\u578b\u7684\u57fa\u7840\u3002</li> </ul> <pre>import torch\n\n# \u521b\u5efa\u4e00\u4e2a\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684 2 \u9636\u5f20\u91cf (\u77e9\u9635)\nx = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\nprint(f\"\u5f20\u91cf\u7684\u5f62\u72b6: {x.shape}\")\nprint(f\"\u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6: {x.requires_grad}\")\n\n# \u8fdb\u884c\u4e00\u4e9b\u64cd\u4f5c\ny = x + 2\nz = y * y * 3\nout = z.mean()\n\n# out.backward()  # \u8c03\u7528 backward \u4f1a\u81ea\u52a8\u8ba1\u7b97\u6240\u6709 require_grad=True \u7684\u5f20\u91cf\u7684\u68af\u5ea6\n</pre> <p>\u603b\u7ed3: \u5f20\u91cf\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6570\u636e\u7684\u8f7d\u4f53\uff0c\u5b83\u4e0d\u4ec5\u80fd\u5b58\u50a8\u591a\u7ef4\u6570\u636e\uff0c\u8fd8\u96c6\u6210\u4e86\u81ea\u52a8\u5fae\u5206\u80fd\u529b\uff0c\u662f\u6784\u5efa\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u77f3\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/test_tensorboard/","title":"Test tensorboard","text":"In\u00a0[\u00a0]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n</pre> from torch.utils.tensorboard import SummaryWriter In\u00a0[\u00a0]: Copied! <pre>writer = SummaryWriter(\"logs\")\n</pre> writer = SummaryWriter(\"logs\") In\u00a0[\u00a0]: Copied! <pre># \u5b9e\u4f8b y = x\nfor i in range(100):\n    writer.add_scalar(tag='y=2x',scalar_value=2*i,global_step=i)\n</pre> # \u5b9e\u4f8b y = x for i in range(100):     writer.add_scalar(tag='y=2x',scalar_value=2*i,global_step=i) In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nimport numpy as np\nimage_path = 'hymenoptera_data/hymenoptera_data/train/ants/0013035.jpg'\nimg = Image.open(image_path)\nimg = np.array(img)\nprint(img.shape) # note : \u9ed8\u8ba4\u662f 3*H*W \u8fd9\u91cc3\u5374\u5728\u6700\u540e\u3002\u56e0\u6b64\u6211\u4eec\u9700\u8981\u989d\u5916\u8bbe\u7f6e\u683c\u5f0f\nwriter.add_image('image',img,global_step=2,dataformats='HWC')\n</pre> from PIL import Image import numpy as np image_path = 'hymenoptera_data/hymenoptera_data/train/ants/0013035.jpg' img = Image.open(image_path) img = np.array(img) print(img.shape) # note : \u9ed8\u8ba4\u662f 3*H*W \u8fd9\u91cc3\u5374\u5728\u6700\u540e\u3002\u56e0\u6b64\u6211\u4eec\u9700\u8981\u989d\u5916\u8bbe\u7f6e\u683c\u5f0f writer.add_image('image',img,global_step=2,dataformats='HWC') In\u00a0[\u00a0]: Copied! <pre>writer.close()\n</pre> writer.close()"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"transform","text":"In\u00a0[1]: Copied! <pre>from torchvision import transforms\n</pre> from torchvision import transforms In\u00a0[2]: Copied! <pre># \u8f6c\u6362\u56fe\u7247\nfrom PIL import Image\nfrom torchvision import transforms\n</pre> # \u8f6c\u6362\u56fe\u7247 from PIL import Image from torchvision import transforms In\u00a0[3]: Copied! <pre>img_path = 'dataset/train/ants/0013035.jpg'\nimg = Image.open(img_path)\nprint(type(img))\n</pre> img_path = 'dataset/train/ants/0013035.jpg' img = Image.open(img_path) print(type(img)) <pre>&lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;\n</pre> In\u00a0[4]: Copied! <pre>tensor_trans = transforms.ToTensor() #\u5b9e\u4f8b\u5316\u7c7b\nimg_tensor = tensor_trans(img) #\u8c03\u7528\u7c7b(\u5c31\u662f\u8c03\u7528__call__\u51fd\u6570)\nprint(type(img_tensor))\n</pre> tensor_trans = transforms.ToTensor() #\u5b9e\u4f8b\u5316\u7c7b img_tensor = tensor_trans(img) #\u8c03\u7528\u7c7b(\u5c31\u662f\u8c03\u7528__call__\u51fd\u6570) print(type(img_tensor)) <pre>&lt;class 'torch.Tensor'&gt;\n</pre> In\u00a0[5]: Copied! <pre>print(img_tensor)\n</pre> print(img_tensor) <pre>tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n         ...,\n         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n\n        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n         ...,\n         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n\n        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n         ...,\n         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n</pre> In\u00a0[7]: Copied! <pre>print(img_tensor.shape)\n</pre> print(img_tensor.shape) <pre>torch.Size([3, 512, 768])\n</pre> <p>\u8bf4\u660e\uff1a\u7ef4\u5ea60 \u6df1\u5ea6\uff08\u901a\u9053\u6570\uff09\uff1b\u7ef4\u5ea61 \u884c\u6570\uff08\u9ad8\uff09\uff0c\u7ef4\u5ea62 \u5217\u6570\uff08\u5bbd\uff09</p>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#torchvision","title":"\ud83d\uddbc\ufe0f TorchVision \u4ecb\u7ecd\u00b6","text":"<p>TorchVision \u662f PyTorch \u5b98\u65b9\u652f\u6301\u7684\u3001\u4e13\u95e8\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9 (Computer Vision) \u4efb\u52a1\u7684\u5e93\u3002\u5b83\u662f PyTorch \u751f\u6001\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u6781\u5927\u5730\u7b80\u5316\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u548c\u5e94\u7528\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002</p> <p>TorchVision \u4e3b\u8981\u7531\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u6784\u6210\uff0c\u5171\u540c\u4e3a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u7684\u652f\u6301\uff1a</p>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#1-torchvisiondatasets","title":"1. <code>torchvision.datasets</code> (\u6570\u636e\u96c6)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5927\u91cf\u9884\u5148\u51c6\u5907\u597d\u7684\u3001\u5e38\u7528\u7684\u516c\u5171\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u76f4\u63a5\u4e0b\u8f7d\u548c\u4f7f\u7528\uff0c\u65e0\u9700\u81ea\u5df1\u7f16\u5199\u590d\u6742\u7684\u6570\u636e\u52a0\u8f7d\u4ee3\u7801\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u65b9\u4fbf\u5730\u52a0\u8f7d\u6807\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002</li> <li>\u5e38\u89c1\u6570\u636e\u96c6\u793a\u4f8b:<ul> <li>Image Classification (\u56fe\u50cf\u5206\u7c7b): <code>MNIST</code>, <code>CIFAR10</code>, <code>ImageNet</code> (\u9700\u8981\u624b\u52a8\u4e0b\u8f7d\u548c\u7ec4\u7ec7\u6587\u4ef6)\u3002</li> <li>Object Detection (\u76ee\u6807\u68c0\u6d4b): <code>CocoDetection</code>, <code>Kitti</code>.</li> <li>Semantic Segmentation (\u8bed\u4e49\u5206\u5272): <code>VOCSegmentation</code>.</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#2-torchvisionmodels","title":"2. <code>torchvision.models</code> (\u9884\u8bad\u7ec3\u6a21\u578b)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5927\u91cf\u9884\u8bad\u7ec3\u7684\u3001\u9ad8\u6027\u80fd\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u5728\u5927\u578b\u6570\u636e\u96c6\uff08\u5982 ImageNet\uff09\u4e0a\u8bad\u7ec3\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u6216\u4f5c\u4e3a\u8fc1\u79fb\u5b66\u4e60 (Transfer Learning) \u7684\u8d77\u70b9\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u5feb\u901f\u83b7\u53d6\u548c\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7ecf\u5178\u6216\u5148\u8fdb\u6a21\u578b\u3002</li> <li>\u5e38\u89c1\u6a21\u578b\u67b6\u6784\u793a\u4f8b:<ul> <li>Classification (\u5206\u7c7b): <code>ResNet</code> (\u5982 <code>resnet50</code>), <code>VGG</code>, <code>MobileNet</code>, <code>EfficientNet</code>.</li> <li>Object Detection (\u76ee\u6807\u68c0\u6d4b): <code>Faster R-CNN</code>, <code>SSD</code>, <code>RetinaNet</code>.</li> <li>Segmentation (\u5206\u5272): <code>FCN</code>, <code>DeepLabV3</code>.</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#3-torchvisiontransforms","title":"3. <code>torchvision.transforms</code> (\u6570\u636e\u53d8\u6362/\u9884\u5904\u7406)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5404\u79cd\u56fe\u50cf\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a (Data Augmentation) \u64cd\u4f5c\u3002\u5728\u5c06\u539f\u59cb\u56fe\u50cf\u8f93\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u901a\u5e38\u9700\u8981\u8fdb\u884c\u8fd9\u4e9b\u5904\u7406\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u5bf9 PIL Image \u6216 Tensor \u8fdb\u884c\u5404\u79cd\u64cd\u4f5c\uff0c\u5982\u5c3a\u5bf8\u8c03\u6574\u3001\u88c1\u526a\u3001\u5f52\u4e00\u5316\u7b49\u3002</li> <li>\u5e38\u89c1\u64cd\u4f5c\u793a\u4f8b:<ul> <li><code>ToTensor()</code>: \u5c06 PIL Image \u6216 NumPy \u6570\u7ec4\u8f6c\u6362\u6210 PyTorch Tensor\u3002</li> <li><code>Normalize()</code>: \u5bf9 Tensor \u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406 (\u51cf\u53bb\u5747\u503c\uff0c\u9664\u4ee5\u6807\u51c6\u5dee)\u3002</li> <li><code>Resize()</code>: \u6539\u53d8\u56fe\u50cf\u5c3a\u5bf8\u3002</li> <li><code>RandomCrop()</code>: \u968f\u673a\u88c1\u526a\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002</li> <li><code>Compose()</code>: \u5c06\u591a\u4e2a\u53d8\u6362\u64cd\u4f5c\u4e32\u8054\u8d77\u6765\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"\u603b\u7ed3\u00b6","text":"<p>TorchVision \u662f PyTorch \u7528\u6237\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u9879\u76ee\u65f6\u7684\u6807\u914d\u5de5\u5177\u7bb1\uff0c\u5b83\u901a\u8fc7\u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u9884\u5904\u7406\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\uff1a</p> <ol> <li>\u5feb\u901f\u542f\u52a8\u9879\u76ee: \u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6807\u51c6\u6570\u636e\u96c6\u5feb\u901f\u6784\u5efa\u539f\u578b\u3002</li> <li>\u4fdd\u8bc1\u6570\u636e\u5904\u7406\u4e00\u81f4\u6027: \u4f7f\u7528\u7edf\u4e00\u7684 <code>transforms</code> API \u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3002</li> <li>\u8282\u7701\u8d44\u6e90: \u907f\u514d\u91cd\u590d\u5b9e\u73b0\u5e38\u89c1\u6a21\u578b\u7684\u4ee3\u7801\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#transform","title":"transform\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#torchvisiontransforms","title":"<code>torchvision.transforms</code> \u4ecb\u7ecd\u00b6","text":"<p><code>torchvision.transforms</code> \u662f TorchVision \u5e93\u4e2d\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u9884\u5904\u7406 (Preprocessing) \u548c\u6570\u636e\u589e\u5f3a (Data Augmentation) \u7684\u6838\u5fc3\u6a21\u5757\u3002</p> <p>\u5728\u5c06\u539f\u59cb\u56fe\u50cf\u6570\u636e\u9001\u5165\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u6211\u4eec\u51e0\u4e4e\u603b\u662f\u9700\u8981\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u8f6c\u6362\u64cd\u4f5c\uff0c\u800c <code>transforms</code> \u6a21\u5757\u5c31\u662f\u4e3a\u6b64\u8bbe\u8ba1\u7684\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"\u6838\u5fc3\u76ee\u7684\u00b6","text":"<ol> <li>\u7edf\u4e00\u8f93\u5165\u683c\u5f0f: \u5c06\u4e0d\u540c\u683c\u5f0f\u7684\u8f93\u5165\uff08\u5982 PIL Image, NumPy Array\uff09\u8f6c\u6362\u4e3a PyTorch \u6a21\u578b\u6240\u9700\u7684 <code>Tensor</code> \u683c\u5f0f\u3002</li> <li>\u6807\u51c6\u5316: \u5bf9\u50cf\u7d20\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u63a5\u8fd1 0\u3001\u6807\u51c6\u5dee\u63a5\u8fd1 1\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u3002</li> <li>\u6570\u636e\u589e\u5f3a: \u901a\u8fc7\u968f\u673a\u53d8\u6362\uff08\u5982\u88c1\u526a\u3001\u7ffb\u8f6c\u3001\u65cb\u8f6c\uff09\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"\u5e38\u89c1\u64cd\u4f5c\u5206\u7c7b\u53ca\u793a\u4f8b\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#1-normalization-format","title":"1. \u57fa\u7840\u8f6c\u6362 (Normalization &amp; Format)\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>ToTensor()</code> \u5c06 PIL Image \u6216 NumPy <code>ndarray</code> \u8f6c\u6362\u4e3a <code>FloatTensor</code>\u3002 \u5fc5\u987b\u7684\u64cd\u4f5c\uff0c\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u80fd\u5904\u7406\u7684 PyTorch \u5f20\u91cf\u3002 <code>Normalize(mean, std)</code> \u6839\u636e\u7ed9\u5b9a\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u5bf9 Tensor \u8fdb\u884c\u6807\u51c6\u5316\u3002 \u5c06\u50cf\u7d20\u503c\u7f29\u653e\u5230\u4e00\u4e2a\u5408\u9002\u7684\u8303\u56f4\uff0c\u52a0\u901f\u6a21\u578b\u6536\u655b\u3002 <code>Resize(size)</code> \u5c06\u8f93\u5165\u56fe\u50cf\u8c03\u6574\u5230\u6307\u5b9a\u7684\u5c3a\u5bf8\u3002 \u786e\u4fdd\u6240\u6709\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u7edf\u4e00\u3002"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#2-augmentation","title":"2. \u6570\u636e\u589e\u5f3a (Augmentation)\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>RandomCrop(size)</code> \u4ece\u56fe\u50cf\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u5757\u6307\u5b9a\u5927\u5c0f\u7684\u533a\u57df\u3002 \u589e\u52a0\u8bad\u7ec3\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u3002 <code>RandomHorizontalFlip(p=0.5)</code> \u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u6c34\u5e73\u968f\u673a\u7ffb\u8f6c\u56fe\u50cf\u3002 \u589e\u52a0\u6570\u636e\u7684\u5bf9\u79f0\u6027\u3002 <code>RandomRotation(degrees)</code> \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u4e00\u4e2a\u89d2\u5ea6\u8303\u56f4\u3002 \u589e\u52a0\u6a21\u578b\u7684\u65cb\u8f6c\u4e0d\u53d8\u6027\u3002 <code>ColorJitter(brightness=0.2, ...)</code> \u968f\u673a\u6539\u53d8\u56fe\u50cf\u7684\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u548c\u8272\u8c03\u3002 \u4f7f\u6a21\u578b\u5bf9\u5149\u7167\u53d8\u5316\u66f4\u5177\u9c81\u68d2\u6027\u3002"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#3","title":"3. \u7ec4\u5408\u64cd\u4f5c\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>Compose([...])</code> \u5c06\u591a\u4e2a\u8f6c\u6362\u64cd\u4f5c\u6309\u987a\u5e8f\u4e32\u8054\u8d77\u6765\u3002 \u8fd9\u662f\u6700\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4e49\u4e00\u4e2a\u5b8c\u6574\u7684\u9884\u5904\u7406\u6d41\u7a0b\u3002"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/#compose","title":"<code>Compose</code> \u793a\u4f8b (\u6807\u51c6\u7684\u9884\u5904\u7406\u6d41\u7a0b)\u00b6","text":"<p>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u901a\u5e38\u4f7f\u7528 <code>transforms.Compose</code> \u5c06\u591a\u4e2a\u64cd\u4f5c\u7ec4\u5408\u6210\u4e00\u4e2a\u6d41\u7a0b\uff1a</p> <pre>from torchvision import transforms\n\n# \u5b9a\u4e49\u4e00\u4e2a\u6807\u51c6\u7684\u9884\u5904\u7406\u6d41\u7a0b\ntransform = transforms.Compose([\n    # 1. \u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u5230 256x256\n    transforms.Resize(256),  \n    \n    # 2. \u4ece 256x256 \u7684\u56fe\u50cf\u4e2d\u968f\u673a\u88c1\u526a 224x224 \u7684\u533a\u57df (\u7528\u4e8e\u6570\u636e\u589e\u5f3a)\n    transforms.RandomCrop(224), \n    \n    # 3. \u4ee5 50% \u7684\u6982\u7387\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf (\u7528\u4e8e\u6570\u636e\u589e\u5f3a)\n    transforms.RandomHorizontalFlip(),\n    \n    # 4. \u5fc5\u987b\u64cd\u4f5c\uff1a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a PyTorch Tensor\n    transforms.ToTensor(),      \n    \n    # 5. \u5fc5\u987b\u64cd\u4f5c\uff1a\u8fdb\u884c\u6807\u51c6\u5316 (\u4f7f\u7528 ImageNet \u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u4f5c\u4e3a\u793a\u4f8b)\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\n# \u5047\u8bbe image \u662f\u4e00\u4e2a PIL \u56fe\u50cf\u5bf9\u8c61\n# processed_tensor = transform(image)\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"\u603b\u7ed3\u00b6","text":"<p><code>torchvision.transforms</code> \u6a21\u5757\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u3001\u7075\u6d3b\u7684 API\uff0c\u4f7f\u5f97\u6df1\u5ea6\u5b66\u4e60\u5de5\u7a0b\u5e08\u53ef\u4ee5\u65b9\u4fbf\u5730\uff1a</p> <ul> <li>\u6784\u5efa\u6a21\u578b\u6240\u9700\u7684\u8f93\u5165\u5f20\u91cf\u3002</li> <li>\u5e94\u7528\u5404\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/test_transfrom/","title":"Test transfrom","text":"In\u00a0[\u00a0]: Copied! <pre>from PIL import Image\nfrom torchvision import transforms\n</pre> from PIL import Image from torchvision import transforms In\u00a0[\u00a0]: Copied! <pre>img_path = 'dataset/train/ants/0013035.jpg'\nimg = Image.open(img_path)\nprint(type(img))\n</pre> img_path = 'dataset/train/ants/0013035.jpg' img = Image.open(img_path) print(type(img)) In\u00a0[\u00a0]: Copied! <pre>tensor_trans = transforms.ToTensor()\nimg_tensor = tensor_trans(img)\nprint(type(img_tensor))\n</pre> tensor_trans = transforms.ToTensor() img_tensor = tensor_trans(img) print(type(img_tensor))"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"transform","text":"<p>\u8be6\u89c1Pytorch\u5b98\u65b9Transformers</p> In\u00a0[1]: Copied! <pre>from torchvision import transforms\n</pre> from torchvision import transforms In\u00a0[2]: Copied! <pre># \u8f6c\u6362\u56fe\u7247\nfrom PIL import Image\nfrom torchvision import transforms\n</pre> # \u8f6c\u6362\u56fe\u7247 from PIL import Image from torchvision import transforms In\u00a0[3]: Copied! <pre>img_path = 'dataset/train/ants/0013035.jpg'\nimg = Image.open(img_path)\nprint(type(img))\n</pre> img_path = 'dataset/train/ants/0013035.jpg' img = Image.open(img_path) print(type(img)) <pre>&lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;\n</pre> In\u00a0[4]: Copied! <pre>tensor_trans = transforms.ToTensor() #\u5b9e\u4f8b\u5316\u7c7b\nimg_tensor = tensor_trans(img) #\u8c03\u7528\u7c7b(\u5c31\u662f\u8c03\u7528__call__\u51fd\u6570)\nprint(type(img_tensor))\n</pre> tensor_trans = transforms.ToTensor() #\u5b9e\u4f8b\u5316\u7c7b img_tensor = tensor_trans(img) #\u8c03\u7528\u7c7b(\u5c31\u662f\u8c03\u7528__call__\u51fd\u6570) print(type(img_tensor)) <pre>&lt;class 'torch.Tensor'&gt;\n</pre> In\u00a0[5]: Copied! <pre>print(img_tensor)\n</pre> print(img_tensor) <pre>tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n         ...,\n         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n\n        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n         ...,\n         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n\n        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n         ...,\n         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n</pre> In\u00a0[1]: Copied! <pre>print(img_tensor.shape)\n</pre> print(img_tensor.shape) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 print(img_tensor.shape)\n\nNameError: name 'img_tensor' is not defined</pre> <p>\u8bf4\u660e\uff1a\u7ef4\u5ea60 \u6df1\u5ea6\uff08\u901a\u9053\u6570\uff09\uff1b\u7ef4\u5ea61 \u884c\u6570\uff08\u9ad8\uff09\uff0c\u7ef4\u5ea62 \u5217\u6570\uff08\u5bbd\uff09</p> In\u00a0[4]: Copied! <pre>import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n# \u5c06\u7279\u5f81\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u7684\u5f20\u91cf\uff0c\u5e76\u5c06\u6807\u7b7e\u8f6c\u6362\u4e3a\u72ec\u70ed\u7f16\u7801\u7684\u5f20\u91cf\nds = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    # \u628a\u8f93\u5165\u7684y\u5217\u8868\u5bf9\u5e94\u7684\u4f4d\u7f6e\u90fd\u5199 1\uff0c\u5176\u4ed6\u5219\u4e3a 0\uff0c\u5e76\u7edf\u4e00\u957f\u5ea6\u4e3a 10\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) #y should be Python List\n)\n</pre> import torch from torchvision import datasets from torchvision.transforms import ToTensor, Lambda # \u5c06\u7279\u5f81\u8f6c\u6362\u4e3a\u5f52\u4e00\u5316\u7684\u5f20\u91cf\uff0c\u5e76\u5c06\u6807\u7b7e\u8f6c\u6362\u4e3a\u72ec\u70ed\u7f16\u7801\u7684\u5f20\u91cf ds = datasets.FashionMNIST(     root=\"data\",     train=True,     download=True,     transform=ToTensor(),     # \u628a\u8f93\u5165\u7684y\u5217\u8868\u5bf9\u5e94\u7684\u4f4d\u7f6e\u90fd\u5199 1\uff0c\u5176\u4ed6\u5219\u4e3a 0\uff0c\u5e76\u7edf\u4e00\u957f\u5ea6\u4e3a 10     target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) #y should be Python List ) <p>\u8fd9\u6bb5\u4ee3\u7801\u4e2d\u7684 <code>target_transform</code> \u90e8\u5206\u662f PyTorch \u6570\u636e\u9884\u5904\u7406\u4e2d\u4e00\u4e2a\u7b80\u6d01\u4e14\u9ad8\u6548\u7684\u6280\u5de7\uff0c\u5b83\u5b9e\u73b0\u4e86\u5c06\u6807\u91cf\u6807\u7b7e\uff08\u7c7b\u522b ID\uff09\u8f6c\u6362\u4e3a\u72ec\u70ed\u7f16\u7801 (One-Hot Encoding)\u3002</p> <p>\u8ba9\u6211\u4eec\u628a\u8fd9\u6bb5\u4ee3\u7801\u62c6\u89e3\u5f00\u6765\uff0c\u4e00\u6b65\u6b65\u8be6\u7ec6\u89e3\u91ca\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#torchvision","title":"\ud83d\uddbc\ufe0f TorchVision \u4ecb\u7ecd\u00b6","text":"<p>TorchVision \u662f PyTorch \u5b98\u65b9\u652f\u6301\u7684\u3001\u4e13\u95e8\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9 (Computer Vision) \u4efb\u52a1\u7684\u5e93\u3002\u5b83\u662f PyTorch \u751f\u6001\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u6781\u5927\u5730\u7b80\u5316\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u548c\u5e94\u7528\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002</p> <p>TorchVision \u4e3b\u8981\u7531\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u6784\u6210\uff0c\u5171\u540c\u4e3a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u7684\u652f\u6301\uff1a</p>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#1-torchvisiondatasets","title":"1. <code>torchvision.datasets</code> (\u6570\u636e\u96c6)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5927\u91cf\u9884\u5148\u51c6\u5907\u597d\u7684\u3001\u5e38\u7528\u7684\u516c\u5171\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u76f4\u63a5\u4e0b\u8f7d\u548c\u4f7f\u7528\uff0c\u65e0\u9700\u81ea\u5df1\u7f16\u5199\u590d\u6742\u7684\u6570\u636e\u52a0\u8f7d\u4ee3\u7801\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u65b9\u4fbf\u5730\u52a0\u8f7d\u6807\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002</li> <li>\u5e38\u89c1\u6570\u636e\u96c6\u793a\u4f8b:<ul> <li>Image Classification (\u56fe\u50cf\u5206\u7c7b): <code>MNIST</code>, <code>CIFAR10</code>, <code>ImageNet</code> (\u9700\u8981\u624b\u52a8\u4e0b\u8f7d\u548c\u7ec4\u7ec7\u6587\u4ef6)\u3002</li> <li>Object Detection (\u76ee\u6807\u68c0\u6d4b): <code>CocoDetection</code>, <code>Kitti</code>.</li> <li>Semantic Segmentation (\u8bed\u4e49\u5206\u5272): <code>VOCSegmentation</code>.</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#2-torchvisionmodels","title":"2. <code>torchvision.models</code> (\u9884\u8bad\u7ec3\u6a21\u578b)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5927\u91cf\u9884\u8bad\u7ec3\u7684\u3001\u9ad8\u6027\u80fd\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u5728\u5927\u578b\u6570\u636e\u96c6\uff08\u5982 ImageNet\uff09\u4e0a\u8bad\u7ec3\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u6216\u4f5c\u4e3a\u8fc1\u79fb\u5b66\u4e60 (Transfer Learning) \u7684\u8d77\u70b9\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u5feb\u901f\u83b7\u53d6\u548c\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7ecf\u5178\u6216\u5148\u8fdb\u6a21\u578b\u3002</li> <li>\u5e38\u89c1\u6a21\u578b\u67b6\u6784\u793a\u4f8b:<ul> <li>Classification (\u5206\u7c7b): <code>ResNet</code> (\u5982 <code>resnet50</code>), <code>VGG</code>, <code>MobileNet</code>, <code>EfficientNet</code>.</li> <li>Object Detection (\u76ee\u6807\u68c0\u6d4b): <code>Faster R-CNN</code>, <code>SSD</code>, <code>RetinaNet</code>.</li> <li>Segmentation (\u5206\u5272): <code>FCN</code>, <code>DeepLabV3</code>.</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#3-torchvisiontransforms","title":"3. <code>torchvision.transforms</code> (\u6570\u636e\u53d8\u6362/\u9884\u5904\u7406)\u00b6","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e86\u5404\u79cd\u56fe\u50cf\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a (Data Augmentation) \u64cd\u4f5c\u3002\u5728\u5c06\u539f\u59cb\u56fe\u50cf\u8f93\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u901a\u5e38\u9700\u8981\u8fdb\u884c\u8fd9\u4e9b\u5904\u7406\u3002</p> <ul> <li>\u6838\u5fc3\u529f\u80fd: \u5bf9 PIL Image \u6216 Tensor \u8fdb\u884c\u5404\u79cd\u64cd\u4f5c\uff0c\u5982\u5c3a\u5bf8\u8c03\u6574\u3001\u88c1\u526a\u3001\u5f52\u4e00\u5316\u7b49\u3002</li> <li>\u5e38\u89c1\u64cd\u4f5c\u793a\u4f8b:<ul> <li><code>ToTensor()</code>: \u5c06 PIL Image \u6216 NumPy \u6570\u7ec4\u8f6c\u6362\u6210 PyTorch Tensor\u3002</li> <li><code>Normalize()</code>: \u5bf9 Tensor \u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406 (\u51cf\u53bb\u5747\u503c\uff0c\u9664\u4ee5\u6807\u51c6\u5dee)\u3002</li> <li><code>Resize()</code>: \u6539\u53d8\u56fe\u50cf\u5c3a\u5bf8\u3002</li> <li><code>RandomCrop()</code>: \u968f\u673a\u88c1\u526a\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002</li> <li><code>Compose()</code>: \u5c06\u591a\u4e2a\u53d8\u6362\u64cd\u4f5c\u4e32\u8054\u8d77\u6765\u3002</li> </ul> </li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"\u603b\u7ed3\u00b6","text":"<p>TorchVision \u662f PyTorch \u7528\u6237\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u9879\u76ee\u65f6\u7684\u6807\u914d\u5de5\u5177\u7bb1\uff0c\u5b83\u901a\u8fc7\u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u9884\u5904\u7406\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\uff1a</p> <ol> <li>\u5feb\u901f\u542f\u52a8\u9879\u76ee: \u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6807\u51c6\u6570\u636e\u96c6\u5feb\u901f\u6784\u5efa\u539f\u578b\u3002</li> <li>\u4fdd\u8bc1\u6570\u636e\u5904\u7406\u4e00\u81f4\u6027: \u4f7f\u7528\u7edf\u4e00\u7684 <code>transforms</code> API \u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u3002</li> <li>\u8282\u7701\u8d44\u6e90: \u907f\u514d\u91cd\u590d\u5b9e\u73b0\u5e38\u89c1\u6a21\u578b\u7684\u4ee3\u7801\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#transform","title":"transform\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/transfrom/#torchvisiontransforms","title":"<code>torchvision.transforms</code> \u4ecb\u7ecd\u00b6","text":"<p><code>torchvision.transforms</code> \u662f TorchVision \u5e93\u4e2d\u4e13\u95e8\u7528\u4e8e\u56fe\u50cf\u9884\u5904\u7406 (Preprocessing) \u548c\u6570\u636e\u589e\u5f3a (Data Augmentation) \u7684\u6838\u5fc3\u6a21\u5757\u3002</p> <p>\u5728\u5c06\u539f\u59cb\u56fe\u50cf\u6570\u636e\u9001\u5165\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u6211\u4eec\u51e0\u4e4e\u603b\u662f\u9700\u8981\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u8f6c\u6362\u64cd\u4f5c\uff0c\u800c <code>transforms</code> \u6a21\u5757\u5c31\u662f\u4e3a\u6b64\u8bbe\u8ba1\u7684\u3002</p>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"\u6838\u5fc3\u76ee\u7684\u00b6","text":"<ol> <li>\u7edf\u4e00\u8f93\u5165\u683c\u5f0f: \u5c06\u4e0d\u540c\u683c\u5f0f\u7684\u8f93\u5165\uff08\u5982 PIL Image, NumPy Array\uff09\u8f6c\u6362\u4e3a PyTorch \u6a21\u578b\u6240\u9700\u7684 <code>Tensor</code> \u683c\u5f0f\u3002</li> <li>\u6807\u51c6\u5316: \u5bf9\u50cf\u7d20\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5176\u5747\u503c\u63a5\u8fd1 0\u3001\u6807\u51c6\u5dee\u63a5\u8fd1 1\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u3002</li> <li>\u6570\u636e\u589e\u5f3a: \u901a\u8fc7\u968f\u673a\u53d8\u6362\uff08\u5982\u88c1\u526a\u3001\u7ffb\u8f6c\u3001\u65cb\u8f6c\uff09\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002</li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"\u5e38\u89c1\u64cd\u4f5c\u5206\u7c7b\u53ca\u793a\u4f8b\u00b6","text":""},{"location":"MachineLearning/Pytorch_Learning/transfrom/#1-normalization-format","title":"1. \u57fa\u7840\u8f6c\u6362 (Normalization &amp; Format)\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>ToTensor()</code> \u5c06 PIL Image \u6216 NumPy <code>ndarray</code> \u8f6c\u6362\u4e3a <code>FloatTensor</code>\u3002 \u5fc5\u987b\u7684\u64cd\u4f5c\uff0c\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u80fd\u5904\u7406\u7684 PyTorch \u5f20\u91cf\u3002 <code>Normalize(mean, std)</code> \u6839\u636e\u7ed9\u5b9a\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u5bf9 Tensor \u8fdb\u884c\u6807\u51c6\u5316\u3002 \u5c06\u50cf\u7d20\u503c\u7f29\u653e\u5230\u4e00\u4e2a\u5408\u9002\u7684\u8303\u56f4\uff0c\u52a0\u901f\u6a21\u578b\u6536\u655b\u3002 <code>Resize(size)</code> \u5c06\u8f93\u5165\u56fe\u50cf\u8c03\u6574\u5230\u6307\u5b9a\u7684\u5c3a\u5bf8\u3002 \u786e\u4fdd\u6240\u6709\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u7edf\u4e00\u3002"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#2-augmentation","title":"2. \u6570\u636e\u589e\u5f3a (Augmentation)\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>RandomCrop(size)</code> \u4ece\u56fe\u50cf\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u5757\u6307\u5b9a\u5927\u5c0f\u7684\u533a\u57df\u3002 \u589e\u52a0\u8bad\u7ec3\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u3002 <code>RandomHorizontalFlip(p=0.5)</code> \u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u6c34\u5e73\u968f\u673a\u7ffb\u8f6c\u56fe\u50cf\u3002 \u589e\u52a0\u6570\u636e\u7684\u5bf9\u79f0\u6027\u3002 <code>RandomRotation(degrees)</code> \u968f\u673a\u65cb\u8f6c\u56fe\u50cf\u4e00\u4e2a\u89d2\u5ea6\u8303\u56f4\u3002 \u589e\u52a0\u6a21\u578b\u7684\u65cb\u8f6c\u4e0d\u53d8\u6027\u3002 <code>ColorJitter(brightness=0.2, ...)</code> \u968f\u673a\u6539\u53d8\u56fe\u50cf\u7684\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u548c\u8272\u8c03\u3002 \u4f7f\u6a21\u578b\u5bf9\u5149\u7167\u53d8\u5316\u66f4\u5177\u9c81\u68d2\u6027\u3002"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#3","title":"3. \u7ec4\u5408\u64cd\u4f5c\u00b6","text":"\u8f6c\u6362\u65b9\u6cd5 \u4f5c\u7528 \u76ee\u7684 <code>Compose([...])</code> \u5c06\u591a\u4e2a\u8f6c\u6362\u64cd\u4f5c\u6309\u987a\u5e8f\u4e32\u8054\u8d77\u6765\u3002 \u8fd9\u662f\u6700\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4e49\u4e00\u4e2a\u5b8c\u6574\u7684\u9884\u5904\u7406\u6d41\u7a0b\u3002"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#compose","title":"<code>Compose</code> \u793a\u4f8b (\u6807\u51c6\u7684\u9884\u5904\u7406\u6d41\u7a0b)\u00b6","text":"<p>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u901a\u5e38\u4f7f\u7528 <code>transforms.Compose</code> \u5c06\u591a\u4e2a\u64cd\u4f5c\u7ec4\u5408\u6210\u4e00\u4e2a\u6d41\u7a0b\uff1a</p> <pre>from torchvision import transforms\n\n# \u5b9a\u4e49\u4e00\u4e2a\u6807\u51c6\u7684\u9884\u5904\u7406\u6d41\u7a0b\ntransform = transforms.Compose([\n    # 1. \u8c03\u6574\u56fe\u50cf\u5c3a\u5bf8\u5230 256x256\n    transforms.Resize(256),  \n    \n    # 2. \u4ece 256x256 \u7684\u56fe\u50cf\u4e2d\u968f\u673a\u88c1\u526a 224x224 \u7684\u533a\u57df (\u7528\u4e8e\u6570\u636e\u589e\u5f3a)\n    transforms.RandomCrop(224), \n    \n    # 3. \u4ee5 50% \u7684\u6982\u7387\u6c34\u5e73\u7ffb\u8f6c\u56fe\u50cf (\u7528\u4e8e\u6570\u636e\u589e\u5f3a)\n    transforms.RandomHorizontalFlip(),\n    \n    # 4. \u5fc5\u987b\u64cd\u4f5c\uff1a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a PyTorch Tensor\n    transforms.ToTensor(),      \n    \n    # 5. \u5fc5\u987b\u64cd\u4f5c\uff1a\u8fdb\u884c\u6807\u51c6\u5316 (\u4f7f\u7528 ImageNet \u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u4f5c\u4e3a\u793a\u4f8b)\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\n# \u5047\u8bbe image \u662f\u4e00\u4e2a PIL \u56fe\u50cf\u5bf9\u8c61\n# processed_tensor = transform(image)\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"\u603b\u7ed3\u00b6","text":"<p><code>torchvision.transforms</code> \u6a21\u5757\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u3001\u7075\u6d3b\u7684 API\uff0c\u4f7f\u5f97\u6df1\u5ea6\u5b66\u4e60\u5de5\u7a0b\u5e08\u53ef\u4ee5\u65b9\u4fbf\u5730\uff1a</p> <ul> <li>\u6784\u5efa\u6a21\u578b\u6240\u9700\u7684\u8f93\u5165\u5f20\u91cf\u3002</li> <li>\u5e94\u7528\u5404\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#target-transform","title":"\u4ee3\u7801\u89e3\u6790\uff1a\u76ee\u6807\u8f6c\u6362 (Target Transform)\u00b6","text":"<pre>target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n</pre>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#1-lambda","title":"1. <code>Lambda(...)</code> \u7684\u4f5c\u7528\u00b6","text":"<ul> <li>\u56de\u987e\uff1a <code>Lambda</code> \u662f <code>torchvision.transforms</code> \u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u7c7b\uff0c\u5b83\u5141\u8bb8\u60a8\u5c06\u4efb\u4f55 \u81ea\u5b9a\u4e49\u7684\u3001\u5355\u884c\u53ef\u6267\u884c\u7684\u51fd\u6570 \u5c01\u88c5\u6210\u4e00\u4e2a\u8f6c\u6362\u64cd\u4f5c\u3002</li> <li>\u8f93\u5165\uff1a \u8fd9\u91cc\u7684 <code>Lambda</code> \u63a5\u6536\u4e00\u4e2a\u533f\u540d\u51fd\u6570 <code>lambda y: ...</code>\u3002</li> <li><code>y</code>\uff1a \u5728 FashionMNIST \u6570\u636e\u96c6\u4e2d\uff0c<code>y</code> \u5c31\u662f\u5f53\u524d\u7684\u76ee\u6807\u6807\u7b7e (Target Label)\uff0c\u5b83\u662f\u4e00\u4e2a\u8868\u793a\u7c7b\u522b\u7d22\u5f15\u7684 \u6574\u6570\u6807\u91cf\uff08\u4f8b\u5982\uff1aT\u6064\u886b\u662f <code>0</code>\uff0c\u88e4\u5b50\u662f <code>1</code>\uff0c\u978b\u5b50\u662f <code>9</code>\uff09\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#2-lambda","title":"2. Lambda \u51fd\u6570\u4f53\uff1a\u6838\u5fc3\u7684\u72ec\u70ed\u7f16\u7801\u903b\u8f91\u00b6","text":"<p>Lambda \u51fd\u6570\u4f53\u6267\u884c\u4e86\u4e09\u4e2a\u6b65\u9aa4\u6765\u5b8c\u6210\u72ec\u70ed\u7f16\u7801\uff1a</p>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#a","title":"A. \u521b\u5efa\u4e00\u4e2a\u5168\u96f6\u5411\u91cf (\u5bb9\u5668)\u00b6","text":"<pre>torch.zeros(10, dtype=torch.float)\n</pre> <ul> <li><code>torch.zeros(10, ...)</code>\uff1a \u521b\u5efa\u4e00\u4e2a\u5305\u542b 10 \u4e2a\u5143\u7d20\u7684 \u4e00\u7ef4\u96f6\u5f20\u91cf\u3002<ul> <li>\u4e3a\u4ec0\u4e48\u662f 10\uff1f\u56e0\u4e3a FashionMNIST \u6570\u636e\u96c6\u6709 10 \u4e2a\u7c7b\u522b\u3002</li> <li>\u8fd9\u4e2a\u5f20\u91cf\u5c31\u662f\u6211\u4eec\u6700\u7ec8\u7684\u72ec\u70ed\u7f16\u7801\u5bb9\u5668\uff0c\u5f62\u72b6\u662f <code>(10,)</code>\u3002</li> </ul> </li> <li><code>dtype=torch.float</code>\uff1a \u786e\u4fdd\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\u662f\u6d6e\u70b9\u6570\uff0c\u8fd9\u901a\u5e38\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u635f\u5931\u51fd\u6570\uff08\u5982\u4ea4\u53c9\u71b5\u635f\u5931\uff09\u6240\u8981\u6c42\u7684\u8f93\u5165\u683c\u5f0f\u3002</li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#b-scatter_","title":"B. \u6838\u5fc3\u64cd\u4f5c\uff1a<code>.scatter_()</code>\u00b6","text":"<p>\u8fd9\u4e2a\u64cd\u4f5c\u662f\u5b9e\u73b0\u72ec\u70ed\u7f16\u7801\u7684\u5173\u952e\u3002\u5b83\u5c06\u4e00\u4e2a\u6570\u503c \u5206\u6563 (scatter) \u5199\u5165\u76ee\u6807\u5f20\u91cf\u7684\u6307\u5b9a\u7d22\u5f15\u4f4d\u7f6e\u3002</p> <ul> <li>\u64cd\u4f5c\u5bf9\u8c61\uff1a \u521a\u624d\u521b\u5efa\u7684\u90a3\u4e2a\u5168\u96f6\u5f20\u91cf\u3002</li> <li>\u8bed\u6cd5\u56de\u987e\uff1a <code>target.scatter_(dim, index, value)</code></li> </ul>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#c-scatter_","title":"C. \u7406\u89e3 <code>.scatter_()</code> \u7684\u53c2\u6570\u00b6","text":"\u53c2\u6570 \u8868\u8fbe\u5f0f \u5b9e\u9645\u503c (\u5047\u8bbe $y=3$) \u4f5c\u7528 <code>dim</code> <code>0</code> <code>0</code> \u5206\u6563\u7ef4\u5ea6\uff1a \u56e0\u4e3a\u76ee\u6807\u5f20\u91cf\u662f 1 \u7ef4\u7684 <code>(10,)</code>\uff0c\u6240\u4ee5\u53ea\u6709 <code>dim=0</code> \u53ef\u9009\u3002\u64cd\u4f5c\u5c06\u6cbf\u7740\u8fd9\u4e2a\u552f\u4e00\u7684\u7ef4\u5ea6\u8fdb\u884c\u3002 <code>index</code> <code>torch.tensor(y)</code> <code>torch.tensor(3)</code> \u7d22\u5f15\u5f20\u91cf\uff1a <code>y</code> \u662f\u4e00\u4e2a Python \u6574\u6570\uff08\u5982 3\uff09\uff0c\u4f46 <code>.scatter_</code> \u8981\u6c42\u7d22\u5f15\u5fc5\u987b\u662f PyTorch \u5f20\u91cf\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u5176\u5305\u88c5\u6210\u4e00\u4e2a\u5355\u5143\u7d20\u5f20\u91cf\u3002 <code>value</code> <code>value=1</code> <code>1</code> \u5199\u5165\u503c\uff1a \u8981\u5199\u5165\u5230\u76ee\u6807\u5f20\u91cf\u6307\u5b9a\u4f4d\u7f6e\u7684\u503c\u3002\u72ec\u70ed\u7f16\u7801\u8981\u6c42\u5c06\u5bf9\u5e94\u7c7b\u522b\u4f4d\u7f6e\u7684\u503c\u8bbe\u7f6e\u4e3a 1\u3002"},{"location":"MachineLearning/Pytorch_Learning/transfrom/#3-y3","title":"3. \u5b8c\u6574\u8fc7\u7a0b\u6f14\u793a\uff08\u5047\u8bbe $y=3$\uff09\u00b6","text":"<p>\u5047\u8bbe\u5f53\u524d\u6837\u672c\u7684\u6807\u7b7e $y=3$\uff08\u5373\u7b2c 4 \u4e2a\u7c7b\u522b\uff09\uff1a</p> <ol> <li><p>\u521d\u59cb\u96f6\u5f20\u91cf (Target): $$T = [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]$$ (\u5f62\u72b6 $(10$, $)$)</p> </li> <li><p>\u7d22\u5f15\u5f20\u91cf (Index): $$I = [3]$$ (\u5f62\u72b6 $(1$, $)$)</p> </li> <li><p>\u6267\u884c <code>.scatter_(0, I, value=1)</code>: \u8fd9\u4e2a\u64cd\u4f5c\u610f\u5473\u7740\uff1a\u5c06\u503c <code>1</code> \u5199\u5165\u5230 $T$ \u4e2d\uff0c\u4f4d\u7f6e\u7531 $I$ \u6307\u5b9a\uff0c\u6cbf\u7740 $dim=0$\u3002</p> <ul> <li>$T[I[0]] = 1$</li> <li>$T[3] = 1$</li> </ul> </li> <li><p>\u6700\u7ec8\u72ec\u70ed\u7f16\u7801\u7ed3\u679c: $$T_{final} = [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]$$</p> </li> </ol>"},{"location":"MachineLearning/Pytorch_Learning/transfrom/","title":"\u603b\u7ed3\uff1a\u8fd9\u6bb5\u4ee3\u7801\u7684\u610f\u4e49\u00b6","text":"<p>\u8fd9\u6bb5 <code>target_transform</code> \u4ee3\u7801\u7684\u76ee\u7684\u662f\u5c06\u539f\u59cb\u7684 \u6574\u6570\u7c7b\u522b\u6807\u7b7e\uff08\u5982 <code>3</code>\uff09\u8f6c\u6362\u6210 \u957f\u5ea6\u4e3a\u7c7b\u522b\u603b\u6570 (10) \u7684\u5411\u91cf\uff0c\u5176\u4e2d\u53ea\u6709\u5bf9\u5e94\u7c7b\u522b\u7684\u7d22\u5f15\u4f4d\u7f6e\u4e3a <code>1</code>\uff0c\u5176\u4ed6\u4f4d\u7f6e\u4e3a <code>0</code>\u3002</p> <p>\u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u6837\u505a\uff1f</p> <ul> <li>\u4ea4\u53c9\u71b5\u635f\u5931 (Cross-Entropy Loss)\uff1a \u5728 PyTorch \u4e2d\uff0c\u867d\u7136 <code>nn.CrossEntropyLoss</code> \u4f18\u5316\u5668\u53ef\u4ee5\u76f4\u63a5\u63a5\u6536\u6574\u6570 ID \u6807\u7b7e\uff0c\u4f46\u5f53\u6d89\u53ca\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u635f\u5931\u51fd\u6570\u3001\u81ea\u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570\uff08\u5982 Sigmoid \u8f93\u51fa\uff09\u6216\u6267\u884c\u67d0\u4e9b\u7279\u6b8a\u7684\u68af\u5ea6\u8ba1\u7b97\u65f6\uff0c\u72ec\u70ed\u7f16\u7801 \u7684\u683c\u5f0f\uff08One-Hot Vector\uff09\u662f\u66f4\u901a\u7528\u548c\u6807\u51c6\u7684\u6807\u7b7e\u683c\u5f0f\u3002</li> </ul>"},{"location":"MachineLearning/part1/Cost_function_Soln/","title":"Optional Lab: Cost Function","text":"In\u00a0[2]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl plt.style.use('./deeplearning.mplstyle') In\u00a0[3]: Copied! <pre>x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)\ny_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)\n</pre> x_train = np.array([1.0, 2.0])           #(size in 1000 square feet) y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars) <p>The code below calculates cost by looping over each example. In each loop:</p> <ul> <li><code>f_wb</code>, a prediction is calculated</li> <li>the difference between the target and the prediction is calculated and squared.</li> <li>this is added to the total cost.</li> </ul> In\u00a0[4]: Copied! <pre>def compute_cost(x, y, w, b): \n    \"\"\"\n    Computes the cost function for linear regression.\n    \n    Args:\n      x (ndarray (m,)): Data, m examples \n      y (ndarray (m,)): target values\n      w,b (scalar)    : model parameters  \n    \n    Returns\n        total_cost (float): The cost of using w,b as the parameters for linear regression\n               to fit the data points in x and y\n    \"\"\"\n    # number of training examples\n    m = x.shape[0] \n    \n    cost_sum = 0 \n    for i in range(m): \n        f_wb = w * x[i] + b   \n        cost = (f_wb - y[i]) ** 2  \n        cost_sum = cost_sum + cost  \n    total_cost = (1 / (2 * m)) * cost_sum  \n\n    return total_cost\n</pre> def compute_cost(x, y, w, b):      \"\"\"     Computes the cost function for linear regression.          Args:       x (ndarray (m,)): Data, m examples        y (ndarray (m,)): target values       w,b (scalar)    : model parameters            Returns         total_cost (float): The cost of using w,b as the parameters for linear regression                to fit the data points in x and y     \"\"\"     # number of training examples     m = x.shape[0]           cost_sum = 0      for i in range(m):          f_wb = w * x[i] + b            cost = (f_wb - y[i]) ** 2           cost_sum = cost_sum + cost       total_cost = (1 / (2 * m)) * cost_sum        return total_cost <p> Your goal is to find a model $f_{w,b}(x) = wx + b$, with parameters $w,b$,  which will accurately predict house values given an input $x$. The cost is a measure of how accurate the model is on the training data.</p> <p>The cost equation (1) above shows that if $w$ and $b$ can be selected such that the predictions $f_{w,b}(x)$ match the target data $y$, the $(f_{w,b}(x^{(i)}) - y^{(i)})^2 $ term will be zero and the cost minimized. In this simple two point example, you can achieve this!</p> <p>In the previous lab, you determined that $b=100$ provided an optimal solution so let's set $b$ to 100 and focus on $w$.</p> <p> Below, use the slider control to select the value of $w$ that minimizes cost. It can take a few seconds for the plot to update.</p> In\u00a0[5]: Copied! <pre>plt_intuition(x_train,y_train)\n</pre> plt_intuition(x_train,y_train) <pre>interactive(children=(IntSlider(value=150, description='w', max=400, step=10), Output()), _dom_classes=('widge\u2026</pre> <p>The plot contains a few points that are worth mentioning.</p> <ul> <li>cost is minimized when $w = 200$, which matches results from the previous lab</li> <li>Because the difference between the target and pediction is squared in the cost equation, the cost increases rapidly when $w$ is either too large or too small.</li> <li>Using the <code>w</code> and <code>b</code> selected by minimizing cost results in a line which is a perfect fit to the data.</li> </ul> In\u00a0[6]: Copied! <pre>x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\ny_train = np.array([250, 300, 480,  430,   630, 730,])\n</pre> x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2]) y_train = np.array([250, 300, 480,  430,   630, 730,]) <p>In the contour plot, click on a point to select <code>w</code> and <code>b</code> to achieve the lowest cost. Use the contours to guide your selections. Note, it can take a few seconds to update the graph.</p> In\u00a0[7]: Copied! <pre>plt.close('all') \nfig, ax, dyn_items = plt_stationary(x_train, y_train)\nupdater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)\n</pre> plt.close('all')  fig, ax, dyn_items = plt_stationary(x_train, y_train) updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)                      Figure                  <p>Above, note the dashed lines in the left plot. These represent the portion of the cost contributed by each example in your training set. In this case, values of approximately $w=209$ and $b=2.4$ provide low cost. Note that, because our training examples are not on a line, the minimum cost is not zero.</p> In\u00a0[8]: Copied! <pre>soup_bowl()\n</pre> soup_bowl()                      Figure                  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part1/Cost_function_Soln/#optional-lab-cost-function","title":"Optional  Lab: Cost Function\u00b6","text":""},{"location":"MachineLearning/part1/Cost_function_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>you will implement and explore the <code>cost</code> function for linear regression with one variable.</li> </ul>"},{"location":"MachineLearning/part1/Cost_function_Soln/#tools","title":"Tools\u00b6","text":"<p>In this lab we will make use of:</p> <ul> <li>NumPy, a popular library for scientific computing</li> <li>Matplotlib, a popular library for plotting data</li> <li>local plotting routines in the lab_utils_uni.py file in the local directory</li> </ul>"},{"location":"MachineLearning/part1/Cost_function_Soln/#problem-statement","title":"Problem Statement\u00b6","text":"<p>You would like a model which can predict housing prices given the size of the house. Let's use the same two data points as before the previous lab- a house with 1000 square feet sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000.</p> Size (1000 sqft) Price (1000s of dollars) 1 300 2 500"},{"location":"MachineLearning/part1/Cost_function_Soln/#computing-cost","title":"Computing Cost\u00b6","text":"<p>The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.</p> <p>The equation for cost with one variable is: $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$</p> <p>where $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$</p> <ul> <li>$f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.</li> <li>$(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.</li> <li>These differences are summed over all the $m$ examples and divided by <code>2m</code> to produce the cost, $J(w,b)$.</li> </ul> <p>Note, in lecture summation ranges are typically from 1 to m, while code will be from 0 to m-1.</p>"},{"location":"MachineLearning/part1/Cost_function_Soln/#cost-function-intuition","title":"Cost Function Intuition\u00b6","text":""},{"location":"MachineLearning/part1/Cost_function_Soln/#cost-function-visualization-3d","title":"Cost Function Visualization- 3D\u00b6","text":"<p>You can see how cost varies with respect to both <code>w</code> and <code>b</code> by plotting in 3D or using a contour plot. It is worth noting that some of the plotting in this course can become quite involved. The plotting routines are provided and while it can be instructive to read through the code to become familiar with the methods, it is not needed to complete the course successfully. The routines are in lab_utils_uni.py in the local directory.</p>"},{"location":"MachineLearning/part1/Cost_function_Soln/#larger-data-set","title":"Larger Data Set\u00b6","text":"<p>It's use instructive to view a scenario with a few more data points. This data set includes data points that do not fall on the same line. What does that mean for the cost equation? Can we find $w$, and $b$ that will give us a cost of 0?</p>"},{"location":"MachineLearning/part1/Cost_function_Soln/#convex-cost-surface","title":"Convex Cost surface\u00b6","text":"<p>The fact that the cost function squares the loss ensures that the 'error surface' is convex like a soup bowl. It will always have a minimum that can be reached by following the gradient in all dimensions. In the previous plot, because the $w$ and $b$ dimensions scale differently, this is not easy to recognize. The following plot, where $w$ and $b$ are symmetric, was shown in lecture:</p>"},{"location":"MachineLearning/part1/Cost_function_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have learned the following:</p> <ul> <li>The cost equation provides a measure of how well your predictions match your training data.</li> <li>Minimizing the cost can provide optimal values of $w$, $b$.</li> </ul>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/","title":"Optional Lab: Gradient Descent for Linear Regression","text":"In\u00a0[1]: Copied! <pre>import math, copy\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nfrom lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients\n</pre> import math, copy import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients In\u00a0[2]: Copied! <pre># Load our data set\nx_train = np.array([1.0, 2.0])   #features\ny_train = np.array([300.0, 500.0])   #target value\n</pre> # Load our data set x_train = np.array([1.0, 2.0])   #features y_train = np.array([300.0, 500.0])   #target value In\u00a0[3]: Copied! <pre>#Function to calculate the cost\ndef compute_cost(x, y, w, b):\n   \n    m = x.shape[0] \n    cost = 0\n    \n    for i in range(m):\n        f_wb = w * x[i] + b\n        cost = cost + (f_wb - y[i])**2\n    total_cost = 1 / (2 * m) * cost\n\n    return total_cost\n</pre> #Function to calculate the cost def compute_cost(x, y, w, b):         m = x.shape[0]      cost = 0          for i in range(m):         f_wb = w * x[i] + b         cost = cost + (f_wb - y[i])**2     total_cost = 1 / (2 * m) * cost      return total_cost <p>In lecture, gradient descent was described as:</p> <p>$$\\begin{align*} \\text{repeat}&amp;\\text{ until convergence:} \\; \\lbrace \\newline \\;  w &amp;= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline   b &amp;= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace \\end{align*}$$ where, parameters $w$, $b$ are updated simultaneously. The gradient is defined as: $$ \\begin{align} \\frac{\\partial J(w,b)}{\\partial w}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\   \\frac{\\partial J(w,b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\ \\end{align} $$</p> <p>Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.</p> <p></p> <p></p> In\u00a0[4]: Copied! <pre>def compute_gradient(x, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      x (ndarray (m,)): Data, m examples \n      y (ndarray (m,)): target values\n      w,b (scalar)    : model parameters  \n    Returns\n      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n     \"\"\"\n    \n    # Number of training examples\n    m = x.shape[0]    \n    dj_dw = 0\n    dj_db = 0\n    \n    for i in range(m):  \n        f_wb = w * x[i] + b \n        dj_dw_i = (f_wb - y[i]) * x[i] \n        dj_db_i = f_wb - y[i] \n        dj_db += dj_db_i\n        dj_dw += dj_dw_i \n    dj_dw = dj_dw / m \n    dj_db = dj_db / m \n        \n    return dj_dw, dj_db\n</pre> def compute_gradient(x, y, w, b):      \"\"\"     Computes the gradient for linear regression      Args:       x (ndarray (m,)): Data, m examples        y (ndarray (m,)): target values       w,b (scalar)    : model parameters       Returns       dj_dw (scalar): The gradient of the cost w.r.t. the parameters w       dj_db (scalar): The gradient of the cost w.r.t. the parameter b           \"\"\"          # Number of training examples     m = x.shape[0]         dj_dw = 0     dj_db = 0          for i in range(m):           f_wb = w * x[i] + b          dj_dw_i = (f_wb - y[i]) * x[i]          dj_db_i = f_wb - y[i]          dj_db += dj_db_i         dj_dw += dj_dw_i      dj_dw = dj_dw / m      dj_db = dj_db / m               return dj_dw, dj_db <p></p> <p> The lectures described how gradient descent utilizes the partial derivative of the cost with respect to a parameter at a point to update that parameter. Let's use our <code>compute_gradient</code> function to find and plot some partial derivatives of our cost function relative to one of the parameters, $w_0$.</p> In\u00a0[5]: Copied! <pre>plt_gradients(x_train,y_train, compute_cost, compute_gradient)\nplt.show()\n</pre> plt_gradients(x_train,y_train, compute_cost, compute_gradient) plt.show() <p>Above, the left plot shows $\\frac{\\partial J(w,b)}{\\partial w}$ or the slope of the cost curve relative to $w$ at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero.</p> <p>The left plot has fixed $b=100$. Gradient descent will utilize both $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ at that point. Note that the gradient points away from the minimum. Review equation (3) above. The scaled gradient is subtracted from the current value of $w$ or $b$. This moves the parameter in a direction that will reduce cost.</p> <p></p> In\u00a0[6]: Copied! <pre>def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n    \"\"\"\n    Performs gradient descent to fit w,b. Updates w,b by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      x (ndarray (m,))  : Data, m examples \n      y (ndarray (m,))  : target values\n      w_in,b_in (scalar): initial values of model parameters  \n      alpha (float):     Learning rate\n      num_iters (int):   number of iterations to run gradient descent\n      cost_function:     function to call to produce cost\n      gradient_function: function to call to produce gradient\n      \n    Returns:\n      w (scalar): Updated value of parameter after running gradient descent\n      b (scalar): Updated value of parameter after running gradient descent\n      J_history (List): History of cost values\n      p_history (list): History of parameters [w,b] \n      \"\"\"\n    \n    w = copy.deepcopy(w_in) # avoid modifying global w_in\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    p_history = []\n    b = b_in\n    w = w_in\n    \n    for i in range(num_iters):\n        # Calculate the gradient and update the parameters using gradient_function\n        dj_dw, dj_db = gradient_function(x, y, w , b)     \n\n        # Update Parameters using equation (3) above\n        b = b - alpha * dj_db                            \n        w = w - alpha * dj_dw                            \n\n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion \n            J_history.append( cost_function(x, y, w , b))\n            p_history.append([w,b])\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n \n    return w, b, J_history, p_history #return w and J,w history for graphing\n</pre> def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):      \"\"\"     Performs gradient descent to fit w,b. Updates w,b by taking      num_iters gradient steps with learning rate alpha          Args:       x (ndarray (m,))  : Data, m examples        y (ndarray (m,))  : target values       w_in,b_in (scalar): initial values of model parameters         alpha (float):     Learning rate       num_iters (int):   number of iterations to run gradient descent       cost_function:     function to call to produce cost       gradient_function: function to call to produce gradient            Returns:       w (scalar): Updated value of parameter after running gradient descent       b (scalar): Updated value of parameter after running gradient descent       J_history (List): History of cost values       p_history (list): History of parameters [w,b]        \"\"\"          w = copy.deepcopy(w_in) # avoid modifying global w_in     # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     p_history = []     b = b_in     w = w_in          for i in range(num_iters):         # Calculate the gradient and update the parameters using gradient_function         dj_dw, dj_db = gradient_function(x, y, w , b)               # Update Parameters using equation (3) above         b = b - alpha * dj_db                                     w = w - alpha * dj_dw                                      # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion              J_history.append( cost_function(x, y, w , b))             p_history.append([w,b])         # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters/10) == 0:             print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",                   f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",                   f\"w: {w: 0.3e}, b:{b: 0.5e}\")       return w, b, J_history, p_history #return w and J,w history for graphing In\u00a0[7]: Copied! <pre># initialize parameters\nw_init = 0\nb_init = 0\n# some gradient descent settings\niterations = 10000\ntmp_alpha = 1.0e-2\n# run gradient descent\nw_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n                                                    iterations, compute_cost, compute_gradient)\nprint(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")\n</pre> # initialize parameters w_init = 0 b_init = 0 # some gradient descent settings iterations = 10000 tmp_alpha = 1.0e-2 # run gradient descent w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,                                                      iterations, compute_cost, compute_gradient) print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\") <pre>Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\nIteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\nIteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\nIteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\nIteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\nIteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\nIteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\nIteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\nIteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\nIteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n(w,b) found by gradient descent: (199.9929,100.0116)\n</pre>   Take a moment and note some characteristics of the gradient descent process printed above.    <ul> <li>The cost starts large and rapidly declines as described in the slide from the lecture.</li> <li>The partial derivatives, <code>dj_dw</code>, and <code>dj_db</code> also get smaller, rapidly at first and then more slowly. As shown in the diagram from the lecture, as the process nears the 'bottom of the bowl' progress is slower due to the smaller value of the derivative at that point.</li> <li>progress slows though the learning rate, alpha, remains fixed</li> </ul> In\u00a0[8]: Copied! <pre># plot cost versus iteration  \nfig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\nax1.plot(J_hist[:100])\nax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\nax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\nax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \nax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \nplt.show()\n</pre> # plot cost versus iteration   fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4)) ax1.plot(J_hist[:100]) ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:]) ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\") ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')  ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')  plt.show() In\u00a0[9]: Copied! <pre>print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\nprint(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\nprint(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")\n</pre> print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\") print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\") print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\") <pre>1000 sqft house prediction 300.0 Thousand dollars\n1200 sqft house prediction 340.0 Thousand dollars\n2000 sqft house prediction 500.0 Thousand dollars\n</pre> <p></p> In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(12, 6))\nplt_contour_wgrad(x_train, y_train, p_hist, ax)\n</pre> fig, ax = plt.subplots(1,1, figsize=(12, 6)) plt_contour_wgrad(x_train, y_train, p_hist, ax) <p>Above, the contour plot shows the $cost(w,b)$ over a range of $w$ and $b$. Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note:</p> <ul> <li>The path makes steady (monotonic) progress toward its goal.</li> <li>initial steps are much larger than the steps near the goal.</li> </ul> <p>Zooming in, we can see that final steps of gradient descent. Note the distance between steps shrinks as the gradient approaches zero.</p> In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(12, 4))\nplt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n            contours=[1,5,10,20],resolution=0.5)\n</pre> fig, ax = plt.subplots(1,1, figsize=(12, 4)) plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],             contours=[1,5,10,20],resolution=0.5) <p></p> In\u00a0[12]: Copied! <pre># initialize parameters\nw_init = 0\nb_init = 0\n# set alpha to a large value\niterations = 10\ntmp_alpha = 8.0e-1\n# run gradient descent\nw_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n                                                    iterations, compute_cost, compute_gradient)\n</pre> # initialize parameters w_init = 0 b_init = 0 # set alpha to a large value iterations = 10 tmp_alpha = 8.0e-1 # run gradient descent w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,                                                      iterations, compute_cost, compute_gradient) <pre>Iteration    0: Cost 2.58e+05  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  5.200e+02, b: 3.20000e+02\nIteration    1: Cost 7.82e+05  dj_dw:  1.130e+03, dj_db:  7.000e+02   w: -3.840e+02, b:-2.40000e+02\nIteration    2: Cost 2.37e+06  dj_dw: -1.970e+03, dj_db: -1.216e+03   w:  1.192e+03, b: 7.32800e+02\nIteration    3: Cost 7.19e+06  dj_dw:  3.429e+03, dj_db:  2.121e+03   w: -1.551e+03, b:-9.63840e+02\nIteration    4: Cost 2.18e+07  dj_dw: -5.974e+03, dj_db: -3.691e+03   w:  3.228e+03, b: 1.98886e+03\nIteration    5: Cost 6.62e+07  dj_dw:  1.040e+04, dj_db:  6.431e+03   w: -5.095e+03, b:-3.15579e+03\nIteration    6: Cost 2.01e+08  dj_dw: -1.812e+04, dj_db: -1.120e+04   w:  9.402e+03, b: 5.80237e+03\nIteration    7: Cost 6.09e+08  dj_dw:  3.156e+04, dj_db:  1.950e+04   w: -1.584e+04, b:-9.80139e+03\nIteration    8: Cost 1.85e+09  dj_dw: -5.496e+04, dj_db: -3.397e+04   w:  2.813e+04, b: 1.73730e+04\nIteration    9: Cost 5.60e+09  dj_dw:  9.572e+04, dj_db:  5.916e+04   w: -4.845e+04, b:-2.99567e+04\n</pre> <p>Above, $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. This is a clear sign that the learning rate is too large and the solution is diverging. Let's visualize this with a plot.</p> In\u00a0[13]: Copied! <pre>plt_divergence(p_hist, J_hist,x_train, y_train)\nplt.show()\n</pre> plt_divergence(p_hist, J_hist,x_train, y_train) plt.show() <p>Above, the left graph shows $w$'s progression over the first few steps of gradient descent. $w$ oscillates from positive to negative and cost grows rapidly. Gradient Descent is operating on both $w$ and $b$ simultaneously, so one needs the 3-D plot on the right for the complete picture.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#optional-lab-gradient-descent-for-linear-regression","title":"Optional Lab: Gradient Descent for Linear Regression\u00b6","text":""},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>automate the process of optimizing $w$ and $b$ using gradient descent.</li> </ul>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#tools","title":"Tools\u00b6","text":"<p>In this lab, we will make use of:</p> <ul> <li>NumPy, a popular library for scientific computing</li> <li>Matplotlib, a popular library for plotting data</li> <li>plotting routines in the lab_utils.py file in the local directory</li> </ul>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#problem-statement","title":"Problem Statement\u00b6","text":"<p>Let's use the same two data points as before - a house with 1000 square feet sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000.</p> Size (1000 sqft) Price (1000s of dollars) 1 300 2 500"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#compute_cost","title":"Compute_Cost\u00b6","text":"<p>This was developed in the last lab. We'll need it again here.</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#gradient-descent-summary","title":"Gradient descent summary\u00b6","text":"<p>So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$: $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$ In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$ $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#implement-gradient-descent","title":"Implement Gradient Descent\u00b6","text":"<p>You will implement gradient descent algorithm for one feature. You will need three functions.</p> <ul> <li><code>compute_gradient</code> implementing equation (4) and (5) above</li> <li><code>compute_cost</code> implementing equation (2) above (code from previous lab)</li> <li><code>gradient_descent</code>, utilizing compute_gradient and compute_cost</li> </ul> <p>Conventions:</p> <ul> <li>The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be <code>dj_db</code>.</li> <li>w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.</li> </ul>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#compute_gradient","title":"compute_gradient\u00b6","text":"<p> <code>compute_gradient</code>  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations.</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#gradient-descent","title":"Gradient Descent\u00b6","text":"<p>Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in <code>gradient_descent</code>. The details of the implementation are described in the comments. Below, you will utilize this function to find optimal values of $w$ and $b$ on the training data.</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#cost-versus-iterations-of-gradient-descent","title":"Cost versus iterations of gradient descent\u00b6","text":"<p>A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step.</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#predictions","title":"Predictions\u00b6","text":"<p>Now that you have discovered the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value.</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#plotting","title":"Plotting\u00b6","text":"<p>You can show the progress of gradient descent during its execution by plotting the cost over iterations on a contour plot of the cost(w,b).</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#increased-learning-rate","title":"Increased Learning Rate\u00b6","text":"In the lecture, there was a discussion related to the proper value of the learning rate, $\\alpha$ in equation(3). The larger $\\alpha$ is, the faster gradient descent will converge to a solution. But, if it is too large, gradient descent will diverge. Above you have an example of a solution which converges nicely.  <p>Let's try increasing the value of  $\\alpha$ and see what happens:</p>"},{"location":"MachineLearning/part1/Gradient_Descent_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>delved into the details of gradient descent for a single variable.</li> <li>developed a routine to compute the gradient</li> <li>visualized what the gradient is</li> <li>completed a gradient descent routine</li> <li>utilized gradient descent to find parameters</li> <li>examined the impact of sizing the learning rate</li> </ul>"},{"location":"MachineLearning/part1/Model_Representation_Soln/","title":"Optional Lab: Model Representation","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') <p>Please run the following code cell to create your <code>x_train</code> and <code>y_train</code> variables. The data is stored in one-dimensional NumPy arrays.</p> In\u00a0[2]: Copied! <pre># x_train is the input variable (size in 1000 square feet)\n# y_train is the target (price in 1000s of dollars)\nx_train = np.array([1.0, 2.0])\ny_train = np.array([300.0, 500.0])\nprint(f\"x_train = {x_train}\")\nprint(f\"y_train = {y_train}\")\n</pre> # x_train is the input variable (size in 1000 square feet) # y_train is the target (price in 1000s of dollars) x_train = np.array([1.0, 2.0]) y_train = np.array([300.0, 500.0]) print(f\"x_train = {x_train}\") print(f\"y_train = {y_train}\") <pre>x_train = [1. 2.]\ny_train = [300. 500.]\n</pre> <p>Note: The course will frequently utilize the python 'f-string' output formatting described here when printing. The content between the curly braces is evaluated when producing the output.</p> In\u00a0[3]: Copied! <pre># m is the number of training examples\nprint(f\"x_train.shape: {x_train.shape}\")\nm = x_train.shape[0]\nprint(f\"Number of training examples is: {m}\")\n</pre> # m is the number of training examples print(f\"x_train.shape: {x_train.shape}\") m = x_train.shape[0] print(f\"Number of training examples is: {m}\") <pre>x_train.shape: (2,)\nNumber of training examples is: 2\n</pre> <p>One can also use the Python <code>len()</code> function as shown below.</p> In\u00a0[4]: Copied! <pre># m is the number of training examples\nm = len(x_train)\nprint(f\"Number of training examples is: {m}\")\n</pre> # m is the number of training examples m = len(x_train) print(f\"Number of training examples is: {m}\") <pre>Number of training examples is: 2\n</pre> In\u00a0[5]: Copied! <pre>i = 0 # Change this to 1 to see (x^1, y^1)\n\nx_i = x_train[i]\ny_i = y_train[i]\nprint(f\"(x^({i}), y^({i})) = ({x_i}, {y_i})\")\n</pre> i = 0 # Change this to 1 to see (x^1, y^1)  x_i = x_train[i] y_i = y_train[i] print(f\"(x^({i}), y^({i})) = ({x_i}, {y_i})\") <pre>(x^(0), y^(0)) = (1.0, 300.0)\n</pre> <p>You can plot these two points using the <code>scatter()</code> function in the <code>matplotlib</code> library, as shown in the cell below.</p> <ul> <li>The function arguments <code>marker</code> and <code>c</code> show the points as red crosses (the default is blue dots).</li> </ul> <p>You can use other functions in the <code>matplotlib</code> library to set the title and labels to display</p> In\u00a0[6]: Copied! <pre># Plot the data points\nplt.scatter(x_train, y_train, marker='x', c='r')\n# Set the title\nplt.title(\"Housing Prices\")\n# Set the y-axis label\nplt.ylabel('Price (in 1000s of dollars)')\n# Set the x-axis label\nplt.xlabel('Size (1000 sqft)')\nplt.show()\n</pre> # Plot the data points plt.scatter(x_train, y_train, marker='x', c='r') # Set the title plt.title(\"Housing Prices\") # Set the y-axis label plt.ylabel('Price (in 1000s of dollars)') # Set the x-axis label plt.xlabel('Size (1000 sqft)') plt.show() In\u00a0[7]: Copied! <pre>w = 100\nb = 100\nprint(f\"w: {w}\")\nprint(f\"b: {b}\")\n</pre> w = 100 b = 100 print(f\"w: {w}\") print(f\"b: {b}\") <pre>w: 100\nb: 100\n</pre> <p>Now, let's compute the value of $f_{w,b}(x^{(i)})$ for your two data points. You can explicitly write this out for each data point as -</p> <p>for $x^{(0)}$, <code>f_wb = w * x[0] + b</code></p> <p>for $x^{(1)}$, <code>f_wb = w * x[1] + b</code></p> <p>For a large number of data points, this can get unwieldy and repetitive. So instead, you can calculate the function output in a <code>for</code> loop as shown in the <code>compute_model_output</code> function below.</p> <p>Note: The argument description <code>(ndarray (m,))</code> describes a Numpy n-dimensional array of shape (m,). <code>(scalar)</code> describes an argument without dimensions, just a magnitude. Note: <code>np.zero(n)</code> will return a one-dimensional numpy array with $n$ entries</p> In\u00a0[8]: Copied! <pre>def compute_model_output(x, w, b):\n    \"\"\"\n    Computes the prediction of a linear model\n    Args:\n      x (ndarray (m,)): Data, m examples \n      w,b (scalar)    : model parameters  \n    Returns\n      y (ndarray (m,)): target values\n    \"\"\"\n    m = x.shape[0]\n    f_wb = np.zeros(m)\n    for i in range(m):\n        f_wb[i] = w * x[i] + b\n        \n    return f_wb\n</pre> def compute_model_output(x, w, b):     \"\"\"     Computes the prediction of a linear model     Args:       x (ndarray (m,)): Data, m examples        w,b (scalar)    : model parameters       Returns       y (ndarray (m,)): target values     \"\"\"     m = x.shape[0]     f_wb = np.zeros(m)     for i in range(m):         f_wb[i] = w * x[i] + b              return f_wb <p>Now let's call the <code>compute_model_output</code> function and plot the output..</p> In\u00a0[9]: Copied! <pre>tmp_f_wb = compute_model_output(x_train, w, b,)\n\n# Plot our model prediction\nplt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')\n\n# Plot the data points\nplt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')\n\n# Set the title\nplt.title(\"Housing Prices\")\n# Set the y-axis label\nplt.ylabel('Price (in 1000s of dollars)')\n# Set the x-axis label\nplt.xlabel('Size (1000 sqft)')\nplt.legend()\nplt.show()\n</pre> tmp_f_wb = compute_model_output(x_train, w, b,)  # Plot our model prediction plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')  # Plot the data points plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')  # Set the title plt.title(\"Housing Prices\") # Set the y-axis label plt.ylabel('Price (in 1000s of dollars)') # Set the x-axis label plt.xlabel('Size (1000 sqft)') plt.legend() plt.show() <p>As you can see, setting $w = 100$ and $b = 100$ does not result in a line that fits our data.</p> Hints <p> <ul> <li>Try $w = 200$ and $b = 100$ </li> </ul> </p> In\u00a0[10]: Copied! <pre>w = 200                         \nb = 100    \nx_i = 1.2\ncost_1200sqft = w * x_i + b    \n\nprint(f\"${cost_1200sqft:.0f} thousand dollars\")\n</pre> w = 200                          b = 100     x_i = 1.2 cost_1200sqft = w * x_i + b      print(f\"${cost_1200sqft:.0f} thousand dollars\") <pre>$340 thousand dollars\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#optional-lab-model-representation","title":"Optional Lab: Model Representation\u00b6","text":""},{"location":"MachineLearning/part1/Model_Representation_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>Learn to implement the model $f_{w,b}$ for linear regression with one variable</li> </ul>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#notation","title":"Notation\u00b6","text":"<p>Here is a summary of some of the notation you will encounter.</p> <p>|General    Notation   | Description| Python (if applicable) | |: ------------|: ------------------------------------------------------------|| | $a$ | scalar, non bold                                                      || | $\\mathbf{a}$ | vector, bold                                                      || | Regression |         |    |     | |  $\\mathbf{x}$ | Training Example feature values (in this lab - Size (1000 sqft))  | <code>x_train</code> | |  $\\mathbf{y}$  | Training Example  targets (in this lab Price (1000s of dollars)).  | <code>y_train</code> |  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | <code>x_i</code>, <code>y_i</code>| | m | Number of training examples | <code>m</code>| |  $w$  |  parameter: weight,                                 | <code>w</code>    | |  $b$           |  parameter: bias                                           | <code>b</code>    | | $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | <code>f_wb</code> |</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#tools","title":"Tools\u00b6","text":"<p>In this lab you will make use of:</p> <ul> <li>NumPy, a popular library for scientific computing</li> <li>Matplotlib, a popular library for plotting data</li> </ul>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#problem-statement","title":"Problem Statement\u00b6","text":"<p>As in the lecture, you will use the motivating example of housing price prediction. This lab will use a simple data set with only two data points - a house with 1000 square feet(sqft) sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000. These two points will constitute our data or training set. In this lab, the units of size are 1000 sqft and the units of price are 1000s of dollars.</p> Size (1000 sqft) Price (1000s of dollars) 1.0 300 2.0 500 <p>You would like to fit a linear regression model (shown above as the blue straight line) through these two points, so you can then predict price for other houses - say, a house with 1200 sqft.</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#number-of-training-examples-m","title":"Number of training examples <code>m</code>\u00b6","text":"<p>You will use <code>m</code> to denote the number of training examples. Numpy arrays have a <code>.shape</code> parameter. <code>x_train.shape</code> returns a python tuple with an entry for each dimension. <code>x_train.shape[0]</code> is the length of the array and number of examples as shown below.</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#training-example-x_i-y_i","title":"Training example <code>x_i, y_i</code>\u00b6","text":"<p>You will use (x$^{(i)}$, y$^{(i)}$) to denote the $i^{th}$ training example. Since Python is zero indexed, (x$^{(0)}$, y$^{(0)}$) is (1.0, 300.0) and (x$^{(1)}$, y$^{(1)}$) is (2.0, 500.0).</p> <p>To access a value in a Numpy array, one indexes the array with the desired offset. For example the syntax to access location zero of <code>x_train</code> is <code>x_train[0]</code>. Run the next code block below to get the $i^{th}$ training example.</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#plotting-the-data","title":"Plotting the data\u00b6","text":""},{"location":"MachineLearning/part1/Model_Representation_Soln/#model-function","title":"Model function\u00b6","text":"<p> As described in lecture, the model function for linear regression (which is a function that maps from <code>x</code> to <code>y</code>) is represented as</p> <p>$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$</p> <p>The formula above is how you can represent straight lines - different values of $w$ and $b$ give you different straight lines on the plot.  </p> <p>Let's try to get a better intuition for this through the code blocks below. Let's start with $w = 100$ and $b = 100$.</p> <p>Note: You can come back to this cell to adjust the model's w and b parameters</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#challenge","title":"Challenge\u00b6","text":"<p>Try experimenting with different values of $w$ and $b$. What should the values be for a line that fits our data?</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#tip","title":"Tip:\u00b6","text":"<p>You can use your mouse to click on the triangle to the left of the green \"Hints\" below to reveal some hints for choosing b and w.</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#prediction","title":"Prediction\u00b6","text":"<p>Now that we have a model, we can use it to make our original prediction. Let's predict the price of a house with 1200 sqft. Since the units of $x$ are in 1000's of sqft, $x$ is 1.2.</p>"},{"location":"MachineLearning/part1/Model_Representation_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you have learned:</p> <ul> <li>Linear regression builds a model which establishes a relationship between features and targets<ul> <li>In the example above, the feature was house size and the target was house price</li> <li>for simple linear regression, the model has two parameters $w$ and $b$ whose values are 'fit' using training data.</li> <li>once a model's parameters have been determined, the model can be used to make predictions on novel data.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/","title":"Optional Lab: Feature Engineering and Polynomial Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng\nnp.set_printoptions(precision=2)  # reduced display precision on numpy arrays\n</pre> import numpy as np import matplotlib.pyplot as plt from lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng np.set_printoptions(precision=2)  # reduced display precision on numpy arrays In\u00a0[2]: Copied! <pre># create target data\nx = np.arange(0, 20, 1)\ny = 1 + x**2\nX = x.reshape(-1, 1)\n\nmodel_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\nplt.plot(x,X@model_w + model_b, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n</pre> # create target data x = np.arange(0, 20, 1) y = 1 + x**2 X = x.reshape(-1, 1)  model_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)  plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\") plt.plot(x,X@model_w + model_b, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show() <pre>Iteration         0, Cost: 1.65756e+03\nIteration       100, Cost: 6.94549e+02\nIteration       200, Cost: 5.88475e+02\nIteration       300, Cost: 5.26414e+02\nIteration       400, Cost: 4.90103e+02\nIteration       500, Cost: 4.68858e+02\nIteration       600, Cost: 4.56428e+02\nIteration       700, Cost: 4.49155e+02\nIteration       800, Cost: 4.44900e+02\nIteration       900, Cost: 4.42411e+02\nw,b found by gradient descent: w: [18.7], b: -52.0834\n</pre> <p>Well, as expected, not a great fit. What is needed is something like $y= w_0x_0^2 + b$, or a polynomial feature. To accomplish this, you can modify the input data to engineer the needed features. If you swap the original data with a version that squares the $x$ value, then you can achieve $y= w_0x_0^2 + b$. Let's try it. Swap <code>X</code> for <code>X**2</code> below:</p> In\u00a0[3]: Copied! <pre># create target data\nx = np.arange(0, 20, 1)\ny = 1 + x**2\n\n# Engineer features \nX = x**2      #&lt;-- added engineered feature\n</pre> # create target data x = np.arange(0, 20, 1) y = 1 + x**2  # Engineer features  X = x**2      #&lt;-- added engineered feature In\u00a0[4]: Copied! <pre>X = X.reshape(-1, 1)  #X should be a 2-D Matrix\nmodel_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\nplt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n</pre> X = X.reshape(-1, 1)  #X should be a 2-D Matrix model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)  plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\") plt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show() <pre>Iteration         0, Cost: 7.32922e+03\nIteration      1000, Cost: 2.24844e-01\nIteration      2000, Cost: 2.22795e-01\nIteration      3000, Cost: 2.20764e-01\nIteration      4000, Cost: 2.18752e-01\nIteration      5000, Cost: 2.16758e-01\nIteration      6000, Cost: 2.14782e-01\nIteration      7000, Cost: 2.12824e-01\nIteration      8000, Cost: 2.10884e-01\nIteration      9000, Cost: 2.08962e-01\nw,b found by gradient descent: w: [1.], b: 0.0490\n</pre> <p>Great! near perfect fit. Notice the values of $\\mathbf{w}$ and b printed right above the graph: <code>w,b found by gradient descent: w: [1.], b: 0.0490</code>. Gradient descent modified our initial values of $\\mathbf{w},b $ to be (1.0,0.049) or a model of $y=1*x_0^2+0.049$, very close to our target of $y=1*x_0^2+1$. If you ran it longer, it could be a better match.</p> In\u00a0[5]: Copied! <pre># create target data\nx = np.arange(0, 20, 1)\ny = x**2\n\n# engineer features .\nX = np.c_[x, x**2, x**3]   #&lt;-- added engineered feature\n</pre> # create target data x = np.arange(0, 20, 1) y = x**2  # engineer features . X = np.c_[x, x**2, x**3]   #&lt;-- added engineered feature In\u00a0[6]: Copied! <pre>model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\")\nplt.plot(x, X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n</pre> model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)  plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\") plt.plot(x, X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show() <pre>Iteration         0, Cost: 1.14029e+03\nIteration      1000, Cost: 3.28539e+02\nIteration      2000, Cost: 2.80443e+02\nIteration      3000, Cost: 2.39389e+02\nIteration      4000, Cost: 2.04344e+02\nIteration      5000, Cost: 1.74430e+02\nIteration      6000, Cost: 1.48896e+02\nIteration      7000, Cost: 1.27100e+02\nIteration      8000, Cost: 1.08495e+02\nIteration      9000, Cost: 9.26132e+01\nw,b found by gradient descent: w: [0.08 0.54 0.03], b: 0.0106\n</pre> <p>Note the value of $\\mathbf{w}$, <code>[0.08 0.54 0.03]</code> and b is <code>0.0106</code>.This implies the model after fitting/training is: $$ 0.08x + 0.54x^2 + 0.03x^3 + 0.0106 $$ Gradient descent has emphasized the data that is the best fit to the $x^2$ data by increasing the $w_1$ term relative to the others.  If you were to run for a very long time, it would continue to reduce the impact of the other terms.</p> <p>Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter</p> <p>Let's review this idea:</p> <ul> <li>Intially, the features were re-scaled so they are comparable to each other</li> <li>less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature useful in fitting the model to the data.</li> <li>above, after fitting, the weight associated with the $x^2$ feature is much larger than the weights for $x$ or $x^3$ as it is the most useful in fitting the data.</li> </ul> In\u00a0[7]: Copied! <pre># create target data\nx = np.arange(0, 20, 1)\ny = x**2\n\n# engineer features .\nX = np.c_[x, x**2, x**3]   #&lt;-- added engineered feature\nX_features = ['x','x^2','x^3']\n</pre> # create target data x = np.arange(0, 20, 1) y = x**2  # engineer features . X = np.c_[x, x**2, x**3]   #&lt;-- added engineered feature X_features = ['x','x^2','x^3'] In\u00a0[8]: Copied! <pre>fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X[:,i],y)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"y\")\nplt.show()\n</pre> fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True) for i in range(len(ax)):     ax[i].scatter(X[:,i],y)     ax[i].set_xlabel(X_features[i]) ax[0].set_ylabel(\"y\") plt.show() <p>Above, it is clear that the $x^2$ feature mapped against the target value $y$ is linear. Linear regression can then easily generate a model using that feature.</p> In\u00a0[9]: Copied! <pre># create target data\nx = np.arange(0,20,1)\nX = np.c_[x, x**2, x**3]\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")\n\n# add mean_normalization \nX = zscore_normalize_features(X)     \nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(X,axis=0)}\")\n</pre> # create target data x = np.arange(0,20,1) X = np.c_[x, x**2, x**3] print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")  # add mean_normalization  X = zscore_normalize_features(X)      print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X,axis=0)}\") <pre>Peak to Peak range by column in Raw        X:[  19  361 6859]\nPeak to Peak range by column in Normalized X:[3.3  3.18 3.28]\n</pre> <p>Now we can try again with a more aggressive value of alpha:</p> In\u00a0[10]: Copied! <pre>x = np.arange(0,20,1)\ny = x**2\n\nX = np.c_[x, x**2, x**3]\nX = zscore_normalize_features(X) \n\nmodel_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\nplt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n</pre> x = np.arange(0,20,1) y = x**2  X = np.c_[x, x**2, x**3] X = zscore_normalize_features(X)   model_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)  plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\") plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show() <pre>Iteration         0, Cost: 9.42147e+03\nIteration     10000, Cost: 3.90938e-01\nIteration     20000, Cost: 2.78389e-02\nIteration     30000, Cost: 1.98242e-03\nIteration     40000, Cost: 1.41169e-04\nIteration     50000, Cost: 1.00527e-05\nIteration     60000, Cost: 7.15855e-07\nIteration     70000, Cost: 5.09763e-08\nIteration     80000, Cost: 3.63004e-09\nIteration     90000, Cost: 2.58497e-10\nw,b found by gradient descent: w: [5.27e-05 1.13e+02 8.43e-05], b: 123.5000\n</pre> <p>Feature scaling allows this to converge much faster. Note again the values of $\\mathbf{w}$. The $w_1$ term, which is the $x^2$ term is the most emphasized. Gradient descent has all but eliminated the $x^3$ term.</p> In\u00a0[11]: Copied! <pre>x = np.arange(0,20,1)\ny = np.cos(x/2)\n\nX = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\nX = zscore_normalize_features(X) \n\nmodel_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)\n\nplt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\nplt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n</pre> x = np.arange(0,20,1) y = np.cos(x/2)  X = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13] X = zscore_normalize_features(X)   model_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)  plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\") plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()  <pre>Iteration         0, Cost: 2.20188e-01\nIteration    100000, Cost: 1.70074e-02\nIteration    200000, Cost: 1.27603e-02\nIteration    300000, Cost: 9.73032e-03\nIteration    400000, Cost: 7.56440e-03\nIteration    500000, Cost: 6.01412e-03\nIteration    600000, Cost: 4.90251e-03\nIteration    700000, Cost: 4.10351e-03\nIteration    800000, Cost: 3.52730e-03\nIteration    900000, Cost: 3.10989e-03\nw,b found by gradient descent: w: [ -1.34 -10.    24.78   5.96 -12.49 -16.26  -9.51   0.59   8.7   11.94\n   9.27   0.79 -12.82], b: -0.0073\n</pre> In\u00a0[11]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#optional-lab-feature-engineering-and-polynomial-regression","title":"Optional Lab: Feature Engineering and Polynomial Regression\u00b6","text":""},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>explore feature engineering and polynomial regression which allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions.</li> </ul>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#tools","title":"Tools\u00b6","text":"<p>You will utilize the function developed in previous labs as well as matplotlib and NumPy.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#feature-engineering-and-polynomial-regression-overview","title":"Feature Engineering and Polynomial Regression Overview\u00b6","text":"<p>Out of the box, linear regression provides a means of building models of the form: $$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$ What if your features/data are non-linear or are combinations of features? For example,  Housing prices do not tend to be linear with living area but penalize very small or very large houses resulting in the curves shown in the graphic above. How can we use the machinery of linear regression to fit this curve? Recall, the 'machinery' we have is the ability to modify the parameters $\\mathbf{w}$, $\\mathbf{b}$ in (1) to 'fit' the equation to the training data. However, no amount of adjusting of $\\mathbf{w}$,$\\mathbf{b}$ in (1) will achieve a fit to a non-linear curve.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#polynomial-features","title":"Polynomial Features\u00b6","text":"<p>Above we were considering a scenario where the data was non-linear. Let's try using what we know so far to fit a non-linear curve. We'll start with a simple quadratic: $y = 1+x^2$</p> <p>You're familiar with all the routines we're using. They are available in the lab_utils.py file for review. We'll use <code>np.c_[..]</code> which is a NumPy routine to concatenate along the column boundary.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#selecting-features","title":"Selecting Features\u00b6","text":"<p> Above, we knew that an $x^2$ term was required. It may not always be obvious which features are required. One could add a variety of potential features to try and find the most useful. For example, what if we had instead tried : $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ?</p> <p>Run the next cells.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#an-alternate-view","title":"An Alternate View\u00b6","text":"<p>Above, polynomial features were chosen based on how well they matched the target data. Another way to think about this is to note that we are still using linear regression once we have created new features. Given that, the best features will be linear relative to the target. This is best understood with an example.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#scaling-features","title":"Scaling features\u00b6","text":"<p>As described in the last lab, if the data set has features with significantly different scales, one should apply feature scaling to speed gradient descent. In the example above, there is $x$, $x^2$ and $x^3$ which will naturally have very different scales. Let's apply Z-score normalization to our example.</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#complex-functions","title":"Complex Functions\u00b6","text":"<p>With feature engineering, even quite complex functions can be modeled:</p>"},{"location":"MachineLearning/part2/FeatEng_PolyReg_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>learned how linear regression can model complex, even highly non-linear functions using feature engineering</li> <li>recognized that it is important to apply feature scaling when doing feature engineering</li> </ul>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/","title":"Optional Lab: Feature scaling and Learning Rate (Multi-variable)","text":"In\u00a0[4]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom lab_utils_multi import  load_house_data, run_gradient_descent \nfrom lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\nfrom lab_utils_common import dlc\nnp.set_printoptions(precision=2)\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np import matplotlib.pyplot as plt from lab_utils_multi import  load_house_data, run_gradient_descent  from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w from lab_utils_common import dlc np.set_printoptions(precision=2) plt.style.use('./deeplearning.mplstyle') In\u00a0[8]: Copied! <pre># load the dataset\nX_train, y_train = load_house_data()\nX_features = ['size(sqft)','bedrooms','floors','age']\n</pre> # load the dataset X_train, y_train = load_house_data() X_features = ['size(sqft)','bedrooms','floors','age'] <p>Let's view the dataset and its features by plotting each feature versus price.</p> In\u00a0[9]: Copied! <pre>fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X_train[:,i],y_train)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"Price (1000's)\")\nplt.show()\n</pre> fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True) for i in range(len(ax)):     ax[i].scatter(X_train[:,i],y_train)     ax[i].set_xlabel(X_features[i]) ax[0].set_ylabel(\"Price (1000's)\") plt.show() <p>Plotting each feature vs. the target, price, provides some indication of which features have the strongest influence on price. Above, increasing size also increases price. Bedrooms and floors don't seem to have a strong impact on price. Newer houses have higher prices than older houses.</p> <p></p> In\u00a0[11]: Copied! <pre>#set alpha to 9.9e-7\n_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)\n</pre> #set alpha to 9.9e-7 _, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7) <pre>Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \n---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n        0 9.55884e+04  5.5e-01  1.0e-03  5.1e-04  1.2e-02  3.6e-04 -5.5e+05 -1.0e+03 -5.2e+02 -1.2e+04 -3.6e+02\n        1 1.28213e+05 -8.8e-02 -1.7e-04 -1.0e-04 -3.4e-03 -4.8e-05  6.4e+05  1.2e+03  6.2e+02  1.6e+04  4.1e+02\n        2 1.72159e+05  6.5e-01  1.2e-03  5.9e-04  1.3e-02  4.3e-04 -7.4e+05 -1.4e+03 -7.0e+02 -1.7e+04 -4.9e+02\n        3 2.31358e+05 -2.1e-01 -4.0e-04 -2.3e-04 -7.5e-03 -1.2e-04  8.6e+05  1.6e+03  8.3e+02  2.1e+04  5.6e+02\n        4 3.11100e+05  7.9e-01  1.4e-03  7.1e-04  1.5e-02  5.3e-04 -1.0e+06 -1.8e+03 -9.5e+02 -2.3e+04 -6.6e+02\n        5 4.18517e+05 -3.7e-01 -7.1e-04 -4.0e-04 -1.3e-02 -2.1e-04  1.2e+06  2.1e+03  1.1e+03  2.8e+04  7.5e+02\n        6 5.63212e+05  9.7e-01  1.7e-03  8.7e-04  1.8e-02  6.6e-04 -1.3e+06 -2.5e+03 -1.3e+03 -3.1e+04 -8.8e+02\n        7 7.58122e+05 -5.8e-01 -1.1e-03 -6.2e-04 -1.9e-02 -3.4e-04  1.6e+06  2.9e+03  1.5e+03  3.8e+04  1.0e+03\n        8 1.02068e+06  1.2e+00  2.2e-03  1.1e-03  2.3e-02  8.3e-04 -1.8e+06 -3.3e+03 -1.7e+03 -4.2e+04 -1.2e+03\n        9 1.37435e+06 -8.7e-01 -1.7e-03 -9.1e-04 -2.7e-02 -5.2e-04  2.1e+06  3.9e+03  2.0e+03  5.1e+04  1.4e+03\nw,b found by gradient descent: w: [-0.87 -0.   -0.   -0.03], b: -0.00\n</pre> <p>It appears the learning rate is too high.  The solution does not converge. Cost is increasing rather than decreasing. Let's plot the result:</p> In\u00a0[12]: Copied! <pre>plot_cost_i_w(X_train, y_train, hist)\n</pre> plot_cost_i_w(X_train, y_train, hist) <p>The plot on the right shows the value of one of the parameters, $w_0$. At each iteration, it is overshooting the optimal value and as a result, cost ends up increasing rather than approaching the minimum. Note that this is not a completely accurate picture as there are 4 parameters being modified each pass rather than just one. This plot is only showing $w_0$ with the other parameters fixed at benign values. In this and later plots you may notice the blue and orange lines being slightly off.</p> In\u00a0[13]: Copied! <pre>#set alpha to 9e-7\n_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)\n</pre> #set alpha to 9e-7 _,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7) <pre>Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \n---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n        0 6.64616e+04  5.0e-01  9.1e-04  4.7e-04  1.1e-02  3.3e-04 -5.5e+05 -1.0e+03 -5.2e+02 -1.2e+04 -3.6e+02\n        1 6.18990e+04  1.8e-02  2.1e-05  2.0e-06 -7.9e-04  1.9e-05  5.3e+05  9.8e+02  5.2e+02  1.3e+04  3.4e+02\n        2 5.76572e+04  4.8e-01  8.6e-04  4.4e-04  9.5e-03  3.2e-04 -5.1e+05 -9.3e+02 -4.8e+02 -1.1e+04 -3.4e+02\n        3 5.37137e+04  3.4e-02  3.9e-05  2.8e-06 -1.6e-03  3.8e-05  4.9e+05  9.1e+02  4.8e+02  1.2e+04  3.2e+02\n        4 5.00474e+04  4.6e-01  8.2e-04  4.1e-04  8.0e-03  3.2e-04 -4.8e+05 -8.7e+02 -4.5e+02 -1.1e+04 -3.1e+02\n        5 4.66388e+04  5.0e-02  5.6e-05  2.5e-06 -2.4e-03  5.6e-05  4.6e+05  8.5e+02  4.5e+02  1.2e+04  2.9e+02\n        6 4.34700e+04  4.5e-01  7.8e-04  3.8e-04  6.4e-03  3.2e-04 -4.4e+05 -8.1e+02 -4.2e+02 -9.8e+03 -2.9e+02\n        7 4.05239e+04  6.4e-02  7.0e-05  1.2e-06 -3.3e-03  7.3e-05  4.3e+05  7.9e+02  4.2e+02  1.1e+04  2.7e+02\n        8 3.77849e+04  4.4e-01  7.5e-04  3.5e-04  4.9e-03  3.2e-04 -4.1e+05 -7.5e+02 -3.9e+02 -9.1e+03 -2.7e+02\n        9 3.52385e+04  7.7e-02  8.3e-05 -1.1e-06 -4.2e-03  8.9e-05  4.0e+05  7.4e+02  3.9e+02  1.0e+04  2.5e+02\nw,b found by gradient descent: w: [ 7.74e-02  8.27e-05 -1.06e-06 -4.20e-03], b: 0.00\n</pre> <p>Cost is decreasing throughout the run showing that alpha is not too large.</p> In\u00a0[14]: Copied! <pre>plot_cost_i_w(X_train, y_train, hist)\n</pre> plot_cost_i_w(X_train, y_train, hist) <p>On the left, you see that cost is decreasing as it should. On the right, you can see that $w_0$ is still oscillating around the minimum, but it is decreasing each iteration rather than increasing. Note above that <code>dj_dw[0]</code> changes sign with each iteration as <code>w[0]</code> jumps over the optimal value. This alpha value will converge. You can vary the number of iterations to see how it behaves.</p> In\u00a0[15]: Copied! <pre>#set alpha to 1e-7\n_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)\n</pre> #set alpha to 1e-7 _,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7) <pre>Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \n---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n        0 4.42313e+04  5.5e-02  1.0e-04  5.2e-05  1.2e-03  3.6e-05 -5.5e+05 -1.0e+03 -5.2e+02 -1.2e+04 -3.6e+02\n        1 2.76461e+04  9.8e-02  1.8e-04  9.2e-05  2.2e-03  6.5e-05 -4.3e+05 -7.9e+02 -4.0e+02 -9.5e+03 -2.8e+02\n        2 1.75102e+04  1.3e-01  2.4e-04  1.2e-04  2.9e-03  8.7e-05 -3.4e+05 -6.1e+02 -3.1e+02 -7.3e+03 -2.2e+02\n        3 1.13157e+04  1.6e-01  2.9e-04  1.5e-04  3.5e-03  1.0e-04 -2.6e+05 -4.8e+02 -2.4e+02 -5.6e+03 -1.8e+02\n        4 7.53002e+03  1.8e-01  3.3e-04  1.7e-04  3.9e-03  1.2e-04 -2.1e+05 -3.7e+02 -1.9e+02 -4.2e+03 -1.4e+02\n        5 5.21639e+03  2.0e-01  3.5e-04  1.8e-04  4.2e-03  1.3e-04 -1.6e+05 -2.9e+02 -1.5e+02 -3.1e+03 -1.1e+02\n        6 3.80242e+03  2.1e-01  3.8e-04  1.9e-04  4.5e-03  1.4e-04 -1.3e+05 -2.2e+02 -1.1e+02 -2.3e+03 -8.6e+01\n        7 2.93826e+03  2.2e-01  3.9e-04  2.0e-04  4.6e-03  1.4e-04 -9.8e+04 -1.7e+02 -8.6e+01 -1.7e+03 -6.8e+01\n        8 2.41013e+03  2.3e-01  4.1e-04  2.1e-04  4.7e-03  1.5e-04 -7.7e+04 -1.3e+02 -6.5e+01 -1.2e+03 -5.4e+01\n        9 2.08734e+03  2.3e-01  4.2e-04  2.1e-04  4.8e-03  1.5e-04 -6.0e+04 -1.0e+02 -4.9e+01 -7.5e+02 -4.3e+01\nw,b found by gradient descent: w: [2.31e-01 4.18e-04 2.12e-04 4.81e-03], b: 0.00\n</pre> <p>Cost is decreasing throughout the run showing that $\\alpha$ is not too large.</p> In\u00a0[16]: Copied! <pre>plot_cost_i_w(X_train,y_train,hist)\n</pre> plot_cost_i_w(X_train,y_train,hist) <p>On the left, you see that cost is decreasing as it should. On the right you can see that $w_0$ is decreasing without crossing the minimum. Note above that <code>dj_w0</code> is negative throughout the run. This solution will also converge, though not quite as quickly as the previous example.</p> Details <p>Let's look again at the situation with $\\alpha$ = 9e-7. This is pretty close to the maximum value we can set $\\alpha$  to without diverging. This is a short run showing the first few iterations:</p> <p>Above, while cost is being decreased, its clear that $w_0$ is making more rapid progress than the other parameters due to its much larger gradient.</p> <p>The graphic below shows the result of a very long run with $\\alpha$ = 9e-7. This takes several hours.</p> <p>Above, you can see cost decreased slowly after its initial reduction. Notice the difference between <code>w0</code> and <code>w1</code>,<code>w2</code>,<code>w3</code> as well as  <code>dj_dw0</code> and <code>dj_dw1-3</code>. <code>w0</code> reaches its near final value very quickly and <code>dj_dw0</code> has quickly decreased to a small value showing that <code>w0</code> is near the final value. The other parameters were reduced much more slowly.</p> <p>Why is this?  Is there something we can improve? See below:</p> <p>The figure above shows why $w$'s are updated unevenly.</p> <ul> <li>$\\alpha$ is shared by all parameter updates ($w$'s and $b$).</li> <li>the common error term is multiplied by the features for the $w$'s. (not $b$).</li> <li>the features vary significantly in magnitude making some features update much faster than others. In this case, $w_0$ is multiplied by 'size(sqft)', which is generally &gt; 1000,  while $w_1$ is multiplied by 'number of bedrooms', which is generally 2-4.</li> </ul> <p>The solution is Feature Scaling.</p> <p>The lectures discussed three different techniques:</p> <ul> <li>Feature scaling, essentially dividing each positive feature by its maximum value, or more generally, rescale each feature by both its minimum and maximum values using (x-min)/(max-min). Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and serves well for the lecture's example, and the latter method works for any features.</li> <li>Mean normalization: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $</li> <li>Z-score normalization which we will explore below.</li> </ul> In\u00a0[17]: Copied! <pre>def zscore_normalize_features(X):\n    \"\"\"\n    computes  X, zcore normalized by column\n    \n    Args:\n      X (ndarray (m,n))     : input data, m examples, n features\n      \n    Returns:\n      X_norm (ndarray (m,n)): input normalized by column\n      mu (ndarray (n,))     : mean of each feature\n      sigma (ndarray (n,))  : standard deviation of each feature\n    \"\"\"\n    # find the mean of each column/feature\n    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n    # find the standard deviation of each column/feature\n    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n    # element-wise, subtract mu for that column from each example, divide by std for that column\n    X_norm = (X - mu) / sigma      \n\n    return (X_norm, mu, sigma)\n \n#check our work\n#from sklearn.preprocessing import scale\n#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n</pre> def zscore_normalize_features(X):     \"\"\"     computes  X, zcore normalized by column          Args:       X (ndarray (m,n))     : input data, m examples, n features            Returns:       X_norm (ndarray (m,n)): input normalized by column       mu (ndarray (n,))     : mean of each feature       sigma (ndarray (n,))  : standard deviation of each feature     \"\"\"     # find the mean of each column/feature     mu     = np.mean(X, axis=0)                 # mu will have shape (n,)     # find the standard deviation of each column/feature     sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)     # element-wise, subtract mu for that column from each example, divide by std for that column     X_norm = (X - mu) / sigma            return (X_norm, mu, sigma)   #check our work #from sklearn.preprocessing import scale #scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True) <p>Let's look at the steps involved in Z-score normalization. The plot below shows the transformation step by step.</p> In\u00a0[19]: Copied! <pre>mu     = np.mean(X_train,axis=0)   \nsigma  = np.std(X_train,axis=0) \nX_mean = (X_train - mu)\nX_norm = (X_train - mu)/sigma      \n\nfig,ax=plt.subplots(1, 3, figsize=(12, 3))\n\nax[0].scatter(X_train[:,0], X_train[:,3])\nax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[0].set_title(\"unnormalized\")\nax[0].axis('equal')\n\nax[1].scatter(X_mean[:,0], X_mean[:,3])\nax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[1].set_title(r\"X - $\\mu$\")\nax[1].axis('equal')\n\nax[2].scatter(X_norm[:,0], X_norm[:,3])\nax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\nax[2].set_title(r\"Z-score normalized\")\nax[2].axis('equal')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nfig.suptitle(\"distribution of features before, during, after normalization\")\nplt.show()\n</pre> mu     = np.mean(X_train,axis=0)    sigma  = np.std(X_train,axis=0)  X_mean = (X_train - mu) X_norm = (X_train - mu)/sigma        fig,ax=plt.subplots(1, 3, figsize=(12, 3))  ax[0].scatter(X_train[:,0], X_train[:,3]) ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]); ax[0].set_title(\"unnormalized\") ax[0].axis('equal')  ax[1].scatter(X_mean[:,0], X_mean[:,3]) ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]); ax[1].set_title(r\"X - $\\mu$\") ax[1].axis('equal')  ax[2].scatter(X_norm[:,0], X_norm[:,3]) ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]); ax[2].set_title(r\"Z-score normalized\") ax[2].axis('equal') plt.tight_layout(rect=[0, 0.03, 1, 0.95]) fig.suptitle(\"distribution of features before, during, after normalization\") plt.show() <p>The plot above shows the relationship between two of the training set parameters, \"age\" and \"size(sqft)\". These are plotted with equal scale.</p> <ul> <li>Left: Unnormalized: The range of values or the variance of the 'size(sqft)' feature is much larger than that of age</li> <li>Middle: The first step removes the mean or average value from each feature. This leaves features that are centered around zero. It's difficult to see the difference for the 'age' feature, but 'size(sqft)' is clearly around zero.</li> <li>Right: The second step divides by the variance. This leaves both features centered at zero with a similar scale.</li> </ul> <p>Let's normalize the data and compare it to the original data.</p> In\u00a0[20]: Copied! <pre># normalize the original features\nX_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\nprint(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")\n</pre> # normalize the original features X_norm, X_mu, X_sigma = zscore_normalize_features(X_train) print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\") print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")    print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\") <pre>X_mu = [1.42e+03 2.72e+00 1.38e+00 3.84e+01], \nX_sigma = [411.62   0.65   0.49  25.78]\nPeak to Peak range by column in Raw        X:[2.41e+03 4.00e+00 1.00e+00 9.50e+01]\nPeak to Peak range by column in Normalized X:[5.85 6.14 2.06 3.69]\n</pre> <p>The peak to peak range of each column is reduced from a factor of thousands to a factor of 2-3 by normalization.</p> In\u00a0[21]: Copied! <pre>fig,ax=plt.subplots(1, 4, figsize=(12, 3))\nfor i in range(len(ax)):\n    norm_plot(ax[i],X_train[:,i],)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"count\");\nfig.suptitle(\"distribution of features before normalization\")\nplt.show()\nfig,ax=plt.subplots(1,4,figsize=(12,3))\nfor i in range(len(ax)):\n    norm_plot(ax[i],X_norm[:,i],)\n    ax[i].set_xlabel(X_features[i])\nax[0].set_ylabel(\"count\"); \nfig.suptitle(\"distribution of features after normalization\")\n\nplt.show()\n</pre> fig,ax=plt.subplots(1, 4, figsize=(12, 3)) for i in range(len(ax)):     norm_plot(ax[i],X_train[:,i],)     ax[i].set_xlabel(X_features[i]) ax[0].set_ylabel(\"count\"); fig.suptitle(\"distribution of features before normalization\") plt.show() fig,ax=plt.subplots(1,4,figsize=(12,3)) for i in range(len(ax)):     norm_plot(ax[i],X_norm[:,i],)     ax[i].set_xlabel(X_features[i]) ax[0].set_ylabel(\"count\");  fig.suptitle(\"distribution of features after normalization\")  plt.show() <p>Notice, above, the range of the normalized data (x-axis) is centered around zero and roughly +/- 2. Most importantly, the range is similar for each feature.</p> <p>Let's re-run our gradient descent algorithm with normalized data. Note the vastly larger value of alpha. This will speed up gradient descent.</p> In\u00a0[14]: Copied! <pre>w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )\n</pre> w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, ) <pre>Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \n---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n        0 5.76170e+04  8.9e+00  3.0e+00  3.3e+00 -6.0e+00  3.6e+01 -8.9e+01 -3.0e+01 -3.3e+01  6.0e+01 -3.6e+02\n      100 2.21086e+02  1.1e+02 -2.0e+01 -3.1e+01 -3.8e+01  3.6e+02 -9.2e-01  4.5e-01  5.3e-01 -1.7e-01 -9.6e-03\n      200 2.19209e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -3.0e-02  1.5e-02  1.7e-02 -6.0e-03 -2.6e-07\n      300 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -1.0e-03  5.1e-04  5.7e-04 -2.0e-04 -6.9e-12\n      400 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -3.4e-05  1.7e-05  1.9e-05 -6.6e-06 -2.7e-13\n      500 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -1.1e-06  5.6e-07  6.2e-07 -2.2e-07 -2.7e-13\n      600 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -3.7e-08  1.9e-08  2.1e-08 -7.3e-09 -2.6e-13\n      700 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -1.2e-09  6.2e-10  6.9e-10 -2.4e-10 -2.6e-13\n      800 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -4.1e-11  2.1e-11  2.3e-11 -8.1e-12 -2.6e-13\n      900 2.19207e+02  1.1e+02 -2.1e+01 -3.3e+01 -3.8e+01  3.6e+02 -1.4e-12  6.9e-13  7.7e-13 -2.7e-13 -2.6e-13\nw,b found by gradient descent: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\n</pre> <p>The scaled features get very accurate results much, much faster!. Notice the gradient of each parameter is tiny by the end of this fairly short run. A learning rate of 0.1 is a good start for regression with normalized features. Let's plot our predictions versus the target values. Note, the prediction is made using the normalized feature while the plot is shown using the original feature values.</p> In\u00a0[15]: Copied! <pre>#predict target using normalized features\nm = X_norm.shape[0]\nyp = np.zeros(m)\nfor i in range(m):\n    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n\n    # plot predictions and targets versus original features    \nfig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n    ax[i].set_xlabel(X_features[i])\n    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predict')\nax[0].set_ylabel(\"Price\"); ax[0].legend();\nfig.suptitle(\"target versus prediction using z-score normalized model\")\nplt.show()\n</pre> #predict target using normalized features m = X_norm.shape[0] yp = np.zeros(m) for i in range(m):     yp[i] = np.dot(X_norm[i], w_norm) + b_norm      # plot predictions and targets versus original features     fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True) for i in range(len(ax)):     ax[i].scatter(X_train[:,i],y_train, label = 'target')     ax[i].set_xlabel(X_features[i])     ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predict') ax[0].set_ylabel(\"Price\"); ax[0].legend(); fig.suptitle(\"target versus prediction using z-score normalized model\") plt.show() <p>The results look good. A few points to note:</p> <ul> <li>with multiple features, we can no longer have a single plot showing results versus features.</li> <li>when generating the plot, the normalized features were used. Any predictions using the parameters learned from a normalized training set must also be normalized.</li> </ul> <p>Prediction The point of generating our model is to use it to predict housing prices that are not in the data set. Let's predict the price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old. Recall, that you must normalize the data with the mean and standard deviation derived when the training data was normalized.</p> In\u00a0[16]: Copied! <pre># First, normalize out example.\nx_house = np.array([1200, 3, 1, 40])\nx_house_norm = (x_house - X_mu) / X_sigma\nprint(x_house_norm)\nx_house_predict = np.dot(x_house_norm, w_norm) + b_norm\nprint(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")\n</pre> # First, normalize out example. x_house = np.array([1200, 3, 1, 40]) x_house_norm = (x_house - X_mu) / X_sigma print(x_house_norm) x_house_predict = np.dot(x_house_norm, w_norm) + b_norm print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\") <pre>[-0.53  0.43 -0.79  0.06]\n predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709\n</pre> <p>Cost Contours Another way to view feature scaling is in terms of the cost contours. When feature scales do not match, the plot of cost versus parameters in a contour plot is asymmetric.</p> <p>In the plot below, the scale of the parameters is matched. The left plot is the cost contour plot of w[0], the square feet versus w[1], the number of bedrooms before normalizing the features. The plot is so asymmetric, the curves completing the contours are not visible. In contrast, when the features are normalized, the cost contour is much more symmetric. The result is that updates to parameters during gradient descent can make equal progress for each parameter.</p> In\u00a0[17]: Copied! <pre>plt_equal_scale(X_train, X_norm, y_train)\n</pre> plt_equal_scale(X_train, X_norm, y_train) In\u00a0[17]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#optional-lab-feature-scaling-and-learning-rate-multi-variable","title":"Optional Lab: Feature scaling and Learning Rate (Multi-variable)\u00b6","text":""},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>Utilize  the multiple variables routines developed in the previous lab</li> <li>run Gradient Descent on a data set with multiple features</li> <li>explore the impact of the learning rate alpha on gradient descent</li> <li>improve performance of gradient descent by feature scaling using z-score normalization</li> </ul>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#tools","title":"Tools\u00b6","text":"<p>You will utilize the functions developed in the last lab as well as matplotlib and NumPy.</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#notation","title":"Notation\u00b6","text":"General Notation Description Python (if applicable) $a$ scalar, non-bold $\\mathbf{a}$ vector, bold $\\mathbf{A}$ matrix, bold capital Regression $\\mathbf{X}$ training example matrix <code>X_train</code> $\\mathbf{y}$ training example targets <code>y_train</code> $\\mathbf{x}^{(i)}$, $y^{(i)}$ $i_{th}$ Training Example <code>X[i]</code>, <code>y[i]</code> $m$ number of training examples <code>m</code> $n$ number of features in each example <code>n</code> $\\mathbf{w}$ parameter: weight <code>w</code> $b$ parameter: bias <code>b</code> $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ The result of the Model evaluation at $\\mathbf{x}^{(i)}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ <code>f_wb</code> $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ the gradient or partial derivative of cost with respect to parameter $w_j$ <code>dj_dw[j]</code> $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ the gradient or partial derivative of cost with respect to parameter $b$ <code>dj_db</code>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#problem-statement","title":"Problem Statement\u00b6","text":"<p>As in the previous labs, you will use the motivating example of housing price prediction. The training data set contains many examples with 4 features (size, bedrooms, floors and age) shown in the table below. Note, in this lab, the Size feature is in sqft while earlier labs utilized 1000 sqft.  This data set is larger than the previous lab.</p> <p>We would like to build a linear regression model using these values so we can then predict the price for other houses - say, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#dataset","title":"Dataset:\u00b6","text":"Size (sqft) Number of Bedrooms Number of floors Age of  Home Price (1000s dollars) 952 2 1 65 271.5 1244 3 2 64 232 1947 3 2 17 509.8 ... ... ... ... ..."},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#gradient-descent-with-multiple-variables","title":"Gradient Descent With Multiple Variables\u00b6","text":"<p>Here are the equations you developed in the last lab on gradient descent for multiple variables.:</p> <p>$$\\begin{align*} \\text{repeat}&amp;\\text{ until convergence:} \\; \\lbrace \\newline\\; &amp; w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; &amp; \\text{for j = 0..n-1}\\newline &amp;b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace \\end{align*}$$</p> <p>where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where</p> <p>$$ \\begin{align} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \\end{align} $$</p> <ul> <li><p>m is the number of training examples in the data set</p> </li> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value</p> </li> </ul>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#learning-rate","title":"Learning Rate\u00b6","text":"The lectures discussed some of the issues related to setting the learning rate $\\alpha$. The learning rate controls the size of the update to the parameters. See equation (1) above. It is shared by all the parameters.    <p>Let's run gradient descent and try a few settings of $\\alpha$ on our data set</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#alpha-99e-7","title":"$\\alpha$ = 9.9e-7\u00b6","text":""},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#alpha-9e-7","title":"$\\alpha$ = 9e-7\u00b6","text":"<p>Let's try a bit smaller value and see what happens.</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#alpha-1e-7","title":"$\\alpha$ = 1e-7\u00b6","text":"<p>Let's try a bit smaller value for $\\alpha$ and see what happens.</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#feature-scaling","title":"Feature Scaling\u00b6","text":"The lectures described the importance of rescaling the dataset so the features have a similar range. If you are interested in the details of why this is the case, click on the 'details' header below. If not, the section below will walk through an implementation of how to do feature scaling."},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#z-score-normalization","title":"z-score normalization\u00b6","text":"<p>After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.</p> <p>To implement z-score normalization, adjust your input values as shown in this formula: $$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $\u00b5_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j). $$ \\begin{align} \\mu_j &amp;= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\ \\sigma^2_j &amp;= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6} \\end{align} $$</p> <p>Implementation Note: When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bed- rooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.</p> <p>Implementation</p>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>utilized the routines for linear regression with multiple features you developed in previous labs</li> <li>explored the impact of the learning rate  $\\alpha$ on convergence</li> <li>discovered the value of feature scaling using z-score normalization in speeding convergence</li> </ul>"},{"location":"MachineLearning/part2/Feature_Scaling_and_Learning_Rate_Soln/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>The housing data was derived from the Ames Housing dataset compiled by Dean De Cock for use in data science education.</p>"},{"location":"MachineLearning/part2/Linear_Regression/","title":"Practice Lab: Linear Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom utils import *\nimport copy\nimport math\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt from utils import * import copy import math %matplotlib inline In\u00a0[2]: Copied! <pre># load the dataset\nx_train, y_train = load_data()\n</pre> # load the dataset x_train, y_train = load_data() In\u00a0[3]: Copied! <pre># print x_train\nprint(\"Type of x_train:\",type(x_train))\nprint(\"First five elements of x_train are:\\n\", x_train[:5]) \n</pre> # print x_train print(\"Type of x_train:\",type(x_train)) print(\"First five elements of x_train are:\\n\", x_train[:5])  <pre>Type of x_train: &lt;class 'numpy.ndarray'&gt;\nFirst five elements of x_train are:\n [6.1101 5.5277 8.5186 7.0032 5.8598]\n</pre> <p><code>x_train</code> is a numpy array that contains decimal values that are all greater than zero.</p> <ul> <li>These values represent the city population times 10,000</li> <li>For example, 6.1101 means that the population for that city is 61,101</li> </ul> <p>Now, let's print <code>y_train</code></p> In\u00a0[4]: Copied! <pre># print y_train\nprint(\"Type of y_train:\",type(y_train))\nprint(\"First five elements of y_train are:\\n\", y_train[:5])  \n</pre> # print y_train print(\"Type of y_train:\",type(y_train)) print(\"First five elements of y_train are:\\n\", y_train[:5])   <pre>Type of y_train: &lt;class 'numpy.ndarray'&gt;\nFirst five elements of y_train are:\n [17.592   9.1302 13.662  11.854   6.8233]\n</pre> <p>Similarly, <code>y_train</code> is a numpy array that has decimal values, some negative, some positive.</p> <ul> <li>These represent your restaurant's average monthly profits in each city, in units of $10,000.<ul> <li>For example, 17.592 represents $175,920 in average monthly profits for that city.</li> <li>-2.6807 represents -$26,807 in average monthly loss for that city.</li> </ul> </li> </ul> In\u00a0[5]: Copied! <pre>print ('The shape of x_train is:', x_train.shape)\nprint ('The shape of y_train is: ', y_train.shape)\nprint ('Number of training examples (m):', len(x_train))\n</pre> print ('The shape of x_train is:', x_train.shape) print ('The shape of y_train is: ', y_train.shape) print ('Number of training examples (m):', len(x_train)) <pre>The shape of x_train is: (97,)\nThe shape of y_train is:  (97,)\nNumber of training examples (m): 97\n</pre> <p>The city population array has 97 data points, and the monthly average profits also has 97 data points. These are NumPy 1D arrays.</p> In\u00a0[6]: Copied! <pre># Create a scatter plot of the data. To change the markers to red \"x\",\n# we used the 'marker' and 'c' parameters\nplt.scatter(x_train, y_train, marker='x', c='r') \n\n# Set the title\nplt.title(\"Profits vs. Population per city\")\n# Set the y-axis label\nplt.ylabel('Profit in $10,000')\n# Set the x-axis label\nplt.xlabel('Population of City in 10,000s')\nplt.show()\n</pre> # Create a scatter plot of the data. To change the markers to red \"x\", # we used the 'marker' and 'c' parameters plt.scatter(x_train, y_train, marker='x', c='r')   # Set the title plt.title(\"Profits vs. Population per city\") # Set the y-axis label plt.ylabel('Profit in $10,000') # Set the x-axis label plt.xlabel('Population of City in 10,000s') plt.show() <p>Your goal is to build a linear regression model to fit this data.</p> <ul> <li>With this model, you can then input a new city's population, and have the model estimate your restaurant's potential monthly profits for that city.</li> </ul> <p></p> <p></p> <p></p> In\u00a0[13]: Copied! <pre># UNQ_C1\n# GRADED FUNCTION: compute_cost\n\ndef compute_cost(x, y, w, b): \n    \"\"\"\n    Computes the cost function for linear regression.\n    \n    Args:\n        x (ndarray): Shape (m,) Input to the model (Population of cities) \n        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n        w, b (scalar): Parameters of the model\n    \n    Returns\n        total_cost (float): The cost of using w,b as the parameters for linear regression\n               to fit the data points in x and y\n    \"\"\"\n    # number of training examples\n    m = x.shape[0] \n    \n    # You need to return this variable correctly\n    total_cost = 0\n\n    ### START CODE HERE ###\n    cost=0\n    for i in range(m):\n        f_wb = w*x[i]+b\n        cost += (f_wb - y[i])**2\n    \n    total_cost = cost/(2*m)\n    \n    ### END CODE HERE ### \n\n    return total_cost\n</pre> # UNQ_C1 # GRADED FUNCTION: compute_cost  def compute_cost(x, y, w, b):      \"\"\"     Computes the cost function for linear regression.          Args:         x (ndarray): Shape (m,) Input to the model (Population of cities)          y (ndarray): Shape (m,) Label (Actual profits for the cities)         w, b (scalar): Parameters of the model          Returns         total_cost (float): The cost of using w,b as the parameters for linear regression                to fit the data points in x and y     \"\"\"     # number of training examples     m = x.shape[0]           # You need to return this variable correctly     total_cost = 0      ### START CODE HERE ###     cost=0     for i in range(m):         f_wb = w*x[i]+b         cost += (f_wb - y[i])**2          total_cost = cost/(2*m)          ### END CODE HERE ###       return total_cost Click for hints <ul> <li><p>You can represent a summation operator eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ in code as follows:</p> <pre>    h = 0\n    for i in range(m):\n   h = h + 2*i\n    ```\n</pre> </li> <li><p>In this case, you can iterate over all the examples in <code>x</code> using a for loop and add the <code>cost</code> from each iteration to a variable (<code>cost_sum</code>) initialized outside the loop.</p> </li> <li><p>Then, you can return the <code>total_cost</code> as <code>cost_sum</code> divided by <code>2m</code>.</p> </li> </ul> <pre><code>&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt; Click for more hints&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n    \n* Here's how you can structure the overall implementation for this function\n```python \ndef compute_cost(x, y, w, b):\n    # number of training examples\n    m = x.shape[0] \n\n    # You need to return this variable correctly\n    total_cost = 0\n\n    ### START CODE HERE ###  \n    # Variable to keep track of sum of cost from each example\n    cost_sum = 0\n\n    # Loop over training examples\n    for i in range(m):\n        # Your code here to get the prediction f_wb for the ith example\n        f_wb = \n        # Your code here to get the cost associated with the ith example\n        cost = \n    \n        # Add to sum of cost for each example\n        cost_sum = cost_sum + cost \n\n    # Get the total cost as the sum divided by (2*m)\n    total_cost = (1 / (2 * m)) * cost_sum\n    ### END CODE HERE ### \n\n    return total_cost\n```\n\nIf you're still stuck, you can check the hints presented below to figure out how to calculate `f_wb` and `cost`.\n\n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate f_wb&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n       &amp;emsp; &amp;emsp; For scalars $a$, $b$ and $c$ (&lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are all scalars), you can calculate the equation $h = ab + c$ in code as &lt;code&gt;h = a * b + c&lt;/code&gt;\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate f&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n           &amp;emsp; &amp;emsp; You can compute f_wb as &lt;code&gt;f_wb = w * x[i] + b &lt;/code&gt;\n       &lt;/details&gt;\n&lt;/details&gt;\n\n &lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate cost&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n      &amp;emsp; &amp;emsp; You can calculate the square of a variable z as z**2\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate cost&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n          &amp;emsp; &amp;emsp; You can compute cost as &lt;code&gt;cost = (f_wb - y[i]) ** 2&lt;/code&gt;\n      &lt;/details&gt;\n&lt;/details&gt;\n    \n&lt;/details&gt;</code></pre> <p>You can check if your implementation was correct by running the following test code:</p> In\u00a0[14]: Copied! <pre># Compute cost with some initial values for paramaters w, b\ninitial_w = 2\ninitial_b = 1\n\ncost = compute_cost(x_train, y_train, initial_w, initial_b)\nprint(type(cost))\nprint(f'Cost at initial w (zeros): {cost:.3f}')\n\n# Public tests\nfrom public_tests import *\ncompute_cost_test(compute_cost)\n</pre> # Compute cost with some initial values for paramaters w, b initial_w = 2 initial_b = 1  cost = compute_cost(x_train, y_train, initial_w, initial_b) print(type(cost)) print(f'Cost at initial w (zeros): {cost:.3f}')  # Public tests from public_tests import * compute_cost_test(compute_cost) <pre>&lt;class 'numpy.float64'&gt;\nCost at initial w (zeros): 75.203\nAll tests passed!\n</pre> <p>Expected Output:</p> Cost at initial w (zeros): 75.203  <p></p> <p>As described in the lecture videos, the gradient descent algorithm is:</p> <p>$$\\begin{align*}&amp; \\text{repeat until convergence:} \\; \\lbrace \\newline \\; &amp; \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; &amp; \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; &amp;  \\newline &amp; \\rbrace\\end{align*}$$</p> <p>where, parameters $w, b$ are both updated simultaniously and where $$ \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2} $$ $$ \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \\tag{3} $$</p> <ul> <li><p>m is the number of training examples in the dataset</p> </li> <li><p>$f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value</p> </li> </ul> <p>You will implement a function called <code>compute_gradient</code> which calculates $\\frac{\\partial J(w)}{\\partial w}$, $\\frac{\\partial J(w)}{\\partial b}$</p> <p></p> In\u00a0[15]: Copied! <pre># UNQ_C2\n# GRADED FUNCTION: compute_gradient\ndef compute_gradient(x, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      x (ndarray): Shape (m,) Input to the model (Population of cities) \n      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n      w, b (scalar): Parameters of the model  \n    Returns\n      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n     \"\"\"\n    \n    # Number of training examples\n    m = x.shape[0]\n    \n    # You need to return the following variables correctly\n    dj_dw = 0\n    dj_db = 0\n    \n    ### START CODE HERE ### \n    for i in range(m):\n        f_wb = w*x[i]+b\n        dj_db += f_wb - y[i]\n        dj_dw += (f_wb - y[i])*x[i]\n    dj_dw /= m\n    dj_db /= m\n    \n    ### END CODE HERE ### \n        \n    return dj_dw, dj_db\n</pre> # UNQ_C2 # GRADED FUNCTION: compute_gradient def compute_gradient(x, y, w, b):      \"\"\"     Computes the gradient for linear regression      Args:       x (ndarray): Shape (m,) Input to the model (Population of cities)        y (ndarray): Shape (m,) Label (Actual profits for the cities)       w, b (scalar): Parameters of the model       Returns       dj_dw (scalar): The gradient of the cost w.r.t. the parameters w       dj_db (scalar): The gradient of the cost w.r.t. the parameter b           \"\"\"          # Number of training examples     m = x.shape[0]          # You need to return the following variables correctly     dj_dw = 0     dj_db = 0          ### START CODE HERE ###      for i in range(m):         f_wb = w*x[i]+b         dj_db += f_wb - y[i]         dj_dw += (f_wb - y[i])*x[i]     dj_dw /= m     dj_db /= m          ### END CODE HERE ###               return dj_dw, dj_db Click for hints <pre><code>   \n* You can represent a summation operator eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ in code as follows:\n ```python \nh = 0\nfor i in range(m):\n    h = h + 2*i\n```\n\n* In this case, you can iterate over all the examples in `x` using a for loop and for each example, keep adding the gradient from that example to the variables `dj_dw` and `dj_db` which are initialized outside the loop. </code></pre> <p>* Then, you can return <code>dj_dw</code> and <code>dj_db</code> both divided by <code>m</code>.  Click for more hints * Here's how you can structure the overall implementation for this function ```python def compute_gradient(x, y, w, b): \"\"\" Computes the gradient for linear regression Args: x (ndarray): Shape (m,) Input to the model (Population of cities) y (ndarray): Shape (m,) Label (Actual profits for the cities) w, b (scalar): Parameters of the model Returns dj_dw (scalar): The gradient of the cost w.r.t. the parameters w dj_db (scalar): The gradient of the cost w.r.t. the parameter b \"\"\"</p> <pre><code>    # Number of training examples\n    m = x.shape[0]\n\n    # You need to return the following variables correctly\n    dj_dw = 0\n    dj_db = 0\n\n    ### START CODE HERE ### \n    # Loop over examples\n    for i in range(m):  \n        # Your code here to get prediction f_wb for the ith example\n        f_wb = \n        \n        # Your code here to get the gradient for w from the ith example \n        dj_dw_i = \n    \n        # Your code here to get the gradient for b from the ith example \n        dj_db_i = \n \n        # Update dj_db : In Python, a += 1  is the same as a = a + 1\n        dj_db += dj_db_i\n    \n        # Update dj_dw\n        dj_dw += dj_dw_i\n\n    # Divide both dj_dw and dj_db by m\n    dj_dw = dj_dw / m\n    dj_db = dj_db / m\n    ### END CODE HERE ### \n    \n    return dj_dw, dj_db\n```\n\nIf you're still stuck, you can check the hints presented below to figure out how to calculate `f_wb` and `cost`.\n\n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate f_wb&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n       &amp;emsp; &amp;emsp; You did this in the previous exercise! For scalars $a$, $b$ and $c$ (&lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;w&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are all scalars), you can calculate the equation $h = ab + c$ in code as &lt;code&gt;h = a * b + c&lt;/code&gt;\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate f&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n           &amp;emsp; &amp;emsp; You can compute f_wb as &lt;code&gt;f_wb = w * x[i] + b &lt;/code&gt;\n       &lt;/details&gt;\n&lt;/details&gt;\n    \n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate dj_dw_i&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n       &amp;emsp; &amp;emsp; For scalars $a$, $b$ and $c$ (&lt;code&gt;f_wb&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt; and &lt;code&gt;x[i]&lt;/code&gt; are all scalars), you can calculate the equation $h = (a - b)c$ in code as &lt;code&gt;h = (a-b)*c&lt;/code&gt;\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate f&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n           &amp;emsp; &amp;emsp; You can compute dj_dw_i as &lt;code&gt;dj_dw_i = (f_wb - y[i]) * x[i] &lt;/code&gt;\n       &lt;/details&gt;\n&lt;/details&gt;\n    \n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate dj_db_i&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n         &amp;emsp; &amp;emsp; You can compute dj_db_i as &lt;code&gt; dj_db_i = f_wb - y[i] &lt;/code&gt;\n&lt;/details&gt;\n    \n&lt;/details&gt;</code></pre> <p>Run the cells below to check your implementation of the <code>compute_gradient</code> function with two different initializations of the parameters $w$,$b$.</p> In\u00a0[16]: Copied! <pre># Compute and display gradient with w initialized to zeroes\ninitial_w = 0\ninitial_b = 0\n\ntmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\nprint('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)\n\ncompute_gradient_test(compute_gradient)\n</pre> # Compute and display gradient with w initialized to zeroes initial_w = 0 initial_b = 0  tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b) print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)  compute_gradient_test(compute_gradient) <pre>Gradient at initial w, b (zeros): -65.32884974555672 -5.83913505154639\nUsing X with shape (4, 1)\nAll tests passed!\n</pre> <p>Now let's run the gradient descent algorithm implemented above on our dataset.</p> <p>Expected Output:</p> Gradient at initial , b (zeros)  -65.32884975 -5.83913505154639 In\u00a0[17]: Copied! <pre># Compute and display cost and gradient with non-zero w\ntest_w = 0.2\ntest_b = 0.2\ntmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)\n\nprint('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db)\n</pre> # Compute and display cost and gradient with non-zero w test_w = 0.2 test_b = 0.2 tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)  print('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db) <pre>Gradient at test w, b: -47.41610118114435 -4.007175051546391\n</pre> <p>Expected Output:</p> Gradient at test w  -47.41610118 -4.007175051546391 <p></p> In\u00a0[18]: Copied! <pre>def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      x :    (ndarray): Shape (m,)\n      y :    (ndarray): Shape (m,)\n      w_in, b_in : (scalar) Initial values of parameters of the model\n      cost_function: function to compute cost\n      gradient_function: function to compute the gradient\n      alpha : (float) Learning rate\n      num_iters : (int) number of iterations to run gradient descent\n    Returns\n      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n          running gradient descent\n      b : (scalar)                Updated value of parameter of the model after\n          running gradient descent\n    \"\"\"\n    \n    # number of training examples\n    m = len(x)\n    \n    # An array to store cost J and w's at each iteration \u2014 primarily for graphing later\n    J_history = []\n    w_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_dw, dj_db = gradient_function(x, y, w, b )  \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n\n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion \n            cost =  cost_function(x, y, w, b)\n            J_history.append(cost)\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters/10) == 0:\n            w_history.append(w)\n            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n        \n    return w, b, J_history, w_history #return w and J,w history for graphing\n</pre> def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):      \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking      num_iters gradient steps with learning rate alpha          Args:       x :    (ndarray): Shape (m,)       y :    (ndarray): Shape (m,)       w_in, b_in : (scalar) Initial values of parameters of the model       cost_function: function to compute cost       gradient_function: function to compute the gradient       alpha : (float) Learning rate       num_iters : (int) number of iterations to run gradient descent     Returns       w : (ndarray): Shape (1,) Updated values of parameters of the model after           running gradient descent       b : (scalar)                Updated value of parameter of the model after           running gradient descent     \"\"\"          # number of training examples     m = len(x)          # An array to store cost J and w's at each iteration \u2014 primarily for graphing later     J_history = []     w_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in          for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_dw, dj_db = gradient_function(x, y, w, b )            # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw                        b = b - alpha * dj_db                         # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion              cost =  cost_function(x, y, w, b)             J_history.append(cost)          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters/10) == 0:             w_history.append(w)             print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")              return w, b, J_history, w_history #return w and J,w history for graphing <p>Now let's run the gradient descent algorithm above to learn the parameters for our dataset.</p> In\u00a0[19]: Copied! <pre># initialize fitting parameters. Recall that the shape of w is (n,)\ninitial_w = 0.\ninitial_b = 0.\n\n# some gradient descent settings\niterations = 1500\nalpha = 0.01\n\nw,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, \n                     compute_cost, compute_gradient, alpha, iterations)\nprint(\"w,b found by gradient descent:\", w, b)\n</pre> # initialize fitting parameters. Recall that the shape of w is (n,) initial_w = 0. initial_b = 0.  # some gradient descent settings iterations = 1500 alpha = 0.01  w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b,                       compute_cost, compute_gradient, alpha, iterations) print(\"w,b found by gradient descent:\", w, b) <pre>Iteration    0: Cost     6.74   \nIteration  150: Cost     5.31   \nIteration  300: Cost     4.96   \nIteration  450: Cost     4.76   \nIteration  600: Cost     4.64   \nIteration  750: Cost     4.57   \nIteration  900: Cost     4.53   \nIteration 1050: Cost     4.51   \nIteration 1200: Cost     4.50   \nIteration 1350: Cost     4.49   \nw,b found by gradient descent: 1.166362350335582 -3.63029143940436\n</pre> <p>Expected Output:</p>  w, b found by gradient descent  1.16636235 -3.63029143940436 <p>We will now use the final parameters from gradient descent to plot the linear fit.</p> <p>Recall that we can get the prediction for a single example $f(x^{(i)})= wx^{(i)}+b$.</p> <p>To calculate the predictions on the entire dataset, we can loop through all the training examples and calculate the prediction for each example. This is shown in the code block below.</p> In\u00a0[20]: Copied! <pre>m = x_train.shape[0]\npredicted = np.zeros(m)\n\nfor i in range(m):\n    predicted[i] = w * x_train[i] + b\n</pre> m = x_train.shape[0] predicted = np.zeros(m)  for i in range(m):     predicted[i] = w * x_train[i] + b <p>We will now plot the predicted values to see the linear fit.</p> In\u00a0[21]: Copied! <pre># Plot the linear fit\nplt.plot(x_train, predicted, c = \"b\")\n\n# Create a scatter plot of the data. \nplt.scatter(x_train, y_train, marker='x', c='r') \n\n# Set the title\nplt.title(\"Profits vs. Population per city\")\n# Set the y-axis label\nplt.ylabel('Profit in $10,000')\n# Set the x-axis label\nplt.xlabel('Population of City in 10,000s')\n</pre> # Plot the linear fit plt.plot(x_train, predicted, c = \"b\")  # Create a scatter plot of the data.  plt.scatter(x_train, y_train, marker='x', c='r')   # Set the title plt.title(\"Profits vs. Population per city\") # Set the y-axis label plt.ylabel('Profit in $10,000') # Set the x-axis label plt.xlabel('Population of City in 10,000s') Out[21]: <pre>Text(0.5, 0, 'Population of City in 10,000s')</pre> <p>Your final values of $w,b$ can also be used to make predictions on profits. Let's predict what the profit would be in areas of 35,000 and 70,000 people.</p> <ul> <li><p>The model takes in population of a city in 10,000s as input.</p> </li> <li><p>Therefore, 35,000 people can be translated into an input to the model as <code>np.array([3.5])</code></p> </li> <li><p>Similarly, 70,000 people can be translated into an input to the model as <code>np.array([7.])</code></p> </li> </ul> In\u00a0[22]: Copied! <pre>predict1 = 3.5 * w + b\nprint('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))\n\npredict2 = 7.0 * w + b\nprint('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000))\n</pre> predict1 = 3.5 * w + b print('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))  predict2 = 7.0 * w + b print('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000)) <pre>For population = 35,000, we predict a profit of $4519.77\nFor population = 70,000, we predict a profit of $45342.45\n</pre> <p>Expected Output:</p>  For population = 35,000, we predict a profit of  $4519.77   For population = 70,000, we predict a profit of  $45342.45"},{"location":"MachineLearning/part2/Linear_Regression/#practice-lab-linear-regression","title":"Practice Lab: Linear Regression\u00b6","text":"<p>Welcome to your first practice lab! In this lab, you will implement linear regression with one variable to predict profits for a restaurant franchise.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#outline","title":"Outline\u00b6","text":"<ul> <li> 1 - Packages </li> <li> 2 - Linear regression with one variable <ul> <li> 2.1 Problem Statement</li> <li> 2.2  Dataset</li> <li> 2.3 Refresher on linear regression</li> <li> 2.4  Compute Cost<ul> <li> Exercise 1</li> </ul> </li> <li> 2.5 Gradient descent <ul> <li> Exercise 2</li> </ul> </li> <li> 2.6 Learning parameters using batch gradient descent </li> </ul> </li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#1-packages","title":"1 - Packages\u00b6","text":"<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p> <ul> <li>numpy is the fundamental package for working with matrices in Python.</li> <li>matplotlib is a famous library to plot graphs in Python.</li> <li><code>utils.py</code> contains helper functions for this assignment. You do not need to modify code in this file.</li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#2-problem-statement","title":"2 -  Problem Statement\u00b6","text":"<p>Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet.</p> <ul> <li>You would like to expand your business to cities that may give your restaurant higher profits.</li> <li>The chain already has restaurants in various cities and you have data for profits and populations from the cities.</li> <li>You also have data on cities that are candidates for a new restaurant.<ul> <li>For these cities, you have the city population.</li> </ul> </li> </ul> <p>Can you use the data to help you identify which cities may potentially give your business higher profits?</p>"},{"location":"MachineLearning/part2/Linear_Regression/#3-dataset","title":"3 - Dataset\u00b6","text":"<p>You will start by loading the dataset for this task.</p> <ul> <li>The <code>load_data()</code> function shown below loads the data into variables <code>x_train</code> and <code>y_train</code><ul> <li><code>x_train</code> is the population of a city</li> <li><code>y_train</code> is the profit of a restaurant in that city. A negative value for profit indicates a loss.</li> <li>Both <code>X_train</code> and <code>y_train</code> are numpy arrays.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#view-the-variables","title":"View the variables\u00b6","text":"<p>Before starting on any task, it is useful to get more familiar with your dataset.</p> <ul> <li>A good place to start is to just print out each variable and see what it contains.</li> </ul> <p>The code below prints the variable <code>x_train</code> and the type of the variable.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#check-the-dimensions-of-your-variables","title":"Check the dimensions of your variables\u00b6","text":"<p>Another useful way to get familiar with your data is to view its dimensions.</p> <p>Please print the shape of <code>x_train</code> and <code>y_train</code> and see how many training examples you have in your dataset.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#visualize-your-data","title":"Visualize your data\u00b6","text":"<p>It is often useful to understand the data by visualizing it.</p> <ul> <li>For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population).</li> <li>Many other problems that you will encounter in real life have more than two properties (for example, population, average household income, monthly profits, monthly sales).When you have more than two properties, you can still use a scatter plot to see the relationship between each pair of properties.</li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#4-refresher-on-linear-regression","title":"4 - Refresher on linear regression\u00b6","text":"<p>In this practice lab, you will fit the linear regression parameters $(w,b)$ to your dataset.</p> <ul> <li><p>The model function for linear regression, which is a function that maps from <code>x</code> (city population) to <code>y</code> (your restaurant's monthly profit for that city) is represented as $$f_{w,b}(x) = wx + b$$</p> </li> <li><p>To train a linear regression model, you want to find the best $(w,b)$ parameters that fit your dataset.</p> <ul> <li><p>To compare how one choice of $(w,b)$ is better or worse than another choice, you can evaluate it with a cost function $J(w,b)$</p> <ul> <li>$J$ is a function of $(w,b)$. That is, the value of the cost $J(w,b)$ depends on the value of $(w,b)$.</li> </ul> </li> <li><p>The choice of $(w,b)$ that fits your data the best is the one that has the smallest cost $J(w,b)$.</p> </li> </ul> </li> <li><p>To find the values $(w,b)$ that gets the smallest possible cost $J(w,b)$, you can use a method called gradient descent.</p> <ul> <li>With each step of gradient descent, your parameters $(w,b)$ come closer to the optimal values that will achieve the lowest cost $J(w,b)$.</li> </ul> </li> <li><p>The trained linear regression model can then take the input feature $x$ (city population) and output a prediction $f_{w,b}(x)$ (predicted monthly profit for a restaurant in that city).</p> </li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#5-compute-cost","title":"5 - Compute Cost\u00b6","text":"<p>Gradient descent involves repeated steps to adjust the value of your parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$.</p> <ul> <li>At each step of gradient descent, it will be helpful for you to monitor your progress by computing the cost $J(w,b)$ as $(w,b)$ gets updated.</li> <li>In this section, you will implement a function to calculate $J(w,b)$ so that you can check the progress of your gradient descent implementation.</li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#cost-function","title":"Cost function\u00b6","text":"<p>As you may recall from the lecture, for one variable, the cost function for linear regression $J(w,b)$ is defined as</p> <p>$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$</p> <ul> <li>You can think of $f_{w,b}(x^{(i)})$ as the model's prediction of your restaurant's profit, as opposed to $y^{(i)}$, which is the actual profit that is recorded in the data.</li> <li>$m$ is the number of training examples in the dataset</li> </ul>"},{"location":"MachineLearning/part2/Linear_Regression/#model-prediction","title":"Model prediction\u00b6","text":"<ul> <li>For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is representented as:</li> </ul> <p>$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$</p> <p>This is the equation for a line, with an intercept $b$ and a slope $w$</p>"},{"location":"MachineLearning/part2/Linear_Regression/#implementation","title":"Implementation\u00b6","text":"<p>Please complete the <code>compute_cost()</code> function below to compute the cost $J(w,b)$.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Complete the <code>compute_cost</code> below to:</p> <ul> <li><p>Iterate over the training examples, and for each example, compute:</p> <ul> <li><p>The prediction of the model for that example $$   f_{wb}(x^{(i)}) =  wx^{(i)} + b    $$</p> </li> <li><p>The cost for that example  $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$</p> </li> </ul> </li> <li><p>Return the total cost over all examples $$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$</p> <ul> <li>Here, $m$ is the number of training examples and $\\sum$ is the summation operator</li> </ul> </li> </ul> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#6-gradient-descent","title":"6 - Gradient descent\u00b6","text":"<p>In this section, you will implement the gradient for parameters $w, b$ for linear regression.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Please complete the <code>compute_gradient</code> function to:</p> <ul> <li><p>Iterate over the training examples, and for each example, compute:</p> <ul> <li><p>The prediction of the model for that example $$   f_{wb}(x^{(i)}) =  wx^{(i)} + b    $$</p> </li> <li><p>The gradient for the parameters $w, b$ from that example $$   \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)})    $$ $$   \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}    $$</p> </li> </ul> </li> <li><p>Return the total gradient update from all the examples $$   \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}   $$</p> <p>$$   \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)}    $$</p> <ul> <li>Here, $m$ is the number of training examples and $\\sum$ is the summation operator</li> </ul> </li> </ul> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part2/Linear_Regression/#26-learning-parameters-using-batch-gradient-descent","title":"2.6 Learning parameters using batch gradient descent\u00b6","text":"<p>You will now find the optimal parameters of a linear regression model by using batch gradient descent. Recall batch refers to running all the examples in one iteration.</p> <ul> <li><p>You don't need to implement anything for this part. Simply run the cells below.</p> </li> <li><p>A good way to verify that gradient descent is working correctly is to look at the value of $J(w,b)$ and check that it is decreasing with each step.</p> </li> <li><p>Assuming you have implemented the gradient and computed the cost correctly and you have an appropriate value for the learning rate alpha, $J(w,b)$ should never increase and should converge to a steady value by the end of the algorithm.</p> </li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/","title":"Optional Lab: Multiple Variable Linear Regression","text":"In\u00a0[1]: Copied! <pre>import copy, math\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('./deeplearning.mplstyle')\nnp.set_printoptions(precision=2)  # reduced display precision on numpy arrays\n</pre> import copy, math import numpy as np import matplotlib.pyplot as plt plt.style.use('./deeplearning.mplstyle') np.set_printoptions(precision=2)  # reduced display precision on numpy arrays In\u00a0[2]: Copied! <pre>X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\n</pre> X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]]) y_train = np.array([460, 232, 178]) In\u00a0[3]: Copied! <pre># data is stored in numpy array/matrix\nprint(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\nprint(X_train)\nprint(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\nprint(y_train)\n</pre> # data is stored in numpy array/matrix print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\") print(X_train) print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\") print(y_train) <pre>X Shape: (3, 4), X Type:&lt;class 'numpy.ndarray'&gt;)\n[[2104    5    1   45]\n [1416    3    2   40]\n [ 852    2    1   35]]\ny Shape: (3,), y Type:&lt;class 'numpy.ndarray'&gt;)\n[460 232 178]\n</pre> <p>For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector.</p> In\u00a0[4]: Copied! <pre>b_init = 785.1811367994083\nw_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\nprint(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")\n</pre> b_init = 785.1811367994083 w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618]) print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\") <pre>w_init shape: (4,), b_init type: &lt;class 'float'&gt;\n</pre> <p></p> <p></p> In\u00a0[5]: Copied! <pre>def predict_single_loop(x, w, b): \n    \"\"\"\n    single predict using linear regression\n    \n    Args:\n      x (ndarray): Shape (n,) example with multiple features\n      w (ndarray): Shape (n,) model parameters    \n      b (scalar):  model parameter     \n      \n    Returns:\n      p (scalar):  prediction\n    \"\"\"\n    n = x.shape[0]\n    p = 0\n    for i in range(n):\n        p_i = x[i] * w[i]  \n        p = p + p_i         \n    p = p + b                \n    return p\n</pre> def predict_single_loop(x, w, b):      \"\"\"     single predict using linear regression          Args:       x (ndarray): Shape (n,) example with multiple features       w (ndarray): Shape (n,) model parameters           b (scalar):  model parameter                 Returns:       p (scalar):  prediction     \"\"\"     n = x.shape[0]     p = 0     for i in range(n):         p_i = x[i] * w[i]           p = p + p_i              p = p + b                     return p In\u00a0[6]: Copied! <pre># get a row from our training data\nx_vec = X_train[0,:]\nprint(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n\n# make a prediction\nf_wb = predict_single_loop(x_vec, w_init, b_init)\nprint(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")\n</pre> # get a row from our training data x_vec = X_train[0,:] print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")  # make a prediction f_wb = predict_single_loop(x_vec, w_init, b_init) print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\") <pre>x_vec shape (4,), x_vec value: [2104    5    1   45]\nf_wb shape (), prediction: 459.9999976194083\n</pre> <p>Note the shape of <code>x_vec</code>. It is a 1-D NumPy vector with 4 elements, (4,). The result, <code>f_wb</code> is a scalar.</p> <p></p> In\u00a0[7]: Copied! <pre>def predict(x, w, b): \n    \"\"\"\n    single predict using linear regression\n    Args:\n      x (ndarray): Shape (n,) example with multiple features\n      w (ndarray): Shape (n,) model parameters   \n      b (scalar):             model parameter \n      \n    Returns:\n      p (scalar):  prediction\n    \"\"\"\n    p = np.dot(x, w) + b     \n    return p    \n</pre> def predict(x, w, b):      \"\"\"     single predict using linear regression     Args:       x (ndarray): Shape (n,) example with multiple features       w (ndarray): Shape (n,) model parameters          b (scalar):             model parameter             Returns:       p (scalar):  prediction     \"\"\"     p = np.dot(x, w) + b          return p     In\u00a0[8]: Copied! <pre># get a row from our training data\nx_vec = X_train[0,:]\nprint(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n\n# make a prediction\nf_wb = predict(x_vec,w_init, b_init)\nprint(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")\n</pre> # get a row from our training data x_vec = X_train[0,:] print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")  # make a prediction f_wb = predict(x_vec,w_init, b_init) print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\") <pre>x_vec shape (4,), x_vec value: [2104    5    1   45]\nf_wb shape (), prediction: 459.99999761940825\n</pre> <p>The results and shapes are the same as the previous version which used looping. Going forward, <code>np.dot</code> will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine.</p> <p></p> <p>Below is an implementation of equations (3) and (4). Note that this uses a standard pattern for this course where a for loop over all <code>m</code> examples is used.</p> In\u00a0[9]: Copied! <pre>def compute_cost(X, y, w, b): \n    \"\"\"\n    compute cost\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      cost (scalar): cost\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):                                \n        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n        cost = cost + (f_wb_i - y[i])**2       #scalar\n    cost = cost / (2 * m)                      #scalar    \n    return cost\n</pre> def compute_cost(X, y, w, b):      \"\"\"     compute cost     Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter            Returns:       cost (scalar): cost     \"\"\"     m = X.shape[0]     cost = 0.0     for i in range(m):                                         f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)         cost = cost + (f_wb_i - y[i])**2       #scalar     cost = cost / (2 * m)                      #scalar         return cost In\u00a0[10]: Copied! <pre># Compute and display cost using our pre-chosen optimal parameters. \ncost = compute_cost(X_train, y_train, w_init, b_init)\nprint(f'Cost at optimal w : {cost}')\n</pre> # Compute and display cost using our pre-chosen optimal parameters.  cost = compute_cost(X_train, y_train, w_init, b_init) print(f'Cost at optimal w : {cost}') <pre>Cost at optimal w : 1.5578904880036537e-12\n</pre> <p>Expected Result: Cost at optimal w : 1.5578904045996674e-12</p> <p></p> <p></p> In\u00a0[11]: Copied! <pre>def compute_gradient(X, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]   \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m                                \n        \n    return dj_db, dj_dw\n</pre> def compute_gradient(X, y, w, b):      \"\"\"     Computes the gradient for linear regression      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter            Returns:       dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape           #(number of examples, number of features)     dj_dw = np.zeros((n,))     dj_db = 0.      for i in range(m):                                      err = (np.dot(X[i], w) + b) - y[i]            for j in range(n):                                      dj_dw[j] = dj_dw[j] + err * X[i, j]             dj_db = dj_db + err                             dj_dw = dj_dw / m                                     dj_db = dj_db / m                                              return dj_db, dj_dw In\u00a0[12]: Copied! <pre>#Compute and display gradient \ntmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\nprint(f'dj_db at initial w,b: {tmp_dj_db}')\nprint(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n</pre> #Compute and display gradient  tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init) print(f'dj_db at initial w,b: {tmp_dj_db}') print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}') <pre>dj_db at initial w,b: -1.673925169143331e-06\ndj_dw at initial w,b: \n [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n</pre> <p>Expected Result: dj_db at initial w,b: -1.6739251122999121e-06 dj_dw at initial w,b: [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]</p> <p></p> In\u00a0[13]: Copied! <pre>def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X (ndarray (m,n))   : Data, m examples with n features\n      y (ndarray (m,))    : target values\n      w_in (ndarray (n,)) : initial model parameters  \n      b_in (scalar)       : initial model parameter\n      cost_function       : function to compute cost\n      gradient_function   : function to compute the gradient\n      alpha (float)       : Learning rate\n      num_iters (int)     : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,)) : Updated values of parameters \n      b (scalar)       : Updated value of parameter \n      \"\"\"\n    \n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               ##None\n        b = b - alpha * dj_db               ##None\n      \n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion \n            J_history.append( cost_function(X, y, w, b))\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n        \n    return w, b, J_history #return final w,b and J history for graphing\n</pre> def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):      \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking      num_iters gradient steps with learning rate alpha          Args:       X (ndarray (m,n))   : Data, m examples with n features       y (ndarray (m,))    : target values       w_in (ndarray (n,)) : initial model parameters         b_in (scalar)       : initial model parameter       cost_function       : function to compute cost       gradient_function   : function to compute the gradient       alpha (float)       : Learning rate       num_iters (int)     : number of iterations to run gradient descent            Returns:       w (ndarray (n,)) : Updated values of parameters        b (scalar)       : Updated value of parameter        \"\"\"          # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in          for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = gradient_function(X, y, w, b)   ##None          # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw               ##None         b = b - alpha * dj_db               ##None                # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion              J_history.append( cost_function(X, y, w, b))          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters / 10) == 0:             print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")              return w, b, J_history #return final w,b and J history for graphing <p>In the next cell you will test the implementation.</p> In\u00a0[14]: Copied! <pre># initialize parameters\ninitial_w = np.zeros_like(w_init)\ninitial_b = 0.\n# some gradient descent settings\niterations = 1000\nalpha = 5.0e-7\n# run gradient descent \nw_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n                                                    compute_cost, compute_gradient, \n                                                    alpha, iterations)\nprint(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\nm,_ = X_train.shape\nfor i in range(m):\n    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")\n</pre> # initialize parameters initial_w = np.zeros_like(w_init) initial_b = 0. # some gradient descent settings iterations = 1000 alpha = 5.0e-7 # run gradient descent  w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,                                                     compute_cost, compute_gradient,                                                      alpha, iterations) print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \") m,_ = X_train.shape for i in range(m):     print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\") <pre>Iteration    0: Cost  2529.46   \nIteration  100: Cost   695.99   \nIteration  200: Cost   694.92   \nIteration  300: Cost   693.86   \nIteration  400: Cost   692.81   \nIteration  500: Cost   691.77   \nIteration  600: Cost   690.73   \nIteration  700: Cost   689.71   \nIteration  800: Cost   688.70   \nIteration  900: Cost   687.69   \nb,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \nprediction: 426.19, target value: 460\nprediction: 286.17, target value: 232\nprediction: 171.47, target value: 178\n</pre> <p>Expected Result: b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] prediction: 426.19, target value: 460 prediction: 286.17, target value: 232 prediction: 171.47, target value: 178</p> In\u00a0[15]: Copied! <pre># plot cost versus iteration  \nfig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\nax1.plot(J_hist)\nax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\nax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\nax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \nax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \nplt.show()\n</pre> # plot cost versus iteration   fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4)) ax1.plot(J_hist) ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:]) ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\") ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost')  ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step')  plt.show() <p>These results are not inspiring! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#optional-lab-multiple-variable-linear-regression","title":"Optional Lab: Multiple Variable Linear Regression\u00b6","text":"<p>In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#outline","title":"Outline\u00b6","text":"<ul> <li>\u00a0\u00a01.1 Goals</li> <li>\u00a0\u00a01.2 Tools</li> <li>\u00a0\u00a01.3 Notation</li> <li>2 Problem Statement</li> <li>\u00a0\u00a02.1 Matrix X containing our examples</li> <li>\u00a0\u00a02.2 Parameter vector w, b</li> <li>3 Model Prediction With Multiple Variables</li> <li>\u00a0\u00a03.1 Single Prediction element by element</li> <li>\u00a0\u00a03.2 Single Prediction, vector</li> <li>4 Compute Cost With Multiple Variables</li> <li>5 Gradient Descent With Multiple Variables</li> <li>\u00a0\u00a05.1 Compute Gradient with Multiple Variables</li> <li>\u00a0\u00a05.2 Gradient Descent With Multiple Variables</li> <li>6 Congratulations</li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#11-goals","title":"1.1 Goals\u00b6","text":"<ul> <li>Extend our regression model  routines to support multiple features<ul> <li>Extend data structures to support multiple features</li> <li>Rewrite prediction, cost and gradient routines to support multiple features</li> <li>Utilize NumPy <code>np.dot</code> to vectorize their implementations for speed and simplicity</li> </ul> </li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#12-tools","title":"1.2 Tools\u00b6","text":"<p>In this lab, we will make use of:</p> <ul> <li>NumPy, a popular library for scientific computing</li> <li>Matplotlib, a popular library for plotting data</li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#13-notation","title":"1.3 Notation\u00b6","text":"<p>Here is a summary of some of the notation you will encounter, updated for multiple features.</p> General Notation Description Python (if applicable) $a$ scalar, non-bold $\\mathbf{a}$ vector, bold $\\mathbf{A}$ matrix, bold capital Regression $\\mathbf{X}$ training example matrix <code>X_train</code> $\\mathbf{y}$ training example targets <code>y_train</code> $\\mathbf{x}^{(i)}$, $y^{(i)}$ $i_{th}$ Training Example <code>X[i]</code>, <code>y[i]</code> $m$ number of training examples <code>m</code> $n$ number of features in each example <code>n</code> $\\mathbf{w}$ parameter: weight <code>w</code> $b$ parameter: bias <code>b</code> $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ The result model evaluation at $\\mathbf{x}^{(i)}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ <code>f_wb</code>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#2-problem-statement","title":"2 Problem Statement\u00b6","text":"<p>You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!</p> Size (sqft) Number of Bedrooms Number of floors Age of  Home Price (1000s dollars) 2104 5 1 45 460 1416 3 2 40 232 852 2 1 35 178 <p>You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.</p> <p>Please run the following code cell to create your <code>X_train</code> and <code>y_train</code> variables.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#21-matrix-x-containing-our-examples","title":"2.1 Matrix X containing our examples\u00b6","text":"<p>Similar to the table above, examples are stored in a NumPy matrix <code>X_train</code>. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).</p> <p>$$\\mathbf{X} =  \\begin{pmatrix}  x^{(0)}_0 &amp; x^{(0)}_1 &amp; \\cdots &amp; x^{(0)}_{n-1} \\\\   x^{(1)}_0 &amp; x^{(1)}_1 &amp; \\cdots &amp; x^{(1)}_{n-1} \\\\  \\cdots \\\\  x^{(m-1)}_0 &amp; x^{(m-1)}_1 &amp; \\cdots &amp; x^{(m-1)}_{n-1}  \\end{pmatrix} $$ notation:</p> <ul> <li>$\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$</li> <li>$x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.</li> </ul> <p>Display the input data.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#22-parameter-vector-w-b","title":"2.2 Parameter vector w, b\u00b6","text":"<ul> <li>$\\mathbf{w}$ is a vector with $n$ elements.<ul> <li>Each element contains the parameter associated with one feature.</li> <li>in our dataset, n is 4.</li> <li>notionally, we draw this as a column vector</li> </ul> </li> </ul> <p>$$\\mathbf{w} = \\begin{pmatrix} w_0 \\\\  w_1 \\\\ \\cdots\\\\ w_{n-1} \\end{pmatrix} $$</p> <ul> <li>$b$ is a scalar parameter.</li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#3-model-prediction-with-multiple-variables","title":"3 Model Prediction With Multiple Variables\u00b6","text":"<p>The model's prediction with multiple variables is given by the linear model:</p> <p>$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$ or in vector notation: $$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ where $\\cdot$ is a vector <code>dot product</code></p> <p>To demonstrate the dot product, we will implement prediction using (1) and (2).</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#31-single-prediction-element-by-element","title":"3.1 Single Prediction element by element\u00b6","text":"<p>Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#32-single-prediction-vector","title":"3.2 Single Prediction, vector\u00b6","text":"<p>Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.</p> <p>Recall from the Python/Numpy lab that NumPy <code>np.dot()</code>[link] can be used to perform a vector dot product.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#4-compute-cost-with-multiple-variables","title":"4 Compute Cost With Multiple Variables\u00b6","text":"<p>The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is: $$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ where: $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$</p> <p>In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#5-gradient-descent-with-multiple-variables","title":"5 Gradient Descent With Multiple Variables\u00b6","text":"<p>Gradient descent for multiple variables:</p> <p>$$\\begin{align*} \\text{repeat}&amp;\\text{ until convergence:} \\; \\lbrace \\newline\\; &amp; w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; &amp; \\text{for j = 0..n-1}\\newline &amp;b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace \\end{align*}$$</p> <p>where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where</p> <p>$$ \\begin{align} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7} \\end{align} $$</p> <ul> <li><p>m is the number of training examples in the data set</p> </li> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value</p> </li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#51-compute-gradient-with-multiple-variables","title":"5.1 Compute Gradient with Multiple Variables\u00b6","text":"<p>An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an</p> <ul> <li>outer loop over all m examples.<ul> <li>$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated</li> <li>in a second loop over all n features:<ul> <li>$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.</li> </ul> </li> </ul> </li> </ul>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#52-gradient-descent-with-multiple-variables","title":"5.2 Gradient Descent With Multiple Variables\u00b6","text":"<p>The routine below implements equation (5) above.</p>"},{"location":"MachineLearning/part2/Multiple_Variable_Soln/#6-congratulations","title":"6 Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>Redeveloped the routines for linear regression, now with multiple variables.</li> <li>Utilized NumPy <code>np.dot</code> to vectorize the implementations</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/","title":"Optional Lab: Python, NumPy and Vectorization","text":"In\u00a0[1]: Copied! <pre>import numpy as np    # it is an unofficial standard to use np for numpy\nimport time\n</pre> import numpy as np    # it is an unofficial standard to use np for numpy import time <p>Data creation routines in NumPy will generally have a first parameter which is the shape of the object. This can either be a single value for a 1-D result or a tuple (n,m,...) specifying the shape of the result. Below are examples of creating vectors using these routines.</p> In\u00a0[2]: Copied! <pre># NumPy routines which allocate memory and fill arrays with value\na = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\na = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\na = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n</pre> # NumPy routines which allocate memory and fill arrays with value a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") <pre>np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\nnp.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\nnp.random.random_sample(4): a = [0.16054681 0.65264985 0.98302218 0.29864795], a shape = (4,), a data type = float64\n</pre> <p>Some data creation routines do not take a shape tuple:</p> In\u00a0[3]: Copied! <pre># NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument\na = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\na = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n</pre> # NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") <pre>np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64\nnp.random.rand(4): a = [0.04467047 0.14854614 0.24949402 0.02417095], a shape = (4,), a data type = float64\n</pre> <p>values can be specified manually as well.</p> In\u00a0[4]: Copied! <pre># NumPy routines which allocate memory and fill with user specified values\na = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\na = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n</pre> # NumPy routines which allocate memory and fill with user specified values a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\") a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\") <pre>np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int64\nnp.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n</pre> <p>These have all created a one-dimensional vector  <code>a</code> with four elements. <code>a.shape</code> returns the dimensions. Here we see a.shape = <code>(4,)</code> indicating a 1-d array with 4 elements.</p> <p></p> In\u00a0[5]: Copied! <pre>#vector indexing operations on 1-D vectors\na = np.arange(10)\nprint(a)\n\n#access an element\nprint(f\"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar\")\n\n# access the last element, negative indexes count from the end\nprint(f\"a[-1] = {a[-1]}\")\n\n#indexs must be within the range of the vector or they will produce and error\ntry:\n    c = a[10]\nexcept Exception as e:\n    print(\"The error message you'll see is:\")\n    print(e)\n</pre> #vector indexing operations on 1-D vectors a = np.arange(10) print(a)  #access an element print(f\"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar\")  # access the last element, negative indexes count from the end print(f\"a[-1] = {a[-1]}\")  #indexs must be within the range of the vector or they will produce and error try:     c = a[10] except Exception as e:     print(\"The error message you'll see is:\")     print(e) <pre>[0 1 2 3 4 5 6 7 8 9]\na[2].shape: () a[2]  = 2, Accessing an element returns a scalar\na[-1] = 9\nThe error message you'll see is:\nindex 10 is out of bounds for axis 0 with size 10\n</pre> <p></p> In\u00a0[6]: Copied! <pre>#vector slicing operations\na = np.arange(10)\nprint(f\"a         = {a}\")\n\n#access 5 consecutive elements (start:stop:step)\nc = a[2:7:1];     print(\"a[2:7:1] = \", c)\n\n# access 3 elements separated by two \nc = a[2:7:2];     print(\"a[2:7:2] = \", c)\n\n# access all elements index 3 and above\nc = a[3:];        print(\"a[3:]    = \", c)\n\n# access all elements below index 3\nc = a[:3];        print(\"a[:3]    = \", c)\n\n# access all elements\nc = a[:];         print(\"a[:]     = \", c)\n</pre> #vector slicing operations a = np.arange(10) print(f\"a         = {a}\")  #access 5 consecutive elements (start:stop:step) c = a[2:7:1];     print(\"a[2:7:1] = \", c)  # access 3 elements separated by two  c = a[2:7:2];     print(\"a[2:7:2] = \", c)  # access all elements index 3 and above c = a[3:];        print(\"a[3:]    = \", c)  # access all elements below index 3 c = a[:3];        print(\"a[:3]    = \", c)  # access all elements c = a[:];         print(\"a[:]     = \", c) <pre>a         = [0 1 2 3 4 5 6 7 8 9]\na[2:7:1] =  [2 3 4 5 6]\na[2:7:2] =  [2 4 6]\na[3:]    =  [3 4 5 6 7 8 9]\na[:3]    =  [0 1 2]\na[:]     =  [0 1 2 3 4 5 6 7 8 9]\n</pre> <p></p> In\u00a0[7]: Copied! <pre>a = np.array([1,2,3,4])\nprint(f\"a             : {a}\")\n# negate elements of a\nb = -a \nprint(f\"b = -a        : {b}\")\n\n# sum all elements of a, returns a scalar\nb = np.sum(a) \nprint(f\"b = np.sum(a) : {b}\")\n\nb = np.mean(a)\nprint(f\"b = np.mean(a): {b}\")\n\nb = a**2\nprint(f\"b = a**2      : {b}\")\n</pre> a = np.array([1,2,3,4]) print(f\"a             : {a}\") # negate elements of a b = -a  print(f\"b = -a        : {b}\")  # sum all elements of a, returns a scalar b = np.sum(a)  print(f\"b = np.sum(a) : {b}\")  b = np.mean(a) print(f\"b = np.mean(a): {b}\")  b = a**2 print(f\"b = a**2      : {b}\") <pre>a             : [1 2 3 4]\nb = -a        : [-1 -2 -3 -4]\nb = np.sum(a) : 10\nb = np.mean(a): 2.5\nb = a**2      : [ 1  4  9 16]\n</pre> <p></p> In\u00a0[8]: Copied! <pre>a = np.array([ 1, 2, 3, 4])\nb = np.array([-1,-2, 3, 4])\nprint(f\"Binary operators work element wise: {a + b}\")\n</pre> a = np.array([ 1, 2, 3, 4]) b = np.array([-1,-2, 3, 4]) print(f\"Binary operators work element wise: {a + b}\") <pre>Binary operators work element wise: [0 0 6 8]\n</pre> <p>Of course, for this to work correctly, the vectors must be of the same size:</p> In\u00a0[9]: Copied! <pre>#try a mismatched vector operation\nc = np.array([1, 2])\ntry:\n    d = a + c\nexcept Exception as e:\n    print(\"The error message you'll see is:\")\n    print(e)\n</pre> #try a mismatched vector operation c = np.array([1, 2]) try:     d = a + c except Exception as e:     print(\"The error message you'll see is:\")     print(e) <pre>The error message you'll see is:\noperands could not be broadcast together with shapes (4,) (2,) \n</pre> <p></p> In\u00a0[10]: Copied! <pre>a = np.array([1, 2, 3, 4])\n\n# multiply a by a scalar\nb = 5 * a \nprint(f\"b = 5 * a : {b}\")\n</pre> a = np.array([1, 2, 3, 4])  # multiply a by a scalar b = 5 * a  print(f\"b = 5 * a : {b}\") <pre>b = 5 * a : [ 5 10 15 20]\n</pre> <p></p> <p>The dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same.</p> <p>Let's implement our own version of the dot product below:</p> <p>Using a for loop, implement a function which returns the dot product of two vectors. The function to return given inputs $a$ and $b$: $$ x = \\sum_{i=0}^{n-1} a_i b_i $$ Assume both <code>a</code> and <code>b</code> are the same shape.</p> In\u00a0[11]: Copied! <pre>def my_dot(a, b): \n    \"\"\"\n   Compute the dot product of two vectors\n \n    Args:\n      a (ndarray (n,)):  input vector \n      b (ndarray (n,)):  input vector with same dimension as a\n    \n    Returns:\n      x (scalar): \n    \"\"\"\n    x=0\n    for i in range(a.shape[0]):\n        x = x + a[i] * b[i]\n    return x\n</pre> def my_dot(a, b):      \"\"\"    Compute the dot product of two vectors       Args:       a (ndarray (n,)):  input vector        b (ndarray (n,)):  input vector with same dimension as a          Returns:       x (scalar):      \"\"\"     x=0     for i in range(a.shape[0]):         x = x + a[i] * b[i]     return x In\u00a0[12]: Copied! <pre># test 1-D\na = np.array([1, 2, 3, 4])\nb = np.array([-1, 4, 3, 2])\nprint(f\"my_dot(a, b) = {my_dot(a, b)}\")\n</pre> # test 1-D a = np.array([1, 2, 3, 4]) b = np.array([-1, 4, 3, 2]) print(f\"my_dot(a, b) = {my_dot(a, b)}\") <pre>my_dot(a, b) = 24\n</pre> <p>Note, the dot product is expected to return a scalar value.</p> <p>Let's try the same operations using <code>np.dot</code>.</p> In\u00a0[13]: Copied! <pre># test 1-D\na = np.array([1, 2, 3, 4])\nb = np.array([-1, 4, 3, 2])\nc = np.dot(a, b)\nprint(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \nc = np.dot(b, a)\nprint(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")\n</pre> # test 1-D a = np.array([1, 2, 3, 4]) b = np.array([-1, 4, 3, 2]) c = np.dot(a, b) print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \")  c = np.dot(b, a) print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")  <pre>NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \nNumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () \n</pre> <p>Above, you will note that the results for 1-D matched our implementation.</p> <p></p> In\u00a0[14]: Copied! <pre>np.random.seed(1)\na = np.random.rand(10000000)  # very large arrays\nb = np.random.rand(10000000)\n\ntic = time.time()  # capture start time\nc = np.dot(a, b)\ntoc = time.time()  # capture end time\n\nprint(f\"np.dot(a, b) =  {c:.4f}\")\nprint(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")\n\ntic = time.time()  # capture start time\nc = my_dot(a,b)\ntoc = time.time()  # capture end time\n\nprint(f\"my_dot(a, b) =  {c:.4f}\")\nprint(f\"loop version duration: {1000*(toc-tic):.4f} ms \")\n\ndel(a);del(b)  #remove these big arrays from memory\n</pre> np.random.seed(1) a = np.random.rand(10000000)  # very large arrays b = np.random.rand(10000000)  tic = time.time()  # capture start time c = np.dot(a, b) toc = time.time()  # capture end time  print(f\"np.dot(a, b) =  {c:.4f}\") print(f\"Vectorized version duration: {1000*(toc-tic):.4f} ms \")  tic = time.time()  # capture start time c = my_dot(a,b) toc = time.time()  # capture end time  print(f\"my_dot(a, b) =  {c:.4f}\") print(f\"loop version duration: {1000*(toc-tic):.4f} ms \")  del(a);del(b)  #remove these big arrays from memory <pre>np.dot(a, b) =  2501072.5817\nVectorized version duration: 29.9268 ms \nmy_dot(a, b) =  2501072.5817\nloop version duration: 2137.4848 ms \n</pre> <p>So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large.</p> <p></p> In\u00a0[15]: Copied! <pre># show common Course 1 example\nX = np.array([[1],[2],[3],[4]])\nw = np.array([2])\nc = np.dot(X[1], w)\n\nprint(f\"X[1] has shape {X[1].shape}\")\nprint(f\"w has shape {w.shape}\")\nprint(f\"c has shape {c.shape}\")\n</pre> # show common Course 1 example X = np.array([[1],[2],[3],[4]]) w = np.array([2]) c = np.dot(X[1], w)  print(f\"X[1] has shape {X[1].shape}\") print(f\"w has shape {w.shape}\") print(f\"c has shape {c.shape}\") <pre>X[1] has shape (1,)\nw has shape (1,)\nc has shape ()\n</pre> <p></p> <p></p> <p></p> <p></p> <p>Below, the shape tuple is provided to achieve a 2-D result. Notice how NumPy uses brackets to denote each dimension. Notice further than NumPy, when printing, will print one row per line.</p> In\u00a0[16]: Copied! <pre>a = np.zeros((1, 5))                                       \nprint(f\"a shape = {a.shape}, a = {a}\")                     \n\na = np.zeros((2, 1))                                                                   \nprint(f\"a shape = {a.shape}, a = {a}\") \n\na = np.random.random_sample((1, 1))  \nprint(f\"a shape = {a.shape}, a = {a}\") \n</pre> a = np.zeros((1, 5))                                        print(f\"a shape = {a.shape}, a = {a}\")                       a = np.zeros((2, 1))                                                                    print(f\"a shape = {a.shape}, a = {a}\")   a = np.random.random_sample((1, 1))   print(f\"a shape = {a.shape}, a = {a}\")  <pre>a shape = (1, 5), a = [[0. 0. 0. 0. 0.]]\na shape = (2, 1), a = [[0.]\n [0.]]\na shape = (1, 1), a = [[0.44236513]]\n</pre> <p>One can also manually specify data. Dimensions are specified with additional brackets matching the format in the printing above.</p> In\u00a0[17]: Copied! <pre># NumPy routines which allocate memory and fill with user specified values\na = np.array([[5], [4], [3]]);   print(f\" a shape = {a.shape}, np.array: a = {a}\")\na = np.array([[5],   # One can also\n              [4],   # separate values\n              [3]]); #into separate rows\nprint(f\" a shape = {a.shape}, np.array: a = {a}\")\n</pre> # NumPy routines which allocate memory and fill with user specified values a = np.array([[5], [4], [3]]);   print(f\" a shape = {a.shape}, np.array: a = {a}\") a = np.array([[5],   # One can also               [4],   # separate values               [3]]); #into separate rows print(f\" a shape = {a.shape}, np.array: a = {a}\") <pre> a shape = (3, 1), np.array: a = [[5]\n [4]\n [3]]\n a shape = (3, 1), np.array: a = [[5]\n [4]\n [3]]\n</pre> <p></p> <p></p> <p>Matrices include a second index. The two indexes describe [row, column]. Access can either return an element or a row/column. See below:</p> In\u00a0[18]: Copied! <pre>#vector indexing operations on matrices\na = np.arange(6).reshape(-1, 2)   #reshape is a convenient way to create matrices\nprint(f\"a.shape: {a.shape}, \\na= {a}\")\n\n#access an element\nprint(f\"\\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\")\n\n#access a row\nprint(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")\n</pre> #vector indexing operations on matrices a = np.arange(6).reshape(-1, 2)   #reshape is a convenient way to create matrices print(f\"a.shape: {a.shape}, \\na= {a}\")  #access an element print(f\"\\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\")  #access a row print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\") <pre>a.shape: (3, 2), \na= [[0 1]\n [2 3]\n [4 5]]\n\na[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = &lt;class 'numpy.int64'&gt; Accessing an element returns a scalar\n\na[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = &lt;class 'numpy.ndarray'&gt;\n</pre> <p>It is worth drawing attention to the last example. Accessing a matrix by just specifying the row will return a 1-D vector.</p> <p>Reshape The previous example used reshape to shape the array. <code>a = np.arange(6).reshape(-1, 2) </code> This line of code first created a 1-D Vector of six elements. It then reshaped that vector into a 2-D array using the reshape command. This could have been written: <code>a = np.arange(6).reshape(3, 2) </code> To arrive at the same 3 row, 2 column array. The -1 argument tells the routine to compute the number of rows given the size of the array and the number of columns.</p> <p></p> In\u00a0[19]: Copied! <pre>#vector 2-D slicing operations\na = np.arange(20).reshape(-1, 10)\nprint(f\"a = \\n{a}\")\n\n#access 5 consecutive elements (start:stop:step)\nprint(\"a[0, 2:7:1] = \", a[0, 2:7:1], \",  a[0, 2:7:1].shape =\", a[0, 2:7:1].shape, \"a 1-D array\")\n\n#access 5 consecutive elements (start:stop:step) in two rows\nprint(\"a[:, 2:7:1] = \\n\", a[:, 2:7:1], \",  a[:, 2:7:1].shape =\", a[:, 2:7:1].shape, \"a 2-D array\")\n\n# access all elements\nprint(\"a[:,:] = \\n\", a[:,:], \",  a[:,:].shape =\", a[:,:].shape)\n\n# access all elements in one row (very common usage)\nprint(\"a[1,:] = \", a[1,:], \",  a[1,:].shape =\", a[1,:].shape, \"a 1-D array\")\n# same as\nprint(\"a[1]   = \", a[1],   \",  a[1].shape   =\", a[1].shape, \"a 1-D array\")\n</pre> #vector 2-D slicing operations a = np.arange(20).reshape(-1, 10) print(f\"a = \\n{a}\")  #access 5 consecutive elements (start:stop:step) print(\"a[0, 2:7:1] = \", a[0, 2:7:1], \",  a[0, 2:7:1].shape =\", a[0, 2:7:1].shape, \"a 1-D array\")  #access 5 consecutive elements (start:stop:step) in two rows print(\"a[:, 2:7:1] = \\n\", a[:, 2:7:1], \",  a[:, 2:7:1].shape =\", a[:, 2:7:1].shape, \"a 2-D array\")  # access all elements print(\"a[:,:] = \\n\", a[:,:], \",  a[:,:].shape =\", a[:,:].shape)  # access all elements in one row (very common usage) print(\"a[1,:] = \", a[1,:], \",  a[1,:].shape =\", a[1,:].shape, \"a 1-D array\") # same as print(\"a[1]   = \", a[1],   \",  a[1].shape   =\", a[1].shape, \"a 1-D array\")  <pre>a = \n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]]\na[0, 2:7:1] =  [2 3 4 5 6] ,  a[0, 2:7:1].shape = (5,) a 1-D array\na[:, 2:7:1] = \n [[ 2  3  4  5  6]\n [12 13 14 15 16]] ,  a[:, 2:7:1].shape = (2, 5) a 2-D array\na[:,:] = \n [[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]] ,  a[:,:].shape = (2, 10)\na[1,:] =  [10 11 12 13 14 15 16 17 18 19] ,  a[1,:].shape = (10,) a 1-D array\na[1]   =  [10 11 12 13 14 15 16 17 18 19] ,  a[1].shape   = (10,) a 1-D array\n</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#optional-lab-python-numpy-and-vectorization","title":"Optional Lab: Python, NumPy and Vectorization\u00b6","text":"<p>A brief introduction to some of the scientific computing used in this course. In particular the NumPy scientific computing package and its use with python.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#outline","title":"Outline\u00b6","text":"<ul> <li>\u00a0\u00a01.1 Goals</li> <li>\u00a0\u00a01.2 Useful References</li> <li>2 Python and NumPy </li> <li>3 Vectors</li> <li>\u00a0\u00a03.1 Abstract</li> <li>\u00a0\u00a03.2 NumPy Arrays</li> <li>\u00a0\u00a03.3 Vector Creation</li> <li>\u00a0\u00a03.4 Operations on Vectors</li> <li>4 Matrices</li> <li>\u00a0\u00a04.1 Abstract</li> <li>\u00a0\u00a04.2 NumPy Arrays</li> <li>\u00a0\u00a04.3 Matrix Creation</li> <li>\u00a0\u00a04.4 Operations on Matrices</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#11-goals","title":"1.1 Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>Review the features of NumPy and Python that are used in Course 1</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#12-useful-references","title":"1.2 Useful References\u00b6","text":"<ul> <li>NumPy Documentation including a basic introduction: NumPy.org</li> <li>A challenging feature topic: NumPy Broadcasting</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#2-python-and-numpy","title":"2 Python and NumPy \u00b6","text":"<p>Python is the programming language we will be using in this course. It has a set of numeric data types and arithmetic operations. NumPy is a library that extends the base capabilities of python to add a richer data set including more numeric types, vectors, matrices, and many matrix functions. NumPy and python  work together fairly seamlessly. Python arithmetic operators work on NumPy data types and many NumPy functions will accept python data types.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#3-vectors","title":"3 Vectors\u00b6","text":""},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#31-abstract","title":"3.1 Abstract\u00b6","text":"<p>Vectors, as you will use them in this course, are ordered arrays of numbers. In notation, vectors are denoted with lower case bold letters such as $\\mathbf{x}$.  The elements of a vector are all the same type. A vector does not, for example, contain both characters and numbers. The number of elements in the array is often referred to as the dimension though mathematicians may prefer rank. The vector shown has a dimension of $n$. The elements of a vector can be referenced with an index. In math settings, indexes typically run from 1 to n. In computer science and these labs, indexing will typically run from 0 to n-1.  In notation, elements of a vector, when referenced individually will indicate the index in a subscript, for example, the $0^{th}$ element, of the vector $\\mathbf{x}$ is $x_0$. Note, the x is not bold in this case.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#32-numpy-arrays","title":"3.2 NumPy Arrays\u00b6","text":"<p>NumPy's basic data structure is an indexable, n-dimensional array containing elements of the same type (<code>dtype</code>). Right away, you may notice we have overloaded the term 'dimension'. Above, it was the number of elements in the vector, here, dimension refers to the number of indexes of an array. A one-dimensional or 1-D array has one index. In Course 1, we will represent vectors as NumPy 1-D arrays.</p> <ul> <li>1-D array, shape (n,): n elements indexed [0] through [n-1]</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#33-vector-creation","title":"3.3 Vector Creation\u00b6","text":""},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#34-operations-on-vectors","title":"3.4 Operations on Vectors\u00b6","text":"<p>Let's explore some operations using vectors. </p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#341-indexing","title":"3.4.1 Indexing\u00b6","text":"<p>Elements of vectors can be accessed via indexing and slicing. NumPy provides a very complete set of indexing and slicing capabilities. We will explore only the basics needed for the course here. Reference Slicing and Indexing for more details. Indexing means referring to an element of an array by its position within the array. Slicing means getting a subset of elements from an array based on their indices. NumPy starts indexing at zero so the 3rd element of an vector $\\mathbf{a}$ is <code>a[2]</code>.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#342-slicing","title":"3.4.2 Slicing\u00b6","text":"<p>Slicing creates an array of indices using a set of three values (<code>start:stop:step</code>). A subset of values is also valid. Its use is best explained by example:</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#343-single-vector-operations","title":"3.4.3 Single vector operations\u00b6","text":"<p>There are a number of useful operations that involve operations on a single vector.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#344-vector-vector-element-wise-operations","title":"3.4.4 Vector Vector element-wise operations\u00b6","text":"<p>Most of the NumPy arithmetic, logical and comparison operations apply to vectors as well. These operators work on an element-by-element basis. For example $$ \\mathbf{a} + \\mathbf{b} = \\sum_{i=0}^{n-1} a_i + b_i $$</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#345-scalar-vector-operations","title":"3.4.5 Scalar Vector operations\u00b6","text":"<p>Vectors can be 'scaled' by scalar values. A scalar value is just a number. The scalar multiplies all the elements of the vector.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#346-vector-vector-dot-product","title":"3.4.6 Vector Vector dot product\u00b6","text":"<p>The dot product is a mainstay of Linear Algebra and NumPy. This is an operation used extensively in this course and should be well understood. The dot product is shown below.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#347-the-need-for-speed-vector-vs-for-loop","title":"3.4.7 The Need for Speed: vector vs for loop\u00b6","text":"<p>We utilized the NumPy  library because it improves speed memory efficiency. Let's demonstrate:</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#348-vector-vector-operations-in-course-1","title":"3.4.8 Vector Vector operations in Course 1\u00b6","text":"<p>Vector Vector operations will appear frequently in course 1. Here is why:</p> <ul> <li>Going forward, our examples will be stored in an array, <code>X_train</code> of dimension (m,n). This will be explained more in context, but here it is important to note it is a 2 Dimensional array or matrix (see next section on matrices).</li> <li><code>w</code> will be a 1-dimensional vector of shape (n,).</li> <li>we will perform operations by looping through the examples, extracting each example to work on individually by indexing X. For example:<code>X[i]</code></li> <li><code>X[i]</code> returns a value of shape (n,), a 1-dimensional vector. Consequently, operations involving <code>X[i]</code> are often vector-vector.</li> </ul> <p>That is a somewhat lengthy explanation, but aligning and understanding the shapes of your operands is important when performing vector operations.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#4-matrices","title":"4 Matrices\u00b6","text":""},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#41-abstract","title":"4.1 Abstract\u00b6","text":"<p>Matrices, are two dimensional arrays. The elements of a matrix are all of the same type. In notation, matrices are denoted with capitol, bold letter such as $\\mathbf{X}$. In this and other labs, <code>m</code> is often the number of rows and <code>n</code> the number of columns. The elements of a matrix can be referenced with a two dimensional index. In math settings, numbers in the index typically run from 1 to n. In computer science and these labs, indexing will run from 0 to n-1.</p>  Generic Matrix Notation, 1st index is row, 2nd is column"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#42-numpy-arrays","title":"4.2 NumPy Arrays\u00b6","text":"<p>NumPy's basic data structure is an indexable, n-dimensional array containing elements of the same type (<code>dtype</code>). These were described earlier. Matrices have a two-dimensional (2-D) index [m,n].</p> <p>In Course 1, 2-D matrices are used to hold training data. Training data is $m$ examples by $n$ features creating an (m,n) array. Course 1 does not do operations directly on matrices but typically extracts an example as a vector and operates on that. Below you will review:</p> <ul> <li>data creation</li> <li>slicing and indexing</li> </ul>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#43-matrix-creation","title":"4.3 Matrix Creation\u00b6","text":"<p>The same functions that created 1-D vectors will create 2-D or n-D arrays. Here are some examples</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#44-operations-on-matrices","title":"4.4 Operations on Matrices\u00b6","text":"<p>Let's explore some operations using matrices.</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#441-indexing","title":"4.4.1 Indexing\u00b6","text":""},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#442-slicing","title":"4.4.2 Slicing\u00b6","text":"<p>Slicing creates an array of indices using a set of three values (<code>start:stop:step</code>). A subset of values is also valid. Its use is best explained by example:</p>"},{"location":"MachineLearning/part2/Python_Numpy_Vectorization_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you mastered the features of Python and NumPy that are needed for Course 1.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/","title":"Optional Lab: Linear Regression using Scikit-Learn","text":"<p>There is an open-source, commercially usable machine learning toolkit called scikit-learn. This toolkit contains implementations of many of the algorithms that you will work with in this course.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install sklearn\n</pre> !pip install sklearn In\u00a0[4]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lab_utils_multi import  load_house_data\nfrom lab_utils_common import dlc\nnp.set_printoptions(precision=2)\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import SGDRegressor from sklearn.preprocessing import StandardScaler from lab_utils_multi import  load_house_data from lab_utils_common import dlc np.set_printoptions(precision=2) plt.style.use('./deeplearning.mplstyle') <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[4], line 3\n      1 import numpy as np\n      2 import matplotlib.pyplot as plt\n----&gt; 3 from sklearn.linear_model import SGDRegressor\n      4 from sklearn.preprocessing import StandardScaler\n      5 from lab_utils_multi import  load_house_data\n\nModuleNotFoundError: No module named 'sklearn'</pre> In\u00a0[\u00a0]: Copied! <pre>X_train, y_train = load_house_data()\nX_features = ['size(sqft)','bedrooms','floors','age']\n</pre> X_train, y_train = load_house_data() X_features = ['size(sqft)','bedrooms','floors','age'] In\u00a0[3]: Copied! <pre>scaler = StandardScaler()\nX_norm = scaler.fit_transform(X_train)\nprint(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \nprint(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")\n</pre> scaler = StandardScaler() X_norm = scaler.fit_transform(X_train) print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")    print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\") <pre>Peak to Peak range by column in Raw        X:[2.41e+03 4.00e+00 1.00e+00 9.50e+01]\nPeak to Peak range by column in Normalized X:[5.85 6.14 2.06 3.69]\n</pre> In\u00a0[4]: Copied! <pre>sgdr = SGDRegressor(max_iter=1000)\nsgdr.fit(X_norm, y_train)\nprint(sgdr)\nprint(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n</pre> sgdr = SGDRegressor(max_iter=1000) sgdr.fit(X_norm, y_train) print(sgdr) print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\") <pre>SGDRegressor()\nnumber of iterations completed: 111, number of weight updates: 10990.0\n</pre> In\u00a0[5]: Copied! <pre>b_norm = sgdr.intercept_\nw_norm = sgdr.coef_\nprint(f\"model parameters:                   w: {w_norm}, b:{b_norm}\")\nprint( \"model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\")\n</pre> b_norm = sgdr.intercept_ w_norm = sgdr.coef_ print(f\"model parameters:                   w: {w_norm}, b:{b_norm}\") print( \"model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\") <pre>model parameters:                   w: [109.95 -20.97 -32.35 -38.07], b:[363.15]\nmodel parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16\n</pre> In\u00a0[6]: Copied! <pre># make a prediction using sgdr.predict()\ny_pred_sgd = sgdr.predict(X_norm)\n# make a prediction using w,b. \ny_pred = np.dot(X_norm, w_norm) + b_norm  \nprint(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n\nprint(f\"Prediction on training set:\\n{y_pred[:4]}\" )\nprint(f\"Target values \\n{y_train[:4]}\")\n</pre> # make a prediction using sgdr.predict() y_pred_sgd = sgdr.predict(X_norm) # make a prediction using w,b.  y_pred = np.dot(X_norm, w_norm) + b_norm   print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")  print(f\"Prediction on training set:\\n{y_pred[:4]}\" ) print(f\"Target values \\n{y_train[:4]}\") <pre>prediction using np.dot() and sgdr.predict match: True\nPrediction on training set:\n[295.17 485.84 389.62 492.  ]\nTarget values \n[300.  509.8 394.  540. ]\n</pre> In\u00a0[7]: Copied! <pre># plot predictions and targets vs original features    \nfig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\nfor i in range(len(ax)):\n    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n    ax[i].set_xlabel(X_features[i])\n    ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict')\nax[0].set_ylabel(\"Price\"); ax[0].legend();\nfig.suptitle(\"target versus prediction using z-score normalized model\")\nplt.show()\n</pre> # plot predictions and targets vs original features     fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True) for i in range(len(ax)):     ax[i].scatter(X_train[:,i],y_train, label = 'target')     ax[i].set_xlabel(X_features[i])     ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict') ax[0].set_ylabel(\"Price\"); ax[0].legend(); fig.suptitle(\"target versus prediction using z-score normalized model\") plt.show() In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#optional-lab-linear-regression-using-scikit-learn","title":"Optional Lab: Linear Regression using Scikit-Learn\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>Utilize  scikit-learn to implement linear regression using Gradient Descent</li> </ul>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#tools","title":"Tools\u00b6","text":"<p>You will utilize functions from scikit-learn as well as matplotlib and NumPy.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#gradient-descent","title":"Gradient Descent\u00b6","text":"<p>Scikit-learn has a gradient descent regression model sklearn.linear_model.SGDRegressor.  Like your previous implementation of gradient descent, this model performs best with normalized inputs. sklearn.preprocessing.StandardScaler will perform z-score normalization as in a previous lab. Here it is referred to as 'standard score'.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#load-the-data-set","title":"Load the data set\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#scalenormalize-the-training-data","title":"Scale/normalize the training data\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#create-and-fit-the-regression-model","title":"Create and fit the regression model\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#view-parameters","title":"View parameters\u00b6","text":"<p>Note, the parameters are associated with the normalized input data. The fit parameters are very close to those found in the previous lab with this data.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#make-predictions","title":"Make predictions\u00b6","text":"<p>Predict the targets of the training data. Use both the <code>predict</code> routine and compute using $w$ and $b$.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#plot-results","title":"Plot Results\u00b6","text":"<p>Let's plot the predictions versus the target values.</p>"},{"location":"MachineLearning/part2/Sklearn_GD_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>utilized an open-source machine learning toolkit, scikit-learn</li> <li>implemented linear regression using gradient descent and feature normalization from that toolkit</li> </ul>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/","title":"Optional Lab: Linear Regression using Scikit-Learn","text":"<p>There is an open-source, commercially usable machine learning toolkit called scikit-learn. This toolkit contains implementations of many of the algorithms that you will work with in this course.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom lab_utils_multi import load_house_data\nplt.style.use('./deeplearning.mplstyle')\nnp.set_printoptions(precision=2)\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from lab_utils_multi import load_house_data plt.style.use('./deeplearning.mplstyle') np.set_printoptions(precision=2) <p></p> In\u00a0[2]: Copied! <pre>X_train = np.array([1.0, 2.0])   #features\ny_train = np.array([300, 500])   #target value\n</pre> X_train = np.array([1.0, 2.0])   #features y_train = np.array([300, 500])   #target value In\u00a0[3]: Copied! <pre>linear_model = LinearRegression()\n#X must be a 2-D Matrix\nlinear_model.fit(X_train.reshape(-1, 1), y_train) \n</pre> linear_model = LinearRegression() #X must be a 2-D Matrix linear_model.fit(X_train.reshape(-1, 1), y_train)  Out[3]: <pre>LinearRegression()</pre> In\u00a0[4]: Copied! <pre>b = linear_model.intercept_\nw = linear_model.coef_\nprint(f\"w = {w:}, b = {b:0.2f}\")\nprint(f\"'manual' prediction: f_wb = wx+b : {1200*w + b}\")\n</pre> b = linear_model.intercept_ w = linear_model.coef_ print(f\"w = {w:}, b = {b:0.2f}\") print(f\"'manual' prediction: f_wb = wx+b : {1200*w + b}\") <pre>w = [200.], b = 100.00\n'manual' prediction: f_wb = wx+b : [240100.]\n</pre> In\u00a0[5]: Copied! <pre>y_pred = linear_model.predict(X_train.reshape(-1, 1))\n\nprint(\"Prediction on training set:\", y_pred)\n\nX_test = np.array([[1200]])\nprint(f\"Prediction for 1200 sqft house: ${linear_model.predict(X_test)[0]:0.2f}\")\n</pre> y_pred = linear_model.predict(X_train.reshape(-1, 1))  print(\"Prediction on training set:\", y_pred)  X_test = np.array([[1200]]) print(f\"Prediction for 1200 sqft house: ${linear_model.predict(X_test)[0]:0.2f}\") <pre>Prediction on training set: [300. 500.]\nPrediction for 1200 sqft house: $240100.00\n</pre> In\u00a0[6]: Copied! <pre># load the dataset\nX_train, y_train = load_house_data()\nX_features = ['size(sqft)','bedrooms','floors','age']\n</pre> # load the dataset X_train, y_train = load_house_data() X_features = ['size(sqft)','bedrooms','floors','age'] In\u00a0[7]: Copied! <pre>linear_model = LinearRegression()\nlinear_model.fit(X_train, y_train) \n</pre> linear_model = LinearRegression() linear_model.fit(X_train, y_train)  Out[7]: <pre>LinearRegression()</pre> In\u00a0[8]: Copied! <pre>b = linear_model.intercept_\nw = linear_model.coef_\nprint(f\"w = {w:}, b = {b:0.2f}\")\n</pre> b = linear_model.intercept_ w = linear_model.coef_ print(f\"w = {w:}, b = {b:0.2f}\") <pre>w = [  0.27 -32.62 -67.25  -1.47], b = 220.42\n</pre> In\u00a0[9]: Copied! <pre>print(f\"Prediction on training set:\\n {linear_model.predict(X_train)[:4]}\" )\nprint(f\"prediction using w,b:\\n {(X_train @ w + b)[:4]}\")\nprint(f\"Target values \\n {y_train[:4]}\")\n\nx_house = np.array([1200, 3,1, 40]).reshape(-1,4)\nx_house_predict = linear_model.predict(x_house)[0]\nprint(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.2f}\")\n</pre> print(f\"Prediction on training set:\\n {linear_model.predict(X_train)[:4]}\" ) print(f\"prediction using w,b:\\n {(X_train @ w + b)[:4]}\") print(f\"Target values \\n {y_train[:4]}\")  x_house = np.array([1200, 3,1, 40]).reshape(-1,4) x_house_predict = linear_model.predict(x_house)[0] print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.2f}\") <pre>Prediction on training set:\n [295.18 485.98 389.52 492.15]\nprediction using w,b:\n [295.18 485.98 389.52 492.15]\nTarget values \n [300.  509.8 394.  540. ]\n predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709.09\n</pre> In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#optional-lab-linear-regression-using-scikit-learn","title":"Optional Lab: Linear Regression using Scikit-Learn\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>Utilize  scikit-learn to implement linear regression using a close form solution based on the normal equation</li> </ul>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#tools","title":"Tools\u00b6","text":"<p>You will utilize functions from scikit-learn as well as matplotlib and NumPy.</p>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#linear-regression-closed-form-solution","title":"Linear Regression, closed-form solution\u00b6","text":"<p>Scikit-learn has the linear regression model which implements a closed-form linear regression.</p> <p>Let's use the data from the early labs - a house with 1000 square feet sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000.</p> Size (1000 sqft) Price (1000s of dollars) 1 300 2 500"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#load-the-data-set","title":"Load the data set\u00b6","text":""},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#create-and-fit-the-model","title":"Create and fit the model\u00b6","text":"<p>The code below performs regression using scikit-learn. The first step creates a regression object. The second step utilizes one of the methods associated with the object, <code>fit</code>. This performs regression, fitting the parameters to the input data. The toolkit expects a two-dimensional X matrix.</p>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#view-parameters","title":"View Parameters\u00b6","text":"<p>The $\\mathbf{w}$ and $\\mathbf{b}$ parameters are referred to as 'coefficients' and 'intercept' in scikit-learn.</p>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#make-predictions","title":"Make Predictions\u00b6","text":"<p>Calling the <code>predict</code> function generates predictions.</p>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#second-example","title":"Second Example\u00b6","text":"<p>The second example is from an earlier lab with multiple features. The final parameter values and predictions are very close to the results from the un-normalized 'long-run' from that lab. That un-normalized run took hours to produce results, while this is nearly instantaneous. The closed-form solution work well on smaller data sets such as these but can be computationally demanding on larger data sets.</p> <p>The closed-form solution does not require normalization.</p>"},{"location":"MachineLearning/part2/Sklearn_Normal_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>utilized an open-source machine learning toolkit, scikit-learn</li> <li>implemented linear regression using a close-form solution from that toolkit</li> </ul>"},{"location":"MachineLearning/part2/lab_utils_common/","title":"Lab utils common","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>lab_utils_common.py functions common to all optional labs, Course 1, Week 2</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.style.use('./deeplearning.mplstyle')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0';\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\ndlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')\n</pre> import numpy as np import matplotlib.pyplot as plt  plt.style.use('./deeplearning.mplstyle') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')  <p>Regression Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>#Function to calculate the cost\ndef compute_cost_matrix(X, y, w, b, verbose=False):\n    \"\"\"\n    Computes the gradient for linear regression\n     Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns\n      cost: (scalar)\n    \"\"\"\n    m = X.shape[0]\n\n    # calculate f_wb for all examples.\n    f_wb = X @ w + b\n    # calculate cost\n    total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)\n\n    if verbose: print(\"f_wb:\")\n    if verbose: print(f_wb)\n\n    return total_cost\n\ndef compute_gradient_matrix(X, y, w, b):\n    \"\"\"\n    Computes the gradient for linear regression\n\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n    Returns\n      dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w.\n      dj_db (scalar):        The gradient of the cost w.r.t. the parameter b.\n\n    \"\"\"\n    m,n = X.shape\n    f_wb = X @ w + b\n    e   = f_wb - y\n    dj_dw  = (1/m) * (X.T @ e)\n    dj_db  = (1/m) * np.sum(e)\n\n    return dj_db,dj_dw\n\n\n# Loop version of multi-variable compute_cost\ndef compute_cost(X, y, w, b):\n    \"\"\"\n    compute cost\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n    Returns\n      cost (scalar)    : cost\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):\n        f_wb_i = np.dot(X[i],w) + b           #(n,)(n,)=scalar\n        cost = cost + (f_wb_i - y[i])**2\n    cost = cost/(2*m)\n    return cost \n\ndef compute_gradient(X, y, w, b):\n    \"\"\"\n    Computes the gradient for linear regression\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n    Returns\n      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.\n      dj_db (scalar):             The gradient of the cost w.r.t. the parameter b.\n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):\n        err = (np.dot(X[i], w) + b) - y[i]\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err * X[i,j]\n        dj_db = dj_db + err\n    dj_dw = dj_dw/m\n    dj_db = dj_db/m\n\n    return dj_db,dj_dw\n</pre> #Function to calculate the cost def compute_cost_matrix(X, y, w, b, verbose=False):     \"\"\"     Computes the gradient for linear regression      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter       verbose : (Boolean) If true, print out intermediate value f_wb     Returns       cost: (scalar)     \"\"\"     m = X.shape[0]      # calculate f_wb for all examples.     f_wb = X @ w + b     # calculate cost     total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)      if verbose: print(\"f_wb:\")     if verbose: print(f_wb)      return total_cost  def compute_gradient_matrix(X, y, w, b):     \"\"\"     Computes the gradient for linear regression      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter     Returns       dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w.       dj_db (scalar):        The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape     f_wb = X @ w + b     e   = f_wb - y     dj_dw  = (1/m) * (X.T @ e)     dj_db  = (1/m) * np.sum(e)      return dj_db,dj_dw   # Loop version of multi-variable compute_cost def compute_cost(X, y, w, b):     \"\"\"     compute cost     Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter     Returns       cost (scalar)    : cost     \"\"\"     m = X.shape[0]     cost = 0.0     for i in range(m):         f_wb_i = np.dot(X[i],w) + b           #(n,)(n,)=scalar         cost = cost + (f_wb_i - y[i])**2     cost = cost/(2*m)     return cost   def compute_gradient(X, y, w, b):     \"\"\"     Computes the gradient for linear regression     Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter     Returns       dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.       dj_db (scalar):             The gradient of the cost w.r.t. the parameter b.     \"\"\"     m,n = X.shape           #(number of examples, number of features)     dj_dw = np.zeros((n,))     dj_db = 0.      for i in range(m):         err = (np.dot(X[i], w) + b) - y[i]         for j in range(n):             dj_dw[j] = dj_dw[j] + err * X[i,j]         dj_db = dj_db + err     dj_dw = dj_dw/m     dj_db = dj_db/m      return dj_db,dj_dw"},{"location":"MachineLearning/part2/lab_utils_multi/","title":"Lab utils multi","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport copy\nimport math\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nfrom matplotlib.ticker import MaxNLocator\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; \nplt.style.use('./deeplearning.mplstyle')\n\ndef load_data_multi():\n    data = np.loadtxt(\"data/ex1data2.txt\", delimiter=',')\n    X = data[:,:2]\n    y = data[:,2]\n    return X, y\n</pre> import numpy as np import copy import math from scipy.stats import norm import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d from matplotlib.ticker import MaxNLocator dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0';  plt.style.use('./deeplearning.mplstyle')  def load_data_multi():     data = np.loadtxt(\"data/ex1data2.txt\", delimiter=',')     X = data[:,:2]     y = data[:,2]     return X, y <p>Plotting Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>def plt_house_x(X, y,f_wb=None, ax=None):\n    ''' plot house with aXis '''\n    if not ax:\n        fig, ax = plt.subplots(1,1)\n    ax.scatter(X, y, marker='x', c='r', label=\"Actual Value\")\n\n    ax.set_title(\"Housing Prices\")\n    ax.set_ylabel('Price (in 1000s of dollars)')\n    ax.set_xlabel(f'Size (1000 sqft)')\n    if f_wb is not None:\n        ax.plot(X, f_wb,  c=dlblue, label=\"Our Prediction\")\n    ax.legend()\n    \n\ndef mk_cost_lines(x,y,w,b, ax):\n    ''' makes vertical cost lines'''\n    cstr = \"cost = (1/2m)*1000*(\"\n    ctot = 0\n    label = 'cost for point'\n    for p in zip(x,y):\n        f_wb_p = w*p[0]+b\n        c_p = ((f_wb_p - p[1])**2)/2\n        c_p_txt = c_p/1000\n        ax.vlines(p[0], p[1],f_wb_p, lw=3, color=dlpurple, ls='dotted', label=label)\n        label='' #just one\n        cxy = [p[0], p[1] + (f_wb_p-p[1])/2]\n        ax.annotate(f'{c_p_txt:0.0f}', xy=cxy, xycoords='data',color=dlpurple, \n            xytext=(5, 0), textcoords='offset points')\n        cstr += f\"{c_p_txt:0.0f} +\"\n        ctot += c_p\n    ctot = ctot/(len(x))\n    cstr = cstr[:-1] + f\") = {ctot:0.0f}\"\n    ax.text(0.15,0.02,cstr, transform=ax.transAxes, color=dlpurple)\n    \n    \ndef inbounds(a,b,xlim,ylim):\n    xlow,xhigh = xlim\n    ylow,yhigh = ylim\n    ax, ay = a\n    bx, by = b\n    if (ax &gt; xlow and ax &lt; xhigh) and (bx &gt; xlow and bx &lt; xhigh) \\\n        and (ay &gt; ylow and ay &lt; yhigh) and (by &gt; ylow and by &lt; yhigh):\n        return(True)\n    else:\n        return(False)\n\nfrom mpl_toolkits.mplot3d import axes3d\ndef plt_contour_wgrad(x, y, hist, ax, w_range=[-100, 500, 5], b_range=[-500, 500, 5], \n                contours = [0.1,50,1000,5000,10000,25000,50000], \n                      resolution=5, w_final=200, b_final=100,step=10 ):\n    b0,w0 = np.meshgrid(np.arange(*b_range),np.arange(*w_range))\n    z=np.zeros_like(b0)\n    n,_ = w0.shape\n    for i in range(w0.shape[0]):\n        for j in range(w0.shape[1]):\n            z[i][j] = compute_cost(x, y, w0[i][j], b0[i][j] )\n   \n    CS = ax.contour(w0, b0, z, contours, linewidths=2,\n                   colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple]) \n    ax.clabel(CS, inline=1, fmt='%1.0f', fontsize=10)\n    ax.set_xlabel(\"w\");  ax.set_ylabel(\"b\")\n    ax.set_title('Contour plot of cost J(w,b), vs b,w with path of gradient descent')\n    w = w_final; b=b_final\n    ax.hlines(b, ax.get_xlim()[0],w, lw=2, color=dlpurple, ls='dotted')\n    ax.vlines(w, ax.get_ylim()[0],b, lw=2, color=dlpurple, ls='dotted')\n\n    base = hist[0]\n    for point in hist[0::step]:\n        edist = np.sqrt((base[0] - point[0])**2 + (base[1] - point[1])**2)\n        if(edist &gt; resolution or point==hist[-1]):\n            if inbounds(point,base, ax.get_xlim(),ax.get_ylim()):\n                plt.annotate('', xy=point, xytext=base,xycoords='data',\n                         arrowprops={'arrowstyle': '-&gt;', 'color': 'r', 'lw': 3},\n                         va='center', ha='center')\n            base=point\n    return\n\n\n# plots p1 vs p2. Prange is an array of entries [min, max, steps]. In feature scaling lab.\ndef plt_contour_multi(x, y, w, b, ax, prange, p1, p2, title=\"\", xlabel=\"\", ylabel=\"\"): \n    contours = [1e2, 2e2,3e2,4e2, 5e2, 6e2, 7e2,8e2,1e3, 1.25e3,1.5e3, 1e4, 1e5, 1e6, 1e7]\n    px,py = np.meshgrid(np.linspace(*(prange[p1])),np.linspace(*(prange[p2])))\n    z=np.zeros_like(px)\n    n,_ = px.shape\n    for i in range(px.shape[0]):\n        for j in range(px.shape[1]):\n            w_ij = w\n            b_ij = b\n            if p1 &lt;= 3: w_ij[p1] = px[i,j]\n            if p1 == 4: b_ij = px[i,j]\n            if p2 &lt;= 3: w_ij[p2] = py[i,j]\n            if p2 == 4: b_ij = py[i,j]\n                \n            z[i][j] = compute_cost(x, y, w_ij, b_ij )\n    CS = ax.contour(px, py, z, contours, linewidths=2,\n                   colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple]) \n    ax.clabel(CS, inline=1, fmt='%1.2e', fontsize=10)\n    ax.set_xlabel(xlabel);  ax.set_ylabel(ylabel)\n    ax.set_title(title, fontsize=14)\n\n\ndef plt_equal_scale(X_train, X_norm, y_train):\n    fig,ax = plt.subplots(1,2,figsize=(12,5))\n    prange = [\n              [ 0.238-0.045, 0.238+0.045,  50],\n              [-25.77326319-0.045, -25.77326319+0.045, 50],\n              [-50000, 0,      50],\n              [-1500,  0,      50],\n              [0, 200000, 50]]\n    w_best = np.array([0.23844318, -25.77326319, -58.11084634,  -1.57727192])\n    b_best = 235\n    plt_contour_multi(X_train, y_train, w_best, b_best, ax[0], prange, 0, 1, \n                      title='Unnormalized, J(w,b), vs w[0],w[1]',\n                      xlabel= \"w[0] (size(sqft))\", ylabel=\"w[1] (# bedrooms)\")\n    #\n    w_best = np.array([111.1972, -16.75480051, -28.51530411, -37.17305735])\n    b_best = 376.949151515151\n    prange = [[ 111-50, 111+50,   75],\n              [-16.75-50,-16.75+50, 75],\n              [-28.5-8, -28.5+8,  50],\n              [-37.1-16,-37.1+16, 50],\n              [376-150, 376+150, 50]]\n    plt_contour_multi(X_norm, y_train, w_best, b_best, ax[1], prange, 0, 1, \n                      title='Normalized, J(w,b), vs w[0],w[1]',\n                      xlabel= \"w[0] (normalized size(sqft))\", ylabel=\"w[1] (normalized # bedrooms)\")\n    fig.suptitle(\"Cost contour with equal scale\", fontsize=18)\n    #plt.tight_layout(rect=(0,0,1.05,1.05))\n    fig.tight_layout(rect=(0,0,1,0.95))\n    plt.show()\n    \ndef plt_divergence(p_hist, J_hist, x_train,y_train):\n\n    x=np.zeros(len(p_hist))\n    y=np.zeros(len(p_hist))\n    v=np.zeros(len(p_hist))\n    for i in range(len(p_hist)):\n        x[i] = p_hist[i][0]\n        y[i] = p_hist[i][1]\n        v[i] = J_hist[i]\n\n    fig = plt.figure(figsize=(12,5))\n    plt.subplots_adjust( wspace=0 )\n    gs = fig.add_gridspec(1, 5)\n    fig.suptitle(f\"Cost escalates when learning rate is too large\")\n    #===============\n    #  First subplot\n    #===============\n    ax = fig.add_subplot(gs[:2], )\n\n    # Print w vs cost to see minimum\n    fix_b = 100\n    w_array = np.arange(-70000, 70000, 1000)\n    cost = np.zeros_like(w_array)\n\n    for i in range(len(w_array)):\n        tmp_w = w_array[i]\n        cost[i] = compute_cost(x_train, y_train, tmp_w, fix_b)\n\n    ax.plot(w_array, cost)\n    ax.plot(x,v, c=dlmagenta)\n    ax.set_title(\"Cost vs w, b set to 100\")\n    ax.set_ylabel('Cost')\n    ax.set_xlabel('w')\n    ax.xaxis.set_major_locator(MaxNLocator(2)) \n\n    #===============\n    # Second Subplot\n    #===============\n\n    tmp_b,tmp_w = np.meshgrid(np.arange(-35000, 35000, 500),np.arange(-70000, 70000, 500))\n    z=np.zeros_like(tmp_b)\n    for i in range(tmp_w.shape[0]):\n        for j in range(tmp_w.shape[1]):\n            z[i][j] = compute_cost(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )\n\n    ax = fig.add_subplot(gs[2:], projection='3d')\n    ax.plot_surface(tmp_w, tmp_b, z,  alpha=0.3, color=dlblue)\n    ax.xaxis.set_major_locator(MaxNLocator(2)) \n    ax.yaxis.set_major_locator(MaxNLocator(2)) \n\n    ax.set_xlabel('w', fontsize=16)\n    ax.set_ylabel('b', fontsize=16)\n    ax.set_zlabel('\\ncost', fontsize=16)\n    plt.title('Cost vs (b, w)')\n    # Customize the view angle \n    ax.view_init(elev=20., azim=-65)\n    ax.plot(x, y, v,c=dlmagenta)\n    \n    return\n\n# draw derivative line\n# y = m*(x - x1) + y1\ndef add_line(dj_dx, x1, y1, d, ax):\n    x = np.linspace(x1-d, x1+d,50)\n    y = dj_dx*(x - x1) + y1\n    ax.scatter(x1, y1, color=dlblue, s=50)\n    ax.plot(x, y, '--', c=dldarkred,zorder=10, linewidth = 1)\n    xoff = 30 if x1 == 200 else 10\n    ax.annotate(r\"$\\frac{\\partial J}{\\partial w}$ =%d\" % dj_dx, fontsize=14,\n                xy=(x1, y1), xycoords='data',\n            xytext=(xoff, 10), textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"-&gt;\"),\n            horizontalalignment='left', verticalalignment='top')\n\ndef plt_gradients(x_train,y_train, f_compute_cost, f_compute_gradient):\n    #===============\n    #  First subplot\n    #===============\n    fig,ax = plt.subplots(1,2,figsize=(12,4))\n\n    # Print w vs cost to see minimum\n    fix_b = 100\n    w_array = np.linspace(-100, 500, 50)\n    w_array = np.linspace(0, 400, 50)\n    cost = np.zeros_like(w_array)\n\n    for i in range(len(w_array)):\n        tmp_w = w_array[i]\n        cost[i] = f_compute_cost(x_train, y_train, tmp_w, fix_b)\n    ax[0].plot(w_array, cost,linewidth=1)\n    ax[0].set_title(\"Cost vs w, with gradient; b set to 100\")\n    ax[0].set_ylabel('Cost')\n    ax[0].set_xlabel('w')\n\n    # plot lines for fixed b=100\n    for tmp_w in [100,200,300]:\n        fix_b = 100\n        dj_dw,dj_db = f_compute_gradient(x_train, y_train, tmp_w, fix_b )\n        j = f_compute_cost(x_train, y_train, tmp_w, fix_b)\n        add_line(dj_dw, tmp_w, j, 30, ax[0])\n\n    #===============\n    # Second Subplot\n    #===============\n\n    tmp_b,tmp_w = np.meshgrid(np.linspace(-200, 200, 10), np.linspace(-100, 600, 10))\n    U = np.zeros_like(tmp_w)\n    V = np.zeros_like(tmp_b)\n    for i in range(tmp_w.shape[0]):\n        for j in range(tmp_w.shape[1]):\n            U[i][j], V[i][j] = f_compute_gradient(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )\n    X = tmp_w\n    Y = tmp_b\n    n=-2\n    color_array = np.sqrt(((V-n)/2)**2 + ((U-n)/2)**2)\n\n    ax[1].set_title('Gradient shown in quiver plot')\n    Q = ax[1].quiver(X, Y, U, V, color_array, units='width', )\n    qk = ax[1].quiverkey(Q, 0.9, 0.9, 2, r'$2 \\frac{m}{s}$', labelpos='E',coordinates='figure')\n    ax[1].set_xlabel(\"w\"); ax[1].set_ylabel(\"b\")\n\ndef norm_plot(ax, data):\n    scale = (np.max(data) - np.min(data))*0.2\n    x = np.linspace(np.min(data)-scale,np.max(data)+scale,50)\n    _,bins, _ = ax.hist(data, x, color=\"xkcd:azure\")\n    #ax.set_ylabel(\"Count\")\n    \n    mu = np.mean(data); \n    std = np.std(data); \n    dist = norm.pdf(bins, loc=mu, scale = std)\n    \n    axr = ax.twinx()\n    axr.plot(bins,dist, color = \"orangered\", lw=2)\n    axr.set_ylim(bottom=0)\n    axr.axis('off')\n    \ndef plot_cost_i_w(X,y,hist):\n    ws = np.array([ p[0] for p in hist[\"params\"]])\n    rng = max(abs(ws[:,0].min()),abs(ws[:,0].max()))\n    wr = np.linspace(-rng+0.27,rng+0.27,20)\n    cst = [compute_cost(X,y,np.array([wr[i],-32, -67, -1.46]), 221) for i in range(len(wr))]\n\n    fig,ax = plt.subplots(1,2,figsize=(12,3))\n    ax[0].plot(hist[\"iter\"], (hist[\"cost\"]));  ax[0].set_title(\"Cost vs Iteration\")\n    ax[0].set_xlabel(\"iteration\"); ax[0].set_ylabel(\"Cost\")\n    ax[1].plot(wr, cst); ax[1].set_title(\"Cost vs w[0]\")\n    ax[1].set_xlabel(\"w[0]\"); ax[1].set_ylabel(\"Cost\")\n    ax[1].plot(ws[:,0],hist[\"cost\"])\n    plt.show()\n</pre> def plt_house_x(X, y,f_wb=None, ax=None):     ''' plot house with aXis '''     if not ax:         fig, ax = plt.subplots(1,1)     ax.scatter(X, y, marker='x', c='r', label=\"Actual Value\")      ax.set_title(\"Housing Prices\")     ax.set_ylabel('Price (in 1000s of dollars)')     ax.set_xlabel(f'Size (1000 sqft)')     if f_wb is not None:         ax.plot(X, f_wb,  c=dlblue, label=\"Our Prediction\")     ax.legend()       def mk_cost_lines(x,y,w,b, ax):     ''' makes vertical cost lines'''     cstr = \"cost = (1/2m)*1000*(\"     ctot = 0     label = 'cost for point'     for p in zip(x,y):         f_wb_p = w*p[0]+b         c_p = ((f_wb_p - p[1])**2)/2         c_p_txt = c_p/1000         ax.vlines(p[0], p[1],f_wb_p, lw=3, color=dlpurple, ls='dotted', label=label)         label='' #just one         cxy = [p[0], p[1] + (f_wb_p-p[1])/2]         ax.annotate(f'{c_p_txt:0.0f}', xy=cxy, xycoords='data',color=dlpurple,              xytext=(5, 0), textcoords='offset points')         cstr += f\"{c_p_txt:0.0f} +\"         ctot += c_p     ctot = ctot/(len(x))     cstr = cstr[:-1] + f\") = {ctot:0.0f}\"     ax.text(0.15,0.02,cstr, transform=ax.transAxes, color=dlpurple)           def inbounds(a,b,xlim,ylim):     xlow,xhigh = xlim     ylow,yhigh = ylim     ax, ay = a     bx, by = b     if (ax &gt; xlow and ax &lt; xhigh) and (bx &gt; xlow and bx &lt; xhigh) \\         and (ay &gt; ylow and ay &lt; yhigh) and (by &gt; ylow and by &lt; yhigh):         return(True)     else:         return(False)  from mpl_toolkits.mplot3d import axes3d def plt_contour_wgrad(x, y, hist, ax, w_range=[-100, 500, 5], b_range=[-500, 500, 5],                  contours = [0.1,50,1000,5000,10000,25000,50000],                        resolution=5, w_final=200, b_final=100,step=10 ):     b0,w0 = np.meshgrid(np.arange(*b_range),np.arange(*w_range))     z=np.zeros_like(b0)     n,_ = w0.shape     for i in range(w0.shape[0]):         for j in range(w0.shape[1]):             z[i][j] = compute_cost(x, y, w0[i][j], b0[i][j] )         CS = ax.contour(w0, b0, z, contours, linewidths=2,                    colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple])      ax.clabel(CS, inline=1, fmt='%1.0f', fontsize=10)     ax.set_xlabel(\"w\");  ax.set_ylabel(\"b\")     ax.set_title('Contour plot of cost J(w,b), vs b,w with path of gradient descent')     w = w_final; b=b_final     ax.hlines(b, ax.get_xlim()[0],w, lw=2, color=dlpurple, ls='dotted')     ax.vlines(w, ax.get_ylim()[0],b, lw=2, color=dlpurple, ls='dotted')      base = hist[0]     for point in hist[0::step]:         edist = np.sqrt((base[0] - point[0])**2 + (base[1] - point[1])**2)         if(edist &gt; resolution or point==hist[-1]):             if inbounds(point,base, ax.get_xlim(),ax.get_ylim()):                 plt.annotate('', xy=point, xytext=base,xycoords='data',                          arrowprops={'arrowstyle': '-&gt;', 'color': 'r', 'lw': 3},                          va='center', ha='center')             base=point     return   # plots p1 vs p2. Prange is an array of entries [min, max, steps]. In feature scaling lab. def plt_contour_multi(x, y, w, b, ax, prange, p1, p2, title=\"\", xlabel=\"\", ylabel=\"\"):      contours = [1e2, 2e2,3e2,4e2, 5e2, 6e2, 7e2,8e2,1e3, 1.25e3,1.5e3, 1e4, 1e5, 1e6, 1e7]     px,py = np.meshgrid(np.linspace(*(prange[p1])),np.linspace(*(prange[p2])))     z=np.zeros_like(px)     n,_ = px.shape     for i in range(px.shape[0]):         for j in range(px.shape[1]):             w_ij = w             b_ij = b             if p1 &lt;= 3: w_ij[p1] = px[i,j]             if p1 == 4: b_ij = px[i,j]             if p2 &lt;= 3: w_ij[p2] = py[i,j]             if p2 == 4: b_ij = py[i,j]                              z[i][j] = compute_cost(x, y, w_ij, b_ij )     CS = ax.contour(px, py, z, contours, linewidths=2,                    colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple])      ax.clabel(CS, inline=1, fmt='%1.2e', fontsize=10)     ax.set_xlabel(xlabel);  ax.set_ylabel(ylabel)     ax.set_title(title, fontsize=14)   def plt_equal_scale(X_train, X_norm, y_train):     fig,ax = plt.subplots(1,2,figsize=(12,5))     prange = [               [ 0.238-0.045, 0.238+0.045,  50],               [-25.77326319-0.045, -25.77326319+0.045, 50],               [-50000, 0,      50],               [-1500,  0,      50],               [0, 200000, 50]]     w_best = np.array([0.23844318, -25.77326319, -58.11084634,  -1.57727192])     b_best = 235     plt_contour_multi(X_train, y_train, w_best, b_best, ax[0], prange, 0, 1,                        title='Unnormalized, J(w,b), vs w[0],w[1]',                       xlabel= \"w[0] (size(sqft))\", ylabel=\"w[1] (# bedrooms)\")     #     w_best = np.array([111.1972, -16.75480051, -28.51530411, -37.17305735])     b_best = 376.949151515151     prange = [[ 111-50, 111+50,   75],               [-16.75-50,-16.75+50, 75],               [-28.5-8, -28.5+8,  50],               [-37.1-16,-37.1+16, 50],               [376-150, 376+150, 50]]     plt_contour_multi(X_norm, y_train, w_best, b_best, ax[1], prange, 0, 1,                        title='Normalized, J(w,b), vs w[0],w[1]',                       xlabel= \"w[0] (normalized size(sqft))\", ylabel=\"w[1] (normalized # bedrooms)\")     fig.suptitle(\"Cost contour with equal scale\", fontsize=18)     #plt.tight_layout(rect=(0,0,1.05,1.05))     fig.tight_layout(rect=(0,0,1,0.95))     plt.show()      def plt_divergence(p_hist, J_hist, x_train,y_train):      x=np.zeros(len(p_hist))     y=np.zeros(len(p_hist))     v=np.zeros(len(p_hist))     for i in range(len(p_hist)):         x[i] = p_hist[i][0]         y[i] = p_hist[i][1]         v[i] = J_hist[i]      fig = plt.figure(figsize=(12,5))     plt.subplots_adjust( wspace=0 )     gs = fig.add_gridspec(1, 5)     fig.suptitle(f\"Cost escalates when learning rate is too large\")     #===============     #  First subplot     #===============     ax = fig.add_subplot(gs[:2], )      # Print w vs cost to see minimum     fix_b = 100     w_array = np.arange(-70000, 70000, 1000)     cost = np.zeros_like(w_array)      for i in range(len(w_array)):         tmp_w = w_array[i]         cost[i] = compute_cost(x_train, y_train, tmp_w, fix_b)      ax.plot(w_array, cost)     ax.plot(x,v, c=dlmagenta)     ax.set_title(\"Cost vs w, b set to 100\")     ax.set_ylabel('Cost')     ax.set_xlabel('w')     ax.xaxis.set_major_locator(MaxNLocator(2))       #===============     # Second Subplot     #===============      tmp_b,tmp_w = np.meshgrid(np.arange(-35000, 35000, 500),np.arange(-70000, 70000, 500))     z=np.zeros_like(tmp_b)     for i in range(tmp_w.shape[0]):         for j in range(tmp_w.shape[1]):             z[i][j] = compute_cost(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )      ax = fig.add_subplot(gs[2:], projection='3d')     ax.plot_surface(tmp_w, tmp_b, z,  alpha=0.3, color=dlblue)     ax.xaxis.set_major_locator(MaxNLocator(2))      ax.yaxis.set_major_locator(MaxNLocator(2))       ax.set_xlabel('w', fontsize=16)     ax.set_ylabel('b', fontsize=16)     ax.set_zlabel('\\ncost', fontsize=16)     plt.title('Cost vs (b, w)')     # Customize the view angle      ax.view_init(elev=20., azim=-65)     ax.plot(x, y, v,c=dlmagenta)          return  # draw derivative line # y = m*(x - x1) + y1 def add_line(dj_dx, x1, y1, d, ax):     x = np.linspace(x1-d, x1+d,50)     y = dj_dx*(x - x1) + y1     ax.scatter(x1, y1, color=dlblue, s=50)     ax.plot(x, y, '--', c=dldarkred,zorder=10, linewidth = 1)     xoff = 30 if x1 == 200 else 10     ax.annotate(r\"$\\frac{\\partial J}{\\partial w}$ =%d\" % dj_dx, fontsize=14,                 xy=(x1, y1), xycoords='data',             xytext=(xoff, 10), textcoords='offset points',             arrowprops=dict(arrowstyle=\"-&gt;\"),             horizontalalignment='left', verticalalignment='top')  def plt_gradients(x_train,y_train, f_compute_cost, f_compute_gradient):     #===============     #  First subplot     #===============     fig,ax = plt.subplots(1,2,figsize=(12,4))      # Print w vs cost to see minimum     fix_b = 100     w_array = np.linspace(-100, 500, 50)     w_array = np.linspace(0, 400, 50)     cost = np.zeros_like(w_array)      for i in range(len(w_array)):         tmp_w = w_array[i]         cost[i] = f_compute_cost(x_train, y_train, tmp_w, fix_b)     ax[0].plot(w_array, cost,linewidth=1)     ax[0].set_title(\"Cost vs w, with gradient; b set to 100\")     ax[0].set_ylabel('Cost')     ax[0].set_xlabel('w')      # plot lines for fixed b=100     for tmp_w in [100,200,300]:         fix_b = 100         dj_dw,dj_db = f_compute_gradient(x_train, y_train, tmp_w, fix_b )         j = f_compute_cost(x_train, y_train, tmp_w, fix_b)         add_line(dj_dw, tmp_w, j, 30, ax[0])      #===============     # Second Subplot     #===============      tmp_b,tmp_w = np.meshgrid(np.linspace(-200, 200, 10), np.linspace(-100, 600, 10))     U = np.zeros_like(tmp_w)     V = np.zeros_like(tmp_b)     for i in range(tmp_w.shape[0]):         for j in range(tmp_w.shape[1]):             U[i][j], V[i][j] = f_compute_gradient(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )     X = tmp_w     Y = tmp_b     n=-2     color_array = np.sqrt(((V-n)/2)**2 + ((U-n)/2)**2)      ax[1].set_title('Gradient shown in quiver plot')     Q = ax[1].quiver(X, Y, U, V, color_array, units='width', )     qk = ax[1].quiverkey(Q, 0.9, 0.9, 2, r'$2 \\frac{m}{s}$', labelpos='E',coordinates='figure')     ax[1].set_xlabel(\"w\"); ax[1].set_ylabel(\"b\")  def norm_plot(ax, data):     scale = (np.max(data) - np.min(data))*0.2     x = np.linspace(np.min(data)-scale,np.max(data)+scale,50)     _,bins, _ = ax.hist(data, x, color=\"xkcd:azure\")     #ax.set_ylabel(\"Count\")          mu = np.mean(data);      std = np.std(data);      dist = norm.pdf(bins, loc=mu, scale = std)          axr = ax.twinx()     axr.plot(bins,dist, color = \"orangered\", lw=2)     axr.set_ylim(bottom=0)     axr.axis('off')      def plot_cost_i_w(X,y,hist):     ws = np.array([ p[0] for p in hist[\"params\"]])     rng = max(abs(ws[:,0].min()),abs(ws[:,0].max()))     wr = np.linspace(-rng+0.27,rng+0.27,20)     cst = [compute_cost(X,y,np.array([wr[i],-32, -67, -1.46]), 221) for i in range(len(wr))]      fig,ax = plt.subplots(1,2,figsize=(12,3))     ax[0].plot(hist[\"iter\"], (hist[\"cost\"]));  ax[0].set_title(\"Cost vs Iteration\")     ax[0].set_xlabel(\"iteration\"); ax[0].set_ylabel(\"Cost\")     ax[1].plot(wr, cst); ax[1].set_title(\"Cost vs w[0]\")     ax[1].set_xlabel(\"w[0]\"); ax[1].set_ylabel(\"Cost\")     ax[1].plot(ws[:,0],hist[\"cost\"])     plt.show()  <p>Regression Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_matrix(X, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X : (array_like Shape (m,n)) variable such as house size \n      y : (array_like Shape (m,1)) actual value \n      w : (array_like Shape (n,1)) Values of parameters of the model      \n      b : (scalar )                Values of parameter of the model      \n    Returns\n      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n                                  \n    \"\"\"\n    m,n = X.shape\n    f_wb = X @ w + b              \n    e   = f_wb - y                \n    dj_dw  = (1/m) * (X.T @ e)    \n    dj_db  = (1/m) * np.sum(e)    \n        \n    return dj_db,dj_dw\n\n#Function to calculate the cost\ndef compute_cost_matrix(X, y, w, b, verbose=False):\n    \"\"\"\n    Computes the gradient for linear regression \n     Args:\n      X : (array_like Shape (m,n)) variable such as house size \n      y : (array_like Shape (m,)) actual value \n      w : (array_like Shape (n,)) parameters of the model \n      b : (scalar               ) parameter of the model \n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns\n      cost: (scalar)                      \n    \"\"\" \n    m,n = X.shape\n\n    # calculate f_wb for all examples.\n    f_wb = X @ w + b  \n    # calculate cost\n    total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)\n\n    if verbose: print(\"f_wb:\")\n    if verbose: print(f_wb)\n        \n    return total_cost\n\n# Loop version of multi-variable compute_cost\ndef compute_cost(X, y, w, b): \n    \"\"\"\n    compute cost\n    Args:\n      X : (ndarray): Shape (m,n) matrix of examples with multiple features\n      w : (ndarray): Shape (n)   parameters for prediction   \n      b : (scalar):              parameter  for prediction   \n    Returns\n      cost: (scalar)             cost\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):                                \n        f_wb_i = np.dot(X[i],w) + b       \n        cost = cost + (f_wb_i - y[i])**2              \n    cost = cost/(2*m)                                 \n    return(np.squeeze(cost)) \n\ndef compute_gradient(X, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X : (ndarray Shape (m,n)) matrix of examples \n      y : (ndarray Shape (m,))  target value of each example\n      w : (ndarray Shape (n,))  parameters of the model      \n      b : (scalar)              parameter of the model      \n    Returns\n      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]   \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i,j]    \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw/m                                \n    dj_db = dj_db/m                                \n        \n    return dj_db,dj_dw\n\n#This version saves more values and is more verbose than the assigment versons\ndef gradient_descent_houses(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X : (array_like Shape (m,n)    matrix of examples \n      y : (array_like Shape (m,))    target value of each example\n      w_in : (array_like Shape (n,)) Initial values of parameters of the model\n      b_in : (scalar)                Initial value of parameter of the model\n      cost_function: function to compute cost\n      gradient_function: function to compute the gradient\n      alpha : (float) Learning rate\n      num_iters : (int) number of iterations to run gradient descent\n    Returns\n      w : (array_like Shape (n,)) Updated values of parameters of the model after\n          running gradient descent\n      b : (scalar)                Updated value of parameter of the model after\n          running gradient descent\n    \"\"\"\n    \n    # number of training examples\n    m = len(X)\n    \n    # An array to store values at each iteration primarily for graphing later\n    hist={}\n    hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];\n    \n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs\n\n    print(f\"Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \")\n    print(f\"---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\")\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = gradient_function(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J,w,b at each save interval for graphing\n        if i == 0 or i % save_interval == 0:     \n            hist[\"cost\"].append(cost_function(X, y, w, b))\n            hist[\"params\"].append([w,b])\n            hist[\"grads\"].append([dj_dw,dj_db])\n            hist[\"iter\"].append(i)\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters/10) == 0:\n            #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")\n            cst = cost_function(X, y, w, b)\n            print(f\"{i:9d} {cst:0.5e} {w[0]: 0.1e} {w[1]: 0.1e} {w[2]: 0.1e} {w[3]: 0.1e} {b: 0.1e} {dj_dw[0]: 0.1e} {dj_dw[1]: 0.1e} {dj_dw[2]: 0.1e} {dj_dw[3]: 0.1e} {dj_db: 0.1e}\")\n       \n    return w, b, hist #return w,b and history for graphing\n\ndef run_gradient_descent(X,y,iterations=1000, alpha = 1e-6):\n\n    m,n = X.shape\n    # initialize parameters\n    initial_w = np.zeros(n)\n    initial_b = 0\n    # run gradient descent\n    w_out, b_out, hist_out = gradient_descent_houses(X ,y, initial_w, initial_b,\n                                               compute_cost, compute_gradient_matrix, alpha, iterations)\n    print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.2f}\")\n    \n    return(w_out, b_out, hist_out)\n\n# compact extaction of hist data\n#x = hist[\"iter\"]\n#J  = np.array([ p    for p in hist[\"cost\"]])\n#ws = np.array([ p[0] for p in hist[\"params\"]])\n#dj_ws = np.array([ p[0] for p in hist[\"grads\"]])\n\n#bs = np.array([ p[1] for p in hist[\"params\"]]) \n\ndef run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-6):\n    m,n = X.shape\n    # initialize parameters\n    initial_w = np.zeros(n)\n    initial_b = 0\n    # run gradient descent\n    w_out, b_out, hist_out = gradient_descent(X ,y, initial_w, initial_b,\n                                               compute_cost, compute_gradient_matrix, alpha, iterations)\n    print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.4f}\")\n    \n    return(w_out, b_out)\n\ndef gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X : (array_like Shape (m,n)    matrix of examples \n      y : (array_like Shape (m,))    target value of each example\n      w_in : (array_like Shape (n,)) Initial values of parameters of the model\n      b_in : (scalar)                Initial value of parameter of the model\n      cost_function: function to compute cost\n      gradient_function: function to compute the gradient\n      alpha : (float) Learning rate\n      num_iters : (int) number of iterations to run gradient descent\n    Returns\n      w : (array_like Shape (n,)) Updated values of parameters of the model after\n          running gradient descent\n      b : (scalar)                Updated value of parameter of the model after\n          running gradient descent\n    \"\"\"\n    \n    # number of training examples\n    m = len(X)\n    \n    # An array to store values at each iteration primarily for graphing later\n    hist={}\n    hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];\n    \n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = gradient_function(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J,w,b at each save interval for graphing\n        if i == 0 or i % save_interval == 0:     \n            hist[\"cost\"].append(cost_function(X, y, w, b))\n            hist[\"params\"].append([w,b])\n            hist[\"grads\"].append([dj_dw,dj_db])\n            hist[\"iter\"].append(i)\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters/10) == 0:\n            #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")\n            cst = cost_function(X, y, w, b)\n            print(f\"Iteration {i:9d}, Cost: {cst:0.5e}\")\n    return w, b, hist #return w,b and history for graphing\n\ndef load_house_data():\n    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n    X = data[:,:4]\n    y = data[:,4]\n    return X, y\n\ndef zscore_normalize_features(X,rtn_ms=False):\n    \"\"\"\n    returns z-score normalized X by column\n    Args:\n      X : (numpy array (m,n)) \n    Returns\n      X_norm: (numpy array (m,n)) input normalized by column\n    \"\"\"\n    mu     = np.mean(X,axis=0)  \n    sigma  = np.std(X,axis=0)\n    X_norm = (X - mu)/sigma      \n\n    if rtn_ms:\n        return(X_norm, mu, sigma)\n    else:\n        return(X_norm)\n    \n    \n</pre> def compute_gradient_matrix(X, y, w, b):      \"\"\"     Computes the gradient for linear regression        Args:       X : (array_like Shape (m,n)) variable such as house size        y : (array_like Shape (m,1)) actual value        w : (array_like Shape (n,1)) Values of parameters of the model             b : (scalar )                Values of parameter of the model           Returns       dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.        dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.                                         \"\"\"     m,n = X.shape     f_wb = X @ w + b                   e   = f_wb - y                     dj_dw  = (1/m) * (X.T @ e)         dj_db  = (1/m) * np.sum(e)                  return dj_db,dj_dw  #Function to calculate the cost def compute_cost_matrix(X, y, w, b, verbose=False):     \"\"\"     Computes the gradient for linear regression       Args:       X : (array_like Shape (m,n)) variable such as house size        y : (array_like Shape (m,)) actual value        w : (array_like Shape (n,)) parameters of the model        b : (scalar               ) parameter of the model        verbose : (Boolean) If true, print out intermediate value f_wb     Returns       cost: (scalar)                           \"\"\"      m,n = X.shape      # calculate f_wb for all examples.     f_wb = X @ w + b       # calculate cost     total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)      if verbose: print(\"f_wb:\")     if verbose: print(f_wb)              return total_cost  # Loop version of multi-variable compute_cost def compute_cost(X, y, w, b):      \"\"\"     compute cost     Args:       X : (ndarray): Shape (m,n) matrix of examples with multiple features       w : (ndarray): Shape (n)   parameters for prediction          b : (scalar):              parameter  for prediction        Returns       cost: (scalar)             cost     \"\"\"     m = X.shape[0]     cost = 0.0     for i in range(m):                                         f_wb_i = np.dot(X[i],w) + b                cost = cost + (f_wb_i - y[i])**2                   cost = cost/(2*m)                                      return(np.squeeze(cost))   def compute_gradient(X, y, w, b):      \"\"\"     Computes the gradient for linear regression      Args:       X : (ndarray Shape (m,n)) matrix of examples        y : (ndarray Shape (m,))  target value of each example       w : (ndarray Shape (n,))  parameters of the model             b : (scalar)              parameter of the model           Returns       dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.        dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape           #(number of examples, number of features)     dj_dw = np.zeros((n,))     dj_db = 0.      for i in range(m):                                      err = (np.dot(X[i], w) + b) - y[i]            for j in range(n):                                      dj_dw[j] = dj_dw[j] + err * X[i,j]             dj_db = dj_db + err                             dj_dw = dj_dw/m                                     dj_db = dj_db/m                                              return dj_db,dj_dw  #This version saves more values and is more verbose than the assigment versons def gradient_descent_houses(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):      \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking      num_iters gradient steps with learning rate alpha          Args:       X : (array_like Shape (m,n)    matrix of examples        y : (array_like Shape (m,))    target value of each example       w_in : (array_like Shape (n,)) Initial values of parameters of the model       b_in : (scalar)                Initial value of parameter of the model       cost_function: function to compute cost       gradient_function: function to compute the gradient       alpha : (float) Learning rate       num_iters : (int) number of iterations to run gradient descent     Returns       w : (array_like Shape (n,)) Updated values of parameters of the model after           running gradient descent       b : (scalar)                Updated value of parameter of the model after           running gradient descent     \"\"\"          # number of training examples     m = len(X)          # An array to store values at each iteration primarily for graphing later     hist={}     hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];          w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in     save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs      print(f\"Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \")     print(f\"---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\")      for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = gradient_function(X, y, w, b)             # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw                        b = b - alpha * dj_db                               # Save cost J,w,b at each save interval for graphing         if i == 0 or i % save_interval == 0:                  hist[\"cost\"].append(cost_function(X, y, w, b))             hist[\"params\"].append([w,b])             hist[\"grads\"].append([dj_dw,dj_db])             hist[\"iter\"].append(i)          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters/10) == 0:             #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")             cst = cost_function(X, y, w, b)             print(f\"{i:9d} {cst:0.5e} {w[0]: 0.1e} {w[1]: 0.1e} {w[2]: 0.1e} {w[3]: 0.1e} {b: 0.1e} {dj_dw[0]: 0.1e} {dj_dw[1]: 0.1e} {dj_dw[2]: 0.1e} {dj_dw[3]: 0.1e} {dj_db: 0.1e}\")             return w, b, hist #return w,b and history for graphing  def run_gradient_descent(X,y,iterations=1000, alpha = 1e-6):      m,n = X.shape     # initialize parameters     initial_w = np.zeros(n)     initial_b = 0     # run gradient descent     w_out, b_out, hist_out = gradient_descent_houses(X ,y, initial_w, initial_b,                                                compute_cost, compute_gradient_matrix, alpha, iterations)     print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.2f}\")          return(w_out, b_out, hist_out)  # compact extaction of hist data #x = hist[\"iter\"] #J  = np.array([ p    for p in hist[\"cost\"]]) #ws = np.array([ p[0] for p in hist[\"params\"]]) #dj_ws = np.array([ p[0] for p in hist[\"grads\"]])  #bs = np.array([ p[1] for p in hist[\"params\"]])   def run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-6):     m,n = X.shape     # initialize parameters     initial_w = np.zeros(n)     initial_b = 0     # run gradient descent     w_out, b_out, hist_out = gradient_descent(X ,y, initial_w, initial_b,                                                compute_cost, compute_gradient_matrix, alpha, iterations)     print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.4f}\")          return(w_out, b_out)  def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):      \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking      num_iters gradient steps with learning rate alpha          Args:       X : (array_like Shape (m,n)    matrix of examples        y : (array_like Shape (m,))    target value of each example       w_in : (array_like Shape (n,)) Initial values of parameters of the model       b_in : (scalar)                Initial value of parameter of the model       cost_function: function to compute cost       gradient_function: function to compute the gradient       alpha : (float) Learning rate       num_iters : (int) number of iterations to run gradient descent     Returns       w : (array_like Shape (n,)) Updated values of parameters of the model after           running gradient descent       b : (scalar)                Updated value of parameter of the model after           running gradient descent     \"\"\"          # number of training examples     m = len(X)          # An array to store values at each iteration primarily for graphing later     hist={}     hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];          w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in     save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs      for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = gradient_function(X, y, w, b)             # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw                        b = b - alpha * dj_db                               # Save cost J,w,b at each save interval for graphing         if i == 0 or i % save_interval == 0:                  hist[\"cost\"].append(cost_function(X, y, w, b))             hist[\"params\"].append([w,b])             hist[\"grads\"].append([dj_dw,dj_db])             hist[\"iter\"].append(i)          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters/10) == 0:             #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")             cst = cost_function(X, y, w, b)             print(f\"Iteration {i:9d}, Cost: {cst:0.5e}\")     return w, b, hist #return w,b and history for graphing  def load_house_data():     data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)     X = data[:,:4]     y = data[:,4]     return X, y  def zscore_normalize_features(X,rtn_ms=False):     \"\"\"     returns z-score normalized X by column     Args:       X : (numpy array (m,n))      Returns       X_norm: (numpy array (m,n)) input normalized by column     \"\"\"     mu     = np.mean(X,axis=0)       sigma  = np.std(X,axis=0)     X_norm = (X - mu)/sigma            if rtn_ms:         return(X_norm, mu, sigma)     else:         return(X_norm)"},{"location":"MachineLearning/part2/public_tests/","title":"Public tests","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def compute_cost_test(target):\n    # print(\"Using X with shape (4, 1)\")\n    # Case 1\n    x = np.array([2, 4, 6, 8]).T\n    y = np.array([7, 11, 15, 19]).T\n    initial_w = 2\n    initial_b = 3.0\n    cost = target(x, y, initial_w, initial_b)\n    assert cost == 0, f\"Case 1: Cost must be 0 for a perfect prediction but got {cost}\"\n    \n    # Case 2\n    x = np.array([2, 4, 6, 8]).T\n    y = np.array([7, 11, 15, 19]).T\n    initial_w = 2.0\n    initial_b = 1.0\n    cost = target(x, y, initial_w, initial_b)\n    assert cost == 2, f\"Case 2: Cost must be 2 but got {cost}\"\n    \n    # print(\"Using X with shape (5, 1)\")\n    # Case 3\n    x = np.array([1.5, 2.5, 3.5, 4.5, 1.5]).T\n    y = np.array([4, 7, 10, 13, 5]).T\n    initial_w = 1\n    initial_b = 0.0\n    cost = target(x, y, initial_w, initial_b)\n    assert np.isclose(cost, 15.325), f\"Case 3: Cost must be 15.325 for a perfect prediction but got {cost}\"\n    \n    # Case 4\n    initial_b = 1.0\n    cost = target(x, y, initial_w, initial_b)\n    assert np.isclose(cost, 10.725), f\"Case 4: Cost must be 10.725 but got {cost}\"\n    \n    # Case 5\n    y = y - 2\n    initial_b = 1.0\n    cost = target(x, y, initial_w, initial_b)\n    assert  np.isclose(cost, 4.525), f\"Case 5: Cost must be 4.525 but got {cost}\"\n    \n    print(\"\\033[92mAll tests passed!\")\n</pre> def compute_cost_test(target):     # print(\"Using X with shape (4, 1)\")     # Case 1     x = np.array([2, 4, 6, 8]).T     y = np.array([7, 11, 15, 19]).T     initial_w = 2     initial_b = 3.0     cost = target(x, y, initial_w, initial_b)     assert cost == 0, f\"Case 1: Cost must be 0 for a perfect prediction but got {cost}\"          # Case 2     x = np.array([2, 4, 6, 8]).T     y = np.array([7, 11, 15, 19]).T     initial_w = 2.0     initial_b = 1.0     cost = target(x, y, initial_w, initial_b)     assert cost == 2, f\"Case 2: Cost must be 2 but got {cost}\"          # print(\"Using X with shape (5, 1)\")     # Case 3     x = np.array([1.5, 2.5, 3.5, 4.5, 1.5]).T     y = np.array([4, 7, 10, 13, 5]).T     initial_w = 1     initial_b = 0.0     cost = target(x, y, initial_w, initial_b)     assert np.isclose(cost, 15.325), f\"Case 3: Cost must be 15.325 for a perfect prediction but got {cost}\"          # Case 4     initial_b = 1.0     cost = target(x, y, initial_w, initial_b)     assert np.isclose(cost, 10.725), f\"Case 4: Cost must be 10.725 but got {cost}\"          # Case 5     y = y - 2     initial_b = 1.0     cost = target(x, y, initial_w, initial_b)     assert  np.isclose(cost, 4.525), f\"Case 5: Cost must be 4.525 but got {cost}\"          print(\"\\033[92mAll tests passed!\") In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_test(target):\n    print(\"Using X with shape (4, 1)\")\n    # Case 1\n    x = np.array([2, 4, 6, 8]).T\n    y = np.array([4.5, 8.5, 12.5, 16.5]).T\n    initial_w = 2.\n    initial_b = 0.5\n    dj_dw, dj_db = target(x, y, initial_w, initial_b)\n    #assert dj_dw.shape == initial_w.shape, f\"Wrong shape for dj_dw. {dj_dw} != {initial_w.shape}\"\n    assert dj_db == 0.0, f\"Case 1: dj_db is wrong: {dj_db} != 0.0\"\n    assert np.allclose(dj_dw, 0), f\"Case 1: dj_dw is wrong: {dj_dw} != [[0.0]]\"\n    \n    # Case 2 \n    x = np.array([2, 4, 6, 8]).T\n    y = np.array([4, 7, 10, 13]).T + 2\n    initial_w = 1.5\n    initial_b = 1\n    dj_dw, dj_db = target(x, y, initial_w, initial_b)\n    #assert dj_dw.shape == initial_w.shape, f\"Wrong shape for dj_dw. {dj_dw} != {initial_w.shape}\"\n    assert dj_db == -2, f\"Case 1: dj_db is wrong: {dj_db} != -2\"\n    assert np.allclose(dj_dw, -10.0), f\"Case 1: dj_dw is wrong: {dj_dw} != -10.0\"   \n    \n    print(\"\\033[92mAll tests passed!\")\n</pre> def compute_gradient_test(target):     print(\"Using X with shape (4, 1)\")     # Case 1     x = np.array([2, 4, 6, 8]).T     y = np.array([4.5, 8.5, 12.5, 16.5]).T     initial_w = 2.     initial_b = 0.5     dj_dw, dj_db = target(x, y, initial_w, initial_b)     #assert dj_dw.shape == initial_w.shape, f\"Wrong shape for dj_dw. {dj_dw} != {initial_w.shape}\"     assert dj_db == 0.0, f\"Case 1: dj_db is wrong: {dj_db} != 0.0\"     assert np.allclose(dj_dw, 0), f\"Case 1: dj_dw is wrong: {dj_dw} != [[0.0]]\"          # Case 2      x = np.array([2, 4, 6, 8]).T     y = np.array([4, 7, 10, 13]).T + 2     initial_w = 1.5     initial_b = 1     dj_dw, dj_db = target(x, y, initial_w, initial_b)     #assert dj_dw.shape == initial_w.shape, f\"Wrong shape for dj_dw. {dj_dw} != {initial_w.shape}\"     assert dj_db == -2, f\"Case 1: dj_db is wrong: {dj_db} != -2\"     assert np.allclose(dj_dw, -10.0), f\"Case 1: dj_dw is wrong: {dj_dw} != -10.0\"             print(\"\\033[92mAll tests passed!\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part2/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def load_data():\n    data = np.loadtxt(\"data/ex1data1.txt\", delimiter=',')\n    X = data[:,0]\n    y = data[:,1]\n    return X, y\n</pre> def load_data():     data = np.loadtxt(\"data/ex1data1.txt\", delimiter=',')     X = data[:,0]     y = data[:,1]     return X, y In\u00a0[\u00a0]: Copied! <pre>def load_data_multi():\n    data = np.loadtxt(\"data/ex1data2.txt\", delimiter=',')\n    X = data[:,:2]\n    y = data[:,2]\n    return X, y\n</pre> def load_data_multi():     data = np.loadtxt(\"data/ex1data2.txt\", delimiter=',')     X = data[:,:2]     y = data[:,2]     return X, y"},{"location":"MachineLearning/part3/Classification_Soln/","title":"Optional Lab: Classification","text":"In\u00a0[6]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import dlc, plot_data\nfrom plt_one_addpt_onclick import plt_one_addpt_onclick\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import dlc, plot_data from plt_one_addpt_onclick import plt_one_addpt_onclick plt.style.use('./deeplearning.mplstyle') In\u00a0[7]: Copied! <pre>x_train = np.array([0., 1, 2, 3, 4, 5])\ny_train = np.array([0,  0, 0, 1, 1, 1])\nX_train2 = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train2 = np.array([0, 0, 0, 1, 1, 1])\n</pre> x_train = np.array([0., 1, 2, 3, 4, 5]) y_train = np.array([0,  0, 0, 1, 1, 1]) X_train2 = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]) y_train2 = np.array([0, 0, 0, 1, 1, 1]) In\u00a0[8]: Copied! <pre># \u521b\u5efa\u5e03\u5c14\u63a9\u7801\npos = y_train == 1  # \u627e\u51fa\u6240\u6709\u6807\u7b7e\u4e3a1\u7684\u4f4d\u7f6e\nneg = y_train == 0  # \u627e\u51fa\u6240\u6709\u6807\u7b7e\u4e3a0\u7684\u4f4d\u7f6e\n\nfig,ax = plt.subplots(1,2,figsize=(8,3))\n#plot 1, single variable\nax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\nax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n              edgecolors=dlc[\"dlblue\"],lw=3)\n\nax[0].set_ylim(-0.08,1.1)\nax[0].set_ylabel('y', fontsize=12)\nax[0].set_xlabel('x', fontsize=12)\nax[0].set_title('one variable plot')\nax[0].legend()\n\n#plot 2, two variables\nplot_data(X_train2, y_train2, ax[1])\nax[1].axis([0, 4, 0, 4])\nax[1].set_ylabel('$x_1$', fontsize=12)\nax[1].set_xlabel('$x_0$', fontsize=12)\nax[1].set_title('two variable plot')\nax[1].legend()\nplt.tight_layout()\nplt.show()\n</pre> # \u521b\u5efa\u5e03\u5c14\u63a9\u7801 pos = y_train == 1  # \u627e\u51fa\u6240\u6709\u6807\u7b7e\u4e3a1\u7684\u4f4d\u7f6e neg = y_train == 0  # \u627e\u51fa\u6240\u6709\u6807\u7b7e\u4e3a0\u7684\u4f4d\u7f6e  fig,ax = plt.subplots(1,2,figsize=(8,3)) #plot 1, single variable ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\") ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',                edgecolors=dlc[\"dlblue\"],lw=3)  ax[0].set_ylim(-0.08,1.1) ax[0].set_ylabel('y', fontsize=12) ax[0].set_xlabel('x', fontsize=12) ax[0].set_title('one variable plot') ax[0].legend()  #plot 2, two variables plot_data(X_train2, y_train2, ax[1]) ax[1].axis([0, 4, 0, 4]) ax[1].set_ylabel('$x_1$', fontsize=12) ax[1].set_xlabel('$x_0$', fontsize=12) ax[1].set_title('two variable plot') ax[1].legend() plt.tight_layout() plt.show()                       Figure                  <p>Note in the plots above:</p> <ul> <li>In the single variable plot, positive results are shown both a red 'X's and as y=1. Negative results are blue 'O's and are located at y=0.<ul> <li>Recall in the case of linear regression, y would not have been limited to two values but could have been any value.</li> </ul> </li> <li>In the two-variable plot, the y axis is not available.  Positive results are shown as red 'X's, while negative results use the blue 'O' symbol.<ul> <li>Recall in the case of linear regression with multiple variables, y would not have been limited to two values and a similar plot would have been three-dimensional.</li> </ul> </li> </ul> In\u00a0[10]: Copied! <pre>w_in = np.zeros((1))\nb_in = 0\nplt.close('all') \naddpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=False)\n</pre> w_in = np.zeros((1)) b_in = 0 plt.close('all')  addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=False) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[10], line 4\n      2 b_in = 0\n      3 plt.close('all') \n----&gt; 4 addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=False)\n\nFile D:\\AI\\Machine-Learning-Specialization-Coursera\\C1 - Supervised Machine Learning - Regression and Classification\\week3\\Optional Labs\\plt_one_addpt_onclick.py:66, in plt_one_addpt_onclick.__init__(self, x, y, w, b, logistic)\n     64 self.bthresh = CheckButtons(axthresh, ('Toggle 0.5 threshold (after regression)',))\n     65 self.bthresh.on_clicked(self.thresh)\n---&gt; 66 self.resize_sq(self.bthresh)\n\nFile D:\\AI\\Machine-Learning-Specialization-Coursera\\C1 - Supervised Machine Learning - Regression and Classification\\week3\\Optional Labs\\plt_one_addpt_onclick.py:179, in plt_one_addpt_onclick.resize_sq(self, bcid)\n    171 \"\"\" resizes the check box \"\"\"\n    172 #future reference\n    173 #print(f\"width  : {bcid.rectangles[0].get_width()}\")\n    174 #print(f\"height : {bcid.rectangles[0].get_height()}\")\n    175 #print(f\"xy     : {bcid.rectangles[0].get_xy()}\")\n    176 #print(f\"bb     : {bcid.rectangles[0].get_bbox()}\")\n    177 #print(f\"points : {bcid.rectangles[0].get_bbox().get_points()}\")  #[[xmin,ymin],[xmax,ymax]]\n--&gt; 179 h = bcid.rectangles[0].get_height()\n    180 bcid.rectangles[0].set_height(3*h)\n    182 ymax = bcid.rectangles[0].get_bbox().y1\n\nAttributeError: 'CheckButtons' object has no attribute 'rectangles'</pre>                      Figure                  <p>The example above demonstrates that the linear model is insufficient to model categorical data. The model can be extended as described in the following lab.</p> In\u00a0[4]: Copied! <pre>\n</pre> In\u00a0[4]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Classification_Soln/#optional-lab-classification","title":"Optional Lab: Classification\u00b6","text":"<p>In this lab, you will contrast regression and classification.</p>"},{"location":"MachineLearning/part3/Classification_Soln/#classification-problems","title":"Classification Problems\u00b6","text":"<p> Examples of classification problems are things like: identifying email as Spam or Not Spam or determining if a tumor is malignant or benign. In particular, these are examples of binary classification where there are two possible outcomes.  Outcomes can be  described in pairs of 'positive'/'negative' such as 'yes'/'no, 'true'/'false' or '1'/'0'.</p> <p>Plots of classification data sets often use symbols to indicate the outcome of an example. In the plots below, 'X' is used to represent the positive values while 'O' represents negative outcomes.</p>"},{"location":"MachineLearning/part3/Classification_Soln/#linear-regression-approach","title":"Linear Regression approach\u00b6","text":"<p>In the previous week, you applied linear regression to build a prediction model. Let's try that approach here using the simple example that was described in the lecture. The model will predict if a tumor is benign or malignant based on tumor size.  Try the following:</p> <ul> <li>Click on 'Run Linear Regression' to find the best linear regression model for the given data.<ul> <li>Note the resulting linear model does not match the data well. One option to improve the results is to apply a threshold.</li> </ul> </li> <li>Tick the box on the 'Toggle 0.5 threshold' to show the predictions if a threshold is applied.<ul> <li>These predictions look good, the predictions match the data</li> </ul> </li> <li>Important: Now, add further 'malignant' data points on the far right, in the large tumor size range (near 10), and re-run linear regression.<ul> <li>Now, the model predicts the larger tumor, but data point at x=3 is being incorrectly predicted!</li> </ul> </li> <li>to clear/renew the plot, rerun the cell containing the plot command.</li> </ul>"},{"location":"MachineLearning/part3/Classification_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you:</p> <ul> <li>explored categorical data sets and plotting</li> <li>determined that linear regression was insufficient for a classification problem.</li> </ul>"},{"location":"MachineLearning/part3/Cost_Function_Soln/","title":"Optional Lab: Cost Function for Logistic Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import  plot_data, sigmoid, dlc\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import  plot_data, sigmoid, dlc plt.style.use('./deeplearning.mplstyle') In\u00a0[2]: Copied! <pre>X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\ny_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)\n</pre> X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n) y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,) <p>We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p> In\u00a0[3]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X_train, y_train, ax)\n\n# Set both axes to be from 0-4\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,4)) plot_data(X_train, y_train, ax)  # Set both axes to be from 0-4 ax.axis([0, 4, 0, 3.5]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.show()                      Figure                  <p></p> In\u00a0[4]: Copied! <pre>def compute_cost_logistic(X, y, w, b):\n    \"\"\"\n    Computes cost\n\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):\n        z_i = np.dot(X[i],w) + b\n        f_wb_i = sigmoid(z_i)\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n             \n    cost = cost / m\n    return cost\n</pre> def compute_cost_logistic(X, y, w, b):     \"\"\"     Computes cost      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter            Returns:       cost (scalar): cost     \"\"\"      m = X.shape[0]     cost = 0.0     for i in range(m):         z_i = np.dot(X[i],w) + b         f_wb_i = sigmoid(z_i)         cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)                   cost = cost / m     return cost  <p>Check the implementation of the cost function using the cell below.</p> In\u00a0[5]: Copied! <pre>w_tmp = np.array([1,1])\nb_tmp = -3\nprint(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))\n</pre> w_tmp = np.array([1,1]) b_tmp = -3 print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp)) <pre>0.36686678640551745\n</pre> <p>Expected output: 0.3668667864055175</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Choose values between 0 and 6\nx0 = np.arange(0,6)\n\n# Plot the two decision boundaries\nx1 = 3 - x0\nx1_other = 4 - x0\n\nfig,ax = plt.subplots(1, 1, figsize=(4,4))\n# Plot the decision boundary\nax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\nax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\nax.axis([0, 4, 0, 4])\n\n# Plot the original data\nplot_data(X_train,y_train,ax)\nax.axis([0, 4, 0, 4])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.legend(loc=\"upper right\")\nplt.title(\"Decision Boundary\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Choose values between 0 and 6 x0 = np.arange(0,6)  # Plot the two decision boundaries x1 = 3 - x0 x1_other = 4 - x0  fig,ax = plt.subplots(1, 1, figsize=(4,4)) # Plot the decision boundary ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\") ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\") ax.axis([0, 4, 0, 4])  # Plot the original data plot_data(X_train,y_train,ax) ax.axis([0, 4, 0, 4]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.legend(loc=\"upper right\") plt.title(\"Decision Boundary\") plt.show()                      Figure                  <p>You can see from this plot that <code>b = -4, w = np.array([1,1])</code> is a worse model for the training data. Let's see if the cost function implementation reflects this.</p> In\u00a0[7]: Copied! <pre>w_array1 = np.array([1,1])\nb_1 = -3\nw_array2 = np.array([1,1])\nb_2 = -4\n\nprint(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\nprint(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))\n</pre> w_array1 = np.array([1,1]) b_1 = -3 w_array2 = np.array([1,1]) b_2 = -4  print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1)) print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2)) <pre>Cost for b = -3 :  0.36686678640551745\nCost for b = -4 :  0.5036808636748461\n</pre> <p>Expected output</p> <p>Cost for b = -3 :  0.3668667864055175</p> <p>Cost for b = -4 :  0.5036808636748461</p> <p>You can see the cost function behaves as expected and the cost for <code>b = -4, w = np.array([1,1])</code> is indeed higher than the cost for <code>b = -3, w = np.array([1,1])</code></p> In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#optional-lab-cost-function-for-logistic-regression","title":"Optional Lab: Cost Function for Logistic Regression\u00b6","text":""},{"location":"MachineLearning/part3/Cost_Function_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>examine the implementation and utilize the cost function for logistic regression.</li> </ul>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#dataset","title":"Dataset\u00b6","text":"<p>Let's start with the same dataset as was used in the decision boundary lab.</p>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#cost-function","title":"Cost function\u00b6","text":"<p>In a previous lab, you developed the logistic loss function. Recall, loss is defined to apply to one example. Here you combine the losses to form the cost, which includes all the examples.</p> <p>Recall that for logistic regression, the cost function is of the form</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$</p> <p>where</p> <ul> <li><p>$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:</p> <p>$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$</p> </li> <li><p>where m is the number of training examples in the data set and: $$ \\begin{align}   f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &amp;= g(z^{(i)})\\tag{3} \\\\   z^{(i)} &amp;= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\   g(z^{(i)}) &amp;= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5}  \\end{align} $$</p> </li> </ul>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#code-description","title":"Code Description\u00b6","text":"<p>The algorithm for <code>compute_cost_logistic</code> loops over all the examples calculating the loss for each example and accumulating the total.</p> <p>Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($\ud835\udc5a$,) respectively, where  $\ud835\udc5b$ is the number of features and $\ud835\udc5a$ is the number of training examples.</p>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#example","title":"Example\u00b6","text":"<p>Now, let's see what the cost function output is for a different value of $w$.</p> <ul> <li><p>In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had <code>b = -3, w = np.array([1,1])</code>.</p> </li> <li><p>Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or <code>b = -4, w = np.array([1,1])</code> provides a better model.</p> </li> </ul> <p>Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.</p> <ul> <li>For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)</li> <li>For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)</li> </ul>"},{"location":"MachineLearning/part3/Cost_Function_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you examined and utilized the cost function for logistic regression.</p>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/","title":"Optional Lab: Logistic Regression, Decision Boundary","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import plot_data, sigmoid, draw_vthresh\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import plot_data, sigmoid, draw_vthresh plt.style.use('./deeplearning.mplstyle') In\u00a0[2]: Copied! <pre>X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1) \n</pre> X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]) y = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1)  In\u00a0[3]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X, y, ax)\n\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$')\nax.set_xlabel('$x_0$')\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,4)) plot_data(X, y, ax)  ax.axis([0, 4, 0, 3.5]) ax.set_ylabel('$x_1$') ax.set_xlabel('$x_0$') plt.show()                      Figure                  In\u00a0[4]: Copied! <pre># Plot sigmoid(z) over a range of values from -10 to 10\nz = np.arange(-10,11)\n\nfig,ax = plt.subplots(1,1,figsize=(5,3))\n# Plot z vs sigmoid(z)\nax.plot(z, sigmoid(z), c=\"b\")\n\nax.set_title(\"Sigmoid function\")\nax.set_ylabel('sigmoid(z)')\nax.set_xlabel('z')\ndraw_vthresh(ax,0)\n</pre> # Plot sigmoid(z) over a range of values from -10 to 10 z = np.arange(-10,11)  fig,ax = plt.subplots(1,1,figsize=(5,3)) # Plot z vs sigmoid(z) ax.plot(z, sigmoid(z), c=\"b\")  ax.set_title(\"Sigmoid function\") ax.set_ylabel('sigmoid(z)') ax.set_xlabel('z') draw_vthresh(ax,0)                      Figure                  <ul> <li><p>As you can see, $g(z) &gt;= 0.5$ for $z &gt;=0$</p> </li> <li><p>For a logistic regression model, $z = \\mathbf{w} \\cdot \\mathbf{x} + b$. Therefore,</p> <p>if $\\mathbf{w} \\cdot \\mathbf{x} + b &gt;= 0$, the model predicts $y=1$</p> <p>if $\\mathbf{w} \\cdot \\mathbf{x} + b &lt; 0$, the model predicts $y=0$</p> </li> </ul> In\u00a0[5]: Copied! <pre># Choose values between 0 and 6\nx0 = np.arange(0,6)\n\nx1 = 3 - x0\nfig,ax = plt.subplots(1,1,figsize=(5,4))\n# Plot the decision boundary\nax.plot(x0,x1, c=\"b\")\nax.axis([0, 4, 0, 3.5])\n\n# Fill the region below the line\nax.fill_between(x0,x1, alpha=0.2)\n\n# Plot the original data\nplot_data(X,y,ax)\nax.set_ylabel(r'$x_1$')\nax.set_xlabel(r'$x_0$')\nplt.show()\n</pre> # Choose values between 0 and 6 x0 = np.arange(0,6)  x1 = 3 - x0 fig,ax = plt.subplots(1,1,figsize=(5,4)) # Plot the decision boundary ax.plot(x0,x1, c=\"b\") ax.axis([0, 4, 0, 3.5])  # Fill the region below the line ax.fill_between(x0,x1, alpha=0.2)  # Plot the original data plot_data(X,y,ax) ax.set_ylabel(r'$x_1$') ax.set_xlabel(r'$x_0$') plt.show()                      Figure                  <ul> <li><p>In the plot above, the blue line represents the line $x_0 + x_1 - 3 = 0$ and it should intersect the x1 axis at 3 (if we set $x_1$ = 3, $x_0$ = 0) and the x0 axis at 3 (if we set $x_1$ = 0, $x_0$ = 3).</p> </li> <li><p>The shaded region represents $-3 + x_0+x_1 &lt; 0$. The region above the line is $-3 + x_0+x_1 &gt; 0$.</p> </li> <li><p>Any point in the shaded region (under the line) is classified as $y=0$.  Any point on or above the line is classified as $y=1$. This line is known as the \"decision boundary\".</p> </li> </ul> <p>As we've seen in the lectures, by using higher order polynomial terms (eg: $f(x) = g( x_0^2 + x_1 -1)$, we can come up with more complex non-linear boundaries.</p> In\u00a0[5]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#optional-lab-logistic-regression-decision-boundary","title":"Optional Lab: Logistic Regression, Decision Boundary\u00b6","text":""},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>Plot the decision boundary for a logistic regression model. This will give you a better sense of what the model is predicting.</li> </ul>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#dataset","title":"Dataset\u00b6","text":"<p>Let's suppose you have following training dataset</p> <ul> <li>The input variable <code>X</code> is a numpy array which has 6 training examples, each with two features</li> <li>The output variable <code>y</code> is also a numpy array with 6 examples, and <code>y</code> is either <code>0</code> or <code>1</code></li> </ul>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#plot-data","title":"Plot data\u00b6","text":"<p>Let's use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#logistic-regression-model","title":"Logistic regression model\u00b6","text":"<ul> <li><p>Suppose you'd like to train a logistic regression model on this data which has the form</p> <p>$f(x) = g(w_0x_0+w_1x_1 + b)$</p> <p>where $g(z) = \\frac{1}{1+e^{-z}}$, which is the sigmoid function</p> </li> <li><p>Let's say that you trained the model and get the parameters as $b = -3, w_0 = 1, w_1 = 1$. That is,</p> <p>$f(x) = g(x_0+x_1-3)$</p> <p>(You'll learn how to fit these parameters to the data further in the course)</p> </li> </ul> <p>Let's try to understand what this trained model is predicting by plotting its decision boundary</p>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#refresher-on-logistic-regression-and-decision-boundary","title":"Refresher on logistic regression and decision boundary\u00b6","text":"<ul> <li><p>Recall that for logistic regression, the model is represented as</p> <p>$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\tag{1}$$</p> <p>where $g(z)$ is known as the sigmoid function and it maps all input values to values between 0 and 1:</p> <p>$g(z) = \\frac{1}{1+e^{-z}}\\tag{2}$ and $\\mathbf{w} \\cdot \\mathbf{x}$ is the vector dot product:</p> <p>$$\\mathbf{w} \\cdot \\mathbf{x} = w_0 x_0 + w_1 x_1$$</p> </li> <li><p>We interpret the output of the model ($f_{\\mathbf{w},b}(x)$) as the probability that $y=1$ given $\\mathbf{x}$ and parameterized by $\\mathbf{w}$ and $b$.</p> </li> <li><p>Therefore, to get a final prediction ($y=0$ or $y=1$) from the logistic regression model, we can use the following heuristic -</p> <p>if $f_{\\mathbf{w},b}(x) &gt;= 0.5$, predict $y=1$</p> <p>if $f_{\\mathbf{w},b}(x) &lt; 0.5$, predict $y=0$</p> </li> <li><p>Let's plot the sigmoid function to see where $g(z) &gt;= 0.5$</p> </li> </ul>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#plotting-decision-boundary","title":"Plotting decision boundary\u00b6","text":"<p>Now, let's go back to our example to understand how the logistic regression model is making predictions.</p> <ul> <li><p>Our logistic regression model has the form</p> <p>$f(\\mathbf{x}) = g(-3 + x_0+x_1)$</p> </li> <li><p>From what you've learnt above, you can see that this model predicts $y=1$ if $-3 + x_0+x_1 &gt;= 0$</p> </li> </ul> <p>Let's see what this looks like graphically. We'll start by plotting $-3 + x_0+x_1 = 0$, which is equivalent to $x_1 = 3 - x_0$.</p>"},{"location":"MachineLearning/part3/Decision_Boundary_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have explored the decision boundary in the context of logistic regression.</p>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/","title":"Optional Lab: Gradient Descent for Logistic Regression","text":"In\u00a0[1]: Copied! <pre>import copy, math\nimport numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\nfrom plt_quad_logistic import plt_quad_logistic, plt_prob\nplt.style.use('./deeplearning.mplstyle')\n</pre> import copy, math import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic from plt_quad_logistic import plt_quad_logistic, plt_prob plt.style.use('./deeplearning.mplstyle') In\u00a0[2]: Copied! <pre>X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])\n</pre> X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]) y_train = np.array([0, 0, 0, 1, 1, 1]) <p>As before, we'll use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p> In\u00a0[3]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X_train, y_train, ax)\n\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,4)) plot_data(X_train, y_train, ax)  ax.axis([0, 4, 0, 3.5]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.show()                      Figure                  In\u00a0[4]: Copied! <pre>def compute_gradient_logistic(X, y, w, b): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n    Returns\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                           #(n,)\n    dj_db = 0.\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n        \n    return dj_db, dj_dw\n</pre> def compute_gradient_logistic(X, y, w, b):      \"\"\"     Computes the gradient for linear regression        Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter     Returns       dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape     dj_dw = np.zeros((n,))                           #(n,)     dj_db = 0.      for i in range(m):         f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar         err_i  = f_wb_i  - y[i]                       #scalar         for j in range(n):             dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar         dj_db = dj_db + err_i     dj_dw = dj_dw/m                                   #(n,)     dj_db = dj_db/m                                   #scalar              return dj_db, dj_dw In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Check the implementation of the gradient function using the cell below.</p> In\u00a0[5]: Copied! <pre>X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_tmp = np.array([0, 0, 0, 1, 1, 1])\nw_tmp = np.array([2.,3.])\nb_tmp = 1.\ndj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\nprint(f\"dj_db: {dj_db_tmp}\" )\nprint(f\"dj_dw: {dj_dw_tmp.tolist()}\" )\n</pre> X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]) y_tmp = np.array([0, 0, 0, 1, 1, 1]) w_tmp = np.array([2.,3.]) b_tmp = 1. dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp) print(f\"dj_db: {dj_db_tmp}\" ) print(f\"dj_dw: {dj_dw_tmp.tolist()}\" ) <pre>dj_db: 0.49861806546328574\ndj_dw: [0.498333393278696, 0.49883942983996693]\n</pre> <p>Expected output</p> <pre><code>dj_db: 0.49861806546328574\ndj_dw: [0.498333393278696, 0.49883942983996693]\n</code></pre> In\u00a0[6]: Copied! <pre>def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent\n    \n    Args:\n      X (ndarray (m,n)   : Data, m examples with n features\n      y (ndarray (m,))   : target values\n      w_in (ndarray (n,)): Initial values of model parameters  \n      b_in (scalar)      : Initial values of model parameter\n      alpha (float)      : Learning rate\n      num_iters (scalar) : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,))   : Updated values of parameters\n      b (scalar)         : Updated value of parameter \n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion \n            J_history.append( compute_cost_logistic(X, y, w, b) )\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n        \n    return w, b, J_history         #return final w,b and J history for graphing\n</pre> def gradient_descent(X, y, w_in, b_in, alpha, num_iters):      \"\"\"     Performs batch gradient descent          Args:       X (ndarray (m,n)   : Data, m examples with n features       y (ndarray (m,))   : target values       w_in (ndarray (n,)): Initial values of model parameters         b_in (scalar)      : Initial values of model parameter       alpha (float)      : Learning rate       num_iters (scalar) : number of iterations to run gradient descent            Returns:       w (ndarray (n,))   : Updated values of parameters       b (scalar)         : Updated value of parameter      \"\"\"     # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in          for i in range(num_iters):         # Calculate the gradient and update the parameters         dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)             # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw                        b = b - alpha * dj_db                               # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion              J_history.append( compute_cost_logistic(X, y, w, b) )          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters / 10) == 0:             print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")              return w, b, J_history         #return final w,b and J history for graphing  <p>Let's run gradient descent on our data set.</p> In\u00a0[7]: Copied! <pre>w_tmp  = np.zeros_like(X_train[0])\nb_tmp  = 0.\nalph = 0.1\niters = 10000\n\nw_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \nprint(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")\n</pre> w_tmp  = np.zeros_like(X_train[0]) b_tmp  = 0. alph = 0.1 iters = 10000  w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters)  print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\") <pre>Iteration    0: Cost 0.684610468560574   \nIteration 1000: Cost 0.1590977666870457   \nIteration 2000: Cost 0.08460064176930078   \nIteration 3000: Cost 0.05705327279402531   \nIteration 4000: Cost 0.04290759421682   \nIteration 5000: Cost 0.03433847729884557   \nIteration 6000: Cost 0.02860379802212006   \nIteration 7000: Cost 0.02450156960879306   \nIteration 8000: Cost 0.02142370332569295   \nIteration 9000: Cost 0.019030137124109114   \n\nupdated parameters: w:[5.28 5.08], b:-14.222409982019837\n</pre> In\u00a0[8]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(5,4))\n# plot the probability \nplt_prob(ax, w_out, b_out)\n\n# Plot the original data\nax.set_ylabel(r'$x_1$')\nax.set_xlabel(r'$x_0$')   \nax.axis([0, 4, 0, 3.5])\nplot_data(X_train,y_train,ax)\n\n# Plot the decision boundary\nx0 = -b_out/w_out[0]\nx1 = -b_out/w_out[1]\nax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(5,4)) # plot the probability  plt_prob(ax, w_out, b_out)  # Plot the original data ax.set_ylabel(r'$x_1$') ax.set_xlabel(r'$x_0$')    ax.axis([0, 4, 0, 3.5]) plot_data(X_train,y_train,ax)  # Plot the decision boundary x0 = -b_out/w_out[0] x1 = -b_out/w_out[1] ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1) plt.show()                      Figure                  <p>In the plot above:</p> <ul> <li>the shading reflects the probability y=1 (result prior to decision boundary)</li> <li>the decision boundary is the line at which the probability = 0.5</li> </ul> In\u00a0[9]: Copied! <pre>x_train = np.array([0., 1, 2, 3, 4, 5])\ny_train = np.array([0,  0, 0, 1, 1, 1])\n</pre> x_train = np.array([0., 1, 2, 3, 4, 5]) y_train = np.array([0,  0, 0, 1, 1, 1]) <p>As before, we'll use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p> In\u00a0[10]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,3))\nplt_tumor_data(x_train, y_train, ax)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,3)) plt_tumor_data(x_train, y_train, ax) plt.show()                      Figure                  <p>In the plot below, try:</p> <ul> <li>changing $w$ and $b$ by clicking within the contour plot on the upper right.<ul> <li>changes may take a second or two</li> <li>note the changing value of cost on the upper left plot.</li> <li>note the cost is accumulated by a loss on each example (vertical dotted lines)</li> </ul> </li> <li>run gradient descent by clicking the orange button.<ul> <li>note the steadily decreasing cost (contour and cost plot are in log(cost)</li> <li>clicking in the contour plot will reset the model for a new run</li> </ul> </li> <li>to reset the plot, rerun the cell</li> </ul> In\u00a0[11]: Copied! <pre>w_range = np.array([-1, 7])\nb_range = np.array([1, -14])\nquad = plt_quad_logistic( x_train, y_train, w_range, b_range )\n</pre> w_range = np.array([-1, 7]) b_range = np.array([1, -14]) quad = plt_quad_logistic( x_train, y_train, w_range, b_range )                      Figure                  In\u00a0[11]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#optional-lab-gradient-descent-for-logistic-regression","title":"Optional Lab: Gradient Descent for Logistic Regression\u00b6","text":""},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>update gradient descent for logistic regression.</li> <li>explore gradient descent on a familiar data set</li> </ul>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#data-set","title":"Data set\u00b6","text":"<p>Let's start with the same two feature data set used in the decision boundary lab.</p>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#logistic-gradient-descent","title":"Logistic Gradient Descent\u00b6","text":"<p>Recall the gradient descent algorithm utilizes the gradient calculation: $$\\begin{align*} &amp;\\text{repeat until convergence:} \\; \\lbrace \\\\ &amp;  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; &amp; \\text{for j := 0..n-1} \\\\  &amp;  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\ &amp;\\rbrace \\end{align*}$$</p> <p>Where each iteration performs simultaneous updates on $w_j$ for all $j$, where $$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}  \\end{align*}$$</p> <ul> <li>m is the number of training examples in the data set</li> <li>$f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target</li> <li>For a logistic regression model $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ $f_{\\mathbf{w},b}(x) = g(z)$ where $g(z)$ is the sigmoid function: $g(z) = \\frac{1}{1+e^{-z}}$</li> </ul>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#gradient-descent-implementation","title":"Gradient Descent Implementation\u00b6","text":"<p>The gradient descent algorithm implementation has two components:</p> <ul> <li>The loop implementing equation (1) above. This is <code>gradient_descent</code> below and is generally provided to you in optional and practice labs.</li> <li>The calculation of the current gradient, equations (2,3) above. This is <code>compute_gradient_logistic</code> below. You will be asked to implement this week's practice lab.</li> </ul>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#calculating-the-gradient-code-description","title":"Calculating the Gradient, Code Description\u00b6","text":"<p>Implements equation (2),(3) above for all $w_j$ and $b$. There are many ways to implement this. Outlined below is this:</p> <ul> <li><p>initialize variables to accumulate <code>dj_dw</code> and <code>dj_db</code></p> </li> <li><p>for each example</p> <ul> <li>calculate the error for that example $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$</li> <li>for each input value $x_{j}^{(i)}$ in this example,<ul> <li>multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of <code>dj_dw</code>. (equation 2 above)</li> </ul> </li> <li>add the error to <code>dj_db</code> (equation 3 above)</li> </ul> </li> <li><p>divide <code>dj_db</code> and <code>dj_dw</code> by total number of examples (m)</p> </li> <li><p>note that $\\mathbf{x}^{(i)}$ in numpy <code>X[i,:]</code> or <code>X[i]</code>  and $x_{j}^{(i)}$ is <code>X[i,j]</code></p> </li> </ul>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#gradient-descent-code","title":"Gradient Descent Code\u00b6","text":"<p>The code implementing equation (1) above is implemented below. Take a moment to locate and compare the functions in the routine to the equations above.</p>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#lets-plot-the-results-of-gradient-descent","title":"Let's plot the results of gradient descent:\u00b6","text":""},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#another-data-set","title":"Another Data set\u00b6","text":"<p>Let's return to a one-variable data set. With just two parameters, $w$, $b$, it is possible to plot the cost function using a contour plot to get a better idea of what gradient descent is up to.</p>"},{"location":"MachineLearning/part3/Gradient_Descent_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have:</p> <ul> <li>examined the formulas and implementation of calculating the gradient for logistic regression</li> <li>utilized those routines in<ul> <li>exploring a single variable data set</li> <li>exploring a two-variable data set</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/LogisticLoss_Soln/","title":"Optional Lab: Logistic Regression, Logistic Loss","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\nfrom plt_logistic_loss import soup_bowl, plt_logistic_squared_error\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example from plt_logistic_loss import soup_bowl, plt_logistic_squared_error plt.style.use('./deeplearning.mplstyle') <p>Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum.</p> In\u00a0[2]: Copied! <pre>soup_bowl()\n</pre> soup_bowl()                      Figure                  <p>This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, $f_{wb}(x)$ now has a non-linear component, the sigmoid function:   $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$.   Let's try a squared error cost on the example from an earlier lab, now including the sigmoid.</p> <p>Here is our training data:</p> In\u00a0[3]: Copied! <pre>x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\ny_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\nplt_simple_example(x_train, y_train)\n</pre> x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble) y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble) plt_simple_example(x_train, y_train)                      Figure                  <p>Now, let's get a surface plot of the cost using a squared error cost: $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$</p> <p>where $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$</p> In\u00a0[4]: Copied! <pre>plt.close('all')\nplt_logistic_squared_error(x_train,y_train)\nplt.show()\n</pre> plt.close('all') plt_logistic_squared_error(x_train,y_train) plt.show()                      Figure                  <p>While this produces a pretty interesting plot, the surface above not nearly as smooth as the 'soup bowl' from linear regression!</p> <p>Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below.</p> <p>Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number.</p> <p>Definition Note:   In this course, these definitions are used: Loss is a measure of the difference of a single example to its target value while the Cost is a measure of the losses over the training set</p> <p>This is defined:</p> <ul> <li>$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:</li> </ul> <p>\\begin{equation}   loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}     - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) &amp; \\text{if $y^{(i)}=1$}\\\\     - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) &amp; \\text{if $y^{(i)}=0$}   \\end{cases} \\end{equation}</p> <ul> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.</p> </li> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.</p> </li> </ul> <p>The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:</p> In\u00a0[5]: Copied! <pre>plt_two_logistic_loss_curves()\n</pre> plt_two_logistic_loss_curves()                      Figure                  <p>Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is $f_{\\mathbf{w},b}$ which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1.</p> <p>The loss function above can be rewritten to be easier to implement. $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$</p> <p>This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces: when $ y^{(i)} = 0$, the left-hand term is eliminated: $$ \\begin{align} loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &amp;= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\ &amp;= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\end{align} $$ and when $ y^{(i)} = 1$, the right-hand term is eliminated: $$ \\begin{align}   loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &amp;=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\   &amp;=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\end{align} $$</p> <p>OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:</p> In\u00a0[6]: Copied! <pre>plt.close('all')\ncst = plt_logistic_cost(x_train,y_train)\n</pre> plt.close('all') cst = plt_logistic_cost(x_train,y_train)                      Figure                  <p>This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline. Reminder: you can rotate the above plots using your mouse.</p> In\u00a0[6]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/LogisticLoss_Soln/#optional-lab-logistic-regression-logistic-loss","title":"Optional Lab: Logistic Regression, Logistic Loss\u00b6","text":"<p>In this ungraded lab, you will:</p> <ul> <li>explore the reason the squared error loss is not appropriate for logistic regression</li> <li>explore the logistic loss function</li> </ul>"},{"location":"MachineLearning/part3/LogisticLoss_Soln/#squared-error-for-logistic-regression","title":"Squared error for logistic regression?\u00b6","text":"<p> Recall for Linear Regression we have used the squared error cost function: The equation for the squared error cost with one variable is: $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$</p> <p>where $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$</p>"},{"location":"MachineLearning/part3/LogisticLoss_Soln/#logistic-loss-function","title":"Logistic Loss Function\u00b6","text":""},{"location":"MachineLearning/part3/LogisticLoss_Soln/#congratulation","title":"Congratulation!\u00b6","text":"<p>You have:</p> <ul> <li>determined a squared error loss function is not suitable for classification tasks</li> <li>developed and examined the logistic loss function which is suitable for classification tasks.</li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/","title":"Logistic Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom utils import *\nimport copy\nimport math\n\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt from utils import * import copy import math  %matplotlib inline In\u00a0[2]: Copied! <pre># load dataset\nX_train, y_train = load_data(\"data/ex2data1.txt\")\n</pre> # load dataset X_train, y_train = load_data(\"data/ex2data1.txt\") In\u00a0[3]: Copied! <pre>print(\"First five elements in X_train are:\\n\", X_train[:5])\nprint(\"Type of X_train:\",type(X_train))\n</pre> print(\"First five elements in X_train are:\\n\", X_train[:5]) print(\"Type of X_train:\",type(X_train)) <pre>First five elements in X_train are:\n [[34.62365962 78.02469282]\n [30.28671077 43.89499752]\n [35.84740877 72.90219803]\n [60.18259939 86.3085521 ]\n [79.03273605 75.34437644]]\nType of X_train: &lt;class 'numpy.ndarray'&gt;\n</pre> <p>Now print the first five values of <code>y_train</code></p> In\u00a0[4]: Copied! <pre>print(\"First five elements in y_train are:\\n\", y_train[:5])\nprint(\"Type of y_train:\",type(y_train))\n</pre> print(\"First five elements in y_train are:\\n\", y_train[:5]) print(\"Type of y_train:\",type(y_train)) <pre>First five elements in y_train are:\n [0. 0. 0. 1. 1.]\nType of y_train: &lt;class 'numpy.ndarray'&gt;\n</pre> In\u00a0[5]: Copied! <pre>print ('The shape of X_train is: ' + str(X_train.shape))\nprint ('The shape of y_train is: ' + str(y_train.shape))\nprint ('We have m = %d training examples' % (len(y_train)))\n</pre> print ('The shape of X_train is: ' + str(X_train.shape)) print ('The shape of y_train is: ' + str(y_train.shape)) print ('We have m = %d training examples' % (len(y_train))) <pre>The shape of X_train is: (100, 2)\nThe shape of y_train is: (100,)\nWe have m = 100 training examples\n</pre> In\u00a0[6]: Copied! <pre># Plot examples\nplot_data(X_train, y_train[:], pos_label=\"Admitted\", neg_label=\"Not admitted\")\n\n# Set the y-axis label\nplt.ylabel('Exam 2 score') \n# Set the x-axis label\nplt.xlabel('Exam 1 score') \nplt.legend(loc=\"upper right\")\nplt.show()\n</pre> # Plot examples plot_data(X_train, y_train[:], pos_label=\"Admitted\", neg_label=\"Not admitted\")  # Set the y-axis label plt.ylabel('Exam 2 score')  # Set the x-axis label plt.xlabel('Exam 1 score')  plt.legend(loc=\"upper right\") plt.show() <p>Your goal is to build a logistic regression model to fit this data.</p> <ul> <li>With this model, you can then predict if a new student will be admitted based on their scores on the two exams.</li> </ul> <p></p> In\u00a0[11]: Copied! <pre># UNQ_C1\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Args:\n        z (ndarray): A scalar, numpy array of any size.\n\n    Returns:\n        g (ndarray): sigmoid(z), with the same shape as z\n         \n    \"\"\"\n          \n    ### START CODE HERE ### \n    g = 1/(1+np.exp(-z))\n    ### END SOLUTION ###  \n    \n    return g\n</pre> # UNQ_C1 # GRADED FUNCTION: sigmoid  def sigmoid(z):     \"\"\"     Compute the sigmoid of z      Args:         z (ndarray): A scalar, numpy array of any size.      Returns:         g (ndarray): sigmoid(z), with the same shape as z               \"\"\"                ### START CODE HERE ###      g = 1/(1+np.exp(-z))     ### END SOLUTION ###            return g Click for hints <pre><code>   </code></pre> <p><code>numpy</code> has a function called <code>np.exp()</code>, which offers a convinient way to calculate the exponential ( $e^{z}$) of all elements in the input array (<code>z</code>).</p>  Click for more hints <pre><code>    </code></pre> <p>- You can translate $e^{-z}$ into code as <code>np.exp(-z)</code></p> <ul> <li><p>You can translate $1/e^{-z}$ into code as <code>1/np.exp(-z)</code></p> <p>If you're still stuck, you can check the hints presented below to figure out how to calculate <code>g</code></p> Hint to calculate g <code>g = 1 / (1 + np.exp(-z))</code> </li> </ul> <p>When you are finished, try testing a few values by calling <code>sigmoid(x)</code> in the cell below.</p> <ul> <li>For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0.</li> <li>Evaluating <code>sigmoid(0)</code> should give you exactly 0.5.</li> </ul> In\u00a0[12]: Copied! <pre>print (\"sigmoid(0) = \" + str(sigmoid(0)))\n</pre> print (\"sigmoid(0) = \" + str(sigmoid(0))) <pre>sigmoid(0) = 0.5\n</pre> <p>Expected Output:</p> sigmoid(0)  0.5  <ul> <li>As mentioned before, your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.</li> </ul> In\u00a0[13]: Copied! <pre>print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n\n# UNIT TESTS\nfrom public_tests import *\nsigmoid_test(sigmoid)\n</pre> print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))  # UNIT TESTS from public_tests import * sigmoid_test(sigmoid) <pre>sigmoid([ -1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]\nAll tests passed!\n</pre> <p>Expected Output:</p> sigmoid([-1, 0, 1, 2]) [0.26894142        0.5           0.73105858        0.88079708] <p></p> In\u00a0[18]: Copied! <pre># UNQ_C2\n# GRADED FUNCTION: compute_cost\ndef compute_cost(X, y, w, b, lambda_= 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X : (ndarray Shape (m,n)) data, m examples by n features\n      y : (array_like Shape (m,)) target value \n      w : (array_like Shape (n,)) Values of parameters of the model      \n      b : scalar Values of bias parameter of the model\n      lambda_: unused placeholder\n    Returns:\n      total_cost: (scalar)         cost \n    \"\"\"\n\n    m, n = X.shape\n    \n    ### START CODE HERE ###\n    cost = 0\n    for i in range(m):\n        z = np.dot(X[i],w) + b\n        f_wb = sigmoid(z)\n        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n    total_cost = cost/m\n    \n    ### END CODE HERE ### \n\n    return total_cost\n</pre> # UNQ_C2 # GRADED FUNCTION: compute_cost def compute_cost(X, y, w, b, lambda_= 1):     \"\"\"     Computes the cost over all examples     Args:       X : (ndarray Shape (m,n)) data, m examples by n features       y : (array_like Shape (m,)) target value        w : (array_like Shape (n,)) Values of parameters of the model             b : scalar Values of bias parameter of the model       lambda_: unused placeholder     Returns:       total_cost: (scalar)         cost      \"\"\"      m, n = X.shape          ### START CODE HERE ###     cost = 0     for i in range(m):         z = np.dot(X[i],w) + b         f_wb = sigmoid(z)         cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)     total_cost = cost/m          ### END CODE HERE ###       return total_cost Click for hints <ul> <li><p>You can represent a summation operator eg: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ in code as follows: <code>python     h = 0    for i in range(m):        h = h + 2*i     </code></p> </li> <li><p>In this case, you can iterate over all the examples in <code>X</code> using a for loop and add the <code>loss</code> from each iteration to a variable (<code>loss_sum</code>) initialized outside the loop.</p> </li> <li><p>Then, you can return the <code>total_cost</code> as <code>loss_sum</code> divided by <code>m</code>.</p> </li> </ul> <pre><code>&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt; Click for more hints&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n    \n* Here's how you can structure the overall implementation for this function\n```python \ndef compute_cost(X, y, w, b, lambda_= 1):\n    m, n = X.shape\n\n    ### START CODE HERE ###\n    loss_sum = 0 \n    \n    # Loop over each training example\n    for i in range(m): \n        \n        # First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n        z_wb = 0 \n        # Loop over each feature\n        for j in range(n): \n            # Add the corresponding term to z_wb\n            z_wb_ij = # Your code here to calculate w[j] * X[i][j]\n            z_wb += z_wb_ij # equivalent to z_wb = z_wb + z_wb_ij\n        # Add the bias term to z_wb\n        z_wb += b # equivalent to z_wb = z_wb + b\n    \n        f_wb = # Your code here to calculate prediction f_wb for a training example\n        loss =  # Your code here to calculate loss for a training example\n        \n        loss_sum += loss # equivalent to loss_sum = loss_sum + loss\n    \n    total_cost = (1 / m) * loss_sum  \n    ### END CODE HERE ### \n    \n    return total_cost\n```\n\nIf you're still stuck, you can check the hints presented below to figure out how to calculate `z_wb_ij`, `f_wb` and `cost`.\n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate z_wb_ij&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n       &amp;emsp; &amp;emsp; &lt;code&gt;z_wb_ij = w[j]*X[i][j] &lt;/code&gt;\n&lt;/details&gt;\n    \n&lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate f_wb&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n       &amp;emsp; &amp;emsp; $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$ where $g$ is the sigmoid function. You can simply call the `sigmoid` function implemented above.\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate f&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n           &amp;emsp; &amp;emsp; You can compute f_wb as &lt;code&gt;f_wb = sigmoid(z_wb) &lt;/code&gt;\n       &lt;/details&gt;\n&lt;/details&gt;\n\n &lt;details&gt;\n      &lt;summary&gt;&lt;font size=\"2\" color=\"darkblue\"&gt;&lt;b&gt;Hint to calculate loss&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n      &amp;emsp; &amp;emsp; You can use the &lt;a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log.html\"&gt;np.log&lt;/a&gt; function to calculate the log\n      &lt;details&gt;\n          &lt;summary&gt;&lt;font size=\"2\" color=\"blue\"&gt;&lt;b&gt;&amp;emsp; &amp;emsp; More hints to calculate loss&lt;/b&gt;&lt;/font&gt;&lt;/summary&gt;\n          &amp;emsp; &amp;emsp; You can compute loss as &lt;code&gt;loss =  -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)&lt;/code&gt;\n      &lt;/details&gt;\n&lt;/details&gt;\n    \n&lt;/details&gt;</code></pre> <p>Run the cells below to check your implementation of the <code>compute_cost</code> function with two different initializations of the parameters $w$</p> In\u00a0[19]: Copied! <pre>m, n = X_train.shape\n\n# Compute and display cost with w initialized to zeroes\ninitial_w = np.zeros(n)\ninitial_b = 0.\ncost = compute_cost(X_train, y_train, initial_w, initial_b)\nprint('Cost at initial w (zeros): {:.3f}'.format(cost))\n</pre> m, n = X_train.shape  # Compute and display cost with w initialized to zeroes initial_w = np.zeros(n) initial_b = 0. cost = compute_cost(X_train, y_train, initial_w, initial_b) print('Cost at initial w (zeros): {:.3f}'.format(cost)) <pre>Cost at initial w (zeros): 0.693\n</pre> <p>Expected Output:</p> Cost at initial w (zeros)  0.693  In\u00a0[20]: Copied! <pre># Compute and display cost with non-zero w\ntest_w = np.array([0.2, 0.2])\ntest_b = -24.\ncost = compute_cost(X_train, y_train, test_w, test_b)\n\nprint('Cost at test w,b: {:.3f}'.format(cost))\n\n\n# UNIT TESTS\ncompute_cost_test(compute_cost)\n</pre> # Compute and display cost with non-zero w test_w = np.array([0.2, 0.2]) test_b = -24. cost = compute_cost(X_train, y_train, test_w, test_b)  print('Cost at test w,b: {:.3f}'.format(cost))   # UNIT TESTS compute_cost_test(compute_cost)  <pre>Cost at test w,b: 0.218\nAll tests passed!\n</pre> <p>Expected Output:</p> Cost at test w,b  0.218  <p></p> <p></p> In\u00a0[21]: Copied! <pre># UNQ_C3\n# GRADED FUNCTION: compute_gradient\ndef compute_gradient(X, y, w, b, lambda_=None): \n    \"\"\"\n    Computes the gradient for logistic regression \n \n    Args:\n      X : (ndarray Shape (m,n)) variable such as house size \n      y : (array_like Shape (m,1)) actual value \n      w : (array_like Shape (n,1)) values of parameters of the model      \n      b : (scalar)                 value of parameter of the model \n      lambda_: unused placeholder.\n    Returns\n      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m, n = X.shape\n    dj_dw = np.zeros(w.shape)\n    dj_db = 0.\n\n    ### START CODE HERE ### \n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          \n        err_i  = f_wb_i  - y[i]                       \n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   \n    dj_db = dj_db/m                                   \n        \n    ### END CODE HERE ###\n\n        \n    return dj_db, dj_dw\n</pre> # UNQ_C3 # GRADED FUNCTION: compute_gradient def compute_gradient(X, y, w, b, lambda_=None):      \"\"\"     Computes the gradient for logistic regression        Args:       X : (ndarray Shape (m,n)) variable such as house size        y : (array_like Shape (m,1)) actual value        w : (array_like Shape (n,1)) values of parameters of the model             b : (scalar)                 value of parameter of the model        lambda_: unused placeholder.     Returns       dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w.        dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b.      \"\"\"     m, n = X.shape     dj_dw = np.zeros(w.shape)     dj_db = 0.      ### START CODE HERE ###      for i in range(m):         f_wb_i = sigmoid(np.dot(X[i],w) + b)                   err_i  = f_wb_i  - y[i]                                for j in range(n):             dj_dw[j] = dj_dw[j] + err_i * X[i,j]               dj_db = dj_db + err_i     dj_dw = dj_dw/m                                        dj_db = dj_db/m                                                 ### END CODE HERE ###               return dj_db, dj_dw Click for hints <ul> <li><p>Here's how you can structure the overall implementation for this function</p> <pre>   def compute_gradient(X, y, w, b, lambda_=None): \n        m, n = X.shape\n        dj_dw = np.zeros(w.shape)\n        dj_db = 0.\n\n        ### START CODE HERE ### \n        for i in range(m):\n            # Calculate f_wb (exactly as you did in the compute_cost function above)\n            f_wb = \n\n            # Calculate the  gradient for b from this example\n            dj_db_i = # Your code here to calculate the error\n\n            # add that to dj_db\n            dj_db += dj_db_i\n\n            # get dj_dw for each attribute\n            for j in range(n):\n                # You code here to calculate the gradient from the i-th example for j-th attribute\n                dj_dw_ij =  \n                dj_dw[j] += dj_dw_ij\n\n        # divide dj_db and dj_dw by total number of examples\n        dj_dw = dj_dw / m\n        dj_db = dj_db / m\n        ### END CODE HERE ###\n\n        return dj_db, dj_dw\n</pre> <p>If you're still stuck, you can check the hints presented below to figure out how to calculate <code>f_wb</code>, <code>dj_db_i</code> and <code>dj_dw_ij</code></p> Hint to calculate f_wb          \u2003 \u2003 Recall that you calculated f_wb in <code>compute_cost</code> above \u2014 for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise           \u2003 \u2003 More hints to calculate f_wb             \u2003 \u2003 You can calculate f_wb as              <pre>\n             for i in range(m):   \n                 # Calculate f_wb (exactly how you did it in the compute_cost function above)\n                 z_wb = 0\n                 # Loop over each feature\n                 for j in range(n): \n                     # Add the corresponding term to z_wb\n                     z_wb_ij = X[i, j] * w[j]\n                     z_wb += z_wb_ij\n\n<pre><code>             # Add bias term \n             z_wb += b\n\n             # Calculate the prediction from the model\n             f_wb = sigmoid(z_wb)</code></pre>\n<p></p></pre> Hint to calculate dj_db_i          \u2003 \u2003 You can calculate dj_db_i as <code>dj_db_i = f_wb - y[i]</code> Hint to calculate dj_dw_ij       \u2003 \u2003 You can calculate dj_dw_ij as <code>dj_dw_ij = (f_wb - y[i])* X[i][j]</code> </li> </ul> <p>Run the cells below to check your implementation of the <code>compute_gradient</code> function with two different initializations of the parameters $w$</p> In\u00a0[22]: Copied! <pre># Compute and display gradient with w initialized to zeroes\ninitial_w = np.zeros(n)\ninitial_b = 0.\n\ndj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\nprint(f'dj_db at initial w (zeros):{dj_db}' )\nprint(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )\n</pre> # Compute and display gradient with w initialized to zeroes initial_w = np.zeros(n) initial_b = 0.  dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b) print(f'dj_db at initial w (zeros):{dj_db}' ) print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' ) <pre>dj_db at initial w (zeros):-0.1\ndj_dw at initial w (zeros):[-12.00921658929115, -11.262842205513591]\n</pre> <p>Expected Output:</p> dj_db at initial w (zeros)  -0.1  ddj_dw at initial w (zeros):  [-12.00921658929115, -11.262842205513591]  In\u00a0[23]: Copied! <pre># Compute and display cost and gradient with non-zero w\ntest_w = np.array([ 0.2, -0.5])\ntest_b = -24\ndj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n\nprint('dj_db at test_w:', dj_db)\nprint('dj_dw at test_w:', dj_dw.tolist())\n\n# UNIT TESTS    \ncompute_gradient_test(compute_gradient)\n</pre> # Compute and display cost and gradient with non-zero w test_w = np.array([ 0.2, -0.5]) test_b = -24 dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)  print('dj_db at test_w:', dj_db) print('dj_dw at test_w:', dj_dw.tolist())  # UNIT TESTS     compute_gradient_test(compute_gradient)  <pre>dj_db at test_w: -0.5999999999991071\ndj_dw at test_w: [-44.831353617873795, -44.37384124953978]\nAll tests passed!\n</pre> <p>Expected Output:</p> dj_db at initial w (zeros)  -0.5999999999991071  ddj_dw at initial w (zeros):   [-44.8313536178737957, -44.37384124953978]  <p></p> In\u00a0[24]: Copied! <pre>def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X :    (array_like Shape (m, n)\n      y :    (array_like Shape (m,))\n      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n      b_in : (scalar)                 Initial value of parameter of the model\n      cost_function:                  function to compute cost\n      alpha : (float)                 Learning rate\n      num_iters : (int)               number of iterations to run gradient descent\n      lambda_ (scalar, float)         regularization constant\n      \n    Returns:\n      w : (array_like Shape (n,)) Updated values of parameters of the model after\n          running gradient descent\n      b : (scalar)                Updated value of parameter of the model after\n          running gradient descent\n    \"\"\"\n    \n    # number of training examples\n    m = len(X)\n    \n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w_history = []\n    \n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w_in = w_in - alpha * dj_dw               \n        b_in = b_in - alpha * dj_db              \n       \n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion \n            cost =  cost_function(X, y, w_in, b_in, lambda_)\n            J_history.append(cost)\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n            w_history.append(w_in)\n            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n        \n    return w_in, b_in, J_history, w_history #return w and J,w history for graphing\n</pre> def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):      \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking      num_iters gradient steps with learning rate alpha          Args:       X :    (array_like Shape (m, n)       y :    (array_like Shape (m,))       w_in : (array_like Shape (n,))  Initial values of parameters of the model       b_in : (scalar)                 Initial value of parameter of the model       cost_function:                  function to compute cost       alpha : (float)                 Learning rate       num_iters : (int)               number of iterations to run gradient descent       lambda_ (scalar, float)         regularization constant            Returns:       w : (array_like Shape (n,)) Updated values of parameters of the model after           running gradient descent       b : (scalar)                Updated value of parameter of the model after           running gradient descent     \"\"\"          # number of training examples     m = len(X)          # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w_history = []          for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)             # Update Parameters using w, b, alpha and gradient         w_in = w_in - alpha * dj_dw                        b_in = b_in - alpha * dj_db                               # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion              cost =  cost_function(X, y, w_in, b_in, lambda_)             J_history.append(cost)          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):             w_history.append(w_in)             print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")              return w_in, b_in, J_history, w_history #return w and J,w history for graphing <p>Now let's run the gradient descent algorithm above to learn the parameters for our dataset.</p> <p>Note</p> <p>The code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the <code>iterations</code> to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.</p> In\u00a0[25]: Copied! <pre>np.random.seed(1)\ninitial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\ninitial_b = -8\n\n\n# Some gradient descent settings\niterations = 10000\nalpha = 0.001\n\nw,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n                                   compute_cost, compute_gradient, alpha, iterations, 0)\n</pre> np.random.seed(1) initial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5) initial_b = -8   # Some gradient descent settings iterations = 10000 alpha = 0.001  w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,                                     compute_cost, compute_gradient, alpha, iterations, 0) <pre>Iteration    0: Cost     1.01   \nIteration 1000: Cost     0.31   \nIteration 2000: Cost     0.30   \nIteration 3000: Cost     0.30   \nIteration 4000: Cost     0.30   \nIteration 5000: Cost     0.30   \nIteration 6000: Cost     0.30   \nIteration 7000: Cost     0.30   \nIteration 8000: Cost     0.30   \nIteration 9000: Cost     0.30   \nIteration 9999: Cost     0.30   \n</pre> Expected Output: Cost     0.30, (Click to see details): <pre><code># With the following settings\nnp.random.seed(1)\nintial_w = 0.01 * (np.random.rand(2).reshape(-1,1) - 0.5)\ninitial_b = -8\niterations = 10000\nalpha = 0.001\n#</code></pre> <pre><code>Iteration    0: Cost     1.01   \nIteration 1000: Cost     0.31   \nIteration 2000: Cost     0.30   \nIteration 3000: Cost     0.30   \nIteration 4000: Cost     0.30   \nIteration 5000: Cost     0.30   \nIteration 6000: Cost     0.30   \nIteration 7000: Cost     0.30   \nIteration 8000: Cost     0.30   \nIteration 9000: Cost     0.30   \nIteration 9999: Cost     0.30   \n</code></pre> <p></p> In\u00a0[26]: Copied! <pre>plot_decision_boundary(w, b, X_train, y_train)\n</pre> plot_decision_boundary(w, b, X_train, y_train) <p></p> <p></p> In\u00a0[29]: Copied! <pre># UNQ_C4\n# GRADED FUNCTION: predict\n\ndef predict(X, w, b): \n    \"\"\"\n    Predict whether the label is 0 or 1 using learned logistic\n    regression parameters w\n    \n    Args:\n    X : (ndarray Shape (m, n))\n    w : (array_like Shape (n,))      Parameters of the model\n    b : (scalar, float)              Parameter of the model\n\n    Returns:\n    p: (ndarray (m,1))\n        The predictions for X using a threshold at 0.5\n    \"\"\"\n    # number of training examples\n    m, n = X.shape   \n    p = np.zeros(m)\n   \n    ### START CODE HERE ### \n    # Loop over each example\n    for i in range(m):   \n        z_wb = np.dot(X[i],w) \n        # Loop over each feature\n        for j in range(n): \n            # Add the corresponding term to z_wb\n            z_wb += 0\n        \n        # Add bias term \n        z_wb += b\n        \n        # Calculate the prediction for this example\n        f_wb = sigmoid(z_wb)\n\n        # Apply the threshold\n        p[i] = 1 if f_wb&gt;0.5 else 0\n        \n    ### END CODE HERE ### \n    return p\n</pre> # UNQ_C4 # GRADED FUNCTION: predict  def predict(X, w, b):      \"\"\"     Predict whether the label is 0 or 1 using learned logistic     regression parameters w          Args:     X : (ndarray Shape (m, n))     w : (array_like Shape (n,))      Parameters of the model     b : (scalar, float)              Parameter of the model      Returns:     p: (ndarray (m,1))         The predictions for X using a threshold at 0.5     \"\"\"     # number of training examples     m, n = X.shape        p = np.zeros(m)         ### START CODE HERE ###      # Loop over each example     for i in range(m):            z_wb = np.dot(X[i],w)          # Loop over each feature         for j in range(n):              # Add the corresponding term to z_wb             z_wb += 0                  # Add bias term          z_wb += b                  # Calculate the prediction for this example         f_wb = sigmoid(z_wb)          # Apply the threshold         p[i] = 1 if f_wb&gt;0.5 else 0              ### END CODE HERE ###      return p Click for hints <ul> <li><p>Here's how you can structure the overall implementation for this function</p> <pre>   def predict(X, w, b): \n        # number of training examples\n        m, n = X.shape   \n        p = np.zeros(m)\n\n        ### START CODE HERE ### \n        # Loop over each example\n        for i in range(m):   \n\n            # Calculate f_wb (exactly how you did it in the compute_cost function above) \n            # using a couple of lines of code\n            f_wb = \n\n            # Calculate the prediction for that training example \n            p[i] = # Your code here to calculate the prediction based on f_wb\n\n        ### END CODE HERE ### \n        return p\n</pre> <p>If you're still stuck, you can check the hints presented below to figure out how to calculate <code>f_wb</code> and <code>p[i]</code></p> Hint to calculate f_wb          \u2003 \u2003 Recall that you calculated f_wb in <code>compute_cost</code> above \u2014 for detailed hints on how to calculate each intermediate term, check out the hints section below that exercise           \u2003 \u2003 More hints to calculate f_wb             \u2003 \u2003 You can calculate f_wb as              <pre>\n             for i in range(m):   \n                 # Calculate f_wb (exactly how you did it in the compute_cost function above)\n                 z_wb = 0\n                 # Loop over each feature\n                 for j in range(n): \n                     # Add the corresponding term to z_wb\n                     z_wb_ij = X[i, j] * w[j]\n                     z_wb += z_wb_ij\n\n<pre><code>             # Add bias term \n             z_wb += b\n\n             # Calculate the prediction from the model\n             f_wb = sigmoid(z_wb)</code></pre>\n<p></p></pre> Hint to calculate p[i]          \u2003 \u2003 As an example, if you'd like to say x = 1 if y is less than 3 and 0 otherwise, you can express it in code as <code>x = y &lt; 3 </code>. Now do the same for p[i] = 1 if f_wb &gt;= 0.5 and 0 otherwise.            \u2003 \u2003 More hints to calculate p[i]             \u2003 \u2003 You can compute p[i] as <code>p[i] = f_wb &gt;= 0.5</code> </li> </ul> <p>Once you have completed the function <code>predict</code>, let's run the code below to report the training accuracy of your classifier by computing the percentage of examples it got correct.</p> In\u00a0[30]: Copied! <pre># Test your predict code\nnp.random.seed(1)\ntmp_w = np.random.randn(2)\ntmp_b = 0.3    \ntmp_X = np.random.randn(4, 2) - 0.5\n\ntmp_p = predict(tmp_X, tmp_w, tmp_b)\nprint(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')\n\n# UNIT TESTS        \npredict_test(predict)\n</pre> # Test your predict code np.random.seed(1) tmp_w = np.random.randn(2) tmp_b = 0.3     tmp_X = np.random.randn(4, 2) - 0.5  tmp_p = predict(tmp_X, tmp_w, tmp_b) print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')  # UNIT TESTS         predict_test(predict) <pre>Output of predict: shape (4,), value [0. 1. 1. 1.]\nAll tests passed!\n</pre> <p>Expected output</p> Output of predict: shape (4,),value [0. 1. 1. 1.] <p>Now let's use this to compute the accuracy on the training set</p> In\u00a0[31]: Copied! <pre>#Compute accuracy on our training set\np = predict(X_train, w,b)\nprint('Train Accuracy: %f'%(np.mean(p == y_train) * 100))\n</pre> #Compute accuracy on our training set p = predict(X_train, w,b) print('Train Accuracy: %f'%(np.mean(p == y_train) * 100)) <pre>Train Accuracy: 92.000000\n</pre> Train Accuracy (approx):  92.00  <p></p> In\u00a0[32]: Copied! <pre># load dataset\nX_train, y_train = load_data(\"data/ex2data2.txt\")\n</pre> # load dataset X_train, y_train = load_data(\"data/ex2data2.txt\") In\u00a0[33]: Copied! <pre># print X_train\nprint(\"X_train:\", X_train[:5])\nprint(\"Type of X_train:\",type(X_train))\n\n# print y_train\nprint(\"y_train:\", y_train[:5])\nprint(\"Type of y_train:\",type(y_train))\n</pre> # print X_train print(\"X_train:\", X_train[:5]) print(\"Type of X_train:\",type(X_train))  # print y_train print(\"y_train:\", y_train[:5]) print(\"Type of y_train:\",type(y_train)) <pre>X_train: [[ 0.051267  0.69956 ]\n [-0.092742  0.68494 ]\n [-0.21371   0.69225 ]\n [-0.375     0.50219 ]\n [-0.51325   0.46564 ]]\nType of X_train: &lt;class 'numpy.ndarray'&gt;\ny_train: [1. 1. 1. 1. 1.]\nType of y_train: &lt;class 'numpy.ndarray'&gt;\n</pre> In\u00a0[34]: Copied! <pre>print ('The shape of X_train is: ' + str(X_train.shape))\nprint ('The shape of y_train is: ' + str(y_train.shape))\nprint ('We have m = %d training examples' % (len(y_train)))\n</pre> print ('The shape of X_train is: ' + str(X_train.shape)) print ('The shape of y_train is: ' + str(y_train.shape)) print ('We have m = %d training examples' % (len(y_train))) <pre>The shape of X_train is: (118, 2)\nThe shape of y_train is: (118,)\nWe have m = 118 training examples\n</pre> In\u00a0[35]: Copied! <pre># Plot examples\nplot_data(X_train, y_train[:], pos_label=\"Accepted\", neg_label=\"Rejected\")\n\n# Set the y-axis label\nplt.ylabel('Microchip Test 2') \n# Set the x-axis label\nplt.xlabel('Microchip Test 1') \nplt.legend(loc=\"upper right\")\nplt.show()\n</pre> # Plot examples plot_data(X_train, y_train[:], pos_label=\"Accepted\", neg_label=\"Rejected\")  # Set the y-axis label plt.ylabel('Microchip Test 2')  # Set the x-axis label plt.xlabel('Microchip Test 1')  plt.legend(loc=\"upper right\") plt.show() <p>Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight forward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.</p> <p></p> In\u00a0[36]: Copied! <pre>print(\"Original shape of data:\", X_train.shape)\n\nmapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\nprint(\"Shape after feature mapping:\", mapped_X.shape)\n</pre> print(\"Original shape of data:\", X_train.shape)  mapped_X =  map_feature(X_train[:, 0], X_train[:, 1]) print(\"Shape after feature mapping:\", mapped_X.shape) <pre>Original shape of data: (118, 2)\nShape after feature mapping: (118, 27)\n</pre> <p>Let's also print the first elements of <code>X_train</code> and <code>mapped_X</code> to see the tranformation.</p> In\u00a0[37]: Copied! <pre>print(\"X_train[0]:\", X_train[0])\nprint(\"mapped X_train[0]:\", mapped_X[0])\n</pre> print(\"X_train[0]:\", X_train[0]) print(\"mapped X_train[0]:\", mapped_X[0]) <pre>X_train[0]: [0.051267 0.69956 ]\nmapped X_train[0]: [5.12670000e-02 6.99560000e-01 2.62830529e-03 3.58643425e-02\n 4.89384194e-01 1.34745327e-04 1.83865725e-03 2.50892595e-02\n 3.42353606e-01 6.90798869e-06 9.42624411e-05 1.28625106e-03\n 1.75514423e-02 2.39496889e-01 3.54151856e-07 4.83255257e-06\n 6.59422333e-05 8.99809795e-04 1.22782870e-02 1.67542444e-01\n 1.81563032e-08 2.47750473e-07 3.38066048e-06 4.61305487e-05\n 6.29470940e-04 8.58939846e-03 1.17205992e-01]\n</pre> <p>While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p> <p></p> <p></p> In\u00a0[51]: Copied! <pre># UNQ_C5\ndef compute_cost_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X : (array_like Shape (m,n)) data, m examples by n features\n      y : (array_like Shape (m,)) target value \n      w : (array_like Shape (n,)) Values of parameters of the model      \n      b : (array_like Shape (n,)) Values of bias parameter of the model\n      lambda_ : (scalar, float)    Controls amount of regularization\n    Returns:\n      total_cost: (scalar)         cost \n    \"\"\"\n\n    m, n = X.shape\n    \n    # Calls the compute_cost function that you implemented above\n    cost_without_reg = compute_cost(X, y, w, b) \n    \n    # You need to calculate this value\n    reg_cost = 0.\n    \n    ### START CODE HERE ###\n    reg_cost = sum(np.square(w))\n    ### END CODE HERE ### \n    \n    # Add the regularization cost to get the total cost\n    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n\n    return total_cost\n</pre> # UNQ_C5 def compute_cost_reg(X, y, w, b, lambda_ = 1):     \"\"\"     Computes the cost over all examples     Args:       X : (array_like Shape (m,n)) data, m examples by n features       y : (array_like Shape (m,)) target value        w : (array_like Shape (n,)) Values of parameters of the model             b : (array_like Shape (n,)) Values of bias parameter of the model       lambda_ : (scalar, float)    Controls amount of regularization     Returns:       total_cost: (scalar)         cost      \"\"\"      m, n = X.shape          # Calls the compute_cost function that you implemented above     cost_without_reg = compute_cost(X, y, w, b)           # You need to calculate this value     reg_cost = 0.          ### START CODE HERE ###     reg_cost = sum(np.square(w))     ### END CODE HERE ###           # Add the regularization cost to get the total cost     total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost      return total_cost Click for hints <ul> <li><p>Here's how you can structure the overall implementation for this function</p> <pre>   def compute_cost_reg(X, y, w, b, lambda_ = 1):\n\n       m, n = X.shape\n\n        # Calls the compute_cost function that you implemented above\n        cost_without_reg = compute_cost(X, y, w, b) \n\n        # You need to calculate this value\n        reg_cost = 0.\n\n        ### START CODE HERE ###\n        for j in range(n):\n            reg_cost_j = # Your code here to calculate the cost from w[j]\n            reg_cost = reg_cost + reg_cost_j\n\n        ### END CODE HERE ### \n\n        # Add the regularization cost to get the total cost\n        total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n\n    return total_cost\n</pre> <p>If you're still stuck, you can check the hints presented below to figure out how to calculate <code>reg_cost_j</code></p> Hint to calculate reg_cost_j          \u2003 \u2003 You can use calculate reg_cost_j as <code>reg_cost_j = w[j]**2 </code> </li></ul> <p>Run the cell below to check your implementation of the <code>compute_cost_reg</code> function.</p> In\u00a0[52]: Copied! <pre>X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\nnp.random.seed(1)\ninitial_w = np.random.rand(X_mapped.shape[1]) - 0.5\ninitial_b = 0.5\nlambda_ = 0.5\ncost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n\nprint(\"Regularized cost :\", cost)\n\n# UNIT TEST    \ncompute_cost_reg_test(compute_cost_reg)\n</pre> X_mapped = map_feature(X_train[:, 0], X_train[:, 1]) np.random.seed(1) initial_w = np.random.rand(X_mapped.shape[1]) - 0.5 initial_b = 0.5 lambda_ = 0.5 cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)  print(\"Regularized cost :\", cost)  # UNIT TEST     compute_cost_reg_test(compute_cost_reg)  <pre>Regularized cost : 0.6618252552483948\nAll tests passed!\n</pre> <p>Expected Output:</p> Regularized cost :   0.6618252552483948  <p></p> <p></p> In\u00a0[56]: Copied! <pre># UNQ_C6\ndef compute_gradient_reg(X, y, w, b, lambda_ = 1): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X : (ndarray Shape (m,n))   variable such as house size \n      y : (ndarray Shape (m,))    actual value \n      w : (ndarray Shape (n,))    values of parameters of the model      \n      b : (scalar)                value of parameter of the model  \n      lambda_ : (scalar,float)    regularization constant\n    Returns\n      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b. \n      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n\n    \"\"\"\n    m, n = X.shape\n    \n    dj_db, dj_dw = compute_gradient(X, y, w, b)\n\n    ### START CODE HERE ###     \n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n    ### END CODE HERE ###         \n        \n    return dj_db, dj_dw\n</pre> # UNQ_C6 def compute_gradient_reg(X, y, w, b, lambda_ = 1):      \"\"\"     Computes the gradient for linear regression        Args:       X : (ndarray Shape (m,n))   variable such as house size        y : (ndarray Shape (m,))    actual value        w : (ndarray Shape (n,))    values of parameters of the model             b : (scalar)                value of parameter of the model         lambda_ : (scalar,float)    regularization constant     Returns       dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b.        dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.       \"\"\"     m, n = X.shape          dj_db, dj_dw = compute_gradient(X, y, w, b)      ### START CODE HERE ###          for j in range(n):         dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]     ### END CODE HERE ###                       return dj_db, dj_dw Click for hints <ul> <li><p>Here's how you can structure the overall implementation for this function</p> <pre>def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n    m, n = X.shape\n\n    dj_db, dj_dw = compute_gradient(X, y, w, b)\n\n    ### START CODE HERE ###     \n    # Loop over the elements of w\n    for j in range(n): \n\n        dj_dw_j_reg = # Your code here to calculate the regularization term for dj_dw[j]\n\n        # Add the regularization term  to the correspoding element of dj_dw\n        dj_dw[j] = dj_dw[j] + dj_dw_j_reg\n\n    ### END CODE HERE ###         \n\n    return dj_db, dj_dw\n</pre> <p>If you're still stuck, you can check the hints presented below to figure out how to calculate <code>dj_dw_j_reg</code></p> Hint to calculate dj_dw_j_reg          \u2003 \u2003 You can use calculate dj_dw_j_reg as <code>dj_dw_j_reg = (lambda_ / m) * w[j] </code> </li></ul> <p>Run the cell below to check your implementation of the <code>compute_gradient_reg</code> function.</p> In\u00a0[57]: Copied! <pre>X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\nnp.random.seed(1) \ninitial_w  = np.random.rand(X_mapped.shape[1]) - 0.5 \ninitial_b = 0.5\n \nlambda_ = 0.5\ndj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n\nprint(f\"dj_db: {dj_db}\", )\nprint(f\"First few elements of regularized dj_dw:\\n {dj_dw[:4].tolist()}\", )\n\n# UNIT TESTS    \ncompute_gradient_reg_test(compute_gradient_reg)\n</pre> X_mapped = map_feature(X_train[:, 0], X_train[:, 1]) np.random.seed(1)  initial_w  = np.random.rand(X_mapped.shape[1]) - 0.5  initial_b = 0.5   lambda_ = 0.5 dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)  print(f\"dj_db: {dj_db}\", ) print(f\"First few elements of regularized dj_dw:\\n {dj_dw[:4].tolist()}\", )  # UNIT TESTS     compute_gradient_reg_test(compute_gradient_reg)  <pre>dj_db: 0.07138288792343662\nFirst few elements of regularized dj_dw:\n [-0.010386028450548701, 0.011409852883280122, 0.0536273463274574, 0.0031402782673134655]\nAll tests passed!\n</pre> <p>Expected Output:</p> dj_db:0.07138288792343656  First few elements of regularized dj_dw:  [[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]]  <p></p> In\u00a0[58]: Copied! <pre># Initialize fitting parameters\nnp.random.seed(1)\ninitial_w = np.random.rand(X_mapped.shape[1])-0.5\ninitial_b = 1.\n\n# Set regularization parameter lambda_ to 1 (you can try varying this)\nlambda_ = 0.01;                                          \n# Some gradient descent settings\niterations = 10000\nalpha = 0.01\n\nw,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n                                    compute_cost_reg, compute_gradient_reg, \n                                    alpha, iterations, lambda_)\n</pre> # Initialize fitting parameters np.random.seed(1) initial_w = np.random.rand(X_mapped.shape[1])-0.5 initial_b = 1.  # Set regularization parameter lambda_ to 1 (you can try varying this) lambda_ = 0.01;                                           # Some gradient descent settings iterations = 10000 alpha = 0.01  w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b,                                      compute_cost_reg, compute_gradient_reg,                                      alpha, iterations, lambda_) <pre>Iteration    0: Cost     0.72   \nIteration 1000: Cost     0.59   \nIteration 2000: Cost     0.56   \nIteration 3000: Cost     0.53   \nIteration 4000: Cost     0.51   \nIteration 5000: Cost     0.50   \nIteration 6000: Cost     0.48   \nIteration 7000: Cost     0.47   \nIteration 8000: Cost     0.46   \nIteration 9000: Cost     0.45   \nIteration 9999: Cost     0.45   \n</pre> Expected Output: Cost &lt; 0.5  (Click for details) <pre><code># Using the following settings\n#np.random.seed(1)\n#initial_w = np.random.rand(X_mapped.shape[1])-0.5\n#initial_b = 1.\n#lambda_ = 0.01;                                          \n#iterations = 10000\n#alpha = 0.01\nIteration    0: Cost     0.72   \nIteration 1000: Cost     0.59   \nIteration 2000: Cost     0.56   \nIteration 3000: Cost     0.53   \nIteration 4000: Cost     0.51   \nIteration 5000: Cost     0.50   \nIteration 6000: Cost     0.48   \nIteration 7000: Cost     0.47   \nIteration 8000: Cost     0.46   \nIteration 9000: Cost     0.45   \nIteration 9999: Cost     0.45       \n    \n</code></pre> <p></p> In\u00a0[59]: Copied! <pre>plot_decision_boundary(w, b, X_mapped, y_train)\n</pre> plot_decision_boundary(w, b, X_mapped, y_train) <p></p> In\u00a0[60]: Copied! <pre>#Compute accuracy on the training set\np = predict(X_mapped, w, b)\n\nprint('Train Accuracy: %f'%(np.mean(p == y_train) * 100))\n</pre> #Compute accuracy on the training set p = predict(X_mapped, w, b)  print('Train Accuracy: %f'%(np.mean(p == y_train) * 100)) <pre>Train Accuracy: 82.203390\n</pre> <p>Expected Output:</p> Train Accuracy:~ 80% In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Logistic_Regression/#logistic-regression","title":"Logistic Regression\u00b6","text":"<p>In this exercise, you will implement logistic regression and apply it to two different datasets.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#outline","title":"Outline\u00b6","text":"<ul> <li> 1 - Packages </li> <li> 2 - Logistic Regression<ul> <li> 2.1 Problem Statement</li> <li> 2.2 Loading and visualizing the data</li> <li> 2.3  Sigmoid function</li> <li> 2.4 Cost function for logistic regression</li> <li> 2.5 Gradient for logistic regression</li> <li> 2.6 Learning parameters using gradient descent </li> <li> 2.7 Plotting the decision boundary</li> <li> 2.8 Evaluating logistic regression</li> </ul> </li> <li> 3 - Regularized Logistic Regression<ul> <li> 3.1 Problem Statement</li> <li> 3.2 Loading and visualizing the data</li> <li> 3.3 Feature mapping</li> <li> 3.4 Cost function for regularized logistic regression</li> <li> 3.5 Gradient for regularized logistic regression</li> <li> 3.6 Learning parameters using gradient descent</li> <li> 3.7 Plotting the decision boundary</li> <li> 3.8 Evaluating regularized logistic regression model</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#1-packages","title":"1 - Packages\u00b6","text":"<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p> <ul> <li>numpy is the fundamental package for scientific computing with Python.</li> <li>matplotlib is a famous library to plot graphs in Python.</li> <li><code>utils.py</code> contains helper functions for this assignment. You do not need to modify code in this file.</li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#2-logistic-regression","title":"2 - Logistic Regression\u00b6","text":"<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.</p> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#21-problem-statement","title":"2.1 Problem Statement\u00b6","text":"<p>Suppose that you are the administrator of a university department and you want to determine each applicant\u2019s chance of admission based on their results on two exams.</p> <ul> <li>You have historical data from previous applicants that you can use as a training set for logistic regression.</li> <li>For each training example, you have the applicant\u2019s scores on two exams and the admissions decision.</li> <li>Your task is to build a classification model that estimates an applicant\u2019s probability of admission based on the scores from those two exams.</li> </ul> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#22-loading-and-visualizing-the-data","title":"2.2 Loading and visualizing the data\u00b6","text":"<p>You will start by loading the dataset for this task.</p> <ul> <li>The <code>load_dataset()</code> function shown below loads the data into variables <code>X_train</code> and <code>y_train</code><ul> <li><code>X_train</code> contains exam scores on two exams for a student</li> <li><code>y_train</code> is the admission decision<ul> <li><code>y_train = 1</code> if the student was admitted</li> <li><code>y_train = 0</code> if the student was not admitted</li> </ul> </li> <li>Both <code>X_train</code> and <code>y_train</code> are numpy arrays.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#view-the-variables","title":"View the variables\u00b6","text":"<p>Let's get more familiar with your dataset.</p> <ul> <li>A good place to start is to just print out each variable and see what it contains.</li> </ul> <p>The code below prints the first five values of <code>X_train</code> and the type of the variable.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#check-the-dimensions-of-your-variables","title":"Check the dimensions of your variables\u00b6","text":"<p>Another useful way to get familiar with your data is to view its dimensions. Let's print the shape of <code>X_train</code> and <code>y_train</code> and see how many training examples we have in our dataset.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#visualize-your-data","title":"Visualize your data\u00b6","text":"<p>Before starting to implement any learning algorithm, it is always good to visualize the data if possible.</p> <ul> <li>The code below displays the data on a 2D plot (as shown below), where the axes are the two exam scores, and the positive and negative examples are shown with different markers.</li> <li>We use a helper function in the <code>utils.py</code> file to generate this plot.</li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#23-sigmoid-function","title":"2.3  Sigmoid function\u00b6","text":"<p>Recall that for logistic regression, the model is represented as</p> <p>$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$ where function $g$ is the sigmoid function. The sigmoid function is defined as:</p> <p>$$g(z) = \\frac{1}{1+e^{-z}}$$</p> <p>Let's implement the sigmoid function first, so it can be used by the rest of this assignment.</p> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-1","title":"Exercise 1\u00b6","text":"<p>Please complete  the <code>sigmoid</code> function to calculate</p> <p>$$g(z) = \\frac{1}{1+e^{-z}}$$</p> <p>Note that</p> <ul> <li><code>z</code> is not always a single number, but can also be an array of numbers.</li> <li>If the input is an array of numbers, we'd like to apply the sigmoid function to each value in the input array.</li> </ul> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#24-cost-function-for-logistic-regression","title":"2.4 Cost function for logistic regression\u00b6","text":"<p>In this section, you will implement the cost function for logistic regression.</p> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-2","title":"Exercise 2\u00b6","text":"<p>Please complete the <code>compute_cost</code> function using the equations below.</p> <p>Recall that for logistic regression, the cost function is of the form</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$</p> <p>where</p> <ul> <li><p>m is the number of training examples in the dataset</p> </li> <li><p>$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -</p> <p>$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$</p> </li> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label</p> </li> <li><p>$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.</p> <ul> <li>It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$</li> </ul> </li> </ul> <p>Note:</p> <ul> <li>As you are doing this, remember that the variables <code>X_train</code> and <code>y_train</code> are not scalar values but matrices of shape ($m, n$) and ($\ud835\udc5a$,1) respectively, where  $\ud835\udc5b$ is the number of features and $\ud835\udc5a$ is the number of training examples.</li> <li>You can use the sigmoid function that you implemented above for this part.</li> </ul> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#25-gradient-for-logistic-regression","title":"2.5 Gradient for logistic regression\u00b6","text":"<p>In this section, you will implement the gradient for logistic regression.</p> <p>Recall that the gradient descent algorithm is:</p> <p>$$\\begin{align*}&amp; \\text{repeat until convergence:} \\; \\lbrace \\newline \\; &amp; b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; &amp; w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; &amp; \\text{for j := 0..n-1}\\newline &amp; \\rbrace\\end{align*}$$</p> <p>where, parameters $b$, $w_j$ are all updated simultaniously</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-3","title":"Exercise 3\u00b6","text":"<p>Please complete the <code>compute_gradient</code> function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.</p> <p>$$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2} $$ $$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3} $$</p> <ul> <li><p>m is the number of training examples in the dataset</p> </li> <li><p>$f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label</p> </li> </ul> <ul> <li>Note: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $f_{\\mathbf{w},b}(x)$.</li> </ul> <p>As before, you can use the sigmoid function that you implemented above and if you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#26-learning-parameters-using-gradient-descent","title":"2.6 Learning parameters using gradient descent\u00b6","text":"<p>Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent.</p> <ul> <li><p>You don't need to implement anything for this part. Simply run the cells below.</p> </li> <li><p>A good way to verify that gradient descent is working correctly is to look at the value of $J(\\mathbf{w},b)$ and check that it is decreasing with each step.</p> </li> <li><p>Assuming you have implemented the gradient and computed the cost correctly, your value of $J(\\mathbf{w},b)$ should never increase, and should converge to a steady value by the end of the algorithm.</p> </li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#27-plotting-the-decision-boundary","title":"2.7 Plotting the decision boundary\u00b6","text":"<p>We will now use the final parameters from gradient descent to plot the linear fit. If you implemented the previous parts correctly, you should see the following plot: </p> <p>We will use a helper function in the <code>utils.py</code> file to create this plot.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#28-evaluating-logistic-regression","title":"2.8 Evaluating logistic regression\u00b6","text":"<p>We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set.</p> <p>You will implement the <code>predict</code> function below to do this.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-4","title":"Exercise 4\u00b6","text":"<p>Please complete the <code>predict</code> function to produce <code>1</code> or <code>0</code> predictions given a dataset and a learned parameter vector $w$ and $b$.</p> <ul> <li><p>First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example</p> <ul> <li>You've implemented this before in the parts above</li> </ul> </li> <li><p>We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.</p> </li> <li><p>Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -</p> <p>if $f(x^{(i)}) &gt;= 0.5$, predict $y^{(i)}=1$</p> <p>if $f(x^{(i)}) &lt; 0.5$, predict $y^{(i)}=0$</p> </li> </ul> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#3-regularized-logistic-regression","title":"3 - Regularized Logistic Regression\u00b6","text":"<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#31-problem-statement","title":"3.1 Problem Statement\u00b6","text":"<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests.</p> <ul> <li>From these two tests, you would like to determine whether the microchips should be accepted or rejected.</li> <li>To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</li> </ul> <p></p>"},{"location":"MachineLearning/part3/Logistic_Regression/#32-loading-and-visualizing-the-data","title":"3.2 Loading and visualizing the data\u00b6","text":"<p>Similar to previous parts of this exercise, let's start by loading the dataset for this task and visualizing it.</p> <ul> <li>The <code>load_dataset()</code> function shown below loads the data into variables <code>X_train</code> and <code>y_train</code><ul> <li><code>X_train</code> contains the test results for the microchips from two tests</li> <li><code>y_train</code> contains the results of the QA<ul> <li><code>y_train = 1</code> if the microchip was accepted</li> <li><code>y_train = 0</code> if the microchip was rejected</li> </ul> </li> <li>Both <code>X_train</code> and <code>y_train</code> are numpy arrays.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#view-the-variables","title":"View the variables\u00b6","text":"<p>The code below prints the first five values of <code>X_train</code> and <code>y_train</code> and the type of the variables.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#check-the-dimensions-of-your-variables","title":"Check the dimensions of your variables\u00b6","text":"<p>Another useful way to get familiar with your data is to view its dimensions. Let's print the shape of <code>X_train</code> and <code>y_train</code> and see how many training examples we have in our dataset.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#visualize-your-data","title":"Visualize your data\u00b6","text":"<p>The helper function <code>plot_data</code> (from <code>utils.py</code>) is used to generate a figure like Figure 3, where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#33-feature-mapping","title":"3.3 Feature mapping\u00b6","text":"<p>One way to fit the data better is to create more features from each data point. In the provided function <code>map_feature</code>, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power.</p> <p>$$\\mathrm{map\\_feature}(x) =  \\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_1^2\\\\ x_1 x_2\\\\ x_2^2\\\\ x_1^3\\\\ \\vdots\\\\ x_1 x_2^5\\\\ x_2^6\\end{array}\\right]$$</p> <p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 27-dimensional vector.</p> <ul> <li>A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will be nonlinear when drawn in our 2-dimensional plot.</li> <li>We have provided the <code>map_feature</code> function for you in utils.py.</li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#34-cost-function-for-regularized-logistic-regression","title":"3.4 Cost function for regularized logistic regression\u00b6","text":"<p>In this part, you will implement the cost function for regularized logistic regression.</p> <p>Recall that for regularized logistic regression, the cost function is of the form $$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$</p> <p>Compare this to the cost function without regularization (which you implemented above), which is of the form</p> <p>$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$</p> <p>The difference is the regularization term, which is $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ Note that the $b$ parameter is not regularized.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-5","title":"Exercise 5\u00b6","text":"<p>Please complete the <code>compute_cost_reg</code> function below to calculate the following term for each element in $w$ $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$</p> <p>The starter code then adds this to the cost without regularization (which you computed above in <code>compute_cost</code>) to calculate the cost with regulatization.</p> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#35-gradient-for-regularized-logistic-regression","title":"3.5 Gradient for regularized logistic regression\u00b6","text":"<p>In this section, you will implement the gradient for regularized logistic regression.</p> <p>The gradient of the regularized cost function has two components. The first, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is a scalar, the other is a vector with the same shape as the parameters $\\mathbf{w}$, where the $j^\\mathrm{th}$ element is defined as follows:</p> <p>$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  $$</p> <p>$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$</p> <p>Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form $$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2} $$ $$ \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3} $$</p> <p>As you can see,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is the same, the difference is the following term in $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, which is $$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#exercise-6","title":"Exercise 6\u00b6","text":"<p>Please complete the <code>compute_gradient_reg</code> function below to modify the code below to calculate the following term</p> <p>$$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$</p> <p>The starter code will add this term to the $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ returned from <code>compute_gradient</code> above to get the gradient for the regularized cost function.</p> <p>If you get stuck, you can check out the hints presented after the cell below to help you with the implementation.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#36-learning-parameters-using-gradient-descent","title":"3.6 Learning parameters using gradient descent\u00b6","text":"<p>Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters $w$,$b$.</p> <ul> <li>If you have completed the cost and gradient for regularized logistic regression correctly, you should be able to step through the next cell to learn the parameters $w$.</li> <li>After training our parameters, we will use it to plot the decision boundary.</li> </ul> <p>Note</p> <p>The code block below takes quite a while to run, especially with a non-vectorized version. You can reduce the <code>iterations</code> to test your implementation and iterate faster. If you hae time, run for 100,000 iterations to see better results.</p>"},{"location":"MachineLearning/part3/Logistic_Regression/#37-plotting-the-decision-boundary","title":"3.7 Plotting the decision boundary\u00b6","text":"<p>To help you visualize the model learned by this classifier, we will use our <code>plot_decision_boundary</code> function which plots the (non-linear) decision boundary that separates the positive and negative examples.</p> <ul> <li><p>In the function, we plotted the non-linear decision boundary by computing the classifier\u2019s predictions on an evenly spaced grid and then drew a contour plot of where the predictions change from y = 0 to y = 1.</p> </li> <li><p>After learning the parameters $w$,$b$, the next step is to plot a decision boundary similar to Figure 4.</p> </li> </ul>"},{"location":"MachineLearning/part3/Logistic_Regression/#38-evaluating-regularized-logistic-regression-model","title":"3.8 Evaluating regularized logistic regression model\u00b6","text":"<p>You will use the <code>predict</code> function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set</p>"},{"location":"MachineLearning/part3/Overfitting_Soln/","title":"Ungraded Lab: Overfitting","text":"In\u00a0[2]: Copied! <pre>!pip install matplotlib\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom ipywidgets import Output\nfrom plt_overfit import overfit_example, output\nplt.style.use('./deeplearning.mplstyle')\n</pre> !pip install matplotlib %matplotlib widget import matplotlib.pyplot as plt from ipywidgets import Output from plt_overfit import overfit_example, output plt.style.use('./deeplearning.mplstyle') <pre>Collecting matplotlib\n  Downloading matplotlib-3.10.7-cp314-cp314-win_amd64.whl.metadata (11 kB)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Downloading contourpy-1.3.3-cp314-cp314-win_amd64.whl.metadata (5.5 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Downloading fonttools-4.61.0-cp314-cp314-win_amd64.whl.metadata (115 kB)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Downloading kiwisolver-1.4.9-cp314-cp314-win_amd64.whl.metadata (6.4 kB)\nCollecting numpy&gt;=1.23 (from matplotlib)\n  Downloading numpy-2.3.5-cp314-cp314-win_amd64.whl.metadata (60 kB)\nRequirement already satisfied: packaging&gt;=20.0 in d:\\anaconda3\\envs\\jupyter_env\\lib\\site-packages (from matplotlib) (25.0)\nCollecting pillow&gt;=8 (from matplotlib)\n  Downloading pillow-12.0.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)\nCollecting pyparsing&gt;=3 (from matplotlib)\n  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: python-dateutil&gt;=2.7 in d:\\anaconda3\\envs\\jupyter_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six&gt;=1.5 in d:\\anaconda3\\envs\\jupyter_env\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\nDownloading matplotlib-3.10.7-cp314-cp314-win_amd64.whl (8.3 MB)\n   ---------------------------------------- 0.0/8.3 MB ? eta -:--:--\n   - -------------------------------------- 0.3/8.3 MB ? eta -:--:--\n   -- ------------------------------------- 0.5/8.3 MB 1.1 MB/s eta 0:00:07\n   ----- ---------------------------------- 1.0/8.3 MB 1.9 MB/s eta 0:00:04\n   ------ --------------------------------- 1.3/8.3 MB 1.8 MB/s eta 0:00:04\n   -------- ------------------------------- 1.8/8.3 MB 1.8 MB/s eta 0:00:04\n   ---------- ----------------------------- 2.1/8.3 MB 1.8 MB/s eta 0:00:04\n   ----------- ---------------------------- 2.4/8.3 MB 1.6 MB/s eta 0:00:04\n   ------------ --------------------------- 2.6/8.3 MB 1.6 MB/s eta 0:00:04\n   ------------- -------------------------- 2.9/8.3 MB 1.6 MB/s eta 0:00:04\n   --------------- ------------------------ 3.1/8.3 MB 1.5 MB/s eta 0:00:04\n   --------------- ------------------------ 3.1/8.3 MB 1.5 MB/s eta 0:00:04\n   ---------------- ----------------------- 3.4/8.3 MB 1.4 MB/s eta 0:00:04\n   ----------------- ---------------------- 3.7/8.3 MB 1.4 MB/s eta 0:00:04\n   ------------------- -------------------- 3.9/8.3 MB 1.4 MB/s eta 0:00:04\n   -------------------- ------------------- 4.2/8.3 MB 1.4 MB/s eta 0:00:03\n   --------------------- ------------------ 4.5/8.3 MB 1.4 MB/s eta 0:00:03\n   ---------------------- ----------------- 4.7/8.3 MB 1.4 MB/s eta 0:00:03\n   ------------------------ --------------- 5.0/8.3 MB 1.3 MB/s eta 0:00:03\n   ------------------------- -------------- 5.2/8.3 MB 1.3 MB/s eta 0:00:03\n   -------------------------- ------------- 5.5/8.3 MB 1.3 MB/s eta 0:00:03\n   -------------------------- ------------- 5.5/8.3 MB 1.3 MB/s eta 0:00:03\n   --------------------------- ------------ 5.8/8.3 MB 1.3 MB/s eta 0:00:02\n   ----------------------------- ---------- 6.0/8.3 MB 1.3 MB/s eta 0:00:02\n   ----------------------------- ---------- 6.0/8.3 MB 1.3 MB/s eta 0:00:02\n   ------------------------------ --------- 6.3/8.3 MB 1.2 MB/s eta 0:00:02\n   ------------------------------- -------- 6.6/8.3 MB 1.2 MB/s eta 0:00:02\n   --------------------------------- ------ 6.8/8.3 MB 1.2 MB/s eta 0:00:02\n   ----------------------------------- ---- 7.3/8.3 MB 1.3 MB/s eta 0:00:01\n   ------------------------------------ --- 7.6/8.3 MB 1.3 MB/s eta 0:00:01\n   ---------------------------------------  8.1/8.3 MB 1.3 MB/s eta 0:00:01\n   ---------------------------------------  8.1/8.3 MB 1.3 MB/s eta 0:00:01\n   ---------------------------------------- 8.3/8.3 MB 1.3 MB/s  0:00:06\nDownloading contourpy-1.3.3-cp314-cp314-win_amd64.whl (232 kB)\nDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.61.0-cp314-cp314-win_amd64.whl (2.3 MB)\n   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n   --------- ------------------------------ 0.5/2.3 MB 1.6 MB/s eta 0:00:02\n   ------------- -------------------------- 0.8/2.3 MB 1.6 MB/s eta 0:00:01\n   ------------------ --------------------- 1.0/2.3 MB 1.5 MB/s eta 0:00:01\n   ---------------------- ----------------- 1.3/2.3 MB 1.4 MB/s eta 0:00:01\n   ------------------------------- -------- 1.8/2.3 MB 1.5 MB/s eta 0:00:01\n   ------------------------------------ --- 2.1/2.3 MB 1.5 MB/s eta 0:00:01\n   ------------------------------------ --- 2.1/2.3 MB 1.5 MB/s eta 0:00:01\n   ---------------------------------------- 2.3/2.3 MB 1.4 MB/s  0:00:01\nDownloading kiwisolver-1.4.9-cp314-cp314-win_amd64.whl (75 kB)\nDownloading numpy-2.3.5-cp314-cp314-win_amd64.whl (12.9 MB)\n   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n   - -------------------------------------- 0.5/12.9 MB 1.3 MB/s eta 0:00:10\n   -- ------------------------------------- 0.8/12.9 MB 1.1 MB/s eta 0:00:11\n   --- ------------------------------------ 1.0/12.9 MB 1.2 MB/s eta 0:00:11\n   --- ------------------------------------ 1.0/12.9 MB 1.2 MB/s eta 0:00:11\n   ---- ----------------------------------- 1.3/12.9 MB 1.1 MB/s eta 0:00:11\n   ---- ----------------------------------- 1.3/12.9 MB 1.1 MB/s eta 0:00:11\n   ---- ----------------------------------- 1.6/12.9 MB 951.2 kB/s eta 0:00:12\n   ----- ---------------------------------- 1.8/12.9 MB 985.4 kB/s eta 0:00:12\n   ------ --------------------------------- 2.1/12.9 MB 1.0 MB/s eta 0:00:11\n   ------- -------------------------------- 2.4/12.9 MB 968.3 kB/s eta 0:00:11\n   ------- -------------------------------- 2.4/12.9 MB 968.3 kB/s eta 0:00:11\n   -------- ------------------------------- 2.6/12.9 MB 915.5 kB/s eta 0:00:12\n   -------- ------------------------------- 2.6/12.9 MB 915.5 kB/s eta 0:00:12\n   -------- ------------------------------- 2.9/12.9 MB 875.9 kB/s eta 0:00:12\n   --------- ------------------------------ 3.1/12.9 MB 904.6 kB/s eta 0:00:11\n   --------- ------------------------------ 3.1/12.9 MB 904.6 kB/s eta 0:00:11\n   --------- ------------------------------ 3.1/12.9 MB 904.6 kB/s eta 0:00:11\n   ----------- ---------------------------- 3.7/12.9 MB 890.6 kB/s eta 0:00:11\n   ----------- ---------------------------- 3.7/12.9 MB 890.6 kB/s eta 0:00:11\n   ------------ --------------------------- 3.9/12.9 MB 884.8 kB/s eta 0:00:11\n   ------------ --------------------------- 3.9/12.9 MB 884.8 kB/s eta 0:00:11\n   ------------ --------------------------- 4.2/12.9 MB 876.0 kB/s eta 0:00:10\n   ------------- -------------------------- 4.5/12.9 MB 888.0 kB/s eta 0:00:10\n   -------------- ------------------------- 4.7/12.9 MB 886.4 kB/s eta 0:00:10\n   -------------- ------------------------- 4.7/12.9 MB 886.4 kB/s eta 0:00:10\n   -------------- ------------------------- 4.7/12.9 MB 886.4 kB/s eta 0:00:10\n   --------------- ------------------------ 5.0/12.9 MB 859.2 kB/s eta 0:00:10\n   ---------------- ----------------------- 5.2/12.9 MB 874.2 kB/s eta 0:00:09\n   ---------------- ----------------------- 5.2/12.9 MB 874.2 kB/s eta 0:00:09\n   ----------------- ---------------------- 5.5/12.9 MB 857.9 kB/s eta 0:00:09\n   ----------------- ---------------------- 5.5/12.9 MB 857.9 kB/s eta 0:00:09\n   ----------------- ---------------------- 5.8/12.9 MB 839.1 kB/s eta 0:00:09\n   ----------------- ---------------------- 5.8/12.9 MB 839.1 kB/s eta 0:00:09\n   ------------------ --------------------- 6.0/12.9 MB 823.3 kB/s eta 0:00:09\n   ------------------ --------------------- 6.0/12.9 MB 823.3 kB/s eta 0:00:09\n   ------------------- -------------------- 6.3/12.9 MB 807.6 kB/s eta 0:00:09\n   ------------------- -------------------- 6.3/12.9 MB 807.6 kB/s eta 0:00:09\n   ------------------- -------------------- 6.3/12.9 MB 807.6 kB/s eta 0:00:09\n   -------------------- ------------------- 6.6/12.9 MB 788.6 kB/s eta 0:00:09\n   -------------------- ------------------- 6.6/12.9 MB 788.6 kB/s eta 0:00:09\n   --------------------- ------------------ 6.8/12.9 MB 766.6 kB/s eta 0:00:08\n   --------------------- ------------------ 6.8/12.9 MB 766.6 kB/s eta 0:00:08\n   --------------------- ------------------ 6.8/12.9 MB 766.6 kB/s eta 0:00:08\n   --------------------- ------------------ 7.1/12.9 MB 751.4 kB/s eta 0:00:08\n   --------------------- ------------------ 7.1/12.9 MB 751.4 kB/s eta 0:00:08\n   ---------------------- ----------------- 7.3/12.9 MB 739.2 kB/s eta 0:00:08\n   ---------------------- ----------------- 7.3/12.9 MB 739.2 kB/s eta 0:00:08\n   ---------------------- ----------------- 7.3/12.9 MB 739.2 kB/s eta 0:00:08\n   ----------------------- ---------------- 7.6/12.9 MB 728.6 kB/s eta 0:00:08\n   ------------------------ --------------- 7.9/12.9 MB 728.4 kB/s eta 0:00:07\n   ------------------------ --------------- 7.9/12.9 MB 728.4 kB/s eta 0:00:07\n   ------------------------- -------------- 8.1/12.9 MB 730.2 kB/s eta 0:00:07\n   ------------------------- -------------- 8.4/12.9 MB 737.0 kB/s eta 0:00:07\n   ------------------------- -------------- 8.4/12.9 MB 737.0 kB/s eta 0:00:07\n   --------------------------- ------------ 8.9/12.9 MB 754.3 kB/s eta 0:00:06\n   ---------------------------- ----------- 9.2/12.9 MB 764.8 kB/s eta 0:00:05\n   ----------------------------- ---------- 9.4/12.9 MB 775.6 kB/s eta 0:00:05\n   ------------------------------ --------- 9.7/12.9 MB 787.0 kB/s eta 0:00:05\n   ------------------------------- -------- 10.2/12.9 MB 812.7 kB/s eta 0:00:04\n   --------------------------------- ------ 10.7/12.9 MB 840.2 kB/s eta 0:00:03\n   ---------------------------------- ----- 11.0/12.9 MB 852.6 kB/s eta 0:00:03\n   ---------------------------------- ----- 11.3/12.9 MB 853.6 kB/s eta 0:00:02\n   ----------------------------------- ---- 11.5/12.9 MB 864.6 kB/s eta 0:00:02\n   ------------------------------------ --- 11.8/12.9 MB 871.7 kB/s eta 0:00:02\n   ------------------------------------- -- 12.1/12.9 MB 874.9 kB/s eta 0:00:01\n   -------------------------------------- - 12.3/12.9 MB 876.5 kB/s eta 0:00:01\n   -------------------------------------- - 12.6/12.9 MB 887.0 kB/s eta 0:00:01\n   ---------------------------------------- 12.9/12.9 MB 893.3 kB/s  0:00:14\nDownloading pillow-12.0.0-cp314-cp314-win_amd64.whl (7.1 MB)\n   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n   - -------------------------------------- 0.3/7.1 MB ? eta -:--:--\n   ---- ----------------------------------- 0.8/7.1 MB 1.5 MB/s eta 0:00:05\n   ----- ---------------------------------- 1.0/7.1 MB 1.5 MB/s eta 0:00:05\n   ------- -------------------------------- 1.3/7.1 MB 1.4 MB/s eta 0:00:05\n   -------- ------------------------------- 1.6/7.1 MB 1.5 MB/s eta 0:00:04\n   ----------- ---------------------------- 2.1/7.1 MB 1.5 MB/s eta 0:00:04\n   ------------- -------------------------- 2.4/7.1 MB 1.5 MB/s eta 0:00:04\n   -------------- ------------------------- 2.6/7.1 MB 1.5 MB/s eta 0:00:03\n   ---------------- ----------------------- 2.9/7.1 MB 1.5 MB/s eta 0:00:03\n   ---------------- ----------------------- 2.9/7.1 MB 1.5 MB/s eta 0:00:03\n   ----------------- ---------------------- 3.1/7.1 MB 1.4 MB/s eta 0:00:03\n   ------------------- -------------------- 3.4/7.1 MB 1.4 MB/s eta 0:00:03\n   ---------------------- ----------------- 3.9/7.1 MB 1.4 MB/s eta 0:00:03\n   ---------------------- ----------------- 3.9/7.1 MB 1.4 MB/s eta 0:00:03\n   ----------------------- ---------------- 4.2/7.1 MB 1.4 MB/s eta 0:00:03\n   ------------------------ --------------- 4.5/7.1 MB 1.3 MB/s eta 0:00:03\n   -------------------------- ------------- 4.7/7.1 MB 1.3 MB/s eta 0:00:02\n   ----------------------------- ---------- 5.2/7.1 MB 1.4 MB/s eta 0:00:02\n   -------------------------------- ------- 5.8/7.1 MB 1.4 MB/s eta 0:00:01\n   ----------------------------------- ---- 6.3/7.1 MB 1.5 MB/s eta 0:00:01\n   ------------------------------------ --- 6.6/7.1 MB 1.5 MB/s eta 0:00:01\n   ---------------------------------------- 7.1/7.1 MB 1.5 MB/s  0:00:04\nDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\nInstalling collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n\n   ---------------------------------------- 0/8 [pyparsing]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ----- ---------------------------------- 1/8 [pillow]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   ---------- ----------------------------- 2/8 [numpy]\n   --------------- ------------------------ 3/8 [kiwisolver]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   -------------------- ------------------- 4/8 [fonttools]\n   ------------------------------ --------- 6/8 [contourpy]\n   ------------------------------ --------- 6/8 [contourpy]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ----------------------------------- ---- 7/8 [matplotlib]\n   ---------------------------------------- 8/8 [matplotlib]\n\nSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 numpy-2.3.5 pillow-12.0.0 pyparsing-3.2.5\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[2], line 2\n      1 get_ipython().system('pip install matplotlib')\n----&gt; 2 get_ipython().run_line_magic('matplotlib', 'widget')\n      3 import matplotlib.pyplot as plt\n      4 from ipywidgets import Output\n\nFile D:\\anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2511, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)\n   2509     kwargs['local_ns'] = self.get_local_scope(stack_depth)\n   2510 with self.builtin_trap:\n-&gt; 2511     result = fn(*args, **kwargs)\n   2513 # The code below prevents the output from being displayed\n   2514 # when using magics with decorator @output_can_be_silenced\n   2515 # when the last Python token in the expression is a ';'.\n   2516 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile D:\\anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\IPython\\core\\magics\\pylab.py:103, in PylabMagics.matplotlib(self, line)\n     98     print(\n     99         \"Available matplotlib backends: %s\"\n    100         % _list_matplotlib_backends_and_gui_loops()\n    101     )\n    102 else:\n--&gt; 103     gui, backend = self.shell.enable_matplotlib(args.gui)\n    104     self._show_matplotlib_backend(args.gui, backend)\n\nFile D:\\anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3801, in InteractiveShell.enable_matplotlib(self, gui)\n   3797         print('Warning: Cannot change to a different GUI toolkit: %s.'\n   3798                 ' Using %s instead.' % (gui, self.pylab_gui_select))\n   3799         gui, backend = pt.find_gui_and_backend(self.pylab_gui_select)\n-&gt; 3801 pt.activate_matplotlib(backend)\n   3803 from matplotlib_inline.backend_inline import configure_inline_support\n   3805 configure_inline_support(self, backend)\n\nFile D:\\anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\IPython\\core\\pylabtools.py:402, in activate_matplotlib(backend)\n    397 matplotlib.interactive(True)\n    399 # Matplotlib had a bug where even switch_backend could not force\n    400 # the rcParam to update. This needs to be set *before* the module\n    401 # magic of switch_backend().\n--&gt; 402 matplotlib.rcParams['backend'] = backend\n    404 # Due to circular imports, pyplot may be only partially initialised\n    405 # when this function runs.\n    406 # So avoid needing matplotlib attribute-lookup to access pyplot.\n    407 from matplotlib import pyplot as plt\n\nFile D:\\anaconda3\\envs\\jupyter_env\\Lib\\site-packages\\matplotlib\\__init__.py:774, in RcParams.__setitem__(self, key, val)\n    772         cval = self.validate[key](val)\n    773     except ValueError as ve:\n--&gt; 774         raise ValueError(f\"Key {key}: {ve}\") from None\n    775     self._set(key, cval)\n    776 except KeyError as err:\n\nValueError: Key backend: 'module://ipympl.backend_nbagg' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template', 'inline']</pre> In\u00a0[\u00a0]: Copied! <pre>plt.close(\"all\")\ndisplay(output)\nofit = overfit_example(False)\n</pre> plt.close(\"all\") display(output) ofit = overfit_example(False) <p>In the plot above you can:</p> <ul> <li>switch between Regression and Categorization examples</li> <li>add data</li> <li>select the degree of the model</li> <li>fit the model to the data</li> </ul> <p>Here are some things you should try:</p> <ul> <li>Fit the data with degree = 1; Note 'underfitting'.</li> <li>Fit the data with degree = 6; Note 'overfitting'</li> <li>tune degree to get the 'best fit'</li> <li>add data:<ul> <li>extreme examples can increase overfitting (assuming they are outliers).</li> <li>nominal examples can reduce overfitting</li> </ul> </li> <li>switch between <code>Regression</code> and <code>Categorical</code> to try both examples.</li> </ul> <p>To reset the plot, re-run the cell. Click slowly to allow the plot to update before receiving the next click.</p> <p>Notes on implementations:</p> <ul> <li>the 'ideal' curves represent the generator model to which noise was added to achieve the data set</li> <li>'fit' does not use pure gradient descent to improve speed. These methods can be used on smaller data sets.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Overfitting_Soln/#ungraded-lab-overfitting","title":"Ungraded Lab:  Overfitting\u00b6","text":""},{"location":"MachineLearning/part3/Overfitting_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will explore:</p> <ul> <li>the situations where overfitting can occur</li> <li>some of the solutions</li> </ul>"},{"location":"MachineLearning/part3/Overfitting_Soln/#overfitting","title":"Overfitting\u00b6","text":"<p>The week's lecture described situations where overfitting can arise. Run the cell below to generate a plot that will allow you to explore overfitting. There are further instructions below the cell.</p>"},{"location":"MachineLearning/part3/Overfitting_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have developed some intuition about the causes and solutions to overfitting. In the next lab, you will explore a commonly used solution, Regularization.</p>"},{"location":"MachineLearning/part3/Regularization_Soln/","title":"Optional Lab - Regularized Cost and Gradient","text":"In\u00a0[9]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_overfit import overfit_example, output\nfrom lab_utils_common import sigmoid\nnp.set_printoptions(precision=8)\n</pre>  import numpy as np %matplotlib widget import matplotlib.pyplot as plt from plt_overfit import overfit_example, output from lab_utils_common import sigmoid np.set_printoptions(precision=8) In\u00a0[10]: Copied! <pre>def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m  = X.shape[0]\n    n  = len(w)\n    cost = 0.\n    for i in range(m):\n        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n    cost = cost / (2 * m)                                              #scalar  \n \n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost                                                  #scalar\n</pre> def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):     \"\"\"     Computes the cost over all examples     Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns:       total_cost (scalar):  cost      \"\"\"      m  = X.shape[0]     n  = len(w)     cost = 0.     for i in range(m):         f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot         cost = cost + (f_wb_i - y[i])**2                               #scalar                  cost = cost / (2 * m)                                              #scalar         reg_cost = 0     for j in range(n):         reg_cost += (w[j]**2)                                          #scalar     reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar          total_cost = cost + reg_cost                                       #scalar     return total_cost                                                  #scalar <p>Run the cell below to see it in action.</p> In\u00a0[11]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,6) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5 b_tmp = 0.5 lambda_tmp = 0.7 cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(\"Regularized cost:\", cost_tmp) <pre>Regularized cost: 0.07917239320214275\n</pre> <p>Expected Output:</p> Regularized cost:  0.07917239320214275  In\u00a0[12]: Copied! <pre>def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m,n  = X.shape\n    cost = 0.\n    for i in range(m):\n        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n        f_wb_i = sigmoid(z_i)                                          #scalar\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n             \n    cost = cost/m                                                      #scalar\n\n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost                                                  #scalar\n</pre> def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):     \"\"\"     Computes the cost over all examples     Args:     Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns:       total_cost (scalar):  cost      \"\"\"      m,n  = X.shape     cost = 0.     for i in range(m):         z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot         f_wb_i = sigmoid(z_i)                                          #scalar         cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar                   cost = cost/m                                                      #scalar      reg_cost = 0     for j in range(n):         reg_cost += (w[j]**2)                                          #scalar     reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar          total_cost = cost + reg_cost                                       #scalar     return total_cost                                                  #scalar <p>Run the cell below to see it in action.</p> In\u00a0[13]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,6) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5 b_tmp = 0.5 lambda_tmp = 0.7 cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(\"Regularized cost:\", cost_tmp) <pre>Regularized cost: 0.6850849138741673\n</pre> <p>Expected Output:</p> Regularized cost:  0.6850849138741673  In\u00a0[14]: Copied! <pre>def compute_gradient_linear_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]                 \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m   \n    \n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw\n</pre> def compute_gradient_linear_reg(X, y, w, b, lambda_):      \"\"\"     Computes the gradient for linear regression      Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization            Returns:       dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape           #(number of examples, number of features)     dj_dw = np.zeros((n,))     dj_db = 0.      for i in range(m):                                      err = (np.dot(X[i], w) + b) - y[i]                          for j in range(n):                                      dj_dw[j] = dj_dw[j] + err * X[i, j]                        dj_db = dj_db + err                             dj_dw = dj_dw / m                                     dj_db = dj_db / m             for j in range(n):         dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]      return dj_db, dj_dw <p>Run the cell below to see it in action.</p> In\u00a0[15]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,3) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]) b_tmp = 0.5 lambda_tmp = 0.7 dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(f\"dj_db: {dj_db_tmp}\", ) print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", ) <pre>dj_db: 0.6648774569425727\nRegularized dj_dw:\n [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n</pre> <p>Expected Output</p> <pre><code>dj_db: 0.6648774569425726\nRegularized dj_dw:\n [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n</code></pre> In\u00a0[16]: Copied! <pre>def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns\n      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                            #(n,)\n    dj_db = 0.0                                       #scalar\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n\n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw  \n</pre> def compute_gradient_logistic_reg(X, y, w, b, lambda_):      \"\"\"     Computes the gradient for linear regression        Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns       dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape     dj_dw = np.zeros((n,))                            #(n,)     dj_db = 0.0                                       #scalar      for i in range(m):         f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar         err_i  = f_wb_i  - y[i]                       #scalar         for j in range(n):             dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar         dj_db = dj_db + err_i     dj_dw = dj_dw/m                                   #(n,)     dj_db = dj_db/m                                   #scalar      for j in range(n):         dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]      return dj_db, dj_dw    <p>Run the cell below to see it in action.</p> In\u00a0[17]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,3) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]) b_tmp = 0.5 lambda_tmp = 0.7 dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(f\"dj_db: {dj_db_tmp}\", ) print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", ) <pre>dj_db: 0.341798994972791\nRegularized dj_dw:\n [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n</pre> <p>Expected Output</p> <pre><code>dj_db: 0.341798994972791\nRegularized dj_dw:\n [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n</code></pre> In\u00a0[18]: Copied! <pre>plt.close(\"all\")\ndisplay(output)\nofit = overfit_example(True)\n</pre> plt.close(\"all\") display(output) ofit = overfit_example(True) <pre>Output()</pre>                      Figure                  <p>In the plot above, try out regularization on the previous example. In particular:</p> <ul> <li>Categorical (logistic regression)<ul> <li>set degree to 6, lambda to 0 (no regularization), fit the data</li> <li>now set lambda to 1 (increase regularization), fit the data, notice the difference.</li> </ul> </li> <li>Regression (linear regression)<ul> <li>try the same procedure.</li> </ul> </li> </ul> In\u00a0[10]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Regularization_Soln/#optional-lab-regularized-cost-and-gradient","title":"Optional Lab - Regularized Cost and Gradient\u00b6","text":""},{"location":"MachineLearning/part3/Regularization_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>extend the previous linear and logistic cost functions with a regularization term.</li> <li>rerun the previous example of over-fitting with a regularization term added.</li> </ul>"},{"location":"MachineLearning/part3/Regularization_Soln/#adding-regularization","title":"Adding regularization\u00b6","text":"<p>The slides above show the cost and gradient functions for both linear and logistic regression. Note:</p> <ul> <li>Cost<ul> <li>The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.</li> </ul> </li> <li>Gradient<ul> <li>The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of $f_{wb}$.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/Regularization_Soln/#cost-functions-with-regularization","title":"Cost functions with regularization\u00b6","text":""},{"location":"MachineLearning/part3/Regularization_Soln/#cost-function-for-regularized-linear-regression","title":"Cost function for regularized linear regression\u00b6","text":"<p>The equation for the cost function regularized linear regression is: $$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ where: $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$</p> <p>Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:</p> <p>$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$</p> <p>The difference is the regularization term,   $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </p> <p>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.</p> <p>Below is an implementation of equations (1) and (2). Note that this uses a standard pattern for this course,   a <code>for loop</code> over all <code>m</code> examples.</p>"},{"location":"MachineLearning/part3/Regularization_Soln/#cost-function-for-regularized-logistic-regression","title":"Cost function for regularized logistic regression\u00b6","text":"<p>For regularized logistic regression, the cost function is of the form $$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$ where: $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$</p> <p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$</p> <p>As was the case in linear regression above, the difference is the regularization term, which is     $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </p> <p>Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.</p>"},{"location":"MachineLearning/part3/Regularization_Soln/#gradient-descent-with-regularization","title":"Gradient descent with regularization\u00b6","text":"<p>The basic algorithm for running gradient descent does not change with regularization, it is: $$\\begin{align*} &amp;\\text{repeat until convergence:} \\; \\lbrace \\\\ &amp;  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; &amp; \\text{for j := 0..n-1} \\\\  &amp;  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\ &amp;\\rbrace \\end{align*}$$ Where each iteration performs simultaneous updates on $w_j$ for all $j$.</p> <p>What changes with regularization is computing the gradients.</p>"},{"location":"MachineLearning/part3/Regularization_Soln/#computing-the-gradient-with-regularization-both-linearlogistic","title":"Computing the Gradient with regularization (both linear/logistic)\u00b6","text":"<p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$. $$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}  \\end{align*}$$</p> <ul> <li><p>m is the number of training examples in the data set</p> </li> <li><p>$f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target</p> </li> <li><p>For a   linear  regression model $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$</p> </li> <li><p>For a  logistic  regression model $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ $f_{\\mathbf{w},b}(x) = g(z)$ where $g(z)$ is the sigmoid function: $g(z) = \\frac{1}{1+e^{-z}}$</p> </li> </ul> <p>The term which adds regularization is  the $\\frac{\\lambda}{m} w_j $.</p>"},{"location":"MachineLearning/part3/Regularization_Soln/#gradient-function-for-regularized-linear-regression","title":"Gradient function for regularized linear regression\u00b6","text":""},{"location":"MachineLearning/part3/Regularization_Soln/#gradient-function-for-regularized-logistic-regression","title":"Gradient function for regularized logistic regression\u00b6","text":""},{"location":"MachineLearning/part3/Regularization_Soln/#rerun-over-fitting-example","title":"Rerun over-fitting example\u00b6","text":""},{"location":"MachineLearning/part3/Regularization_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have:</p> <ul> <li>examples of cost and gradient routines with regularization added for both linear and logistic regression</li> <li>developed some intuition on how regularization can reduce over-fitting</li> </ul>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/","title":"Ungraded Lab: Logistic Regression using Scikit-Learn","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\nX = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny = np.array([0, 0, 0, 1, 1, 1])\n</pre> import numpy as np  X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]]) y = np.array([0, 0, 0, 1, 1, 1]) In\u00a0[3]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(X, y)\n</pre> from sklearn.linear_model import LogisticRegression  lr_model = LogisticRegression() lr_model.fit(X, y) Out[3]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre> In\u00a0[4]: Copied! <pre>y_pred = lr_model.predict(X)\n\nprint(\"Prediction on training set:\", y_pred)\n</pre> y_pred = lr_model.predict(X)  print(\"Prediction on training set:\", y_pred) <pre>Prediction on training set: [0 0 0 1 1 1]\n</pre> In\u00a0[5]: Copied! <pre>print(\"Accuracy on training set:\", lr_model.score(X, y))\n</pre> print(\"Accuracy on training set:\", lr_model.score(X, y)) <pre>Accuracy on training set: 1.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#ungraded-lab-logistic-regression-using-scikit-learn","title":"Ungraded Lab:  Logistic Regression using Scikit-Learn\u00b6","text":""},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#goals","title":"Goals\u00b6","text":"<p>In this lab you will:</p> <ul> <li>Train a logistic regression model using scikit-learn.</li> </ul>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#dataset","title":"Dataset\u00b6","text":"<p>Let's start with the same dataset as before.</p>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#fit-the-model","title":"Fit the model\u00b6","text":"<p>The code below imports the logistic regression model from scikit-learn. You can fit this model on the training data by calling <code>fit</code> function.</p>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#make-predictions","title":"Make Predictions\u00b6","text":"<p>You can see the predictions made by this model by calling the <code>predict</code> function.</p>"},{"location":"MachineLearning/part3/Scikit_Learn_Soln/#calculate-accuracy","title":"Calculate accuracy\u00b6","text":"<p>You can calculate this accuracy of this model by calling the <code>score</code> function.</p>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/","title":"Optional Lab: Logistic Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_one_addpt_onclick import plt_one_addpt_onclick\nfrom lab_utils_common import draw_vthresh\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from plt_one_addpt_onclick import plt_one_addpt_onclick from lab_utils_common import draw_vthresh plt.style.use('./deeplearning.mplstyle') <p>NumPy has a function called <code>exp()</code>, which offers a convenient way to calculate the exponential ( $e^{z}$) of all elements in the input array (<code>z</code>).</p> <p>It also works with a single number as an input, as shown below.</p> In\u00a0[2]: Copied! <pre># Input is an array. \ninput_array = np.array([1,2,3])\nexp_array = np.exp(input_array)\n\nprint(\"Input to exp:\", input_array)\nprint(\"Output of exp:\", exp_array)\n\n# Input is a single number\ninput_val = 1  \nexp_val = np.exp(input_val)\n\nprint(\"Input to exp:\", input_val)\nprint(\"Output of exp:\", exp_val)\n</pre> # Input is an array.  input_array = np.array([1,2,3]) exp_array = np.exp(input_array)  print(\"Input to exp:\", input_array) print(\"Output of exp:\", exp_array)  # Input is a single number input_val = 1   exp_val = np.exp(input_val)  print(\"Input to exp:\", input_val) print(\"Output of exp:\", exp_val) <pre>Input to exp: [1 2 3]\nOutput of exp: [ 2.72  7.39 20.09]\nInput to exp: 1\nOutput of exp: 2.718281828459045\n</pre> <p>The <code>sigmoid</code> function is implemented in python as shown in the cell below.</p> In\u00a0[3]: Copied! <pre>def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Args:\n        z (ndarray): A scalar, numpy array of any size.\n\n    Returns:\n        g (ndarray): sigmoid(z), with the same shape as z\n         \n    \"\"\"\n\n    g = 1/(1+np.exp(-z))\n   \n    return g\n</pre> def sigmoid(z):     \"\"\"     Compute the sigmoid of z      Args:         z (ndarray): A scalar, numpy array of any size.      Returns:         g (ndarray): sigmoid(z), with the same shape as z               \"\"\"      g = 1/(1+np.exp(-z))         return g <p>Let's see what the output of this function is for various value of <code>z</code></p> In\u00a0[4]: Copied! <pre># Generate an array of evenly spaced values between -10 and 10\nz_tmp = np.arange(-10,11)\n\n# Use the function implemented above to get the sigmoid values\ny = sigmoid(z_tmp)\n\n# Code for pretty printing the two arrays next to each other\nnp.set_printoptions(precision=3) \nprint(\"Input (z), Output (sigmoid(z))\")\nprint(np.c_[z_tmp, y])\n</pre> # Generate an array of evenly spaced values between -10 and 10 z_tmp = np.arange(-10,11)  # Use the function implemented above to get the sigmoid values y = sigmoid(z_tmp)  # Code for pretty printing the two arrays next to each other np.set_printoptions(precision=3)  print(\"Input (z), Output (sigmoid(z))\") print(np.c_[z_tmp, y]) <pre>Input (z), Output (sigmoid(z))\n[[-1.000e+01  4.540e-05]\n [-9.000e+00  1.234e-04]\n [-8.000e+00  3.354e-04]\n [-7.000e+00  9.111e-04]\n [-6.000e+00  2.473e-03]\n [-5.000e+00  6.693e-03]\n [-4.000e+00  1.799e-02]\n [-3.000e+00  4.743e-02]\n [-2.000e+00  1.192e-01]\n [-1.000e+00  2.689e-01]\n [ 0.000e+00  5.000e-01]\n [ 1.000e+00  7.311e-01]\n [ 2.000e+00  8.808e-01]\n [ 3.000e+00  9.526e-01]\n [ 4.000e+00  9.820e-01]\n [ 5.000e+00  9.933e-01]\n [ 6.000e+00  9.975e-01]\n [ 7.000e+00  9.991e-01]\n [ 8.000e+00  9.997e-01]\n [ 9.000e+00  9.999e-01]\n [ 1.000e+01  1.000e+00]]\n</pre> <p>The values in the left column are <code>z</code>, and the values in the right column are <code>sigmoid(z)</code>. As you can see, the input values to the sigmoid range from -10 to 10, and the output values range from 0 to 1.</p> <p>Now, let's try to plot this function using the <code>matplotlib</code> library.</p> In\u00a0[5]: Copied! <pre># Plot z vs sigmoid(z)\nfig,ax = plt.subplots(1,1,figsize=(5,3))\nax.plot(z_tmp, y, c=\"b\")\n\nax.set_title(\"Sigmoid function\")\nax.set_ylabel('sigmoid(z)')\nax.set_xlabel('z')\ndraw_vthresh(ax,0)\n</pre> # Plot z vs sigmoid(z) fig,ax = plt.subplots(1,1,figsize=(5,3)) ax.plot(z_tmp, y, c=\"b\")  ax.set_title(\"Sigmoid function\") ax.set_ylabel('sigmoid(z)') ax.set_xlabel('z') draw_vthresh(ax,0)                      Figure                  <p>As you can see, the sigmoid function approaches  <code>0</code> as <code>z</code> goes to large negative values and approaches <code>1</code> as <code>z</code> goes to large positive values.</p> <p>Let's apply logistic regression to the categorical data example of tumor classification. First, load the examples and initial values for the parameters.</p> In\u00a0[6]: Copied! <pre>x_train = np.array([0., 1, 2, 3, 4, 5])\ny_train = np.array([0,  0, 0, 1, 1, 1])\n\nw_in = np.zeros((1))\nb_in = 0\n</pre> x_train = np.array([0., 1, 2, 3, 4, 5]) y_train = np.array([0,  0, 0, 1, 1, 1])  w_in = np.zeros((1)) b_in = 0 <p>Try the following steps:</p> <ul> <li>Click on 'Run Logistic Regression' to find the best logistic regression model for the given training data<ul> <li>Note the resulting model fits the data quite well.</li> <li>Note, the orange line is '$z$' or $\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$  above. It does not match the line in a linear regression model. Further improve these results by applying a threshold.</li> </ul> </li> <li>Tick the box on the 'Toggle 0.5 threshold' to show the predictions if a threshold is applied.<ul> <li>These predictions look good. The predictions match the data</li> <li>Now, add further data points in the large tumor size range (near 10), and re-run logistic regression.</li> <li>unlike the linear regression model, this model continues to make correct predictions</li> </ul> </li> </ul> In\u00a0[7]: Copied! <pre>plt.close('all') \naddpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=True)\n</pre> plt.close('all')  addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=True) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[7], line 2\n      1 plt.close('all') \n----&gt; 2 addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=True)\n\nFile D:\\AI\\Machine-Learning-Specialization-Coursera\\C1 - Supervised Machine Learning - Regression and Classification\\week3\\Optional Labs\\plt_one_addpt_onclick.py:66, in plt_one_addpt_onclick.__init__(self, x, y, w, b, logistic)\n     64 self.bthresh = CheckButtons(axthresh, ('Toggle 0.5 threshold (after regression)',))\n     65 self.bthresh.on_clicked(self.thresh)\n---&gt; 66 self.resize_sq(self.bthresh)\n\nFile D:\\AI\\Machine-Learning-Specialization-Coursera\\C1 - Supervised Machine Learning - Regression and Classification\\week3\\Optional Labs\\plt_one_addpt_onclick.py:179, in plt_one_addpt_onclick.resize_sq(self, bcid)\n    171 \"\"\" resizes the check box \"\"\"\n    172 #future reference\n    173 #print(f\"width  : {bcid.rectangles[0].get_width()}\")\n    174 #print(f\"height : {bcid.rectangles[0].get_height()}\")\n    175 #print(f\"xy     : {bcid.rectangles[0].get_xy()}\")\n    176 #print(f\"bb     : {bcid.rectangles[0].get_bbox()}\")\n    177 #print(f\"points : {bcid.rectangles[0].get_bbox().get_points()}\")  #[[xmin,ymin],[xmax,ymax]]\n--&gt; 179 h = bcid.rectangles[0].get_height()\n    180 bcid.rectangles[0].set_height(3*h)\n    182 ymax = bcid.rectangles[0].get_bbox().y1\n\nAttributeError: 'CheckButtons' object has no attribute 'rectangles'</pre>                      Figure                  In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/#optional-lab-logistic-regression","title":"Optional Lab: Logistic Regression\u00b6","text":"<p>In this ungraded lab, you will</p> <ul> <li>explore the sigmoid function (also known as the logistic function)</li> <li>explore logistic regression; which uses the sigmoid function</li> </ul>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/#sigmoid-or-logistic-function","title":"Sigmoid or Logistic Function\u00b6","text":"<p>As discussed in the lecture videos, for a classification task, we can start by using our linear regression model, $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$, to predict $y$ given $x$.</p> <ul> <li>However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.</li> <li>This can be accomplished by using a \"sigmoid function\" which maps all input values to values between 0 and 1.</li> </ul> <p>Let's implement the sigmoid function and see this for ourselves.</p>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/#formula-for-sigmoid-function","title":"Formula for Sigmoid function\u00b6","text":"<p>The formula for a sigmoid function is as follows -</p> <p>$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$</p> <p>In the case of logistic regression, z (the input to the sigmoid function), is the output of a linear regression model.</p> <ul> <li>In the case of a single example, $z$ is scalar.</li> <li>in the case of multiple examples, $z$ may be a vector consisting of $m$ values, one for each example.</li> <li>The implementation of the sigmoid function should cover both of these potential input formats. Let's implement this in Python.</li> </ul>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/#logistic-regression","title":"Logistic Regression\u00b6","text":"<p> A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:</p> <p>$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} $$</p> <p>where</p> <p>$g(z) = \\frac{1}{1+e^{-z}}\\tag{3}$</p>"},{"location":"MachineLearning/part3/Sigmoid_function_Soln/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have explored the use of the sigmoid function in logistic regression.</p>"},{"location":"MachineLearning/part3/lab_utils_common/","title":"Lab utils common","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>lab_utils_common contains common routines and variable definitions used by all the labs in this week. by contrast, specific, large plotting routines will be in separate files and are generally imported into the week where they are used. those files will import this file</p> In\u00a0[\u00a0]: Copied! <pre>import copy\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch\nfrom ipywidgets import Output\n\nnp.set_printoptions(precision=2)\n\ndlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')\ndlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'\ndlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\nplt.style.use('./deeplearning.mplstyle')\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Parameters\n    ----------\n    z : array_like\n        A scalar or numpy array of any size.\n\n    Returns\n    -------\n     g : array_like\n         sigmoid(z)\n    \"\"\"\n    z = np.clip( z, -500, 500 )           # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n\n    return g\n</pre> import copy import math import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import FancyArrowPatch from ipywidgets import Output  np.set_printoptions(precision=2)  dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0') dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0' dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple] plt.style.use('./deeplearning.mplstyle')  def sigmoid(z):     \"\"\"     Compute the sigmoid of z      Parameters     ----------     z : array_like         A scalar or numpy array of any size.      Returns     -------      g : array_like          sigmoid(z)     \"\"\"     z = np.clip( z, -500, 500 )           # protect against overflow     g = 1.0/(1.0+np.exp(-z))      return g <p>Regression Routines #########################################################</p> In\u00a0[\u00a0]: Copied! <pre>def predict_logistic(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return sigmoid(X @ w + b)\n\ndef predict_linear(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return X @ w + b\n\ndef compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n    \"\"\"\n    Computes cost using logistic loss, non-matrix version\n\n    Args:\n      X (ndarray): Shape (m,n)  matrix of examples with n features\n      y (ndarray): Shape (m,)   target values\n      w (ndarray): Shape (n,)   parameters for prediction\n      b (scalar):               parameter  for prediction\n      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n      safe : (boolean)          True-selects under/overflow safe algorithm\n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m,n = X.shape\n    cost = 0.0\n    for i in range(m):\n        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n        if safe:  #avoids overflows\n            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n        else:\n            f_wb_i = sigmoid(z_i)                                                   #(n,)\n            cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n    cost = cost/m\n\n    reg_cost = 0\n    if lambda_ != 0:\n        for j in range(n):\n            reg_cost += (w[j]**2)                                               # scalar\n        reg_cost = (lambda_/(2*m))*reg_cost\n\n    return cost + reg_cost\n\n\ndef log_1pexp(x, maximum=20):\n    ''' approximate log(1+exp^x)\n        https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice\n    Args:\n    x   : (ndarray Shape (n,1) or (n,)  input\n    out : (ndarray Shape matches x      output ~= np.log(1+exp(x))\n    '''\n\n    out  = np.zeros_like(x,dtype=float)\n    i    = x &lt;= maximum\n    ni   = np.logical_not(i)\n\n    out[i]  = np.log(1 + np.exp(x[i]))\n    out[ni] = x[ni]\n    return out\n\n\ndef compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):\n    \"\"\"\n    Computes the cost using  using matrices\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model\n      b : (scalar )                       Values of parameter of the model\n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns:\n      total_cost: (scalar)                cost\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n    if logistic:\n        if safe:  #safe from overflow\n            z = X @ w + b                                                           #(m,n)(n,1)=(m,1)\n            cost = -(y * z) + log_1pexp(z)\n            cost = np.sum(cost)/m                                                   # (scalar)\n        else:\n            f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)\n            cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)\n            cost = cost[0,0]                                                        # scalar\n    else:\n        f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)\n        cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar\n\n    reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar\n\n    total_cost = cost + reg_cost                                                # scalar\n\n    return total_cost                                                           # scalar\n\ndef compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):\n    \"\"\"\n    Computes the gradient using matrices\n\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model\n      b : (scalar )                       Values of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n    Returns\n      dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w\n      dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n\n    f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)\n    err   = f_wb - y                                              # (m,1)\n    dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)\n    dj_db = (1/m) * np.sum(err)                                   # scalar\n\n    dj_dw += (lambda_/m) * w        # regularize                  # (n,1)\n\n    return dj_db, dj_dw                                           # scalar, (n,1)\n\ndef gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True):\n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking\n    num_iters gradient steps with learning rate alpha\n\n    Args:\n      X (ndarray):    Shape (m,n)         matrix of examples\n      y (ndarray):    Shape (m,) or (m,1) target value of each example\n      w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model\n      b_in (scalar):                      Initial value of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n      alpha (float):                      Learning rate\n      num_iters (int):                    number of iterations to run gradient descent\n\n    Returns:\n      w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape\n      b (scalar):                         Updated value of parameter\n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    w = w.reshape(-1,1)      #prep for matrix operations\n    y = y.reshape(-1,1)\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion\n            J_history.append( compute_cost_matrix(X, y, w, b, logistic, lambda_) )\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            if verbose: print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n\n    return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing\n\ndef zscore_normalize_features(X):\n    \"\"\"\n    computes  X, zcore normalized by column\n\n    Args:\n      X (ndarray): Shape (m,n) input data, m examples, n features\n\n    Returns:\n      X_norm (ndarray): Shape (m,n)  input normalized by column\n      mu (ndarray):     Shape (n,)   mean of each feature\n      sigma (ndarray):  Shape (n,)   standard deviation of each feature\n    \"\"\"\n    # find the mean of each column/feature\n    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n    # find the standard deviation of each column/feature\n    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n    # element-wise, subtract mu for that column from each example, divide by std for that column\n    X_norm = (X - mu) / sigma\n\n    return X_norm, mu, sigma\n\n#check our work\n#from sklearn.preprocessing import scale\n#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n</pre> def predict_logistic(X, w, b):     \"\"\" performs prediction \"\"\"     return sigmoid(X @ w + b)  def predict_linear(X, w, b):     \"\"\" performs prediction \"\"\"     return X @ w + b  def compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):     \"\"\"     Computes cost using logistic loss, non-matrix version      Args:       X (ndarray): Shape (m,n)  matrix of examples with n features       y (ndarray): Shape (m,)   target values       w (ndarray): Shape (n,)   parameters for prediction       b (scalar):               parameter  for prediction       lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization       safe : (boolean)          True-selects under/overflow safe algorithm     Returns:       cost (scalar): cost     \"\"\"      m,n = X.shape     cost = 0.0     for i in range(m):         z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()         if safe:  #avoids overflows             cost += -(y[i] * z_i ) + log_1pexp(z_i)         else:             f_wb_i = sigmoid(z_i)                                                   #(n,)             cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar     cost = cost/m      reg_cost = 0     if lambda_ != 0:         for j in range(n):             reg_cost += (w[j]**2)                                               # scalar         reg_cost = (lambda_/(2*m))*reg_cost      return cost + reg_cost   def log_1pexp(x, maximum=20):     ''' approximate log(1+exp^x)         https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice     Args:     x   : (ndarray Shape (n,1) or (n,)  input     out : (ndarray Shape matches x      output ~= np.log(1+exp(x))     '''      out  = np.zeros_like(x,dtype=float)     i    = x &lt;= maximum     ni   = np.logical_not(i)      out[i]  = np.log(1 + np.exp(x[i]))     out[ni] = x[ni]     return out   def compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):     \"\"\"     Computes the cost using  using matrices     Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model       b : (scalar )                       Values of parameter of the model       verbose : (Boolean) If true, print out intermediate value f_wb     Returns:       total_cost: (scalar)                cost     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D     if logistic:         if safe:  #safe from overflow             z = X @ w + b                                                           #(m,n)(n,1)=(m,1)             cost = -(y * z) + log_1pexp(z)             cost = np.sum(cost)/m                                                   # (scalar)         else:             f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)             cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)             cost = cost[0,0]                                                        # scalar     else:         f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)         cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar      reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar      total_cost = cost + reg_cost                                                # scalar      return total_cost                                                           # scalar  def compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):     \"\"\"     Computes the gradient using matrices      Args:       X : (ndarray, Shape (m,n))          matrix of examples       y : (ndarray  Shape (m,) or (m,1))  target value of each example       w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model       b : (scalar )                       Values of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero     Returns       dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w       dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b     \"\"\"     m = X.shape[0]     y = y.reshape(-1,1)             # ensure 2D     w = w.reshape(-1,1)             # ensure 2D      f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)     err   = f_wb - y                                              # (m,1)     dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)     dj_db = (1/m) * np.sum(err)                                   # scalar      dj_dw += (lambda_/m) * w        # regularize                  # (n,1)      return dj_db, dj_dw                                           # scalar, (n,1)  def gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True):     \"\"\"     Performs batch gradient descent to learn theta. Updates theta by taking     num_iters gradient steps with learning rate alpha      Args:       X (ndarray):    Shape (m,n)         matrix of examples       y (ndarray):    Shape (m,) or (m,1) target value of each example       w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model       b_in (scalar):                      Initial value of parameter of the model       logistic: (boolean)                 linear if false, logistic if true       lambda_:  (float)                   applies regularization if non-zero       alpha (float):                      Learning rate       num_iters (int):                    number of iterations to run gradient descent      Returns:       w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape       b (scalar):                         Updated value of parameter     \"\"\"     # An array to store cost J and w's at each iteration primarily for graphing later     J_history = []     w = copy.deepcopy(w_in)  #avoid modifying global w within function     b = b_in     w = w.reshape(-1,1)      #prep for matrix operations     y = y.reshape(-1,1)      for i in range(num_iters):          # Calculate the gradient and update the parameters         dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)          # Update Parameters using w, b, alpha and gradient         w = w - alpha * dj_dw         b = b - alpha * dj_db          # Save cost J at each iteration         if i&lt;100000:      # prevent resource exhaustion             J_history.append( compute_cost_matrix(X, y, w, b, logistic, lambda_) )          # Print cost every at intervals 10 times or as many iterations if &lt; 10         if i% math.ceil(num_iters / 10) == 0:             if verbose: print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")      return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing  def zscore_normalize_features(X):     \"\"\"     computes  X, zcore normalized by column      Args:       X (ndarray): Shape (m,n) input data, m examples, n features      Returns:       X_norm (ndarray): Shape (m,n)  input normalized by column       mu (ndarray):     Shape (n,)   mean of each feature       sigma (ndarray):  Shape (n,)   standard deviation of each feature     \"\"\"     # find the mean of each column/feature     mu     = np.mean(X, axis=0)                 # mu will have shape (n,)     # find the standard deviation of each column/feature     sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)     # element-wise, subtract mu for that column from each example, divide by std for that column     X_norm = (X - mu) / sigma      return X_norm, mu, sigma  #check our work #from sklearn.preprocessing import scale #scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True) <p>Common Plotting Routines #####################################################</p> In\u00a0[\u00a0]: Copied! <pre>def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n    \"\"\" plots logistic data with two axis \"\"\"\n    # Find Indices of Positive and Negative Examples\n    pos = y == 1\n    neg = y == 0\n    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n    neg = neg.reshape(-1,)\n\n    # Plot examples\n    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n    ax.legend(loc=loc)\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\ndef plt_tumor_data(x, y, ax):\n    \"\"\" plots tumor data on one axis \"\"\"\n    pos = y == 1\n    neg = y == 0\n\n    ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n    ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n    ax.set_ylim(-0.175,1.1)\n    ax.set_ylabel('y')\n    ax.set_xlabel('Tumor Size')\n    ax.set_title(\"Logistic Regression on Categorical Data\")\n\n    ax.figure.canvas.toolbar_visible = False\n    ax.figure.canvas.header_visible = False\n    ax.figure.canvas.footer_visible = False\n\n# Draws a threshold at 0.5\ndef draw_vthresh(ax,x):\n    \"\"\" draws a threshold \"\"\"\n    ylim = ax.get_ylim()\n    xlim = ax.get_xlim()\n    ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)\n    ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)\n    ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',\n                xytext=[30,5],textcoords='offset points')\n    d = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(d)\n    ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',\n                 xytext=[-50,5],textcoords='offset points', ha='left')\n    f = FancyArrowPatch(\n        posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,\n        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n    )\n    ax.add_artist(f)\n</pre>  def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):     \"\"\" plots logistic data with two axis \"\"\"     # Find Indices of Positive and Negative Examples     pos = y == 1     neg = y == 0     pos = pos.reshape(-1,)  #work with 1D or 1D y vectors     neg = neg.reshape(-1,)      # Plot examples     ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)     ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)     ax.legend(loc=loc)      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  def plt_tumor_data(x, y, ax):     \"\"\" plots tumor data on one axis \"\"\"     pos = y == 1     neg = y == 0      ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")     ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)     ax.set_ylim(-0.175,1.1)     ax.set_ylabel('y')     ax.set_xlabel('Tumor Size')     ax.set_title(\"Logistic Regression on Categorical Data\")      ax.figure.canvas.toolbar_visible = False     ax.figure.canvas.header_visible = False     ax.figure.canvas.footer_visible = False  # Draws a threshold at 0.5 def draw_vthresh(ax,x):     \"\"\" draws a threshold \"\"\"     ylim = ax.get_ylim()     xlim = ax.get_xlim()     ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)     ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)     ax.annotate(\"z &gt;= 0\", xy= [x,0.5], xycoords='data',                 xytext=[30,5],textcoords='offset points')     d = FancyArrowPatch(         posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(d)     ax.annotate(\"z &lt; 0\", xy= [x,0.5], xycoords='data',                  xytext=[-50,5],textcoords='offset points', ha='left')     f = FancyArrowPatch(         posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,         arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',     )     ax.add_artist(f)"},{"location":"MachineLearning/part3/plt_logistic_loss/","title":"Plt logistic loss","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"----------------------------------------------------------------\n logistic_loss plotting routines and support\n\"\"\"\n</pre> \"\"\"----------------------------------------------------------------  logistic_loss plotting routines and support \"\"\" In\u00a0[\u00a0]: Copied! <pre>from matplotlib import cm\nfrom lab_utils_common import sigmoid, dlblue, dlorange, np, plt, compute_cost_matrix\n</pre> from matplotlib import cm from lab_utils_common import sigmoid, dlblue, dlorange, np, plt, compute_cost_matrix In\u00a0[\u00a0]: Copied! <pre>def compute_cost_logistic_sq_err(X, y, w, b):\n    \"\"\"\n    compute sq error cost on logicist data (for negative example only, not used in practice)\n    Args:\n      X (ndarray): Shape (m,n) matrix of examples with multiple features\n      w (ndarray): Shape (n)   parameters for prediction\n      b (scalar):              parameter  for prediction\n    Returns:\n      cost (scalar): cost\n    \"\"\"\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):\n        z_i = np.dot(X[i],w) + b\n        f_wb_i = sigmoid(z_i)                 #add sigmoid to normal sq error cost for linear regression\n        cost = cost + (f_wb_i - y[i])**2\n    cost = cost / (2 * m)\n    return np.squeeze(cost)\n</pre> def compute_cost_logistic_sq_err(X, y, w, b):     \"\"\"     compute sq error cost on logicist data (for negative example only, not used in practice)     Args:       X (ndarray): Shape (m,n) matrix of examples with multiple features       w (ndarray): Shape (n)   parameters for prediction       b (scalar):              parameter  for prediction     Returns:       cost (scalar): cost     \"\"\"     m = X.shape[0]     cost = 0.0     for i in range(m):         z_i = np.dot(X[i],w) + b         f_wb_i = sigmoid(z_i)                 #add sigmoid to normal sq error cost for linear regression         cost = cost + (f_wb_i - y[i])**2     cost = cost / (2 * m)     return np.squeeze(cost) In\u00a0[\u00a0]: Copied! <pre>def plt_logistic_squared_error(X,y):\n    \"\"\" plots logistic squared error for demonstration \"\"\"\n    wx, by = np.meshgrid(np.linspace(-6,12,50),\n                         np.linspace(10, -20, 40))\n    points = np.c_[wx.ravel(), by.ravel()]\n    cost = np.zeros(points.shape[0])\n\n    for i in range(points.shape[0]):\n        w,b = points[i]\n        cost[i] = compute_cost_logistic_sq_err(X.reshape(-1,1), y, w, b)\n    cost = cost.reshape(wx.shape)\n\n    fig = plt.figure()\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n    ax = fig.add_subplot(1, 1, 1, projection='3d')\n    ax.plot_surface(wx, by, cost, alpha=0.6,cmap=cm.jet,)\n\n    ax.set_xlabel('w', fontsize=16)\n    ax.set_ylabel('b', fontsize=16)\n    ax.set_zlabel(\"Cost\", rotation=90, fontsize=16)\n    ax.set_title('\"Logistic\" Squared Error Cost vs (w, b)')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n</pre> def plt_logistic_squared_error(X,y):     \"\"\" plots logistic squared error for demonstration \"\"\"     wx, by = np.meshgrid(np.linspace(-6,12,50),                          np.linspace(10, -20, 40))     points = np.c_[wx.ravel(), by.ravel()]     cost = np.zeros(points.shape[0])      for i in range(points.shape[0]):         w,b = points[i]         cost[i] = compute_cost_logistic_sq_err(X.reshape(-1,1), y, w, b)     cost = cost.reshape(wx.shape)      fig = plt.figure()     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False     ax = fig.add_subplot(1, 1, 1, projection='3d')     ax.plot_surface(wx, by, cost, alpha=0.6,cmap=cm.jet,)      ax.set_xlabel('w', fontsize=16)     ax.set_ylabel('b', fontsize=16)     ax.set_zlabel(\"Cost\", rotation=90, fontsize=16)     ax.set_title('\"Logistic\" Squared Error Cost vs (w, b)')     ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0)) In\u00a0[\u00a0]: Copied! <pre>def plt_logistic_cost(X,y):\n    \"\"\" plots logistic cost \"\"\"\n    wx, by = np.meshgrid(np.linspace(-6,12,50),\n                         np.linspace(0, -20, 40))\n    points = np.c_[wx.ravel(), by.ravel()]\n    cost = np.zeros(points.shape[0],dtype=np.longdouble)\n\n    for i in range(points.shape[0]):\n        w,b = points[i]\n        cost[i] = compute_cost_matrix(X.reshape(-1,1), y, w, b, logistic=True, safe=True)\n    cost = cost.reshape(wx.shape)\n\n    fig = plt.figure(figsize=(9,5))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n    ax.plot_surface(wx, by, cost, alpha=0.6,cmap=cm.jet,)\n\n    ax.set_xlabel('w', fontsize=16)\n    ax.set_ylabel('b', fontsize=16)\n    ax.set_zlabel(\"Cost\", rotation=90, fontsize=16)\n    ax.set_title('Logistic Cost vs (w, b)')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n\n    ax = fig.add_subplot(1, 2, 2, projection='3d')\n\n    ax.plot_surface(wx, by, np.log(cost), alpha=0.6,cmap=cm.jet,)\n\n    ax.set_xlabel('w', fontsize=16)\n    ax.set_ylabel('b', fontsize=16)\n    ax.set_zlabel('\\nlog(Cost)', fontsize=16)\n    ax.set_title('log(Logistic Cost) vs (w, b)')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n\n    plt.show()\n    return cost\n</pre> def plt_logistic_cost(X,y):     \"\"\" plots logistic cost \"\"\"     wx, by = np.meshgrid(np.linspace(-6,12,50),                          np.linspace(0, -20, 40))     points = np.c_[wx.ravel(), by.ravel()]     cost = np.zeros(points.shape[0],dtype=np.longdouble)      for i in range(points.shape[0]):         w,b = points[i]         cost[i] = compute_cost_matrix(X.reshape(-1,1), y, w, b, logistic=True, safe=True)     cost = cost.reshape(wx.shape)      fig = plt.figure(figsize=(9,5))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False     ax = fig.add_subplot(1, 2, 1, projection='3d')     ax.plot_surface(wx, by, cost, alpha=0.6,cmap=cm.jet,)      ax.set_xlabel('w', fontsize=16)     ax.set_ylabel('b', fontsize=16)     ax.set_zlabel(\"Cost\", rotation=90, fontsize=16)     ax.set_title('Logistic Cost vs (w, b)')     ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))      ax = fig.add_subplot(1, 2, 2, projection='3d')      ax.plot_surface(wx, by, np.log(cost), alpha=0.6,cmap=cm.jet,)      ax.set_xlabel('w', fontsize=16)     ax.set_ylabel('b', fontsize=16)     ax.set_zlabel('\\nlog(Cost)', fontsize=16)     ax.set_title('log(Logistic Cost) vs (w, b)')     ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))      plt.show()     return cost In\u00a0[\u00a0]: Copied! <pre>def soup_bowl():\n    \"\"\" creates 3D quadratic error surface \"\"\"\n    #Create figure and plot with a 3D projection\n    fig = plt.figure(figsize=(4,4))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n\n    #Plot configuration\n    ax = fig.add_subplot(111, projection='3d')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_rotate_label(False)\n    ax.view_init(15, -120)\n\n    #Useful linearspaces to give values to the parameters w and b\n    w = np.linspace(-20, 20, 100)\n    b = np.linspace(-20, 20, 100)\n\n    #Get the z value for a bowl-shaped cost function\n    z=np.zeros((len(w), len(b)))\n    j=0\n    for x in w:\n        i=0\n        for y in b:\n            z[i,j] = x**2 + y**2\n            i+=1\n        j+=1\n\n    #Meshgrid used for plotting 3D functions\n    W, B = np.meshgrid(w, b)\n\n    #Create the 3D surface plot of the bowl-shaped cost function\n    ax.plot_surface(W, B, z, cmap = \"Spectral_r\", alpha=0.7, antialiased=False)\n    ax.plot_wireframe(W, B, z, color='k', alpha=0.1)\n    ax.set_xlabel(\"$w$\")\n    ax.set_ylabel(\"$b$\")\n    ax.set_zlabel(\"Cost\", rotation=90)\n    ax.set_title(\"Squared Error Cost used in Linear Regression\")\n\n    plt.show()\n</pre> def soup_bowl():     \"\"\" creates 3D quadratic error surface \"\"\"     #Create figure and plot with a 3D projection     fig = plt.figure(figsize=(4,4))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False      #Plot configuration     ax = fig.add_subplot(111, projection='3d')     ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))     ax.zaxis.set_rotate_label(False)     ax.view_init(15, -120)      #Useful linearspaces to give values to the parameters w and b     w = np.linspace(-20, 20, 100)     b = np.linspace(-20, 20, 100)      #Get the z value for a bowl-shaped cost function     z=np.zeros((len(w), len(b)))     j=0     for x in w:         i=0         for y in b:             z[i,j] = x**2 + y**2             i+=1         j+=1      #Meshgrid used for plotting 3D functions     W, B = np.meshgrid(w, b)      #Create the 3D surface plot of the bowl-shaped cost function     ax.plot_surface(W, B, z, cmap = \"Spectral_r\", alpha=0.7, antialiased=False)     ax.plot_wireframe(W, B, z, color='k', alpha=0.1)     ax.set_xlabel(\"$w$\")     ax.set_ylabel(\"$b$\")     ax.set_zlabel(\"Cost\", rotation=90)     ax.set_title(\"Squared Error Cost used in Linear Regression\")      plt.show() In\u00a0[\u00a0]: Copied! <pre>def plt_simple_example(x, y):\n    \"\"\" plots tumor data \"\"\"\n    pos = y == 1\n    neg = y == 0\n\n    fig,ax = plt.subplots(1,1,figsize=(5,3))\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n\n    ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n    ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n    ax.set_ylim(-0.075,1.1)\n    ax.set_ylabel('y')\n    ax.set_xlabel('Tumor Size')\n    ax.legend(loc='lower right')\n    ax.set_title(\"Example of Logistic Regression on Categorical Data\")\n</pre> def plt_simple_example(x, y):     \"\"\" plots tumor data \"\"\"     pos = y == 1     neg = y == 0      fig,ax = plt.subplots(1,1,figsize=(5,3))     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False      ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")     ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)     ax.set_ylim(-0.075,1.1)     ax.set_ylabel('y')     ax.set_xlabel('Tumor Size')     ax.legend(loc='lower right')     ax.set_title(\"Example of Logistic Regression on Categorical Data\") In\u00a0[\u00a0]: Copied! <pre>def plt_two_logistic_loss_curves():\n    \"\"\" plots the logistic loss \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(6,3),sharey=True)\n    fig.canvas.toolbar_visible = False\n    fig.canvas.header_visible = False\n    fig.canvas.footer_visible = False\n    x = np.linspace(0.01,1-0.01,20)\n    ax[0].plot(x,-np.log(x))\n    #ax[0].set_title(\"y = 1\")\n    ax[0].text(0.5, 4.0, \"y = 1\", fontsize=12)\n    ax[0].set_ylabel(\"loss\")\n    ax[0].set_xlabel(r\"$f_{w,b}(x)$\")\n    ax[1].plot(x,-np.log(1-x))\n    #ax[1].set_title(\"y = 0\")\n    ax[1].text(0.5, 4.0, \"y = 0\", fontsize=12)\n    ax[1].set_xlabel(r\"$f_{w,b}(x)$\")\n    ax[0].annotate(\"prediction \\nmatches \\ntarget \", xy= [1,0], xycoords='data',\n                 xytext=[-10,30],textcoords='offset points', ha=\"right\", va=\"center\",\n                   arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)\n    ax[0].annotate(\"loss increases as prediction\\n differs from target\", xy= [0.1,-np.log(0.1)], xycoords='data',\n                 xytext=[10,30],textcoords='offset points', ha=\"left\", va=\"center\",\n                   arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)\n    ax[1].annotate(\"prediction \\nmatches \\ntarget \", xy= [0,0], xycoords='data',\n                 xytext=[10,30],textcoords='offset points', ha=\"left\", va=\"center\",\n                   arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)\n    ax[1].annotate(\"loss increases as prediction\\n differs from target\", xy= [0.9,-np.log(1-0.9)], xycoords='data',\n                 xytext=[-10,30],textcoords='offset points', ha=\"right\", va=\"center\",\n                   arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)\n    plt.suptitle(\"Loss Curves for Two Categorical Target Values\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\n</pre> def plt_two_logistic_loss_curves():     \"\"\" plots the logistic loss \"\"\"     fig,ax = plt.subplots(1,2,figsize=(6,3),sharey=True)     fig.canvas.toolbar_visible = False     fig.canvas.header_visible = False     fig.canvas.footer_visible = False     x = np.linspace(0.01,1-0.01,20)     ax[0].plot(x,-np.log(x))     #ax[0].set_title(\"y = 1\")     ax[0].text(0.5, 4.0, \"y = 1\", fontsize=12)     ax[0].set_ylabel(\"loss\")     ax[0].set_xlabel(r\"$f_{w,b}(x)$\")     ax[1].plot(x,-np.log(1-x))     #ax[1].set_title(\"y = 0\")     ax[1].text(0.5, 4.0, \"y = 0\", fontsize=12)     ax[1].set_xlabel(r\"$f_{w,b}(x)$\")     ax[0].annotate(\"prediction \\nmatches \\ntarget \", xy= [1,0], xycoords='data',                  xytext=[-10,30],textcoords='offset points', ha=\"right\", va=\"center\",                    arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)     ax[0].annotate(\"loss increases as prediction\\n differs from target\", xy= [0.1,-np.log(0.1)], xycoords='data',                  xytext=[10,30],textcoords='offset points', ha=\"left\", va=\"center\",                    arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)     ax[1].annotate(\"prediction \\nmatches \\ntarget \", xy= [0,0], xycoords='data',                  xytext=[10,30],textcoords='offset points', ha=\"left\", va=\"center\",                    arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)     ax[1].annotate(\"loss increases as prediction\\n differs from target\", xy= [0.9,-np.log(1-0.9)], xycoords='data',                  xytext=[-10,30],textcoords='offset points', ha=\"right\", va=\"center\",                    arrowprops={'arrowstyle': '-&gt;', 'color': dlorange, 'lw': 3},)     plt.suptitle(\"Loss Curves for Two Categorical Target Values\", fontsize=12)     plt.tight_layout()     plt.show()"},{"location":"MachineLearning/part3/plt_one_addpt_onclick/","title":"Plt one addpt onclick","text":"In\u00a0[\u00a0]: Copied! <pre>import time\nimport copy\nfrom ipywidgets import Output\nfrom matplotlib.widgets import Button, CheckButtons\nfrom matplotlib.patches import FancyArrowPatch\nfrom lab_utils_common import np, plt, dlblue, dlorange, sigmoid, dldarkred, gradient_descent\n</pre> import time import copy from ipywidgets import Output from matplotlib.widgets import Button, CheckButtons from matplotlib.patches import FancyArrowPatch from lab_utils_common import np, plt, dlblue, dlorange, sigmoid, dldarkred, gradient_descent <p>for debug output = Output() # sends hidden error messages to display when using widgets display(output)</p> In\u00a0[\u00a0]: Copied! <pre>class plt_one_addpt_onclick:\n    \"\"\" class to run one interactive plot \"\"\"\n    def __init__(self, x, y, w, b, logistic=True):\n        self.logistic=logistic\n        pos = y == 1\n        neg = y == 0\n\n        fig,ax = plt.subplots(1,1,figsize=(8,4))\n        fig.canvas.toolbar_visible = False\n        fig.canvas.header_visible = False\n        fig.canvas.footer_visible = False\n\n        plt.subplots_adjust(bottom=0.25)\n        ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n        ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n        ax.set_ylim(-0.05,1.1)\n        xlim = ax.get_xlim()\n        ax.set_xlim(xlim[0],xlim[1]*2)\n        ax.set_ylabel('y')\n        ax.set_xlabel('Tumor Size')\n        self.alegend = ax.legend(loc='lower right')\n        if self.logistic:\n            ax.set_title(\"Example of Logistic Regression on Categorical Data\")\n        else:\n            ax.set_title(\"Example of Linear Regression on Categorical Data\")\n\n        ax.text(0.65,0.8,\"[Click to add data points]\", size=10, transform=ax.transAxes)\n\n        axcalc   = plt.axes([0.1, 0.05, 0.38, 0.075])  #l,b,w,h\n        axthresh = plt.axes([0.5, 0.05, 0.38, 0.075])  #l,b,w,h\n        self.tlist = []\n\n        self.fig = fig\n        self.ax = [ax,axcalc,axthresh]\n        self.x = x\n        self.y = y\n        self.w = copy.deepcopy(w)\n        self.b = b\n        f_wb = np.matmul(self.x.reshape(-1,1), self.w) + self.b\n        if self.logistic:\n            self.aline = self.ax[0].plot(self.x, sigmoid(f_wb), color=dlblue)\n            self.bline = self.ax[0].plot(self.x, f_wb, color=dlorange,lw=1)\n        else:\n            self.aline = self.ax[0].plot(self.x, sigmoid(f_wb), color=dlblue)\n\n        self.cid = fig.canvas.mpl_connect('button_press_event', self.add_data)\n        if self.logistic:\n            self.bcalc = Button(axcalc, 'Run Logistic Regression (click)', color=dlblue)\n            self.bcalc.on_clicked(self.calc_logistic)\n        else:\n            self.bcalc = Button(axcalc, 'Run Linear Regression (click)', color=dlblue)\n            self.bcalc.on_clicked(self.calc_linear)\n        self.bthresh = CheckButtons(axthresh, ('Toggle 0.5 threshold (after regression)',))\n        self.bthresh.on_clicked(self.thresh)\n        self.resize_sq(self.bthresh)\n\n #   @output.capture()  # debug\n    def add_data(self, event):\n        #self.ax[0].text(0.1,0.1, f\"in onclick\")\n        if event.inaxes == self.ax[0]:\n            x_coord = event.xdata\n            y_coord = event.ydata\n\n            if y_coord &gt; 0.5:\n                self.ax[0].scatter(x_coord, 1, marker='x', s=80, c = 'red' )\n                self.y = np.append(self.y,1)\n            else:\n                self.ax[0].scatter(x_coord, 0, marker='o', s=100, facecolors='none', edgecolors=dlblue,lw=3)\n                self.y = np.append(self.y,0)\n            self.x = np.append(self.x,x_coord)\n        self.fig.canvas.draw()\n</pre> class plt_one_addpt_onclick:     \"\"\" class to run one interactive plot \"\"\"     def __init__(self, x, y, w, b, logistic=True):         self.logistic=logistic         pos = y == 1         neg = y == 0          fig,ax = plt.subplots(1,1,figsize=(8,4))         fig.canvas.toolbar_visible = False         fig.canvas.header_visible = False         fig.canvas.footer_visible = False          plt.subplots_adjust(bottom=0.25)         ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")         ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)         ax.set_ylim(-0.05,1.1)         xlim = ax.get_xlim()         ax.set_xlim(xlim[0],xlim[1]*2)         ax.set_ylabel('y')         ax.set_xlabel('Tumor Size')         self.alegend = ax.legend(loc='lower right')         if self.logistic:             ax.set_title(\"Example of Logistic Regression on Categorical Data\")         else:             ax.set_title(\"Example of Linear Regression on Categorical Data\")          ax.text(0.65,0.8,\"[Click to add data points]\", size=10, transform=ax.transAxes)          axcalc   = plt.axes([0.1, 0.05, 0.38, 0.075])  #l,b,w,h         axthresh = plt.axes([0.5, 0.05, 0.38, 0.075])  #l,b,w,h         self.tlist = []          self.fig = fig         self.ax = [ax,axcalc,axthresh]         self.x = x         self.y = y         self.w = copy.deepcopy(w)         self.b = b         f_wb = np.matmul(self.x.reshape(-1,1), self.w) + self.b         if self.logistic:             self.aline = self.ax[0].plot(self.x, sigmoid(f_wb), color=dlblue)             self.bline = self.ax[0].plot(self.x, f_wb, color=dlorange,lw=1)         else:             self.aline = self.ax[0].plot(self.x, sigmoid(f_wb), color=dlblue)          self.cid = fig.canvas.mpl_connect('button_press_event', self.add_data)         if self.logistic:             self.bcalc = Button(axcalc, 'Run Logistic Regression (click)', color=dlblue)             self.bcalc.on_clicked(self.calc_logistic)         else:             self.bcalc = Button(axcalc, 'Run Linear Regression (click)', color=dlblue)             self.bcalc.on_clicked(self.calc_linear)         self.bthresh = CheckButtons(axthresh, ('Toggle 0.5 threshold (after regression)',))         self.bthresh.on_clicked(self.thresh)         self.resize_sq(self.bthresh)   #   @output.capture()  # debug     def add_data(self, event):         #self.ax[0].text(0.1,0.1, f\"in onclick\")         if event.inaxes == self.ax[0]:             x_coord = event.xdata             y_coord = event.ydata              if y_coord &gt; 0.5:                 self.ax[0].scatter(x_coord, 1, marker='x', s=80, c = 'red' )                 self.y = np.append(self.y,1)             else:                 self.ax[0].scatter(x_coord, 0, marker='o', s=100, facecolors='none', edgecolors=dlblue,lw=3)                 self.y = np.append(self.y,0)             self.x = np.append(self.x,x_coord)         self.fig.canvas.draw() In\u00a0[\u00a0]: Copied! <pre>#   @output.capture()  # debug\n    def calc_linear(self, event):\n        if self.bthresh.get_status()[0]:\n            self.remove_thresh()\n        for it in [1,1,1,1,1,2,4,8,16,32,64,128,256]:\n            self.w, self.b, _ = gradient_descent(self.x.reshape(-1,1), self.y.reshape(-1,1),\n                                                 self.w.reshape(-1,1), self.b, 0.01, it,\n                                                 logistic=False, lambda_=0, verbose=False)\n            self.aline[0].remove()\n            self.alegend.remove()\n            y_hat = np.matmul(self.x.reshape(-1,1), self.w) + self.b\n            self.aline = self.ax[0].plot(self.x, y_hat, color=dlblue,\n                                         label=f\"y = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")\n            self.alegend = self.ax[0].legend(loc='lower right')\n            time.sleep(0.3)\n            self.fig.canvas.draw()\n        if self.bthresh.get_status()[0]:\n            self.draw_thresh()\n            self.fig.canvas.draw()\n\n    def calc_logistic(self, event):\n        if self.bthresh.get_status()[0]:\n            self.remove_thresh()\n        for it in [1, 8,16,32,64,128,256,512,1024,2048,4096]:\n            self.w, self.b, _ = gradient_descent(self.x.reshape(-1,1), self.y.reshape(-1,1),\n                                                 self.w.reshape(-1,1), self.b, 0.1, it,\n                                                 logistic=True, lambda_=0, verbose=False)\n            self.aline[0].remove()\n            self.bline[0].remove()\n            self.alegend.remove()\n            xlim  = self.ax[0].get_xlim()\n            x_hat = np.linspace(*xlim, 30)\n            y_hat = sigmoid(np.matmul(x_hat.reshape(-1,1), self.w) + self.b)\n            self.aline = self.ax[0].plot(x_hat, y_hat, color=dlblue,\n                                         label=\"y = sigmoid(z)\")\n            f_wb = np.matmul(x_hat.reshape(-1,1), self.w) + self.b\n            self.bline = self.ax[0].plot(x_hat, f_wb, color=dlorange, lw=1,\n                                         label=f\"z = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")\n            self.alegend = self.ax[0].legend(loc='lower right')\n            time.sleep(0.3)\n            self.fig.canvas.draw()\n        if self.bthresh.get_status()[0]:\n            self.draw_thresh()\n            self.fig.canvas.draw()\n\n\n    def thresh(self, event):\n        if self.bthresh.get_status()[0]:\n            #plt.figtext(0,0, f\"in thresh {self.bthresh.get_status()}\")\n            self.draw_thresh()\n        else:\n            #plt.figtext(0,0.3, f\"in thresh {self.bthresh.get_status()}\")\n            self.remove_thresh()\n\n    def draw_thresh(self):\n        ws = np.squeeze(self.w)\n        xp5 = -self.b/ws if self.logistic else (0.5 - self.b) / ws\n        ylim = self.ax[0].get_ylim()\n        xlim = self.ax[0].get_xlim()\n        a = self.ax[0].fill_between([xlim[0], xp5], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)\n        b = self.ax[0].fill_between([xp5, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)\n        c = self.ax[0].annotate(\"Malignant\", xy= [xp5,0.5], xycoords='data',\n             xytext=[30,5],textcoords='offset points')\n        d = FancyArrowPatch(\n            posA=(xp5, 0.5), posB=(xp5+1.5, 0.5), color=dldarkred,\n            arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n        )\n        self.ax[0].add_artist(d)\n\n        e = self.ax[0].annotate(\"Benign\", xy= [xp5,0.5], xycoords='data',\n                     xytext=[-70,5],textcoords='offset points', ha='left')\n        f = FancyArrowPatch(\n            posA=(xp5, 0.5), posB=(xp5-1.5, 0.5), color=dlblue,\n            arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n        )\n        self.ax[0].add_artist(f)\n        self.tlist = [a,b,c,d,e,f]\n\n        self.fig.canvas.draw()\n\n    def remove_thresh(self):\n        #plt.figtext(0.5,0.0, f\"rem thresh {self.bthresh.get_status()}\")\n        for artist in self.tlist:\n            artist.remove()\n        self.fig.canvas.draw()\n\n    def resize_sq(self, bcid):\n        \"\"\" resizes the check box \"\"\"\n        #future reference\n        #print(f\"width  : {bcid.rectangles[0].get_width()}\")\n        #print(f\"height : {bcid.rectangles[0].get_height()}\")\n        #print(f\"xy     : {bcid.rectangles[0].get_xy()}\")\n        #print(f\"bb     : {bcid.rectangles[0].get_bbox()}\")\n        #print(f\"points : {bcid.rectangles[0].get_bbox().get_points()}\")  #[[xmin,ymin],[xmax,ymax]]\n\n        h = bcid.rectangles[0].get_height()\n        bcid.rectangles[0].set_height(3*h)\n\n        ymax = bcid.rectangles[0].get_bbox().y1\n        ymin = bcid.rectangles[0].get_bbox().y0\n\n        bcid.lines[0][0].set_ydata([ymax,ymin])\n        bcid.lines[0][1].set_ydata([ymin,ymax])\n</pre> #   @output.capture()  # debug     def calc_linear(self, event):         if self.bthresh.get_status()[0]:             self.remove_thresh()         for it in [1,1,1,1,1,2,4,8,16,32,64,128,256]:             self.w, self.b, _ = gradient_descent(self.x.reshape(-1,1), self.y.reshape(-1,1),                                                  self.w.reshape(-1,1), self.b, 0.01, it,                                                  logistic=False, lambda_=0, verbose=False)             self.aline[0].remove()             self.alegend.remove()             y_hat = np.matmul(self.x.reshape(-1,1), self.w) + self.b             self.aline = self.ax[0].plot(self.x, y_hat, color=dlblue,                                          label=f\"y = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")             self.alegend = self.ax[0].legend(loc='lower right')             time.sleep(0.3)             self.fig.canvas.draw()         if self.bthresh.get_status()[0]:             self.draw_thresh()             self.fig.canvas.draw()      def calc_logistic(self, event):         if self.bthresh.get_status()[0]:             self.remove_thresh()         for it in [1, 8,16,32,64,128,256,512,1024,2048,4096]:             self.w, self.b, _ = gradient_descent(self.x.reshape(-1,1), self.y.reshape(-1,1),                                                  self.w.reshape(-1,1), self.b, 0.1, it,                                                  logistic=True, lambda_=0, verbose=False)             self.aline[0].remove()             self.bline[0].remove()             self.alegend.remove()             xlim  = self.ax[0].get_xlim()             x_hat = np.linspace(*xlim, 30)             y_hat = sigmoid(np.matmul(x_hat.reshape(-1,1), self.w) + self.b)             self.aline = self.ax[0].plot(x_hat, y_hat, color=dlblue,                                          label=\"y = sigmoid(z)\")             f_wb = np.matmul(x_hat.reshape(-1,1), self.w) + self.b             self.bline = self.ax[0].plot(x_hat, f_wb, color=dlorange, lw=1,                                          label=f\"z = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")             self.alegend = self.ax[0].legend(loc='lower right')             time.sleep(0.3)             self.fig.canvas.draw()         if self.bthresh.get_status()[0]:             self.draw_thresh()             self.fig.canvas.draw()       def thresh(self, event):         if self.bthresh.get_status()[0]:             #plt.figtext(0,0, f\"in thresh {self.bthresh.get_status()}\")             self.draw_thresh()         else:             #plt.figtext(0,0.3, f\"in thresh {self.bthresh.get_status()}\")             self.remove_thresh()      def draw_thresh(self):         ws = np.squeeze(self.w)         xp5 = -self.b/ws if self.logistic else (0.5 - self.b) / ws         ylim = self.ax[0].get_ylim()         xlim = self.ax[0].get_xlim()         a = self.ax[0].fill_between([xlim[0], xp5], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)         b = self.ax[0].fill_between([xp5, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)         c = self.ax[0].annotate(\"Malignant\", xy= [xp5,0.5], xycoords='data',              xytext=[30,5],textcoords='offset points')         d = FancyArrowPatch(             posA=(xp5, 0.5), posB=(xp5+1.5, 0.5), color=dldarkred,             arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',         )         self.ax[0].add_artist(d)          e = self.ax[0].annotate(\"Benign\", xy= [xp5,0.5], xycoords='data',                      xytext=[-70,5],textcoords='offset points', ha='left')         f = FancyArrowPatch(             posA=(xp5, 0.5), posB=(xp5-1.5, 0.5), color=dlblue,             arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',         )         self.ax[0].add_artist(f)         self.tlist = [a,b,c,d,e,f]          self.fig.canvas.draw()      def remove_thresh(self):         #plt.figtext(0.5,0.0, f\"rem thresh {self.bthresh.get_status()}\")         for artist in self.tlist:             artist.remove()         self.fig.canvas.draw()      def resize_sq(self, bcid):         \"\"\" resizes the check box \"\"\"         #future reference         #print(f\"width  : {bcid.rectangles[0].get_width()}\")         #print(f\"height : {bcid.rectangles[0].get_height()}\")         #print(f\"xy     : {bcid.rectangles[0].get_xy()}\")         #print(f\"bb     : {bcid.rectangles[0].get_bbox()}\")         #print(f\"points : {bcid.rectangles[0].get_bbox().get_points()}\")  #[[xmin,ymin],[xmax,ymax]]          h = bcid.rectangles[0].get_height()         bcid.rectangles[0].set_height(3*h)          ymax = bcid.rectangles[0].get_bbox().y1         ymin = bcid.rectangles[0].get_bbox().y0          bcid.lines[0][0].set_ydata([ymax,ymin])         bcid.lines[0][1].set_ydata([ymin,ymax])"},{"location":"MachineLearning/part3/plt_overfit/","title":"Plt overfit","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nplot_overfit\n    class and assocaited routines that plot an interactive example of overfitting and its solutions\n\"\"\"\nimport math\nfrom ipywidgets import Output\nfrom matplotlib.gridspec import GridSpec\nfrom matplotlib.widgets import Button, CheckButtons\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom lab_utils_common import np, plt, dlc, predict_logistic, plot_data, zscore_normalize_features\n</pre> \"\"\" plot_overfit     class and assocaited routines that plot an interactive example of overfitting and its solutions \"\"\" import math from ipywidgets import Output from matplotlib.gridspec import GridSpec from matplotlib.widgets import Button, CheckButtons from sklearn.linear_model import LogisticRegression, Ridge from lab_utils_common import np, plt, dlc, predict_logistic, plot_data, zscore_normalize_features In\u00a0[\u00a0]: Copied! <pre>def map_one_feature(X1, degree):\n    \"\"\"\n    Feature mapping function to polynomial features\n    \"\"\"\n    X1 = np.atleast_1d(X1)\n    out = []\n    string = \"\"\n    k = 0\n    for i in range(1, degree+1):\n        out.append((X1**i))\n        string = string + f\"w_{{{k}}}{munge('x_0',i)} + \"\n        k += 1\n    string = string + ' b' #add b to text equation, not to data\n    return np.stack(out, axis=1), string\n</pre> def map_one_feature(X1, degree):     \"\"\"     Feature mapping function to polynomial features     \"\"\"     X1 = np.atleast_1d(X1)     out = []     string = \"\"     k = 0     for i in range(1, degree+1):         out.append((X1**i))         string = string + f\"w_{{{k}}}{munge('x_0',i)} + \"         k += 1     string = string + ' b' #add b to text equation, not to data     return np.stack(out, axis=1), string In\u00a0[\u00a0]: Copied! <pre>def map_feature(X1, X2, degree):\n    \"\"\"\n    Feature mapping function to polynomial features\n    \"\"\"\n    X1 = np.atleast_1d(X1)\n    X2 = np.atleast_1d(X2)\n\n    out = []\n    string = \"\"\n    k = 0\n    for i in range(1, degree+1):\n        for j in range(i + 1):\n            out.append((X1**(i-j) * (X2**j)))\n            string = string + f\"w_{{{k}}}{munge('x_0',i-j)}{munge('x_1',j)} + \"\n            k += 1\n    #print(string + 'b')\n    return np.stack(out, axis=1), string + ' b'\n</pre> def map_feature(X1, X2, degree):     \"\"\"     Feature mapping function to polynomial features     \"\"\"     X1 = np.atleast_1d(X1)     X2 = np.atleast_1d(X2)      out = []     string = \"\"     k = 0     for i in range(1, degree+1):         for j in range(i + 1):             out.append((X1**(i-j) * (X2**j)))             string = string + f\"w_{{{k}}}{munge('x_0',i-j)}{munge('x_1',j)} + \"             k += 1     #print(string + 'b')     return np.stack(out, axis=1), string + ' b' In\u00a0[\u00a0]: Copied! <pre>def munge(base, exp):\n    if exp == 0:\n        return ''\n    if exp == 1:\n        return base\n    return base + f'^{{{exp}}}'\n</pre> def munge(base, exp):     if exp == 0:         return ''     if exp == 1:         return base     return base + f'^{{{exp}}}' In\u00a0[\u00a0]: Copied! <pre>def plot_decision_boundary(ax, x0r,x1r, predict,  w, b, scaler = False, mu=None, sigma=None, degree=None):\n    \"\"\"\n    Plots a decision boundary\n     Args:\n      x0r : (array_like Shape (1,1)) range (min, max) of x0\n      x1r : (array_like Shape (1,1)) range (min, max) of x1\n      predict : function to predict z values\n      scalar : (boolean) scale data or not\n    \"\"\"\n\n    h = .01  # step size in the mesh\n    # create a mesh to plot in\n    xx, yy = np.meshgrid(np.arange(x0r[0], x0r[1], h),\n                         np.arange(x1r[0], x1r[1], h))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    points = np.c_[xx.ravel(), yy.ravel()]\n    Xm,_ = map_feature(points[:, 0], points[:, 1],degree)\n    if scaler:\n        Xm = (Xm - mu)/sigma\n    Z = predict(Xm, w, b)\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    contour = ax.contour(xx, yy, Z, levels = [0.5], colors='g')\n    return contour\n</pre> def plot_decision_boundary(ax, x0r,x1r, predict,  w, b, scaler = False, mu=None, sigma=None, degree=None):     \"\"\"     Plots a decision boundary      Args:       x0r : (array_like Shape (1,1)) range (min, max) of x0       x1r : (array_like Shape (1,1)) range (min, max) of x1       predict : function to predict z values       scalar : (boolean) scale data or not     \"\"\"      h = .01  # step size in the mesh     # create a mesh to plot in     xx, yy = np.meshgrid(np.arange(x0r[0], x0r[1], h),                          np.arange(x1r[0], x1r[1], h))      # Plot the decision boundary. For that, we will assign a color to each     # point in the mesh [x_min, m_max]x[y_min, y_max].     points = np.c_[xx.ravel(), yy.ravel()]     Xm,_ = map_feature(points[:, 0], points[:, 1],degree)     if scaler:         Xm = (Xm - mu)/sigma     Z = predict(Xm, w, b)      # Put the result into a color plot     Z = Z.reshape(xx.shape)     contour = ax.contour(xx, yy, Z, levels = [0.5], colors='g')     return contour In\u00a0[\u00a0]: Copied! <pre># use this to test the above routine\ndef plot_decision_boundary_sklearn(x0r, x1r, predict, degree,  scaler = False):\n    \"\"\"\n    Plots a decision boundary\n     Args:\n      x0r : (array_like Shape (1,1)) range (min, max) of x0\n      x1r : (array_like Shape (1,1)) range (min, max) of x1\n      degree: (int)                  degree of polynomial\n      predict : function to predict z values\n      scaler  : not sure\n    \"\"\"\n\n    h = .01  # step size in the mesh\n    # create a mesh to plot in\n    xx, yy = np.meshgrid(np.arange(x0r[0], x0r[1], h),\n                         np.arange(x1r[0], x1r[1], h))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    points = np.c_[xx.ravel(), yy.ravel()]\n    Xm = map_feature(points[:, 0], points[:, 1],degree)\n    if scaler:\n        Xm = scaler.transform(Xm)\n    Z = predict(Xm)\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, colors='g')\n    #plot_data(X_train,y_train)\n</pre> # use this to test the above routine def plot_decision_boundary_sklearn(x0r, x1r, predict, degree,  scaler = False):     \"\"\"     Plots a decision boundary      Args:       x0r : (array_like Shape (1,1)) range (min, max) of x0       x1r : (array_like Shape (1,1)) range (min, max) of x1       degree: (int)                  degree of polynomial       predict : function to predict z values       scaler  : not sure     \"\"\"      h = .01  # step size in the mesh     # create a mesh to plot in     xx, yy = np.meshgrid(np.arange(x0r[0], x0r[1], h),                          np.arange(x1r[0], x1r[1], h))      # Plot the decision boundary. For that, we will assign a color to each     # point in the mesh [x_min, m_max]x[y_min, y_max].     points = np.c_[xx.ravel(), yy.ravel()]     Xm = map_feature(points[:, 0], points[:, 1],degree)     if scaler:         Xm = scaler.transform(Xm)     Z = predict(Xm)      # Put the result into a color plot     Z = Z.reshape(xx.shape)     plt.contour(xx, yy, Z, colors='g')     #plot_data(X_train,y_train) <p>for debug, uncomment the #@output statments below for routines you want to get error output from In the notebook that will call these routines, import  <code>output</code> from plt_overfit import overfit_example, output then, in a cell where the error messages will be the output of.. display(output)</p> In\u00a0[\u00a0]: Copied! <pre>output = Output() # sends hidden error messages to display when using widgets\n</pre> output = Output() # sends hidden error messages to display when using widgets In\u00a0[\u00a0]: Copied! <pre>class button_manager:\n    ''' Handles some missing features of matplotlib check buttons\n    on init:\n        creates button, links to button_click routine,\n        calls call_on_click with active index and firsttime=True\n    on click:\n        maintains single button on state, calls call_on_click\n    '''\n\n    @output.capture()  # debug\n    def __init__(self,fig, dim, labels, init, call_on_click):\n        '''\n        dim: (list)     [leftbottom_x,bottom_y,width,height]\n        labels: (list)  for example ['1','2','3','4','5','6']\n        init: (list)    for example [True, False, False, False, False, False]\n        '''\n        self.fig = fig\n        self.ax = plt.axes(dim)  #lx,by,w,h\n        self.init_state = init\n        self.call_on_click = call_on_click\n        self.button  = CheckButtons(self.ax,labels,init)\n        self.button.on_clicked(self.button_click)\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True),firsttime=True)\n\n    @output.capture()  # debug\n    def reinit(self):\n        self.status = self.init_state\n        self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status\n\n    @output.capture()  # debug\n    def button_click(self, event):\n        ''' maintains one-on state. If on-button is clicked, will process correctly '''\n        #new_status = self.button.get_status()\n        #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]\n        #newidx = new.index(True)\n        self.button.eventson = False\n        self.button.set_active(self.status.index(True))  #turn off old or reenable if same\n        self.button.eventson = True\n        self.status = self.button.get_status()\n        self.call_on_click(self.status.index(True))\n</pre> class button_manager:     ''' Handles some missing features of matplotlib check buttons     on init:         creates button, links to button_click routine,         calls call_on_click with active index and firsttime=True     on click:         maintains single button on state, calls call_on_click     '''      @output.capture()  # debug     def __init__(self,fig, dim, labels, init, call_on_click):         '''         dim: (list)     [leftbottom_x,bottom_y,width,height]         labels: (list)  for example ['1','2','3','4','5','6']         init: (list)    for example [True, False, False, False, False, False]         '''         self.fig = fig         self.ax = plt.axes(dim)  #lx,by,w,h         self.init_state = init         self.call_on_click = call_on_click         self.button  = CheckButtons(self.ax,labels,init)         self.button.on_clicked(self.button_click)         self.status = self.button.get_status()         self.call_on_click(self.status.index(True),firsttime=True)      @output.capture()  # debug     def reinit(self):         self.status = self.init_state         self.button.set_active(self.status.index(True))      #turn off old, will trigger update and set to status      @output.capture()  # debug     def button_click(self, event):         ''' maintains one-on state. If on-button is clicked, will process correctly '''         #new_status = self.button.get_status()         #new = [self.status[i] ^ new_status[i] for i in range(len(self.status))]         #newidx = new.index(True)         self.button.eventson = False         self.button.set_active(self.status.index(True))  #turn off old or reenable if same         self.button.eventson = True         self.status = self.button.get_status()         self.call_on_click(self.status.index(True)) In\u00a0[\u00a0]: Copied! <pre>class overfit_example():\n    \"\"\" plot overfit example \"\"\"\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-locals\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self, regularize=False):\n        self.regularize=regularize\n        self.lambda_=0\n        fig = plt.figure( figsize=(8,6))\n        fig.canvas.toolbar_visible = False\n        fig.canvas.header_visible = False\n        fig.canvas.footer_visible = False\n        fig.set_facecolor('#ffffff') #white\n        gs  = GridSpec(5, 3, figure=fig)\n        ax0 = fig.add_subplot(gs[0:3, :])\n        ax1 = fig.add_subplot(gs[-2, :])\n        ax2 = fig.add_subplot(gs[-1, :])\n        ax1.set_axis_off()\n        ax2.set_axis_off()\n        self.ax = [ax0,ax1,ax2]\n        self.fig = fig\n\n        self.axfitdata = plt.axes([0.26,0.124,0.12,0.1 ])  #lx,by,w,h\n        self.bfitdata  = Button(self.axfitdata , 'fit data', color=dlc['dlblue'])\n        self.bfitdata.label.set_fontsize(12)\n        self.bfitdata.on_clicked(self.fitdata_clicked)\n\n        #clear data is a future enhancement\n        #self.axclrdata = plt.axes([0.26,0.06,0.12,0.05 ])  #lx,by,w,h\n        #self.bclrdata  = Button(self.axclrdata , 'clear data', color='white')\n        #self.bclrdata.label.set_fontsize(12)\n        #self.bclrdata.on_clicked(self.clrdata_clicked)\n\n        self.cid = fig.canvas.mpl_connect('button_press_event', self.add_data)\n\n        self.typebut = button_manager(fig, [0.4, 0.07,0.15,0.15], [\"Regression\", \"Categorical\"],\n                                       [False,True], self.toggle_type)\n\n        self.fig.text(0.1, 0.02+0.21, \"Degree\", fontsize=12)\n        self.degrbut = button_manager(fig,[0.1,0.02,0.15,0.2 ], ['1','2','3','4','5','6'],\n                                        [True, False, False, False, False, False], self.update_equation)\n        if self.regularize:\n            self.fig.text(0.6, 0.02+0.21, r\"lambda($\\lambda$)\", fontsize=12)\n            self.lambut = button_manager(fig,[0.6,0.02,0.15,0.2 ], ['0.0','0.2','0.4','0.6','0.8','1'],\n                                        [True, False, False, False, False, False], self.updt_lambda)\n\n        #self.regbut =  button_manager(fig, [0.8, 0.08,0.24,0.15], [\"Regularize\"],\n        #                               [False], self.toggle_reg)\n        #self.logistic_data()\n\n    def updt_lambda(self, idx, firsttime=False):\n      # pylint: disable=unused-argument\n        self.lambda_ = idx * 0.2\n\n    def toggle_type(self, idx, firsttime=False):\n        self.logistic = idx==1\n        self.ax[0].clear()\n        if self.logistic:\n            self.logistic_data()\n        else:\n            self.linear_data()\n        if not firsttime:\n            self.degrbut.reinit()\n\n    @output.capture()  # debug\n    def logistic_data(self,redraw=False):\n        if not redraw:\n            m = 50\n            n = 2\n            np.random.seed(2)\n            X_train = 2*(np.random.rand(m,n)-[0.5,0.5])\n            y_train = X_train[:,1]+0.5  &gt; X_train[:,0]**2 + 0.5*np.random.rand(m) #quadratic + random\n            y_train = y_train + 0  #convert from boolean to integer\n            self.X = X_train\n            self.y = y_train\n            self.x_ideal = np.sort(X_train[:,0])\n            self.y_ideal =  self.x_ideal**2\n\n\n        self.ax[0].plot(self.x_ideal, self.y_ideal, \"--\", color = \"orangered\", label=\"ideal\", lw=1)\n        plot_data(self.X, self.y, self.ax[0], s=10, loc='lower right')\n        self.ax[0].set_title(\"OverFitting Example: Categorical data set with noise\")\n        self.ax[0].text(0.5,0.93, \"Click on plot to add data. Hold [Shift] for blue(y=0) data.\",\n                        fontsize=12, ha='center',transform=self.ax[0].transAxes, color=dlc[\"dlblue\"])\n        self.ax[0].set_xlabel(r\"$x_0$\")\n        self.ax[0].set_ylabel(r\"$x_1$\")\n\n    def linear_data(self,redraw=False):\n        if not redraw:\n            m = 30\n            c = 0\n            x_train = np.arange(0,m,1)\n            np.random.seed(1)\n            y_ideal = x_train**2 + c\n            y_train = y_ideal + 0.7 * y_ideal*(np.random.sample((m,))-0.5)\n            self.x_ideal = x_train #for redraw when new data included in X\n            self.X = x_train\n            self.y = y_train\n            self.y_ideal = y_ideal\n        else:\n            self.ax[0].set_xlim(self.xlim)\n            self.ax[0].set_ylim(self.ylim)\n\n        self.ax[0].scatter(self.X,self.y, label=\"y\")\n        self.ax[0].plot(self.x_ideal, self.y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n        self.ax[0].set_title(\"OverFitting Example: Regression Data Set (quadratic with noise)\",fontsize = 14)\n        self.ax[0].set_xlabel(\"x\")\n        self.ax[0].set_ylabel(\"y\")\n        self.ax0ledgend = self.ax[0].legend(loc='lower right')\n        self.ax[0].text(0.5,0.93, \"Click on plot to add data\",\n                        fontsize=12, ha='center',transform=self.ax[0].transAxes, color=dlc[\"dlblue\"])\n        if not redraw:\n            self.xlim = self.ax[0].get_xlim()\n            self.ylim = self.ax[0].get_ylim()\n\n\n    @output.capture()  # debug\n    def add_data(self, event):\n        if self.logistic:\n            self.add_data_logistic(event)\n        else:\n            self.add_data_linear(event)\n\n    @output.capture()  # debug\n    def add_data_logistic(self, event):\n        if event.inaxes == self.ax[0]:\n            x0_coord = event.xdata\n            x1_coord = event.ydata\n\n            if event.key is None:  #shift not pressed\n                self.ax[0].scatter(x0_coord, x1_coord, marker='x', s=10, c = 'red', label=\"y=1\")\n                self.y = np.append(self.y,1)\n            else:\n                self.ax[0].scatter(x0_coord, x1_coord, marker='o', s=10, label=\"y=0\", facecolors='none',\n                                   edgecolors=dlc['dlblue'],lw=3)\n                self.y = np.append(self.y,0)\n            self.X = np.append(self.X,np.array([[x0_coord, x1_coord]]),axis=0)\n        self.fig.canvas.draw()\n\n    def add_data_linear(self, event):\n        if event.inaxes == self.ax[0]:\n            x_coord = event.xdata\n            y_coord = event.ydata\n\n            self.ax[0].scatter(x_coord, y_coord, marker='o', s=10, facecolors='none',\n                                   edgecolors=dlc['dlblue'],lw=3)\n            self.y = np.append(self.y,y_coord)\n            self.X = np.append(self.X,x_coord)\n            self.fig.canvas.draw()\n\n    #@output.capture()  # debug\n    #def clrdata_clicked(self,event):\n    #    if self.logistic == True:\n    #        self.X = np.\n    #    else:\n    #        self.linear_regression()\n\n\n    @output.capture()  # debug\n    def fitdata_clicked(self,event):\n        if self.logistic:\n            self.logistic_regression()\n        else:\n            self.linear_regression()\n\n    def linear_regression(self):\n        self.ax[0].clear()\n        self.fig.canvas.draw()\n\n        # create and fit the model using our mapped_X feature set.\n        self.X_mapped, _ =  map_one_feature(self.X, self.degree)\n        self.X_mapped_scaled, self.X_mu, self.X_sigma  = zscore_normalize_features(self.X_mapped)\n\n        #linear_model = LinearRegression()\n        linear_model = Ridge(alpha=self.lambda_, normalize=True, max_iter=10000)\n        linear_model.fit(self.X_mapped_scaled, self.y )\n        self.w = linear_model.coef_.reshape(-1,)\n        self.b = linear_model.intercept_\n        x = np.linspace(*self.xlim,30)  #plot line idependent of data which gets disordered\n        xm, _ =  map_one_feature(x, self.degree)\n        xms = (xm - self.X_mu)/ self.X_sigma\n        y_pred = linear_model.predict(xms)\n\n        #self.fig.canvas.draw()\n        self.linear_data(redraw=True)\n        self.ax0yfit = self.ax[0].plot(x, y_pred, color = \"blue\", label=\"y_fit\")\n        self.ax0ledgend = self.ax[0].legend(loc='lower right')\n        self.fig.canvas.draw()\n\n    def logistic_regression(self):\n        self.ax[0].clear()\n        self.fig.canvas.draw()\n\n        # create and fit the model using our mapped_X feature set.\n        self.X_mapped, _ =  map_feature(self.X[:, 0], self.X[:, 1], self.degree)\n        self.X_mapped_scaled, self.X_mu, self.X_sigma  = zscore_normalize_features(self.X_mapped)\n        if not self.regularize or self.lambda_ == 0:\n            lr = LogisticRegression(penalty='none', max_iter=10000)\n        else:\n            C = 1/self.lambda_\n            lr = LogisticRegression(C=C, max_iter=10000)\n\n        lr.fit(self.X_mapped_scaled,self.y)\n        #print(lr.score(self.X_mapped_scaled, self.y))\n        self.w = lr.coef_.reshape(-1,)\n        self.b = lr.intercept_\n        #print(self.w, self.b)\n        self.logistic_data(redraw=True)\n        self.contour = plot_decision_boundary(self.ax[0],[-1,1],[-1,1], predict_logistic, self.w, self.b,\n                       scaler=True, mu=self.X_mu, sigma=self.X_sigma, degree=self.degree )\n        self.fig.canvas.draw()\n\n    @output.capture()  # debug\n    def update_equation(self, idx, firsttime=False):\n        #print(f\"Update equation, index = {idx}, firsttime={firsttime}\")\n        self.degree = idx+1\n        if firsttime:\n            self.eqtext = []\n        else:\n            for artist in self.eqtext:\n                #print(artist)\n                artist.remove()\n            self.eqtext = []\n        if self.logistic:\n            _, equation =  map_feature(self.X[:, 0], self.X[:, 1], self.degree)\n            string = 'f_{wb} = sigmoid('\n        else:\n            _, equation =  map_one_feature(self.X, self.degree)\n            string = 'f_{wb} = ('\n        bz = 10\n        seq = equation.split('+')\n        blks = math.ceil(len(seq)/bz)\n        for i in range(blks):\n            if i == 0:\n                string = string +  '+'.join(seq[bz*i:bz*i+bz])\n            else:\n                string = '+'.join(seq[bz*i:bz*i+bz])\n            string = string + ')' if i == blks-1 else string + '+'\n            ei = self.ax[1].text(0.01,(0.75-i*0.25), f\"${string}$\",fontsize=9,\n                                 transform = self.ax[1].transAxes, ma='left', va='top' )\n            self.eqtext.append(ei)\n        self.fig.canvas.draw()\n</pre> class overfit_example():     \"\"\" plot overfit example \"\"\"     # pylint: disable=too-many-instance-attributes     # pylint: disable=too-many-locals     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self, regularize=False):         self.regularize=regularize         self.lambda_=0         fig = plt.figure( figsize=(8,6))         fig.canvas.toolbar_visible = False         fig.canvas.header_visible = False         fig.canvas.footer_visible = False         fig.set_facecolor('#ffffff') #white         gs  = GridSpec(5, 3, figure=fig)         ax0 = fig.add_subplot(gs[0:3, :])         ax1 = fig.add_subplot(gs[-2, :])         ax2 = fig.add_subplot(gs[-1, :])         ax1.set_axis_off()         ax2.set_axis_off()         self.ax = [ax0,ax1,ax2]         self.fig = fig          self.axfitdata = plt.axes([0.26,0.124,0.12,0.1 ])  #lx,by,w,h         self.bfitdata  = Button(self.axfitdata , 'fit data', color=dlc['dlblue'])         self.bfitdata.label.set_fontsize(12)         self.bfitdata.on_clicked(self.fitdata_clicked)          #clear data is a future enhancement         #self.axclrdata = plt.axes([0.26,0.06,0.12,0.05 ])  #lx,by,w,h         #self.bclrdata  = Button(self.axclrdata , 'clear data', color='white')         #self.bclrdata.label.set_fontsize(12)         #self.bclrdata.on_clicked(self.clrdata_clicked)          self.cid = fig.canvas.mpl_connect('button_press_event', self.add_data)          self.typebut = button_manager(fig, [0.4, 0.07,0.15,0.15], [\"Regression\", \"Categorical\"],                                        [False,True], self.toggle_type)          self.fig.text(0.1, 0.02+0.21, \"Degree\", fontsize=12)         self.degrbut = button_manager(fig,[0.1,0.02,0.15,0.2 ], ['1','2','3','4','5','6'],                                         [True, False, False, False, False, False], self.update_equation)         if self.regularize:             self.fig.text(0.6, 0.02+0.21, r\"lambda($\\lambda$)\", fontsize=12)             self.lambut = button_manager(fig,[0.6,0.02,0.15,0.2 ], ['0.0','0.2','0.4','0.6','0.8','1'],                                         [True, False, False, False, False, False], self.updt_lambda)          #self.regbut =  button_manager(fig, [0.8, 0.08,0.24,0.15], [\"Regularize\"],         #                               [False], self.toggle_reg)         #self.logistic_data()      def updt_lambda(self, idx, firsttime=False):       # pylint: disable=unused-argument         self.lambda_ = idx * 0.2      def toggle_type(self, idx, firsttime=False):         self.logistic = idx==1         self.ax[0].clear()         if self.logistic:             self.logistic_data()         else:             self.linear_data()         if not firsttime:             self.degrbut.reinit()      @output.capture()  # debug     def logistic_data(self,redraw=False):         if not redraw:             m = 50             n = 2             np.random.seed(2)             X_train = 2*(np.random.rand(m,n)-[0.5,0.5])             y_train = X_train[:,1]+0.5  &gt; X_train[:,0]**2 + 0.5*np.random.rand(m) #quadratic + random             y_train = y_train + 0  #convert from boolean to integer             self.X = X_train             self.y = y_train             self.x_ideal = np.sort(X_train[:,0])             self.y_ideal =  self.x_ideal**2           self.ax[0].plot(self.x_ideal, self.y_ideal, \"--\", color = \"orangered\", label=\"ideal\", lw=1)         plot_data(self.X, self.y, self.ax[0], s=10, loc='lower right')         self.ax[0].set_title(\"OverFitting Example: Categorical data set with noise\")         self.ax[0].text(0.5,0.93, \"Click on plot to add data. Hold [Shift] for blue(y=0) data.\",                         fontsize=12, ha='center',transform=self.ax[0].transAxes, color=dlc[\"dlblue\"])         self.ax[0].set_xlabel(r\"$x_0$\")         self.ax[0].set_ylabel(r\"$x_1$\")      def linear_data(self,redraw=False):         if not redraw:             m = 30             c = 0             x_train = np.arange(0,m,1)             np.random.seed(1)             y_ideal = x_train**2 + c             y_train = y_ideal + 0.7 * y_ideal*(np.random.sample((m,))-0.5)             self.x_ideal = x_train #for redraw when new data included in X             self.X = x_train             self.y = y_train             self.y_ideal = y_ideal         else:             self.ax[0].set_xlim(self.xlim)             self.ax[0].set_ylim(self.ylim)          self.ax[0].scatter(self.X,self.y, label=\"y\")         self.ax[0].plot(self.x_ideal, self.y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)         self.ax[0].set_title(\"OverFitting Example: Regression Data Set (quadratic with noise)\",fontsize = 14)         self.ax[0].set_xlabel(\"x\")         self.ax[0].set_ylabel(\"y\")         self.ax0ledgend = self.ax[0].legend(loc='lower right')         self.ax[0].text(0.5,0.93, \"Click on plot to add data\",                         fontsize=12, ha='center',transform=self.ax[0].transAxes, color=dlc[\"dlblue\"])         if not redraw:             self.xlim = self.ax[0].get_xlim()             self.ylim = self.ax[0].get_ylim()       @output.capture()  # debug     def add_data(self, event):         if self.logistic:             self.add_data_logistic(event)         else:             self.add_data_linear(event)      @output.capture()  # debug     def add_data_logistic(self, event):         if event.inaxes == self.ax[0]:             x0_coord = event.xdata             x1_coord = event.ydata              if event.key is None:  #shift not pressed                 self.ax[0].scatter(x0_coord, x1_coord, marker='x', s=10, c = 'red', label=\"y=1\")                 self.y = np.append(self.y,1)             else:                 self.ax[0].scatter(x0_coord, x1_coord, marker='o', s=10, label=\"y=0\", facecolors='none',                                    edgecolors=dlc['dlblue'],lw=3)                 self.y = np.append(self.y,0)             self.X = np.append(self.X,np.array([[x0_coord, x1_coord]]),axis=0)         self.fig.canvas.draw()      def add_data_linear(self, event):         if event.inaxes == self.ax[0]:             x_coord = event.xdata             y_coord = event.ydata              self.ax[0].scatter(x_coord, y_coord, marker='o', s=10, facecolors='none',                                    edgecolors=dlc['dlblue'],lw=3)             self.y = np.append(self.y,y_coord)             self.X = np.append(self.X,x_coord)             self.fig.canvas.draw()      #@output.capture()  # debug     #def clrdata_clicked(self,event):     #    if self.logistic == True:     #        self.X = np.     #    else:     #        self.linear_regression()       @output.capture()  # debug     def fitdata_clicked(self,event):         if self.logistic:             self.logistic_regression()         else:             self.linear_regression()      def linear_regression(self):         self.ax[0].clear()         self.fig.canvas.draw()          # create and fit the model using our mapped_X feature set.         self.X_mapped, _ =  map_one_feature(self.X, self.degree)         self.X_mapped_scaled, self.X_mu, self.X_sigma  = zscore_normalize_features(self.X_mapped)          #linear_model = LinearRegression()         linear_model = Ridge(alpha=self.lambda_, normalize=True, max_iter=10000)         linear_model.fit(self.X_mapped_scaled, self.y )         self.w = linear_model.coef_.reshape(-1,)         self.b = linear_model.intercept_         x = np.linspace(*self.xlim,30)  #plot line idependent of data which gets disordered         xm, _ =  map_one_feature(x, self.degree)         xms = (xm - self.X_mu)/ self.X_sigma         y_pred = linear_model.predict(xms)          #self.fig.canvas.draw()         self.linear_data(redraw=True)         self.ax0yfit = self.ax[0].plot(x, y_pred, color = \"blue\", label=\"y_fit\")         self.ax0ledgend = self.ax[0].legend(loc='lower right')         self.fig.canvas.draw()      def logistic_regression(self):         self.ax[0].clear()         self.fig.canvas.draw()          # create and fit the model using our mapped_X feature set.         self.X_mapped, _ =  map_feature(self.X[:, 0], self.X[:, 1], self.degree)         self.X_mapped_scaled, self.X_mu, self.X_sigma  = zscore_normalize_features(self.X_mapped)         if not self.regularize or self.lambda_ == 0:             lr = LogisticRegression(penalty='none', max_iter=10000)         else:             C = 1/self.lambda_             lr = LogisticRegression(C=C, max_iter=10000)          lr.fit(self.X_mapped_scaled,self.y)         #print(lr.score(self.X_mapped_scaled, self.y))         self.w = lr.coef_.reshape(-1,)         self.b = lr.intercept_         #print(self.w, self.b)         self.logistic_data(redraw=True)         self.contour = plot_decision_boundary(self.ax[0],[-1,1],[-1,1], predict_logistic, self.w, self.b,                        scaler=True, mu=self.X_mu, sigma=self.X_sigma, degree=self.degree )         self.fig.canvas.draw()      @output.capture()  # debug     def update_equation(self, idx, firsttime=False):         #print(f\"Update equation, index = {idx}, firsttime={firsttime}\")         self.degree = idx+1         if firsttime:             self.eqtext = []         else:             for artist in self.eqtext:                 #print(artist)                 artist.remove()             self.eqtext = []         if self.logistic:             _, equation =  map_feature(self.X[:, 0], self.X[:, 1], self.degree)             string = 'f_{wb} = sigmoid('         else:             _, equation =  map_one_feature(self.X, self.degree)             string = 'f_{wb} = ('         bz = 10         seq = equation.split('+')         blks = math.ceil(len(seq)/bz)         for i in range(blks):             if i == 0:                 string = string +  '+'.join(seq[bz*i:bz*i+bz])             else:                 string = '+'.join(seq[bz*i:bz*i+bz])             string = string + ')' if i == blks-1 else string + '+'             ei = self.ax[1].text(0.01,(0.75-i*0.25), f\"${string}$\",fontsize=9,                                  transform = self.ax[1].transAxes, ma='left', va='top' )             self.eqtext.append(ei)         self.fig.canvas.draw()"},{"location":"MachineLearning/part3/plt_quad_logistic/","title":"Plt quad logistic","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nplt_quad_logistic.py\n    interactive plot and supporting routines showing logistic regression\n\"\"\"\n</pre> \"\"\" plt_quad_logistic.py     interactive plot and supporting routines showing logistic regression \"\"\" In\u00a0[\u00a0]: Copied! <pre>import time\nfrom matplotlib import cm\nimport matplotlib.colors as colors\nfrom matplotlib.gridspec import GridSpec\nfrom matplotlib.widgets import Button\nfrom matplotlib.patches import FancyArrowPatch\nfrom ipywidgets import Output\nfrom lab_utils_common import np, plt, dlc, dlcolors, sigmoid, compute_cost_matrix, gradient_descent\n</pre> import time from matplotlib import cm import matplotlib.colors as colors from matplotlib.gridspec import GridSpec from matplotlib.widgets import Button from matplotlib.patches import FancyArrowPatch from ipywidgets import Output from lab_utils_common import np, plt, dlc, dlcolors, sigmoid, compute_cost_matrix, gradient_descent <p>for debug output = Output() # sends hidden error messages to display when using widgets display(output)</p> In\u00a0[\u00a0]: Copied! <pre>class plt_quad_logistic:\n    ''' plots a quad plot showing logistic regression '''\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-locals\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self, x_train,y_train, w_range, b_range):\n        # setup figure\n        fig = plt.figure( figsize=(10,6))\n        fig.canvas.toolbar_visible = False\n        fig.canvas.header_visible = False\n        fig.canvas.footer_visible = False\n        fig.set_facecolor('#ffffff') #white\n        gs  = GridSpec(2, 2, figure=fig)\n        ax0 = fig.add_subplot(gs[0, 0])\n        ax1 = fig.add_subplot(gs[0, 1])\n        ax2 = fig.add_subplot(gs[1, 0],  projection='3d')\n        ax3 = fig.add_subplot(gs[1,1])\n        pos = ax3.get_position().get_points()  ##[[lb_x,lb_y], [rt_x, rt_y]]\n        h = 0.05 \n        width = 0.2\n        axcalc   = plt.axes([pos[1,0]-width, pos[1,1]-h, width, h])  #lx,by,w,h\n        ax = np.array([ax0, ax1, ax2, ax3, axcalc])\n        self.fig = fig\n        self.ax = ax\n        self.x_train = x_train\n        self.y_train = y_train\n\n        self.w = 0. #initial point, non-array\n        self.b = 0.\n\n        # initialize subplots\n        self.dplot = data_plot(ax[0], x_train, y_train, self.w, self.b)\n        self.con_plot = contour_and_surface_plot(ax[1], ax[2], x_train, y_train, w_range, b_range, self.w, self.b)\n        self.cplot = cost_plot(ax[3])\n\n        # setup events\n        self.cid = fig.canvas.mpl_connect('button_press_event', self.click_contour)\n        self.bcalc = Button(axcalc, 'Run Gradient Descent \\nfrom current w,b (click)', color=dlc[\"dlorange\"])\n        self.bcalc.on_clicked(self.calc_logistic)\n</pre> class plt_quad_logistic:     ''' plots a quad plot showing logistic regression '''     # pylint: disable=too-many-instance-attributes     # pylint: disable=too-many-locals     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self, x_train,y_train, w_range, b_range):         # setup figure         fig = plt.figure( figsize=(10,6))         fig.canvas.toolbar_visible = False         fig.canvas.header_visible = False         fig.canvas.footer_visible = False         fig.set_facecolor('#ffffff') #white         gs  = GridSpec(2, 2, figure=fig)         ax0 = fig.add_subplot(gs[0, 0])         ax1 = fig.add_subplot(gs[0, 1])         ax2 = fig.add_subplot(gs[1, 0],  projection='3d')         ax3 = fig.add_subplot(gs[1,1])         pos = ax3.get_position().get_points()  ##[[lb_x,lb_y], [rt_x, rt_y]]         h = 0.05          width = 0.2         axcalc   = plt.axes([pos[1,0]-width, pos[1,1]-h, width, h])  #lx,by,w,h         ax = np.array([ax0, ax1, ax2, ax3, axcalc])         self.fig = fig         self.ax = ax         self.x_train = x_train         self.y_train = y_train          self.w = 0. #initial point, non-array         self.b = 0.          # initialize subplots         self.dplot = data_plot(ax[0], x_train, y_train, self.w, self.b)         self.con_plot = contour_and_surface_plot(ax[1], ax[2], x_train, y_train, w_range, b_range, self.w, self.b)         self.cplot = cost_plot(ax[3])          # setup events         self.cid = fig.canvas.mpl_connect('button_press_event', self.click_contour)         self.bcalc = Button(axcalc, 'Run Gradient Descent \\nfrom current w,b (click)', color=dlc[\"dlorange\"])         self.bcalc.on_clicked(self.calc_logistic) In\u00a0[\u00a0]: Copied! <pre>#    @output.capture()  # debug\n    def click_contour(self, event):\n        ''' called when click in contour '''\n        if event.inaxes == self.ax[1]:   #contour plot\n            self.w = event.xdata\n            self.b = event.ydata\n\n            self.cplot.re_init()\n            self.dplot.update(self.w, self.b)\n            self.con_plot.update_contour_wb_lines(self.w, self.b)\n            self.con_plot.path.re_init(self.w, self.b)\n\n            self.fig.canvas.draw()\n</pre> #    @output.capture()  # debug     def click_contour(self, event):         ''' called when click in contour '''         if event.inaxes == self.ax[1]:   #contour plot             self.w = event.xdata             self.b = event.ydata              self.cplot.re_init()             self.dplot.update(self.w, self.b)             self.con_plot.update_contour_wb_lines(self.w, self.b)             self.con_plot.path.re_init(self.w, self.b)              self.fig.canvas.draw() In\u00a0[\u00a0]: Copied! <pre>#    @output.capture()  # debug\n    def calc_logistic(self, event):\n        ''' called on run gradient event '''\n        for it in [1, 8,16,32,64,128,256,512,1024,2048,4096]:\n            w, self.b, J_hist = gradient_descent(self.x_train.reshape(-1,1), self.y_train.reshape(-1,1),\n                                                 np.array(self.w).reshape(-1,1), self.b, 0.1, it,\n                                                 logistic=True, lambda_=0, verbose=False)\n            self.w = w[0,0]\n            self.dplot.update(self.w, self.b)\n            self.con_plot.update_contour_wb_lines(self.w, self.b)\n            self.con_plot.path.add_path_item(self.w,self.b)\n            self.cplot.add_cost(J_hist)\n\n            time.sleep(0.3)\n            self.fig.canvas.draw()\n</pre> #    @output.capture()  # debug     def calc_logistic(self, event):         ''' called on run gradient event '''         for it in [1, 8,16,32,64,128,256,512,1024,2048,4096]:             w, self.b, J_hist = gradient_descent(self.x_train.reshape(-1,1), self.y_train.reshape(-1,1),                                                  np.array(self.w).reshape(-1,1), self.b, 0.1, it,                                                  logistic=True, lambda_=0, verbose=False)             self.w = w[0,0]             self.dplot.update(self.w, self.b)             self.con_plot.update_contour_wb_lines(self.w, self.b)             self.con_plot.path.add_path_item(self.w,self.b)             self.cplot.add_cost(J_hist)              time.sleep(0.3)             self.fig.canvas.draw() In\u00a0[\u00a0]: Copied! <pre>class data_plot:\n    ''' handles data plot '''\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self, ax, x_train, y_train, w, b):\n        self.ax = ax\n        self.x_train = x_train\n        self.y_train = y_train\n        self.m = x_train.shape[0]\n        self.w = w\n        self.b = b\n\n        self.plt_tumor_data()\n        self.draw_logistic_lines(firsttime=True)\n        self.mk_cost_lines(firsttime=True)\n\n        self.ax.autoscale(enable=False) # leave plot scales the same after initial setup\n\n    def plt_tumor_data(self):\n        x = self.x_train\n        y = self.y_train\n        pos = y == 1\n        neg = y == 0\n        self.ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n        self.ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none',\n                   edgecolors=dlc[\"dlblue\"],lw=3)\n        self.ax.set_ylim(-0.175,1.1)\n        self.ax.set_ylabel('y')\n        self.ax.set_xlabel('Tumor Size')\n        self.ax.set_title(\"Logistic Regression on Categorical Data\")\n\n    def update(self, w, b):\n        self.w = w\n        self.b = b\n        self.draw_logistic_lines()\n        self.mk_cost_lines()\n\n    def draw_logistic_lines(self, firsttime=False):\n        if not firsttime:\n            self.aline[0].remove()\n            self.bline[0].remove()\n            self.alegend.remove()\n\n        xlim  = self.ax.get_xlim()\n        x_hat = np.linspace(*xlim, 30)\n        y_hat = sigmoid(np.dot(x_hat.reshape(-1,1), self.w) + self.b)\n        self.aline = self.ax.plot(x_hat, y_hat, color=dlc[\"dlblue\"],\n                                     label=\"y = sigmoid(z)\")\n        f_wb = np.dot(x_hat.reshape(-1,1), self.w) + self.b\n        self.bline = self.ax.plot(x_hat, f_wb, color=dlc[\"dlorange\"], lw=1,\n                                     label=f\"z = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")\n        self.alegend = self.ax.legend(loc='upper left')\n\n    def mk_cost_lines(self, firsttime=False):\n        ''' makes vertical cost lines'''\n        if not firsttime:\n            for artist in self.cost_items:\n                artist.remove()\n        self.cost_items = []\n        cstr = f\"cost = (1/{self.m})*(\"\n        ctot = 0\n        label = 'cost for point'\n        addedbreak = False\n        for p in zip(self.x_train,self.y_train):\n            f_wb_p = sigmoid(self.w*p[0]+self.b)\n            c_p = compute_cost_matrix(p[0].reshape(-1,1), p[1],np.array(self.w), self.b, logistic=True, lambda_=0, safe=True)\n            c_p_txt = c_p\n            a = self.ax.vlines(p[0], p[1],f_wb_p, lw=3, color=dlc[\"dlpurple\"], ls='dotted', label=label)\n            label='' #just one\n            cxy = [p[0], p[1] + (f_wb_p-p[1])/2]\n            b = self.ax.annotate(f'{c_p_txt:0.1f}', xy=cxy, xycoords='data',color=dlc[\"dlpurple\"],\n                        xytext=(5, 0), textcoords='offset points')\n            cstr += f\"{c_p_txt:0.1f} +\"\n            if len(cstr) &gt; 38 and addedbreak is False:\n                cstr += \"\\n\"\n                addedbreak = True\n            ctot += c_p\n            self.cost_items.extend((a,b))\n        ctot = ctot/(len(self.x_train))\n        cstr = cstr[:-1] + f\") = {ctot:0.2f}\"\n        ## todo.. figure out how to get this textbox to extend to the width of the subplot\n        c = self.ax.text(0.05,0.02,cstr, transform=self.ax.transAxes, color=dlc[\"dlpurple\"])\n        self.cost_items.append(c)\n</pre> class data_plot:     ''' handles data plot '''     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self, ax, x_train, y_train, w, b):         self.ax = ax         self.x_train = x_train         self.y_train = y_train         self.m = x_train.shape[0]         self.w = w         self.b = b          self.plt_tumor_data()         self.draw_logistic_lines(firsttime=True)         self.mk_cost_lines(firsttime=True)          self.ax.autoscale(enable=False) # leave plot scales the same after initial setup      def plt_tumor_data(self):         x = self.x_train         y = self.y_train         pos = y == 1         neg = y == 0         self.ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")         self.ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none',                    edgecolors=dlc[\"dlblue\"],lw=3)         self.ax.set_ylim(-0.175,1.1)         self.ax.set_ylabel('y')         self.ax.set_xlabel('Tumor Size')         self.ax.set_title(\"Logistic Regression on Categorical Data\")      def update(self, w, b):         self.w = w         self.b = b         self.draw_logistic_lines()         self.mk_cost_lines()      def draw_logistic_lines(self, firsttime=False):         if not firsttime:             self.aline[0].remove()             self.bline[0].remove()             self.alegend.remove()          xlim  = self.ax.get_xlim()         x_hat = np.linspace(*xlim, 30)         y_hat = sigmoid(np.dot(x_hat.reshape(-1,1), self.w) + self.b)         self.aline = self.ax.plot(x_hat, y_hat, color=dlc[\"dlblue\"],                                      label=\"y = sigmoid(z)\")         f_wb = np.dot(x_hat.reshape(-1,1), self.w) + self.b         self.bline = self.ax.plot(x_hat, f_wb, color=dlc[\"dlorange\"], lw=1,                                      label=f\"z = {np.squeeze(self.w):0.2f}x+({self.b:0.2f})\")         self.alegend = self.ax.legend(loc='upper left')      def mk_cost_lines(self, firsttime=False):         ''' makes vertical cost lines'''         if not firsttime:             for artist in self.cost_items:                 artist.remove()         self.cost_items = []         cstr = f\"cost = (1/{self.m})*(\"         ctot = 0         label = 'cost for point'         addedbreak = False         for p in zip(self.x_train,self.y_train):             f_wb_p = sigmoid(self.w*p[0]+self.b)             c_p = compute_cost_matrix(p[0].reshape(-1,1), p[1],np.array(self.w), self.b, logistic=True, lambda_=0, safe=True)             c_p_txt = c_p             a = self.ax.vlines(p[0], p[1],f_wb_p, lw=3, color=dlc[\"dlpurple\"], ls='dotted', label=label)             label='' #just one             cxy = [p[0], p[1] + (f_wb_p-p[1])/2]             b = self.ax.annotate(f'{c_p_txt:0.1f}', xy=cxy, xycoords='data',color=dlc[\"dlpurple\"],                         xytext=(5, 0), textcoords='offset points')             cstr += f\"{c_p_txt:0.1f} +\"             if len(cstr) &gt; 38 and addedbreak is False:                 cstr += \"\\n\"                 addedbreak = True             ctot += c_p             self.cost_items.extend((a,b))         ctot = ctot/(len(self.x_train))         cstr = cstr[:-1] + f\") = {ctot:0.2f}\"         ## todo.. figure out how to get this textbox to extend to the width of the subplot         c = self.ax.text(0.05,0.02,cstr, transform=self.ax.transAxes, color=dlc[\"dlpurple\"])         self.cost_items.append(c) In\u00a0[\u00a0]: Copied! <pre>class contour_and_surface_plot:\n    ''' plots combined in class as they have similar operations '''\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self, axc, axs, x_train, y_train, w_range, b_range, w, b):\n\n        self.x_train = x_train\n        self.y_train = y_train\n        self.axc = axc\n        self.axs = axs\n\n        #setup useful ranges and common linspaces\n        b_space  = np.linspace(*b_range, 100)\n        w_space  = np.linspace(*w_range, 100)\n\n        # get cost for w,b ranges for contour and 3D\n        tmp_b,tmp_w = np.meshgrid(b_space,w_space)\n        z = np.zeros_like(tmp_b)\n        for i in range(tmp_w.shape[0]):\n            for j in range(tmp_w.shape[1]):\n                z[i,j] = compute_cost_matrix(x_train.reshape(-1,1), y_train, tmp_w[i,j], tmp_b[i,j],\n                                             logistic=True, lambda_=0, safe=True)\n                if z[i,j] == 0:\n                    z[i,j] = 1e-9\n\n        ### plot contour ###\n        CS = axc.contour(tmp_w, tmp_b, np.log(z),levels=12, linewidths=2, alpha=0.7,colors=dlcolors)\n        axc.set_title('log(Cost(w,b))')\n        axc.set_xlabel('w', fontsize=10)\n        axc.set_ylabel('b', fontsize=10)\n        axc.set_xlim(w_range)\n        axc.set_ylim(b_range)\n        self.update_contour_wb_lines(w, b, firsttime=True)\n        axc.text(0.7,0.05,\"Click to choose w,b\",  bbox=dict(facecolor='white', ec = 'black'), fontsize = 10,\n                transform=axc.transAxes, verticalalignment = 'center', horizontalalignment= 'center')\n\n        #Surface plot of the cost function J(w,b)\n        axs.plot_surface(tmp_w, tmp_b, z,  cmap = cm.jet, alpha=0.3, antialiased=True)\n        axs.plot_wireframe(tmp_w, tmp_b, z, color='k', alpha=0.1)\n        axs.set_xlabel(\"$w$\")\n        axs.set_ylabel(\"$b$\")\n        axs.zaxis.set_rotate_label(False)\n        axs.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        axs.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        axs.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        axs.set_zlabel(\"J(w, b)\", rotation=90)\n        axs.view_init(30, -120)\n\n        axs.autoscale(enable=False)\n        axc.autoscale(enable=False)\n\n        self.path = path(self.w,self.b, self.axc)  # initialize an empty path, avoids existance check\n\n    def update_contour_wb_lines(self, w, b, firsttime=False):\n        self.w = w\n        self.b = b\n        cst = compute_cost_matrix(self.x_train.reshape(-1,1), self.y_train, np.array(self.w), self.b,\n                                  logistic=True, lambda_=0, safe=True)\n\n        # remove lines and re-add on contour plot and 3d plot\n        if not firsttime:\n            for artist in self.dyn_items:\n                artist.remove()\n        a = self.axc.scatter(self.w, self.b, s=100, color=dlc[\"dlblue\"], zorder= 10, label=\"cost with \\ncurrent w,b\")\n        b = self.axc.hlines(self.b, self.axc.get_xlim()[0], self.w, lw=4, color=dlc[\"dlpurple\"], ls='dotted')\n        c = self.axc.vlines(self.w, self.axc.get_ylim()[0] ,self.b, lw=4, color=dlc[\"dlpurple\"], ls='dotted')\n        d = self.axc.annotate(f\"Cost: {cst:0.2f}\", xy= (self.w, self.b), xytext = (4,4), textcoords = 'offset points',\n                           bbox=dict(facecolor='white'), size = 10)\n        #Add point in 3D surface plot\n        e = self.axs.scatter3D(self.w, self.b, cst , marker='X', s=100)\n\n        self.dyn_items = [a,b,c,d,e]\n</pre> class contour_and_surface_plot:     ''' plots combined in class as they have similar operations '''     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self, axc, axs, x_train, y_train, w_range, b_range, w, b):          self.x_train = x_train         self.y_train = y_train         self.axc = axc         self.axs = axs          #setup useful ranges and common linspaces         b_space  = np.linspace(*b_range, 100)         w_space  = np.linspace(*w_range, 100)          # get cost for w,b ranges for contour and 3D         tmp_b,tmp_w = np.meshgrid(b_space,w_space)         z = np.zeros_like(tmp_b)         for i in range(tmp_w.shape[0]):             for j in range(tmp_w.shape[1]):                 z[i,j] = compute_cost_matrix(x_train.reshape(-1,1), y_train, tmp_w[i,j], tmp_b[i,j],                                              logistic=True, lambda_=0, safe=True)                 if z[i,j] == 0:                     z[i,j] = 1e-9          ### plot contour ###         CS = axc.contour(tmp_w, tmp_b, np.log(z),levels=12, linewidths=2, alpha=0.7,colors=dlcolors)         axc.set_title('log(Cost(w,b))')         axc.set_xlabel('w', fontsize=10)         axc.set_ylabel('b', fontsize=10)         axc.set_xlim(w_range)         axc.set_ylim(b_range)         self.update_contour_wb_lines(w, b, firsttime=True)         axc.text(0.7,0.05,\"Click to choose w,b\",  bbox=dict(facecolor='white', ec = 'black'), fontsize = 10,                 transform=axc.transAxes, verticalalignment = 'center', horizontalalignment= 'center')          #Surface plot of the cost function J(w,b)         axs.plot_surface(tmp_w, tmp_b, z,  cmap = cm.jet, alpha=0.3, antialiased=True)         axs.plot_wireframe(tmp_w, tmp_b, z, color='k', alpha=0.1)         axs.set_xlabel(\"$w$\")         axs.set_ylabel(\"$b$\")         axs.zaxis.set_rotate_label(False)         axs.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         axs.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         axs.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         axs.set_zlabel(\"J(w, b)\", rotation=90)         axs.view_init(30, -120)          axs.autoscale(enable=False)         axc.autoscale(enable=False)          self.path = path(self.w,self.b, self.axc)  # initialize an empty path, avoids existance check      def update_contour_wb_lines(self, w, b, firsttime=False):         self.w = w         self.b = b         cst = compute_cost_matrix(self.x_train.reshape(-1,1), self.y_train, np.array(self.w), self.b,                                   logistic=True, lambda_=0, safe=True)          # remove lines and re-add on contour plot and 3d plot         if not firsttime:             for artist in self.dyn_items:                 artist.remove()         a = self.axc.scatter(self.w, self.b, s=100, color=dlc[\"dlblue\"], zorder= 10, label=\"cost with \\ncurrent w,b\")         b = self.axc.hlines(self.b, self.axc.get_xlim()[0], self.w, lw=4, color=dlc[\"dlpurple\"], ls='dotted')         c = self.axc.vlines(self.w, self.axc.get_ylim()[0] ,self.b, lw=4, color=dlc[\"dlpurple\"], ls='dotted')         d = self.axc.annotate(f\"Cost: {cst:0.2f}\", xy= (self.w, self.b), xytext = (4,4), textcoords = 'offset points',                            bbox=dict(facecolor='white'), size = 10)         #Add point in 3D surface plot         e = self.axs.scatter3D(self.w, self.b, cst , marker='X', s=100)          self.dyn_items = [a,b,c,d,e] In\u00a0[\u00a0]: Copied! <pre>class cost_plot:\n    \"\"\" manages cost plot for plt_quad_logistic \"\"\"\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self,ax):\n        self.ax = ax\n        self.ax.set_ylabel(\"log(cost)\")\n        self.ax.set_xlabel(\"iteration\")\n        self.costs = []\n        self.cline = self.ax.plot(0,0, color=dlc[\"dlblue\"])\n\n    def re_init(self):\n        self.ax.clear()\n        self.__init__(self.ax)\n\n    def add_cost(self,J_hist):\n        self.costs.extend(J_hist)\n        self.cline[0].remove()\n        self.cline = self.ax.plot(self.costs)\n</pre> class cost_plot:     \"\"\" manages cost plot for plt_quad_logistic \"\"\"     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self,ax):         self.ax = ax         self.ax.set_ylabel(\"log(cost)\")         self.ax.set_xlabel(\"iteration\")         self.costs = []         self.cline = self.ax.plot(0,0, color=dlc[\"dlblue\"])      def re_init(self):         self.ax.clear()         self.__init__(self.ax)      def add_cost(self,J_hist):         self.costs.extend(J_hist)         self.cline[0].remove()         self.cline = self.ax.plot(self.costs) In\u00a0[\u00a0]: Copied! <pre>class path:\n    ''' tracks paths during gradient descent on contour plot '''\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=attribute-defined-outside-init\n    def __init__(self, w, b, ax):\n        ''' w, b at start of path '''\n        self.path_items = []\n        self.w = w\n        self.b = b\n        self.ax = ax\n\n    def re_init(self, w, b):\n        for artist in self.path_items:\n            artist.remove()\n        self.path_items = []\n        self.w = w\n        self.b = b\n\n    def add_path_item(self, w, b):\n        a = FancyArrowPatch(\n            posA=(self.w, self.b), posB=(w, b), color=dlc[\"dlblue\"],\n            arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n        )\n        self.ax.add_artist(a)\n        self.path_items.append(a)\n        self.w = w\n        self.b = b\n</pre> class path:     ''' tracks paths during gradient descent on contour plot '''     # pylint: disable=missing-function-docstring     # pylint: disable=attribute-defined-outside-init     def __init__(self, w, b, ax):         ''' w, b at start of path '''         self.path_items = []         self.w = w         self.b = b         self.ax = ax      def re_init(self, w, b):         for artist in self.path_items:             artist.remove()         self.path_items = []         self.w = w         self.b = b      def add_path_item(self, w, b):         a = FancyArrowPatch(             posA=(self.w, self.b), posB=(w, b), color=dlc[\"dlblue\"],             arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',         )         self.ax.add_artist(a)         self.path_items.append(a)         self.w = w         self.b = b In\u00a0[\u00a0]: Copied! <pre>def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n    \"\"\" truncates color map \"\"\"\n    new_cmap = colors.LinearSegmentedColormap.from_list(\n        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n        cmap(np.linspace(minval, maxval, n)))\n    return new_cmap\n</pre> def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):     \"\"\" truncates color map \"\"\"     new_cmap = colors.LinearSegmentedColormap.from_list(         'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),         cmap(np.linspace(minval, maxval, n)))     return new_cmap In\u00a0[\u00a0]: Copied! <pre>def plt_prob(ax, w_out,b_out):\n    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n    #setup useful ranges and common linspaces\n    x0_space  = np.linspace(0, 4 , 100)\n    x1_space  = np.linspace(0, 4 , 100)\n\n    # get probability for x0,x1 ranges\n    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n    z = np.zeros_like(tmp_x0)\n    for i in range(tmp_x0.shape[0]):\n        for j in range(tmp_x1.shape[1]):\n            z[i,j] = sigmoid(np.dot(w_out, np.array([tmp_x0[i,j],tmp_x1[i,j]])) + b_out)\n\n\n    cmap = plt.get_cmap('Blues')\n    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n    ax.figure.colorbar(pcm, ax=ax)\n</pre> def plt_prob(ax, w_out,b_out):     \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"     #setup useful ranges and common linspaces     x0_space  = np.linspace(0, 4 , 100)     x1_space  = np.linspace(0, 4 , 100)      # get probability for x0,x1 ranges     tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)     z = np.zeros_like(tmp_x0)     for i in range(tmp_x0.shape[0]):         for j in range(tmp_x1.shape[1]):             z[i,j] = sigmoid(np.dot(w_out, np.array([tmp_x0[i,j],tmp_x1[i,j]])) + b_out)       cmap = plt.get_cmap('Blues')     new_cmap = truncate_colormap(cmap, 0.0, 0.5)     pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,                    norm=cm.colors.Normalize(vmin=0, vmax=1),                    cmap=new_cmap, shading='nearest', alpha = 0.9)     ax.figure.colorbar(pcm, ax=ax)"},{"location":"MachineLearning/part3/plt_quad_logistic/#related-to-the-logistic-gradient-descent-lab","title":"related to the logistic gradient descent lab\u00b6","text":""},{"location":"MachineLearning/part3/public_tests/","title":"Public tests","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport math\n</pre> import numpy as np import math In\u00a0[\u00a0]: Copied! <pre>def sigmoid_test(target):\n    assert np.isclose(target(3.0), 0.9525741268224334), \"Failed for scalar input\"\n    assert np.allclose(target(np.array([2.5, 0])), [0.92414182, 0.5]), \"Failed for 1D array\"\n    assert np.allclose(target(np.array([[2.5, -2.5], [0, 1]])), \n                       [[0.92414182, 0.07585818], [0.5, 0.73105858]]), \"Failed for 2D array\"\n    print('\\033[92mAll tests passed!')\n</pre> def sigmoid_test(target):     assert np.isclose(target(3.0), 0.9525741268224334), \"Failed for scalar input\"     assert np.allclose(target(np.array([2.5, 0])), [0.92414182, 0.5]), \"Failed for 1D array\"     assert np.allclose(target(np.array([[2.5, -2.5], [0, 1]])),                         [[0.92414182, 0.07585818], [0.5, 0.73105858]]), \"Failed for 2D array\"     print('\\033[92mAll tests passed!') In\u00a0[\u00a0]: Copied! <pre>def compute_cost_test(target):\n    X = np.array([[0, 0, 0, 0]]).T\n    y = np.array([0, 0, 0, 0])\n    w = np.array([0])\n    b = 1\n    result = target(X, y, w, b)\n    if math.isinf(result):\n        raise ValueError(\"Did you get the sigmoid of z_wb?\")\n    \n    np.random.seed(17)  \n    X = np.random.randn(5, 2)\n    y = np.array([1, 0, 0, 1, 1])\n    w = np.random.randn(2)\n    b = 0\n    result = target(X, y, w, b)\n    assert np.isclose(result, 2.15510667), f\"Wrong output. Expected: {2.15510667} got: {result}\"\n    \n    X = np.random.randn(4, 3)\n    y = np.array([1, 1, 0, 0])\n    w = np.random.randn(3)\n    b = 0\n    \n    result = target(X, y, w, b)\n    assert np.isclose(result, 0.80709376), f\"Wrong output. Expected: {0.80709376} got: {result}\"\n\n    X = np.random.randn(4, 3)\n    y = np.array([1, 0,1, 0])\n    w = np.random.randn(3)\n    b = 3\n    result = target(X, y, w, b)\n    assert np.isclose(result, 0.4529660647), f\"Wrong output. Expected: {0.4529660647} got: {result}. Did you inizialized z_wb = b?\"\n    \n    print('\\033[92mAll tests passed!')\n</pre> def compute_cost_test(target):     X = np.array([[0, 0, 0, 0]]).T     y = np.array([0, 0, 0, 0])     w = np.array([0])     b = 1     result = target(X, y, w, b)     if math.isinf(result):         raise ValueError(\"Did you get the sigmoid of z_wb?\")          np.random.seed(17)       X = np.random.randn(5, 2)     y = np.array([1, 0, 0, 1, 1])     w = np.random.randn(2)     b = 0     result = target(X, y, w, b)     assert np.isclose(result, 2.15510667), f\"Wrong output. Expected: {2.15510667} got: {result}\"          X = np.random.randn(4, 3)     y = np.array([1, 1, 0, 0])     w = np.random.randn(3)     b = 0          result = target(X, y, w, b)     assert np.isclose(result, 0.80709376), f\"Wrong output. Expected: {0.80709376} got: {result}\"      X = np.random.randn(4, 3)     y = np.array([1, 0,1, 0])     w = np.random.randn(3)     b = 3     result = target(X, y, w, b)     assert np.isclose(result, 0.4529660647), f\"Wrong output. Expected: {0.4529660647} got: {result}. Did you inizialized z_wb = b?\"          print('\\033[92mAll tests passed!') In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_test(target):\n    np.random.seed(1)\n    X = np.random.randn(7, 3)\n    y = np.array([1, 0, 1, 0, 1, 1, 0])\n    test_w = np.array([1, 0.5, -0.35])\n    test_b = 1.7\n    dj_db, dj_dw  = target(X, y, test_w, test_b)\n    \n    assert np.isclose(dj_db, 0.28936094), f\"Wrong value for dj_db. Expected: {0.28936094} got: {dj_db}\" \n    assert dj_dw.shape == test_w.shape, f\"Wrong shape for dj_dw. Expected: {test_w.shape} got: {dj_dw.shape}\" \n    assert np.allclose(dj_dw, [-0.11999166, 0.41498775, -0.71968405]), f\"Wrong values for dj_dw. Got: {dj_dw}\"\n\n    print('\\033[92mAll tests passed!') \n</pre> def compute_gradient_test(target):     np.random.seed(1)     X = np.random.randn(7, 3)     y = np.array([1, 0, 1, 0, 1, 1, 0])     test_w = np.array([1, 0.5, -0.35])     test_b = 1.7     dj_db, dj_dw  = target(X, y, test_w, test_b)          assert np.isclose(dj_db, 0.28936094), f\"Wrong value for dj_db. Expected: {0.28936094} got: {dj_db}\"      assert dj_dw.shape == test_w.shape, f\"Wrong shape for dj_dw. Expected: {test_w.shape} got: {dj_dw.shape}\"      assert np.allclose(dj_dw, [-0.11999166, 0.41498775, -0.71968405]), f\"Wrong values for dj_dw. Got: {dj_dw}\"      print('\\033[92mAll tests passed!')  In\u00a0[\u00a0]: Copied! <pre>def predict_test(target):\n    np.random.seed(5)\n    b = 0.5    \n    w = np.random.randn(3)\n    X = np.random.randn(8, 3)\n    \n    result = target(X, w, b)\n    wrong_1 = [1., 1., 0., 0., 1., 0., 0., 1.]\n    expected_1 = [1., 1., 1., 0., 1., 0., 0., 1.]\n    if np.allclose(result, wrong_1):\n        raise ValueError(\"Did you apply the sigmoid before applying the threshold?\")\n    assert result.shape == (len(X),), f\"Wrong length. Expected : {(len(X),)} got: {result.shape}\"\n    assert np.allclose(result, expected_1), f\"Wrong output: Expected : {expected_1} got: {result}\"\n    \n    b = -1.7    \n    w = np.random.randn(4) + 0.6\n    X = np.random.randn(6, 4)\n    \n    result = target(X, w, b)\n    expected_2 = [0., 0., 0., 1., 1., 0.]\n    assert result.shape == (len(X),), f\"Wrong length. Expected : {(len(X),)} got: {result.shape}\"\n    assert np.allclose(result,expected_2), f\"Wrong output: Expected : {expected_2} got: {result}\"\n\n    print('\\033[92mAll tests passed!')\n</pre> def predict_test(target):     np.random.seed(5)     b = 0.5         w = np.random.randn(3)     X = np.random.randn(8, 3)          result = target(X, w, b)     wrong_1 = [1., 1., 0., 0., 1., 0., 0., 1.]     expected_1 = [1., 1., 1., 0., 1., 0., 0., 1.]     if np.allclose(result, wrong_1):         raise ValueError(\"Did you apply the sigmoid before applying the threshold?\")     assert result.shape == (len(X),), f\"Wrong length. Expected : {(len(X),)} got: {result.shape}\"     assert np.allclose(result, expected_1), f\"Wrong output: Expected : {expected_1} got: {result}\"          b = -1.7         w = np.random.randn(4) + 0.6     X = np.random.randn(6, 4)          result = target(X, w, b)     expected_2 = [0., 0., 0., 1., 1., 0.]     assert result.shape == (len(X),), f\"Wrong length. Expected : {(len(X),)} got: {result.shape}\"     assert np.allclose(result,expected_2), f\"Wrong output: Expected : {expected_2} got: {result}\"      print('\\033[92mAll tests passed!') In\u00a0[\u00a0]: Copied! <pre>def compute_cost_reg_test(target):\n    np.random.seed(1)\n    w = np.random.randn(3)\n    b = 0.4\n    X = np.random.randn(6, 3)\n    y = np.array([0, 1, 1, 0, 1, 1])\n    lambda_ = 0.1\n    expected_output = target(X, y, w, b, lambda_)\n    \n    assert np.isclose(expected_output, 0.5469746792761936), f\"Wrong output. Expected: {0.5469746792761936} got:{expected_output}\"\n    \n    w = np.random.randn(5)\n    b = -0.6\n    X = np.random.randn(8, 5)\n    y = np.array([1, 0, 1, 0, 0, 1, 0, 1])\n    lambda_ = 0.01\n    output = target(X, y, w, b, lambda_)\n    assert np.isclose(output, 1.2608591964119995), f\"Wrong output. Expected: {1.2608591964119995} got:{output}\"\n    \n    w = np.array([2, 2, 2, 2, 2])\n    b = 0\n    X = np.zeros((8, 5))\n    y = np.array([0.5] * 8)\n    lambda_ = 3\n    output = target(X, y, w, b, lambda_)\n    expected = -np.log(0.5) + 3. / (2. * 8.) * 20.\n    assert np.isclose(output, expected), f\"Wrong output. Expected: {expected} got:{output}\"\n    \n    print('\\033[92mAll tests passed!') \n</pre> def compute_cost_reg_test(target):     np.random.seed(1)     w = np.random.randn(3)     b = 0.4     X = np.random.randn(6, 3)     y = np.array([0, 1, 1, 0, 1, 1])     lambda_ = 0.1     expected_output = target(X, y, w, b, lambda_)          assert np.isclose(expected_output, 0.5469746792761936), f\"Wrong output. Expected: {0.5469746792761936} got:{expected_output}\"          w = np.random.randn(5)     b = -0.6     X = np.random.randn(8, 5)     y = np.array([1, 0, 1, 0, 0, 1, 0, 1])     lambda_ = 0.01     output = target(X, y, w, b, lambda_)     assert np.isclose(output, 1.2608591964119995), f\"Wrong output. Expected: {1.2608591964119995} got:{output}\"          w = np.array([2, 2, 2, 2, 2])     b = 0     X = np.zeros((8, 5))     y = np.array([0.5] * 8)     lambda_ = 3     output = target(X, y, w, b, lambda_)     expected = -np.log(0.5) + 3. / (2. * 8.) * 20.     assert np.isclose(output, expected), f\"Wrong output. Expected: {expected} got:{output}\"          print('\\033[92mAll tests passed!')  In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_reg_test(target):\n    np.random.seed(1)\n    w = np.random.randn(5)\n    b = 0.2\n    X = np.random.randn(7, 5)\n    y = np.array([0, 1, 1, 0, 1, 1, 0])\n    lambda_ = 0.1\n    expected1 = (-0.1506447567869257, np.array([ 0.19530838, -0.00632206,  0.19687367,  0.15741161,  0.02791437]))\n    dj_db, dj_dw = target(X, y, w, b, lambda_)\n    \n    assert np.isclose(dj_db, expected1[0]), f\"Wrong dj_db. Expected: {expected1[0]} got: {dj_db}\"\n    assert np.allclose(dj_dw, expected1[1]), f\"Wrong dj_dw. Expected: {expected1[1]} got: {dj_dw}\"\n\n    \n    w = np.random.randn(7)\n    b = 0\n    X = np.random.randn(7, 7)\n    y = np.array([1, 0, 0, 0, 1, 1, 0])\n    lambda_ = 0\n    expected2 = (0.02660329857573818, np.array([ 0.23567643, -0.06921029, -0.19705212, -0.0002884 ,  0.06490588,\n        0.26948175,  0.10777992]))\n    dj_db, dj_dw = target(X, y, w, b, lambda_)\n    assert np.isclose(dj_db, expected2[0]), f\"Wrong dj_db. Expected: {expected2[0]} got: {dj_db}\"\n    assert np.allclose(dj_dw, expected2[1]), f\"Wrong dj_dw. Expected: {expected2[1]} got: {dj_dw}\"\n    \n    print('\\033[92mAll tests passed!') \n</pre> def compute_gradient_reg_test(target):     np.random.seed(1)     w = np.random.randn(5)     b = 0.2     X = np.random.randn(7, 5)     y = np.array([0, 1, 1, 0, 1, 1, 0])     lambda_ = 0.1     expected1 = (-0.1506447567869257, np.array([ 0.19530838, -0.00632206,  0.19687367,  0.15741161,  0.02791437]))     dj_db, dj_dw = target(X, y, w, b, lambda_)          assert np.isclose(dj_db, expected1[0]), f\"Wrong dj_db. Expected: {expected1[0]} got: {dj_db}\"     assert np.allclose(dj_dw, expected1[1]), f\"Wrong dj_dw. Expected: {expected1[1]} got: {dj_dw}\"           w = np.random.randn(7)     b = 0     X = np.random.randn(7, 7)     y = np.array([1, 0, 0, 0, 1, 1, 0])     lambda_ = 0     expected2 = (0.02660329857573818, np.array([ 0.23567643, -0.06921029, -0.19705212, -0.0002884 ,  0.06490588,         0.26948175,  0.10777992]))     dj_db, dj_dw = target(X, y, w, b, lambda_)     assert np.isclose(dj_db, expected2[0]), f\"Wrong dj_db. Expected: {expected2[0]} got: {dj_db}\"     assert np.allclose(dj_dw, expected2[1]), f\"Wrong dj_dw. Expected: {expected2[1]} got: {dj_dw}\"          print('\\033[92mAll tests passed!')"},{"location":"MachineLearning/part3/test_utils/","title":"Test utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom copy import deepcopy\n</pre> import numpy as np from copy import deepcopy In\u00a0[\u00a0]: Copied! <pre>def datatype_check(expected_output, target_output, error):\n    success = 0\n    if isinstance(target_output, dict):\n        for key in target_output.keys():\n            try:\n                success += datatype_check(expected_output[key],\n                                          target_output[key], error)\n            except:\n                print(\"Error: {} in variable {}. Got {} but expected type {}\".format(error,\n                                                                                     key,\n                                                                                     type(\n                                                                                         target_output[key]),\n                                                                                     type(expected_output[key])))\n        if success == len(target_output.keys()):\n            return 1\n        else:\n            return 0\n    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n        for i in range(len(target_output)):\n            try:\n                success += datatype_check(expected_output[i],\n                                          target_output[i], error)\n            except:\n                print(\"Error: {} in variable {}, expected type: {}  but expected type {}\".format(error,\n                                                                                                 i,\n                                                                                                 type(\n                                                                                                     target_output[i]),\n                                                                                                 type(expected_output[i]\n                                                                                                      )))\n        if success == len(target_output):\n            return 1\n        else:\n            return 0\n\n    else:\n        assert isinstance(target_output, type(expected_output))\n        return 1\n</pre> def datatype_check(expected_output, target_output, error):     success = 0     if isinstance(target_output, dict):         for key in target_output.keys():             try:                 success += datatype_check(expected_output[key],                                           target_output[key], error)             except:                 print(\"Error: {} in variable {}. Got {} but expected type {}\".format(error,                                                                                      key,                                                                                      type(                                                                                          target_output[key]),                                                                                      type(expected_output[key])))         if success == len(target_output.keys()):             return 1         else:             return 0     elif isinstance(target_output, tuple) or isinstance(target_output, list):         for i in range(len(target_output)):             try:                 success += datatype_check(expected_output[i],                                           target_output[i], error)             except:                 print(\"Error: {} in variable {}, expected type: {}  but expected type {}\".format(error,                                                                                                  i,                                                                                                  type(                                                                                                      target_output[i]),                                                                                                  type(expected_output[i]                                                                                                       )))         if success == len(target_output):             return 1         else:             return 0      else:         assert isinstance(target_output, type(expected_output))         return 1 In\u00a0[\u00a0]: Copied! <pre>def equation_output_check(expected_output, target_output, error):\n    success = 0\n    if isinstance(target_output, dict):\n        for key in target_output.keys():\n            try:\n                success += equation_output_check(expected_output[key],\n                                                 target_output[key], error)\n            except:\n                print(\"Error: {} for variable {}.\".format(error,\n                                                          key))\n        if success == len(target_output.keys()):\n            return 1\n        else:\n            return 0\n    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n        for i in range(len(target_output)):\n            try:\n                success += equation_output_check(expected_output[i],\n                                                 target_output[i], error)\n            except:\n                print(\"Error: {} for variable in position {}.\".format(error, i))\n        if success == len(target_output):\n            return 1\n        else:\n            return 0\n\n    else:\n        if hasattr(target_output, 'shape'):\n            np.testing.assert_array_almost_equal(\n                target_output, expected_output)\n        else:\n            assert target_output == expected_output\n        return 1\n</pre> def equation_output_check(expected_output, target_output, error):     success = 0     if isinstance(target_output, dict):         for key in target_output.keys():             try:                 success += equation_output_check(expected_output[key],                                                  target_output[key], error)             except:                 print(\"Error: {} for variable {}.\".format(error,                                                           key))         if success == len(target_output.keys()):             return 1         else:             return 0     elif isinstance(target_output, tuple) or isinstance(target_output, list):         for i in range(len(target_output)):             try:                 success += equation_output_check(expected_output[i],                                                  target_output[i], error)             except:                 print(\"Error: {} for variable in position {}.\".format(error, i))         if success == len(target_output):             return 1         else:             return 0      else:         if hasattr(target_output, 'shape'):             np.testing.assert_array_almost_equal(                 target_output, expected_output)         else:             assert target_output == expected_output         return 1 In\u00a0[\u00a0]: Copied! <pre>def shape_check(expected_output, target_output, error):\n    success = 0\n    if isinstance(target_output, dict):\n        for key in target_output.keys():\n            try:\n                success += shape_check(expected_output[key],\n                                       target_output[key], error)\n            except:\n                print(\"Error: {} for variable {}.\".format(error, key))\n        if success == len(target_output.keys()):\n            return 1\n        else:\n            return 0\n    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n        for i in range(len(target_output)):\n            try:\n                success += shape_check(expected_output[i],\n                                       target_output[i], error)\n            except:\n                print(\"Error: {} for variable {}.\".format(error, i))\n        if success == len(target_output):\n            return 1\n        else:\n            return 0\n\n    else:\n        if hasattr(target_output, 'shape'):\n            assert target_output.shape == expected_output.shape\n        return 1\n</pre> def shape_check(expected_output, target_output, error):     success = 0     if isinstance(target_output, dict):         for key in target_output.keys():             try:                 success += shape_check(expected_output[key],                                        target_output[key], error)             except:                 print(\"Error: {} for variable {}.\".format(error, key))         if success == len(target_output.keys()):             return 1         else:             return 0     elif isinstance(target_output, tuple) or isinstance(target_output, list):         for i in range(len(target_output)):             try:                 success += shape_check(expected_output[i],                                        target_output[i], error)             except:                 print(\"Error: {} for variable {}.\".format(error, i))         if success == len(target_output):             return 1         else:             return 0      else:         if hasattr(target_output, 'shape'):             assert target_output.shape == expected_output.shape         return 1 In\u00a0[\u00a0]: Copied! <pre>def single_test(test_cases, target):\n    success = 0\n    for test_case in test_cases:\n        try:\n            if test_case['name'] == \"datatype_check\":\n                assert isinstance(target(*test_case['input']),\n                                  type(test_case[\"expected\"]))\n                success += 1\n            if test_case['name'] == \"equation_output_check\":\n                assert np.allclose(test_case[\"expected\"],\n                                   target(*test_case['input']))\n                success += 1\n            if test_case['name'] == \"shape_check\":\n                assert test_case['expected'].shape == target(\n                    *test_case['input']).shape\n                success += 1\n        except:\n            print(\"Error: \" + test_case['error'])\n\n    if success == len(test_cases):\n        print(\"\\033[92m All tests passed.\")\n    else:\n        print('\\033[92m', success, \" Tests passed\")\n        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n        raise AssertionError(\n            \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n</pre> def single_test(test_cases, target):     success = 0     for test_case in test_cases:         try:             if test_case['name'] == \"datatype_check\":                 assert isinstance(target(*test_case['input']),                                   type(test_case[\"expected\"]))                 success += 1             if test_case['name'] == \"equation_output_check\":                 assert np.allclose(test_case[\"expected\"],                                    target(*test_case['input']))                 success += 1             if test_case['name'] == \"shape_check\":                 assert test_case['expected'].shape == target(                     *test_case['input']).shape                 success += 1         except:             print(\"Error: \" + test_case['error'])      if success == len(test_cases):         print(\"\\033[92m All tests passed.\")     else:         print('\\033[92m', success, \" Tests passed\")         print('\\033[91m', len(test_cases) - success, \" Tests failed\")         raise AssertionError(             \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__)) In\u00a0[\u00a0]: Copied! <pre>def multiple_test(test_cases, target):\n    success = 0\n    for test_case in test_cases:\n        try:\n            test_input = deepcopy(test_case['input'])\n            target_answer = target(*test_input)\n            if test_case['name'] == \"datatype_check\":\n                success += datatype_check(test_case['expected'],\n                                          target_answer, test_case['error'])\n            if test_case['name'] == \"equation_output_check\":\n                success += equation_output_check(\n                    test_case['expected'], target_answer, test_case['error'])\n            if test_case['name'] == \"shape_check\":\n                success += shape_check(test_case['expected'],\n                                       target_answer, test_case['error'])\n        except:\n            print('\\33[30m', \"Error: \" + test_case['error'])\n\n    if success == len(test_cases):\n        print(\"\\033[92m All tests passed.\")\n    else:\n        print('\\033[92m', success, \" Tests passed\")\n        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n        raise AssertionError(\n            \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n</pre> def multiple_test(test_cases, target):     success = 0     for test_case in test_cases:         try:             test_input = deepcopy(test_case['input'])             target_answer = target(*test_input)             if test_case['name'] == \"datatype_check\":                 success += datatype_check(test_case['expected'],                                           target_answer, test_case['error'])             if test_case['name'] == \"equation_output_check\":                 success += equation_output_check(                     test_case['expected'], target_answer, test_case['error'])             if test_case['name'] == \"shape_check\":                 success += shape_check(test_case['expected'],                                        target_answer, test_case['error'])         except:             print('\\33[30m', \"Error: \" + test_case['error'])      if success == len(test_cases):         print(\"\\033[92m All tests passed.\")     else:         print('\\033[92m', success, \" Tests passed\")         print('\\033[91m', len(test_cases) - success, \" Tests failed\")         raise AssertionError(             \"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))"},{"location":"MachineLearning/part3/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>def load_data(filename):\n    data = np.loadtxt(filename, delimiter=',')\n    X = data[:,:2]\n    y = data[:,2]\n    return X, y\n</pre> def load_data(filename):     data = np.loadtxt(filename, delimiter=',')     X = data[:,:2]     y = data[:,2]     return X, y In\u00a0[\u00a0]: Copied! <pre>def sig(z):\n \n    return 1/(1+np.exp(-z))\n</pre> def sig(z):       return 1/(1+np.exp(-z)) In\u00a0[\u00a0]: Copied! <pre>def map_feature(X1, X2):\n    \"\"\"\n    Feature mapping function to polynomial features    \n    \"\"\"\n    X1 = np.atleast_1d(X1)\n    X2 = np.atleast_1d(X2)\n    degree = 6\n    out = []\n    for i in range(1, degree+1):\n        for j in range(i + 1):\n            out.append((X1**(i-j) * (X2**j)))\n    return np.stack(out, axis=1)\n</pre> def map_feature(X1, X2):     \"\"\"     Feature mapping function to polynomial features         \"\"\"     X1 = np.atleast_1d(X1)     X2 = np.atleast_1d(X2)     degree = 6     out = []     for i in range(1, degree+1):         for j in range(i + 1):             out.append((X1**(i-j) * (X2**j)))     return np.stack(out, axis=1) In\u00a0[\u00a0]: Copied! <pre>def plot_data(X, y, pos_label=\"y=1\", neg_label=\"y=0\"):\n    positive = y == 1\n    negative = y == 0\n    \n    # Plot examples\n    plt.plot(X[positive, 0], X[positive, 1], 'k+', label=pos_label)\n    plt.plot(X[negative, 0], X[negative, 1], 'yo', label=neg_label)\n</pre> def plot_data(X, y, pos_label=\"y=1\", neg_label=\"y=0\"):     positive = y == 1     negative = y == 0          # Plot examples     plt.plot(X[positive, 0], X[positive, 1], 'k+', label=pos_label)     plt.plot(X[negative, 0], X[negative, 1], 'yo', label=neg_label) In\u00a0[\u00a0]: Copied! <pre>def plot_decision_boundary(w, b, X, y):\n    # Credit to dibgerge on Github for this plotting code\n     \n    plot_data(X[:, 0:2], y)\n    \n    if X.shape[1] &lt;= 2:\n        plot_x = np.array([min(X[:, 0]), max(X[:, 0])])\n        plot_y = (-1. / w[1]) * (w[0] * plot_x + b)\n        \n        plt.plot(plot_x, plot_y, c=\"b\")\n        \n    else:\n        u = np.linspace(-1, 1.5, 50)\n        v = np.linspace(-1, 1.5, 50)\n        \n        z = np.zeros((len(u), len(v)))\n\n        # Evaluate z = theta*x over the grid\n        for i in range(len(u)):\n            for j in range(len(v)):\n                z[i,j] = sig(np.dot(map_feature(u[i], v[j]), w) + b)\n        \n        # important to transpose z before calling contour       \n        z = z.T\n        \n        # Plot z = 0\n        plt.contour(u,v,z, levels = [0.5], colors=\"g\")\n</pre> def plot_decision_boundary(w, b, X, y):     # Credit to dibgerge on Github for this plotting code           plot_data(X[:, 0:2], y)          if X.shape[1] &lt;= 2:         plot_x = np.array([min(X[:, 0]), max(X[:, 0])])         plot_y = (-1. / w[1]) * (w[0] * plot_x + b)                  plt.plot(plot_x, plot_y, c=\"b\")              else:         u = np.linspace(-1, 1.5, 50)         v = np.linspace(-1, 1.5, 50)                  z = np.zeros((len(u), len(v)))          # Evaluate z = theta*x over the grid         for i in range(len(u)):             for j in range(len(v)):                 z[i,j] = sig(np.dot(map_feature(u[i], v[j]), w) + b)                  # important to transpose z before calling contour                z = z.T                  # Plot z = 0         plt.contour(u,v,z, levels = [0.5], colors=\"g\") In\u00a0[\u00a0]: Copied! <pre>        \n</pre>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/","title":"Optional Lab: Cost Function for Logistic Regression","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import  plot_data, sigmoid, dlc\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import  plot_data, sigmoid, dlc plt.style.use('./deeplearning.mplstyle') In\u00a0[\u00a0]: Copied! <pre>X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\ny_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)\n</pre> X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n) y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,) <p>We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p> In\u00a0[\u00a0]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X_train, y_train, ax)\n\n# Set both axes to be from 0-4\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,4)) plot_data(X_train, y_train, ax)  # Set both axes to be from 0-4 ax.axis([0, 4, 0, 3.5]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>def compute_cost_logistic(X, y, w, b):\n    \"\"\"\n    Computes cost\n\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):\n        z_i = np.dot(X[i],w) + b\n        f_wb_i = sigmoid(z_i)\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n             \n    cost = cost / m\n    return cost\n</pre> def compute_cost_logistic(X, y, w, b):     \"\"\"     Computes cost      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter            Returns:       cost (scalar): cost     \"\"\"      m = X.shape[0]     cost = 0.0     for i in range(m):         z_i = np.dot(X[i],w) + b         f_wb_i = sigmoid(z_i)         cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)                   cost = cost / m     return cost  <p>Check the implementation of the cost function using the cell below.</p> In\u00a0[\u00a0]: Copied! <pre>w_tmp = np.array([1,1])\nb_tmp = -3\nprint(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))\n</pre> w_tmp = np.array([1,1]) b_tmp = -3 print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp)) <p>Expected output: 0.3668667864055175</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Choose values between 0 and 6\nx0 = np.arange(0,6)\n\n# Plot the two decision boundaries\nx1 = 3 - x0\nx1_other = 4 - x0\n\nfig,ax = plt.subplots(1, 1, figsize=(4,4))\n# Plot the decision boundary\nax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\nax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\nax.axis([0, 4, 0, 4])\n\n# Plot the original data\nplot_data(X_train,y_train,ax)\nax.axis([0, 4, 0, 4])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.legend(loc=\"upper right\")\nplt.title(\"Decision Boundary\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Choose values between 0 and 6 x0 = np.arange(0,6)  # Plot the two decision boundaries x1 = 3 - x0 x1_other = 4 - x0  fig,ax = plt.subplots(1, 1, figsize=(4,4)) # Plot the decision boundary ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\") ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\") ax.axis([0, 4, 0, 4])  # Plot the original data plot_data(X_train,y_train,ax) ax.axis([0, 4, 0, 4]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.legend(loc=\"upper right\") plt.title(\"Decision Boundary\") plt.show() <p>You can see from this plot that <code>w = np.array([-4,1,1])</code> is a worse model for the training data. Let's see if the cost function implementation reflects this.</p> In\u00a0[\u00a0]: Copied! <pre>w_array1 = np.array([1,1])\nb_1 = -3\nw_array2 = np.array([1,1])\nb_2 = -4\n\nprint(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\nprint(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))\n</pre> w_array1 = np.array([1,1]) b_1 = -3 w_array2 = np.array([1,1]) b_2 = -4  print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1)) print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2)) <p>Expected output</p> <p>Cost for b = -3 :  0.3668667864055175</p> <p>Cost for b = -4 :  0.5036808636748461</p> <p>You can see the cost function behaves as expected and the cost for <code>w = np.array([-4,1,1])</code> is indeed higher than the cost for <code>w = np.array([-3,1,1])</code></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#optional-lab-cost-function-for-logistic-regression","title":"Optional Lab: Cost Function for Logistic Regression\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>examine the implementation and utilize the cost function for logistic regression.</li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#dataset","title":"Dataset\u00b6","text":"<p>Let's start with the same dataset as was used in the decision boundary lab.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#cost-function","title":"Cost function\u00b6","text":"<p>In a previous lab, you developed the logistic loss function. Recall, loss is defined to apply to one example. Here you combine the losses to form the cost, which includes all the examples.</p> <p>Recall that for logistic regression, the cost function is of the form</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$</p> <p>where</p> <ul> <li><p>$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:</p> <p>$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$</p> </li> <li><p>where m is the number of training examples in the data set and: $$ \\begin{align}   f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &amp;= g(z^{(i)})\\tag{3} \\\\   z^{(i)} &amp;= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\   g(z^{(i)}) &amp;= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5}  \\end{align} $$</p> </li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#code-description","title":"Code Description\u00b6","text":"<p>The algorithm for <code>compute_cost_logistic</code> loops over all the examples calculating the loss for each example summing.</p> <p>Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($\ud835\udc5a$,) respectively, where  $\ud835\udc5b$ is the number of features and $\ud835\udc5a$ is the number of training examples.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#example","title":"Example\u00b6","text":"<p>Now, let's see what the cost function output is for a different value of $w$.</p> <ul> <li><p>In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had <code>w = np.array([-3,1,1])</code>.</p> </li> <li><p>Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or <code>w = np.array([-4,1,1])</code> provides a better model.</p> </li> </ul> <p>Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.</p> <ul> <li>For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)</li> <li>For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)</li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy1/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you examined and utilized the cost function for logistic regression.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/","title":"Optional Lab: Cost Function for Logistic Regression","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom lab_utils_common import  plot_data, sigmoid, dlc\nplt.style.use('./deeplearning.mplstyle')\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from lab_utils_common import  plot_data, sigmoid, dlc plt.style.use('./deeplearning.mplstyle') In\u00a0[\u00a0]: Copied! <pre>X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\ny_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)\n</pre> X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n) y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,) <p>We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles.</p> In\u00a0[\u00a0]: Copied! <pre>fig,ax = plt.subplots(1,1,figsize=(4,4))\nplot_data(X_train, y_train, ax)\n\n# Set both axes to be from 0-4\nax.axis([0, 4, 0, 3.5])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.show()\n</pre> fig,ax = plt.subplots(1,1,figsize=(4,4)) plot_data(X_train, y_train, ax)  # Set both axes to be from 0-4 ax.axis([0, 4, 0, 3.5]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.show() <p></p> In\u00a0[\u00a0]: Copied! <pre>def compute_cost_logistic(X, y, w, b):\n    \"\"\"\n    Computes cost\n\n    Args:\n      X (ndarray (m,n)): Data, m examples with n features\n      y (ndarray (m,)) : target values\n      w (ndarray (n,)) : model parameters  \n      b (scalar)       : model parameter\n      \n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m = X.shape[0]\n    cost = 0.0\n    for i in range(m):\n        z_i = np.dot(X[i],w) + b\n        f_wb_i = sigmoid(z_i)\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n             \n    cost = cost / m\n    return cost\n</pre> def compute_cost_logistic(X, y, w, b):     \"\"\"     Computes cost      Args:       X (ndarray (m,n)): Data, m examples with n features       y (ndarray (m,)) : target values       w (ndarray (n,)) : model parameters         b (scalar)       : model parameter            Returns:       cost (scalar): cost     \"\"\"      m = X.shape[0]     cost = 0.0     for i in range(m):         z_i = np.dot(X[i],w) + b         f_wb_i = sigmoid(z_i)         cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)                   cost = cost / m     return cost  <p>Check the implementation of the cost function using the cell below.</p> In\u00a0[\u00a0]: Copied! <pre>w_tmp = np.array([1,1])\nb_tmp = -3\nprint(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))\n</pre> w_tmp = np.array([1,1]) b_tmp = -3 print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp)) <p>Expected output: 0.3668667864055175</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Choose values between 0 and 6\nx0 = np.arange(0,6)\n\n# Plot the two decision boundaries\nx1 = 3 - x0\nx1_other = 4 - x0\n\nfig,ax = plt.subplots(1, 1, figsize=(4,4))\n# Plot the decision boundary\nax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\nax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\nax.axis([0, 4, 0, 4])\n\n# Plot the original data\nplot_data(X_train,y_train,ax)\nax.axis([0, 4, 0, 4])\nax.set_ylabel('$x_1$', fontsize=12)\nax.set_xlabel('$x_0$', fontsize=12)\nplt.legend(loc=\"upper right\")\nplt.title(\"Decision Boundary\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Choose values between 0 and 6 x0 = np.arange(0,6)  # Plot the two decision boundaries x1 = 3 - x0 x1_other = 4 - x0  fig,ax = plt.subplots(1, 1, figsize=(4,4)) # Plot the decision boundary ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\") ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\") ax.axis([0, 4, 0, 4])  # Plot the original data plot_data(X_train,y_train,ax) ax.axis([0, 4, 0, 4]) ax.set_ylabel('$x_1$', fontsize=12) ax.set_xlabel('$x_0$', fontsize=12) plt.legend(loc=\"upper right\") plt.title(\"Decision Boundary\") plt.show() <p>You can see from this plot that <code>w = np.array([-4,1,1])</code> is a worse model for the training data. Let's see if the cost function implementation reflects this.</p> In\u00a0[\u00a0]: Copied! <pre>w_array1 = np.array([1,1])\nb_1 = -3\nw_array2 = np.array([1,1])\nb_2 = -4\n\nprint(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\nprint(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))\n</pre> w_array1 = np.array([1,1]) b_1 = -3 w_array2 = np.array([1,1]) b_2 = -4  print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1)) print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2)) <p>Expected output</p> <p>Cost for b = -3 :  0.3668667864055175</p> <p>Cost for b = -4 :  0.5036808636748461</p> <p>You can see the cost function behaves as expected and the cost for <code>w = np.array([-4,1,1])</code> is indeed higher than the cost for <code>w = np.array([-3,1,1])</code></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#optional-lab-cost-function-for-logistic-regression","title":"Optional Lab: Cost Function for Logistic Regression\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>examine the implementation and utilize the cost function for logistic regression.</li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#dataset","title":"Dataset\u00b6","text":"<p>Let's start with the same dataset as was used in the decision boundary lab.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#cost-function","title":"Cost function\u00b6","text":"<p>In a previous lab, you developed the logistic loss function. Recall, loss is defined to apply to one example. Here you combine the losses to form the cost, which includes all the examples.</p> <p>Recall that for logistic regression, the cost function is of the form</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$</p> <p>where</p> <ul> <li><p>$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:</p> <p>$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$</p> </li> <li><p>where m is the number of training examples in the data set and: $$ \\begin{align}   f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &amp;= g(z^{(i)})\\tag{3} \\\\   z^{(i)} &amp;= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\   g(z^{(i)}) &amp;= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5}  \\end{align} $$</p> </li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#code-description","title":"Code Description\u00b6","text":"<p>The algorithm for <code>compute_cost_logistic</code> loops over all the examples calculating the loss for each example and accumulating the total.</p> <p>Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($\ud835\udc5a$,) respectively, where  $\ud835\udc5b$ is the number of features and $\ud835\udc5a$ is the number of training examples.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#example","title":"Example\u00b6","text":"<p>Now, let's see what the cost function output is for a different value of $w$.</p> <ul> <li><p>In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had <code>w = np.array([-3,1,1])</code>.</p> </li> <li><p>Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or <code>w = np.array([-4,1,1])</code> provides a better model.</p> </li> </ul> <p>Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.</p> <ul> <li>For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)</li> <li>For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)</li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab05_Cost_Function_Soln-Copy2/#congratulations","title":"Congratulations!\u00b6","text":"<p>In this lab you examined and utilized the cost function for logistic regression.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/","title":"Optional Lab - Regularized Cost and Gradient","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n%matplotlib widget\nimport matplotlib.pyplot as plt\nfrom plt_overfit import overfit_example, output\nfrom lab_utils_common import sigmoid\nnp.set_printoptions(precision=8)\n</pre> import numpy as np %matplotlib widget import matplotlib.pyplot as plt from plt_overfit import overfit_example, output from lab_utils_common import sigmoid np.set_printoptions(precision=8) In\u00a0[\u00a0]: Copied! <pre>def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m  = X.shape[0]\n    n  = len(w)\n    cost = 0.\n    for i in range(m):\n        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n    cost = cost / (2 * m)                                              #scalar  \n \n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost                                                  #scalar\n</pre> def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):     \"\"\"     Computes the cost over all examples     Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns:       total_cost (scalar):  cost      \"\"\"      m  = X.shape[0]     n  = len(w)     cost = 0.     for i in range(m):         f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot         cost = cost + (f_wb_i - y[i])**2                               #scalar                  cost = cost / (2 * m)                                              #scalar         reg_cost = 0     for j in range(n):         reg_cost += (w[j]**2)                                          #scalar     reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar          total_cost = cost + reg_cost                                       #scalar     return total_cost                                                  #scalar <p>Run the cell below to see it in action.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,6) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5 b_tmp = 0.5 lambda_tmp = 0.7 cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(\"Regularized cost:\", cost_tmp) <p>Expected Output:</p> Regularized cost:  0.07917239320214275  In\u00a0[\u00a0]: Copied! <pre>def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns:\n      total_cost (scalar):  cost \n    \"\"\"\n\n    m,n  = X.shape\n    cost = 0.\n    for i in range(m):\n        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n        f_wb_i = sigmoid(z_i)                                          #scalar\n        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n             \n    cost = cost/m                                                      #scalar\n\n    reg_cost = 0\n    for j in range(n):\n        reg_cost += (w[j]**2)                                          #scalar\n    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n    \n    total_cost = cost + reg_cost                                       #scalar\n    return total_cost                                                  #scalar\n</pre> def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):     \"\"\"     Computes the cost over all examples     Args:     Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns:       total_cost (scalar):  cost      \"\"\"      m,n  = X.shape     cost = 0.     for i in range(m):         z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot         f_wb_i = sigmoid(z_i)                                          #scalar         cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar                   cost = cost/m                                                      #scalar      reg_cost = 0     for j in range(n):         reg_cost += (w[j]**2)                                          #scalar     reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar          total_cost = cost + reg_cost                                       #scalar     return total_cost                                                  #scalar <p>Run the cell below to see it in action.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,6)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\nb_tmp = 0.5\nlambda_tmp = 0.7\ncost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(\"Regularized cost:\", cost_tmp)\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,6) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5 b_tmp = 0.5 lambda_tmp = 0.7 cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(\"Regularized cost:\", cost_tmp) <p>Expected Output:</p> Regularized cost:  0.6850849138741673  In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_linear_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n      \n    Returns:\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):                             \n        err = (np.dot(X[i], w) + b) - y[i]                 \n        for j in range(n):                         \n            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n        dj_db = dj_db + err                        \n    dj_dw = dj_dw / m                                \n    dj_db = dj_db / m   \n    \n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw\n</pre> def compute_gradient_linear_reg(X, y, w, b, lambda_):      \"\"\"     Computes the gradient for linear regression      Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization            Returns:       dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape           #(number of examples, number of features)     dj_dw = np.zeros((n,))     dj_db = 0.      for i in range(m):                                      err = (np.dot(X[i], w) + b) - y[i]                          for j in range(n):                                      dj_dw[j] = dj_dw[j] + err * X[i, j]                        dj_db = dj_db + err                             dj_dw = dj_dw / m                                     dj_db = dj_db / m             for j in range(n):         dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]      return dj_db, dj_dw <p>Run the cell below to see it in action.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,3) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]) b_tmp = 0.5 lambda_tmp = 0.7 dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(f\"dj_db: {dj_db_tmp}\", ) print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", ) <p>Expected Output</p> <pre><code>dj_db: 0.6648774569425726\nRegularized dj_dw:\n [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n      lambda_ (scalar): Controls amount of regularization\n    Returns\n      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                            #(n,)\n    dj_db = 0.0                                       #scalar\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n\n    for j in range(n):\n        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n\n    return dj_db, dj_dw  \n</pre> def compute_gradient_logistic_reg(X, y, w, b, lambda_):      \"\"\"     Computes the gradient for linear regression        Args:       X (ndarray (m,n): Data, m examples with n features       y (ndarray (m,)): target values       w (ndarray (n,)): model parameters         b (scalar)      : model parameter       lambda_ (scalar): Controls amount of regularization     Returns       dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.        dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.      \"\"\"     m,n = X.shape     dj_dw = np.zeros((n,))                            #(n,)     dj_db = 0.0                                       #scalar      for i in range(m):         f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar         err_i  = f_wb_i  - y[i]                       #scalar         for j in range(n):             dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar         dj_db = dj_db + err_i     dj_dw = dj_dw/m                                   #(n,)     dj_db = dj_db/m                                   #scalar      for j in range(n):         dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]      return dj_db, dj_dw    <p>Run the cell below to see it in action.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(1)\nX_tmp = np.random.rand(5,3)\ny_tmp = np.array([0,1,0,1,0])\nw_tmp = np.random.rand(X_tmp.shape[1])\nb_tmp = 0.5\nlambda_tmp = 0.7\ndj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n\nprint(f\"dj_db: {dj_db_tmp}\", )\nprint(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )\n</pre> np.random.seed(1) X_tmp = np.random.rand(5,3) y_tmp = np.array([0,1,0,1,0]) w_tmp = np.random.rand(X_tmp.shape[1]) b_tmp = 0.5 lambda_tmp = 0.7 dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)  print(f\"dj_db: {dj_db_tmp}\", ) print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", ) <p>Expected Output</p> <pre><code>dj_db: 0.341798994972791\nRegularized dj_dw:\n [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>plt.close(\"all\")\ndisplay(output)\nofit = overfit_example(True)\n</pre> plt.close(\"all\") display(output) ofit = overfit_example(True) <p>In the plot above, try out regularization on the previous example. In particular:</p> <ul> <li>Categorical (logistic regression)<ul> <li>set degree to 6, lambda to 0 (no regularization), fit the data</li> <li>now set lambda to 1 (increase regularization), fit the data, notice the difference.</li> </ul> </li> <li>Regression (linear regression)<ul> <li>try the same procedure.</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#optional-lab-regularized-cost-and-gradient","title":"Optional Lab - Regularized Cost and Gradient\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#goals","title":"Goals\u00b6","text":"<p>In this lab, you will:</p> <ul> <li>extend the previous linear and logistic cost functions with a regularization term.</li> <li>rerun the previous example of over-fitting with a regularization term added.</li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#adding-regularization","title":"Adding regularization\u00b6","text":"<p>The slides above show the cost and gradient functions for both linear and logistic regression. Note:</p> <ul> <li>Cost<ul> <li>The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.</li> </ul> </li> <li>Gradient<ul> <li>The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of $f_{wb}$.</li> </ul> </li> </ul>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#cost-functions-with-regularization","title":"Cost functions with regularization\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#cost-function-for-regularized-linear-regression","title":"Cost function for regularized linear regression\u00b6","text":"<p>The equation for the cost function regularized linear regression is: $$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ where: $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$</p> <p>Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:</p> <p>$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$</p> <p>The difference is the regularization term,   $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </p> <p>Including this term incentives gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.</p> <p>Below is an implementation of equations (1) and (2). Note that this uses a standard pattern for this course,   a <code>for loop</code> over all <code>m</code> examples.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#cost-function-for-regularized-logistic-regression","title":"Cost function for regularized logistic regression\u00b6","text":"<p>For regularized logistic regression, the cost function is of the form $$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$ where: $$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$</p> <p>Compare this to the cost function without regularization (which you implemented in  a previous lab):</p> <p>$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$</p> <p>As was the case in linear regression above, the difference is the regularization term, which is     $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </p> <p>Including this term incentives gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#gradient-descent-with-regularization","title":"Gradient descent with regularization\u00b6","text":"<p>The basic algorithm for running gradient descent does not change with regularization, it is: $$\\begin{align*} &amp;\\text{repeat until convergence:} \\; \\lbrace \\\\ &amp;  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; &amp; \\text{for j := 0..n-1} \\\\  &amp;  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\ &amp;\\rbrace \\end{align*}$$ Where each iteration performs simultaneous updates on $w_j$ for all $j$.</p> <p>What changes with regularization is computing the gradients.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#computing-the-gradient-with-regularization-both-linearlogistic","title":"Computing the Gradient with regularization (both linear/logistic)\u00b6","text":"<p>The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$. $$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\ \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &amp;= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}  \\end{align*}$$</p> <ul> <li><p>m is the number of training examples in the data set</p> </li> <li><p>$f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target</p> </li> <li><p>For a   linear  regression model $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$</p> </li> <li><p>For a  logistic  regression model $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ $f_{\\mathbf{w},b}(x) = g(z)$ where $g(z)$ is the sigmoid function: $g(z) = \\frac{1}{1+e^{-z}}$</p> </li> </ul> <p>The term which adds regularization is  the $\\frac{\\lambda}{m} w_j $.</p>"},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#gradient-function-for-regularized-linear-regression","title":"Gradient function for regularized linear regression\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#gradient-function-for-regularized-logistic-regression","title":"Gradient function for regularized logistic regression\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#rerun-over-fitting-example","title":"Rerun over-fitting example\u00b6","text":""},{"location":"MachineLearning/part3/archive/C1_W3_Lab09_Regularization_Soln-Copy1/#congratulations","title":"Congratulations!\u00b6","text":"<p>You have:</p> <ul> <li>examples of cost and gradient routines with regression added for both linear and logistic regression</li> <li>developed some intuition on how regularization can reduce over-fitting</li> </ul>"}]}